# 2025-03-07

# Table of Contents
* [Rethinking Video Tokenization A Conditioned Diffusion-based Approach](#Rethinking-Video-Tokenization-A-Conditioned-Diffusion-based-Approach)
* [Ambiguity-Free Broadband DOA Estimation Relying on Parameterized Time-Frequency Transform](#Ambiguity-Free-Broadband-DOA-Estimation-Relying-on-Parameterized-Time-Frequency-Transform)
* [Chunking the Critic A Transformer-based Soft Actor-Critic with N-Step Returns](#Chunking-the-Critic-A-Transformer-based-Soft-Actor-Critic-with-N-Step-Returns)
* [Feature-Level Insights into Artificial Text Detection with Sparse Autoencoders](#Feature-Level-Insights-into-Artificial-Text-Detection-with-Sparse-Autoencoders)
* [English K_Quantization of LLMs Does Not Disproportionately Diminish Multilingual Performance](#English-K_Quantization-of-LLMs-Does-Not-Disproportionately-Diminish-Multilingual-Performance)
* [PowerAttention Exponentially Scaling of Receptive Fields for Effective Sparse Attention](#PowerAttention-Exponentially-Scaling-of-Receptive-Fields-for-Effective-Sparse-Attention)
* [Domain Consistent Industrial Decarbonisation of Global Coal Power Plants](#Domain-Consistent-Industrial-Decarbonisation-of-Global-Coal-Power-Plants)
* [NeuGrasp Generalizable Neural Surface Reconstruction with Background Priors for Material-Agnostic Object Grasp Detection](#NeuGrasp-Generalizable-Neural-Surface-Reconstruction-with-Background-Priors-for-Material-Agnostic-Object-Grasp-Detection)
* [Collaborative Expert LLMs Guided Multi-Objective Molecular Optimization](#Collaborative-Expert-LLMs-Guided-Multi-Objective-Molecular-Optimization)
* [Visualising Policy-Reward Interplay to Inform Zeroth-Order Preference Optimisation of Large Language Models](#Visualising-Policy-Reward-Interplay-to-Inform-Zeroth-Order-Preference-Optimisation-of-Large-Language-Models)
* [Skeletonisation Scale-Spaces](#Skeletonisation-Scale-Spaces)
* [JamMa Ultra-lightweight Local Feature Matching with Joint Mamba](#JamMa-Ultra-lightweight-Local-Feature-Matching-with-Joint-Mamba)
* [RASD Retrieval-Augmented Speculative Decoding](#RASD-Retrieval-Augmented-Speculative-Decoding)
* [The Gaia Astrometric Catalogue and Secular Aberration Drift in Proper Motions](#The-Gaia-Astrometric-Catalogue-and-Secular-Aberration-Drift-in-Proper-Motions)
* [On the Relation Between Speech Quality and Quantized Latent Representations of Neural Codecs](#On-the-Relation-Between-Speech-Quality-and-Quantized-Latent-Representations-of-Neural-Codecs)
* [A 262 TOPS Hyperdimensional Photonic AI Accelerator powered by a Si3N4 microcomb laser](#A-262-TOPS-Hyperdimensional-Photonic-AI-Accelerator-powered-by-a-Si3N4-microcomb-laser)
* [BAT Learning Event-based Optical Flow with Bidirectional Adaptive Temporal Correlation](#BAT-Learning-Event-based-Optical-Flow-with-Bidirectional-Adaptive-Temporal-Correlation)
* [A Multimodal Framework for Topic Propagation Classification in Social Networks](#A-Multimodal-Framework-for-Topic-Propagation-Classification-in-Social-Networks)
* [SAFE A Sparse Autoencoder-Based Framework for Robust Query Enrichment and Hallucination Mitigation in LLMs](#SAFE-A-Sparse-Autoencoder-Based-Framework-for-Robust-Query-Enrichment-and-Hallucination-Mitigation-in-LLMs)
* [FairSense-AI Responsible AI Meets Sustainability](#FairSense-AI-Responsible-AI-Meets-Sustainability)
* [Privacy and Accuracy-Aware AI/ML Model Deduplication](#Privacy-and-Accuracy-Aware-AI/ML-Model-Deduplication)
* [(How) Do Language Models Track State?](#(How)-Do-Language-Models-Track-State?)
* [Boltzmann Attention Sampling for Image Analysis with Small Objects](#Boltzmann-Attention-Sampling-for-Image-Analysis-with-Small-Objects)
* [AlignDistil Token-Level Language Model Alignment as Adaptive Policy Distillation](#AlignDistil-Token-Level-Language-Model-Alignment-as-Adaptive-Policy-Distillation)
* [Q-Filters Leveraging QK Geometry for Efficient KV Cache Compression](#Q-Filters-Leveraging-QK-Geometry-for-Efficient-KV-Cache-Compression)
* [RAAD-LLM Adaptive Anomaly Detection Using LLMs and RAG Integration](#RAAD-LLM-Adaptive-Anomaly-Detection-Using-LLMs-and-RAG-Integration)
* [ArcPro Architectural Programs for Structured 3D Abstraction of Sparse Points](#ArcPro-Architectural-Programs-for-Structured-3D-Abstraction-of-Sparse-Points)
* [TReND Transformer derived features and Regularized NMF for neonatal functional network Delineation](#TReND-Transformer-derived-features-and-Regularized-NMF-for-neonatal-functional-network-Delineation)
* [TeTRA-VPR A Ternary Transformer Approach for Compact Visual Place Recognition](#TeTRA-VPR-A-Ternary-Transformer-Approach-for-Compact-Visual-Place-Recognition)
* [Q&C When Quantization Meets Cache in Efficient Image Generation](#Q&C-When-Quantization-Meets-Cache-in-Efficient-Image-Generation)
* [The Distributionally Robust Optimization Model of Sparse Principal Component Analysis](#The-Distributionally-Robust-Optimization-Model-of-Sparse-Principal-Component-Analysis)
* [Sparse Meets Dense Unified Generative Recommendations with Cascaded Sparse-Dense Representations](#Sparse-Meets-Dense-Unified-Generative-Recommendations-with-Cascaded-Sparse-Dense-Representations)
* [Joint Tensor and Inter-View Low-Rank Recovery for Incomplete Multiview Clustering](#Joint-Tensor-and-Inter-View-Low-Rank-Recovery-for-Incomplete-Multiview-Clustering)
* [NodeNAS Node-Specific Graph Neural Architecture Search for Out-of-Distribution Generalization](#NodeNAS-Node-Specific-Graph-Neural-Architecture-Search-for-Out-of-Distribution-Generalization)
* [BHViT Binarized Hybrid Vision Transformer](#BHViT-Binarized-Hybrid-Vision-Transformer)
* [Cosmic Acceleration from Nothing](#Cosmic-Acceleration-from-Nothing)
* [Teaching Metric Distance to Autoregressive Multimodal Foundational Models](#Teaching-Metric-Distance-to-Autoregressive-Multimodal-Foundational-Models)
* [BdSLW401 Transformer-Based Word-Level Bangla Sign Language Recognition Using Relative Quantization Encoding (RQE)](#BdSLW401-Transformer-Based-Word-Level-Bangla-Sign-Language-Recognition-Using-Relative-Quantization-Encoding-(RQE))
* [VQ-LLM High-performance Code Generation for Vector Quantization Augmented LLM Inference](#VQ-LLM-High-performance-Code-Generation-for-Vector-Quantization-Augmented-LLM-Inference)
* [Empowering Sparse-Input Neural Radiance Fields with Dual-Level Semantic Guidance from Dense Novel Views](#Empowering-Sparse-Input-Neural-Radiance-Fields-with-Dual-Level-Semantic-Guidance-from-Dense-Novel-Views)
* [DivPrune Diversity-based Visual Token Pruning for Large Multimodal Models](#DivPrune-Diversity-based-Visual-Token-Pruning-for-Large-Multimodal-Models)
* [Leveraging Large Language Models for Enhanced Digital Twin Modeling Trends, Methods, and Challenges](#Leveraging-Large-Language-Models-for-Enhanced-Digital-Twin-Modeling-Trends,-Methods,-and-Challenges)
* [A broadband solid impedance transformer for acoustic transmission between water and air](#A-broadband-solid-impedance-transformer-for-acoustic-transmission-between-water-and-air)
* [Analyzing the Safety of Japanese Large Language Models in Stereotype-Triggering Prompts](#Analyzing-the-Safety-of-Japanese-Large-Language-Models-in-Stereotype-Triggering-Prompts)
* [From superposition to sparse codes interpretable representations in neural networks](#From-superposition-to-sparse-codes-interpretable-representations-in-neural-networks)
* [RSQ Learning from Important Tokens Leads to Better Quantized LLMs](#RSQ-Learning-from-Important-Tokens-Leads-to-Better-Quantized-LLMs)
* [LLMInit A Free Lunch from Large Language Models for Selective Initialization of Recommendation](#LLMInit-A-Free-Lunch-from-Large-Language-Models-for-Selective-Initialization-of-Recommendation)
* [DILEMMA Joint LLM Quantization and Distributed LLM Inference Over Edge Computing Systems](#DILEMMA-Joint-LLM-Quantization-and-Distributed-LLM-Inference-Over-Edge-Computing-Systems)
* [Galaxy populations of ProtoClusters in cosmological hydrodynamical simulations](#Galaxy-populations-of-ProtoClusters-in-cosmological-hydrodynamical-simulations)
* [CoPL Collaborative Preference Learning for Personalizing LLMs](#CoPL-Collaborative-Preference-Learning-for-Personalizing-LLMs)
* [An Efficient Approach to Detecting Lung Nodules Using Swin Transformer](#An-Efficient-Approach-to-Detecting-Lung-Nodules-Using-Swin-Transformer)
* [EliteKV Scalable KV Cache Compression via RoPE Frequency Selection and Joint Low-Rank Projection](#EliteKV-Scalable-KV-Cache-Compression-via-RoPE-Frequency-Selection-and-Joint-Low-Rank-Projection)
* [MRI super-resolution reconstruction using efficient diffusion probabilistic model with residual shifting](#MRI-super-resolution-reconstruction-using-efficient-diffusion-probabilistic-model-with-residual-shifting)
* [Attention Condensation via Sparsity Induced Regularized Training](#Attention-Condensation-via-Sparsity-Induced-Regularized-Training)
* [Revisiting Large Language Model Pruning using Neuron Semantic Attribution](#Revisiting-Large-Language-Model-Pruning-using-Neuron-Semantic-Attribution)
* [KurTail  Kurtosis-based LLM Quantization](#KurTail--Kurtosis-based-LLM-Quantization)
* [Transferring between sparse and dense matching via probabilistic reweighting](#Transferring-between-sparse-and-dense-matching-via-probabilistic-reweighting)
* [Structural Deep Encoding for Table Question Answering](#Structural-Deep-Encoding-for-Table-Question-Answering)
* [MeshPad Interactive Sketch Conditioned Artistic-designed Mesh Generation and Editing](#MeshPad-Interactive-Sketch-Conditioned-Artistic-designed-Mesh-Generation-and-Editing)
* [Enhancing Social Media Rumor Detection A Semantic and Graph Neural Network Approach for the 2024 Global Election](#Enhancing-Social-Media-Rumor-Detection-A-Semantic-and-Graph-Neural-Network-Approach-for-the-2024-Global-Election)
* [Wavelet-Enhanced Desnowing A Novel Single Image Restoration Approach for Traffic Surveillance under Adverse Weather Conditions](#Wavelet-Enhanced-Desnowing-A-Novel-Single-Image-Restoration-Approach-for-Traffic-Surveillance-under-Adverse-Weather-Conditions)
* [WeightedKV Attention Scores Weighted Key-Value Cache Merging for Large Language Models](#WeightedKV-Attention-Scores-Weighted-Key-Value-Cache-Merging-for-Large-Language-Models)
* [PipeOffload Improving Scalability of Pipeline Parallelism with Memory Optimization](#PipeOffload-Improving-Scalability-of-Pipeline-Parallelism-with-Memory-Optimization)
* [PROPER A Progressive Learning Framework for Personalized Large Language Models with Group-Level Adaptation](#PROPER-A-Progressive-Learning-Framework-for-Personalized-Large-Language-Models-with-Group-Level-Adaptation)
* [SVDC Consistent Direct Time-of-Flight Video Depth Completion with Frequency Selective Fusion](#SVDC-Consistent-Direct-Time-of-Flight-Video-Depth-Completion-with-Frequency-Selective-Fusion)
* [NM-SpMM Accelerating Matrix Multiplication Using NM Sparsity with GPGPU](#NM-SpMM-Accelerating-Matrix-Multiplication-Using-NM-Sparsity-with-GPGPU)
* [STGAN Spatial-temporal Graph Autoregression Network for Pavement Distress Deterioration Prediction](#STGAN-Spatial-temporal-Graph-Autoregression-Network-for-Pavement-Distress-Deterioration-Prediction)
* [Large AI Model for Delay-Doppler Domain Channel Prediction in 6G OTFS-Based Vehicular Networks](#Large-AI-Model-for-Delay-Doppler-Domain-Channel-Prediction-in-6G-OTFS-Based-Vehicular-Networks)
* [MAPS Multi-Fidelity AI-Augmented Photonic Simulation and Inverse Design Infrastructure](#MAPS-Multi-Fidelity-AI-Augmented-Photonic-Simulation-and-Inverse-Design-Infrastructure)
* [MedUnifier Unifying Vision-and-Language Pre-training on Medical Data with Vision Generation Task using Discrete Visual Representations](#MedUnifier-Unifying-Vision-and-Language-Pre-training-on-Medical-Data-with-Vision-Generation-Task-using-Discrete-Visual-Representations)
* [Dialogue Without Limits Constant-Sized KV Caches for Extended Responses in LLMs](#Dialogue-Without-Limits-Constant-Sized-KV-Caches-for-Extended-Responses-in-LLMs)
* [Training-Free Dataset Pruning for Instance Segmentation](#Training-Free-Dataset-Pruning-for-Instance-Segmentation)
* [STAR-Edge Structure-aware Local Spherical Curve Representation for Thin-walled Edge Extraction from Unstructured Point Clouds](#STAR-Edge-Structure-aware-Local-Spherical-Curve-Representation-for-Thin-walled-Edge-Extraction-from-Unstructured-Point-Clouds)
* [Pruning Deep Neural Networks via a Combination of the Marchenko-Pastur Distribution and Regularization](#Pruning-Deep-Neural-Networks-via-a-Combination-of-the-Marchenko-Pastur-Distribution-and-Regularization)
* [Streaming Video Question-Answering with In-context Video KV-Cache Retrieval](#Streaming-Video-Question-Answering-with-In-context-Video-KV-Cache-Retrieval)
* [Tutorial Proposal Speculative Decoding for Efficient LLM Inference](#Tutorial-Proposal-Speculative-Decoding-for-Efficient-LLM-Inference)
* [Leveraging Compute-in-Memory for Efficient Generative Model Inference in TPUs](#Leveraging-Compute-in-Memory-for-Efficient-Generative-Model-Inference-in-TPUs)
* [Split Adaptation for Pre-trained Vision Transformers](#Split-Adaptation-for-Pre-trained-Vision-Transformers)
* [Progressive Sparse Attention Algorithm and System Co-design for Efficient Attention in LLM Serving](#Progressive-Sparse-Attention-Algorithm-and-System-Co-design-for-Efficient-Attention-in-LLM-Serving)
* [Strong Solutions and Quantization-Based Numerical Schemes for a Class of Non-Markovian Volatility Models](#Strong-Solutions-and-Quantization-Based-Numerical-Schemes-for-a-Class-of-Non-Markovian-Volatility-Models)
* [AnalogGenie A Generative Engine for Automatic Discovery of Analog Circuit Topologies](#AnalogGenie-A-Generative-Engine-for-Automatic-Discovery-of-Analog-Circuit-Topologies)
* [Steering Large Language Model Activations in Sparse Spaces](#Steering-Large-Language-Model-Activations-in-Sparse-Spaces)
* [BixBench a Comprehensive Benchmark for LLM-based Agents in Computational Biology](#BixBench-a-Comprehensive-Benchmark-for-LLM-based-Agents-in-Computational-Biology)
* [Oscillatory finite-time singularities in rockbursts](#Oscillatory-finite-time-singularities-in-rockbursts)
* [A Method of Selective Attention for Reservoir Based Agents](#A-Method-of-Selective-Attention-for-Reservoir-Based-Agents)
* [AMPLE Event-Driven Accelerator for Mixed-Precision Inference of Graph Neural Networks](#AMPLE-Event-Driven-Accelerator-for-Mixed-Precision-Inference-of-Graph-Neural-Networks)
* [Tracks to Modernity Railroads, Growth, and Social Movements in Denmark](#Tracks-to-Modernity-Railroads,-Growth,-and-Social-Movements-in-Denmark)
* [Training-free and Adaptive Sparse Attention for Efficient Long Video Generation](#Training-free-and-Adaptive-Sparse-Attention-for-Efficient-Long-Video-Generation)
* [Fast 3D point clouds retrieval for Large-scale 3D Place Recognition](#Fast-3D-point-clouds-retrieval-for-Large-scale-3D-Place-Recognition)
* [The amplifier effect of artificial agents in social contagion](#The-amplifier-effect-of-artificial-agents-in-social-contagion)
* [Preconditioned Block Encodings for Quantum Linear Systems](#Preconditioned-Block-Encodings-for-Quantum-Linear-Systems)
* [Oscillation-Reduced MXFP4 Training for Vision Transformers](#Oscillation-Reduced-MXFP4-Training-for-Vision-Transformers)
* [Identifying Sensitive Weights via Post-quantization Integral](#Identifying-Sensitive-Weights-via-Post-quantization-Integral)
* [FlexPrefill A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference](#FlexPrefill-A-Context-Aware-Sparse-Attention-Mechanism-for-Efficient-Long-Sequence-Inference)
* [Enhanced Performance and Stability of Perovskite Solar Cells with Ag-Cu-Zn Alloy Electrodes](#Enhanced-Performance-and-Stability-of-Perovskite-Solar-Cells-with-Ag-Cu-Zn-Alloy-Electrodes)
* [Generating Clinically Realistic EHR Data via a Hierarchy- and Semantics-Guided Transformer](#Generating-Clinically-Realistic-EHR-Data-via-a-Hierarchy--and-Semantics-Guided-Transformer)
* [EDENet Echo Direction Encoding Network for Place Recognition Based on Ground Penetrating Radar](#EDENet-Echo-Direction-Encoding-Network-for-Place-Recognition-Based-on-Ground-Penetrating-Radar)


## Rethinking Video Tokenization A Conditioned Diffusion-based Approach

>Authors: Nianzu Yang, Pandeng Li, Liming Zhao, Yang Li, Chen-Wei Xie, Yehui Tang, Xudong Lu, Zhihang Liu, Yun Zheng, Yu Liu, Junchi Yan

>2025-03-05

> http://arxiv.org/abs/2503.03708v1

Video tokenizers, which transform videos into compact latent representations,
are key to video generation. Existing video tokenizers are based on the VAE
architecture and follow a paradigm where an encoder compresses videos into
compact latents, and a deterministic decoder reconstructs the original videos
from these latents. In this paper, we propose a novel
\underline{\textbf{C}}onditioned \underline{\textbf{D}}iffusion-based video
\underline{\textbf{T}}okenizer entitled \textbf{\ourmethod}, which departs from
previous methods by replacing the deterministic decoder with a 3D causal
diffusion model. The reverse diffusion generative process of the decoder is
conditioned on the latent representations derived via the encoder. With a
feature caching and sampling **acceleration**, the framework efficiently
reconstructs high-fidelity videos of arbitrary lengths. Results show that
{\ourmethod} achieves state-of-the-art performance in video reconstruction
tasks using just a single-step sampling. Even a smaller version of {\ourmethod}
still achieves reconstruction results on par with the top two baselines.
Furthermore, the latent video generation model trained using {\ourmethod} also
shows superior performance.


## Ambiguity-Free Broadband DOA Estimation Relying on Parameterized Time-Frequency Transform

>Authors: Wei Wang, Shefeng Yan, Linlin Mao, Zeping Sui, Jirui Yang

>2025-03-05

> http://arxiv.org/abs/2503.03691v1

An ambiguity-free direction-of-arrival (DOA) estimation scheme is proposed
for **sparse** uniform linear arrays under low signal-to-noise ratios (SNRs) and
non-stationary broadband signals. First, for achieving better DOA estimation
performance at low SNRs while using non-stationary signals compared to the
conventional frequency-difference (FD) paradigms, we propose parameterized
time-frequency transform-based FD processing. Then, the unambiguous compressive
FD beamforming is conceived to compensate the resolution loss induced by
difference operation. Finally, we further derive a coarse-to-fine histogram
statistics scheme to alleviate the perturbation in compressive FD beamforming
with good DOA estimation accuracy. Simulation results demonstrate the superior
performance of our proposed algorithm regarding robustness, resolution, and DOA
estimation accuracy.


## Chunking the Critic A Transformer-based Soft Actor-Critic with N-Step Returns

>Authors: Dong Tian, Ge Li, Hongyi Zhou, Onur Celik, Gerhard Neumann

>2025-03-05

> http://arxiv.org/abs/2503.03660v2

Soft Actor-Critic (SAC) critically depends on its critic network, which
typically evaluates a single state-action pair to guide policy updates. Using
N-step returns is a common practice to reduce the bias in the target values of
the critic. However, using N-step returns can again introduce high variance and
necessitates importance sampling, often destabilizing training. Recent
algorithms have also explored action chunking-such as direct action repetition
and movement primitives-to enhance exploration. In this paper, we propose a
Transformer-based Critic Network for SAC that integrates the N-returns
framework in a stable and efficient manner. Unlike approaches that perform
chunking in the actor network, we feed chunked actions into the critic network
to explore potential performance gains. Our architecture leverages the
Transformer's ability to process sequential information, facilitating more
robust value estimation. Empirical results show that this method not only
achieves efficient, stable training but also excels in **sparse**
reward/multi-phase environments-traditionally a challenge for step-based
methods. These findings underscore the promise of combining Transformer-based
critics with N-returns to advance reinforcement learning performance


## Feature-Level Insights into Artificial Text Detection with Sparse Autoencoders

>Authors: Kristian Kuznetsov, Laida Kushnareva, Polina Druzhinina, Anton Razzhigaev, Anastasia Voznyuk, Irina Piontkovskaya, Evgeny Burnaev, Serguei Barannikov

>2025-03-05

> http://arxiv.org/abs/2503.03601v1

Artificial Text Detection (ATD) is becoming increasingly important with the
rise of advanced Large Language Models (LLMs). Despite numerous efforts, no
single algorithm performs consistently well across different types of unseen
text or guarantees effective generalization to new LLMs. Interpretability plays
a crucial role in achieving this goal. In this study, we enhance ATD
interpretability by using Sparse Autoencoders (SAE) to extract features from
Gemma-2-2b residual stream. We identify both interpretable and efficient
features, analyzing their semantics and relevance through domain- and
model-specific statistics, a steering approach, and manual or LLM-based
interpretation. Our methods offer valuable insights into how texts from various
models differ from human-written content. We show that modern LLMs have a
distinct writing style, especially in information-dense domains, even though
they can produce human-like outputs with personalized prompts.


## English K_Quantization of LLMs Does Not Disproportionately Diminish Multilingual Performance

>Authors: Karl Audun Borgersen

>2025-03-05

> http://arxiv.org/abs/2503.03592v1

For consumer usage of locally deployed LLMs, the GGUF format and
k_**quantization** are invaluable tools for maintaining the performance of the
original model while reducing it to sizes deployable with consumer-grade
hardware. The number of bits dedicated to each weight from the original model
is reduced based on how important they are thought to be during model
inference. This importance is arrived at through the application of an
'importance matrix'-a relatively small text document meant to be representative
of the LLM's standard use-cases. In the vast majority of quants available
online, this document is primarily written in English. It was therefore an open
question whether performance on English language tasks was preserved through
the sacrifice of multilingual performance and whether it can be preserved with
alternate importance matrices. This article investigates these hypotheses by
quantizing Llama3.3 70B on importance matrices written in three languages
(English, Norwegian, and Malayalam) and evaluating them on the MixEval dataset
in both English and Norwegian. All experiments related to k_**quantization**
yielded non-significant results (In all cases p > 0.237) indicating that
current **quantization** practices do not disproportionately harm multilingual
performance.


## PowerAttention Exponentially Scaling of Receptive Fields for Effective Sparse Attention

>Authors: Lida Chen, Dong Xu, Chenxin An, Xintao Wang, Yikai Zhang, Jiangjie Chen, Zujie Liang, Feng Wei, Jiaqing Liang, Yanghua Xiao, Wei Wang

>2025-03-05

> http://arxiv.org/abs/2503.03588v1

Large Language Models (LLMs) face efficiency bottlenecks due to the quadratic
complexity of the attention mechanism when processing long contexts. Sparse
attention methods offer a promising solution, but existing approaches often
suffer from incomplete effective context and/or require complex implementation
of pipeline. We present a comprehensive analysis of **sparse** attention for
autoregressive LLMs from the respective of receptive field, recognize the
suboptimal nature of existing methods for expanding the receptive field, and
introduce PowerAttention, a novel **sparse** attention design that facilitates
effective and complete context extension through the theoretical analysis.
PowerAttention achieves exponential receptive field growth in $d$-layer LLMs,
allowing each output token to attend to $2^d$ tokens, ensuring completeness and
continuity of the receptive field. Experiments demonstrate that PowerAttention
outperforms existing static **sparse** attention methods by $5\sim 40\%$,
especially on tasks demanding long-range dependencies like Passkey Retrieval
and RULER, while maintaining a comparable time complexity to sliding window
attention. Efficiency evaluations further highlight PowerAttention's superior
speedup in both prefilling and decoding phases compared with dynamic **sparse**
attentions and full attention ($3.0\times$ faster on 128K context), making it a
highly effective and user-friendly solution for processing long sequences in
LLMs.


## Domain Consistent Industrial Decarbonisation of Global Coal Power Plants

>Authors: Waqar Muhammad Ashraf, Vivek Dua, Ramit Debnath

>2025-03-05

> http://arxiv.org/abs/2503.03571v1

Machine learning and optimisation techniques (MLOPT) hold significant
potential to accelerate the decarbonisation of industrial systems by enabling
data-driven operational improvements. However, the practical application of
MLOPT in industrial settings is often hindered by a lack of domain compliance
and system-specific consistency, resulting in suboptimal solutions with limited
real-world applicability. To address this challenge, we propose a novel
human-in-the-loop (HITL) constraint-based optimisation framework that
integrates domain expertise with data-driven methods, ensuring solutions are
both technically sound and operationally feasible. We demonstrate the efficacy
of this framework through a case study focused on enhancing the thermal
efficiency and reducing the turbine heat rate of a 660 MW supercritical
coal-fired power plant. By embedding domain knowledge as constraints within the
optimisation process, our approach yields solutions that align with the plant's
operational patterns and are seamlessly integrated into its control systems.
Empirical validation confirms a mean improvement in thermal efficiency of
0.64\% and a mean reduction in turbine heat rate of 93 kJ/kWh. Scaling our
analysis to 59 global coal power plants with comparable capacity and fuel type,
we estimate a cumulative lifetime reduction of 156.4 million tons of carbon
emissions. These results underscore the transformative potential of our
HITL-MLOPT framework in delivering domain-compliant, implementable solutions
for industrial decarbonisation, offering a scalable pathway to mitigate the
environmental impact of coal-based power generation worldwide.


## NeuGrasp Generalizable Neural Surface Reconstruction with Background Priors for Material-Agnostic Object Grasp Detection

>Authors: Qingyu Fan, Yinghao Cai, Chao Li, Wenzhe He, Xudong Zheng, Tao Lu, Bin Liang, Shuo Wang

>2025-03-05

> http://arxiv.org/abs/2503.03511v1

Robotic grasping in scenes with transparent and specular objects presents
great challenges for methods relying on accurate depth information. In this
paper, we introduce NeuGrasp, a neural surface reconstruction method that
leverages background priors for material-agnostic grasp detection. NeuGrasp
integrates transformers and global prior volumes to aggregate multi-view
features with spatial encoding, enabling robust surface reconstruction in
narrow and **sparse** viewing conditions. By focusing on foreground objects through
residual feature enhancement and refining spatial perception with an
occupancy-prior volume, NeuGrasp excels in handling objects with transparent
and specular surfaces. Extensive experiments in both simulated and real-world
scenarios show that NeuGrasp outperforms state-of-the-art methods in grasping
while maintaining comparable reconstruction quality. More details are available
at https://neugrasp.github.io/.


## Collaborative Expert LLMs Guided Multi-Objective Molecular Optimization

>Authors: Jiajun Yu, Yizhen Zheng, Huan Yee Koh, Shirui Pan, Tianyue Wang, Haishuai Wang

>2025-03-05

> http://arxiv.org/abs/2503.03503v1

Molecular optimization is a crucial yet complex and time-intensive process
that often acts as a bottleneck for drug development. Traditional methods rely
heavily on trial and error, making multi-objective optimization both
time-consuming and resource-intensive. Current AI-based methods have shown
limited success in handling multi-objective optimization tasks, hampering their
practical utilization. To address this challenge, we present MultiMol, a
collaborative large language model (LLM) system designed to guide
multi-objective molecular optimization. MultiMol comprises two agents,
including a data-driven worker agent and a literature-guided research agent.
The data-driven worker agent is a large language model being fine-tuned to
learn how to generate optimized molecules considering multiple objectives,
while the literature-guided research agent is responsible for searching
task-related literature to find useful prior knowledge that facilitates
identifying the most promising optimized candidates. In evaluations across six
multi-objective optimization tasks, MultiMol significantly outperforms existing
methods, achieving a 82.30% success rate, in sharp contrast to the 27.50%
success rate of current strongest methods. To further validate its practical
impact, we tested MultiMol on two real-world challenges. First, we enhanced the
selectivity of Xanthine Amine Congener (XAC), a promiscuous ligand that binds
both A1R and A2AR, successfully biasing it towards A1R. Second, we improved the
bioavailability of Saquinavir, an HIV-1 protease inhibitor with known
bioavailability limitations. Overall, these results indicate that MultiMol
represents a highly promising approach for multi-objective molecular
optimization, holding great potential to accelerate the drug development
process and contribute to the advancement of pharmaceutical research.


## Visualising Policy-Reward Interplay to Inform Zeroth-Order Preference Optimisation of Large Language Models

>Authors: Alessio Galatolo, Zhenbang Dai, Katie Winkle, Meriem Beloucif

>2025-03-05

> http://arxiv.org/abs/2503.03460v1

Fine-tuning LLMs with first-order methods like back-propagation is
computationally intensive. Zeroth-Order (ZO) optimisation, using function
evaluations instead of gradients, reduces memory usage but suffers from slow
convergence in high-dimensional models. As a result, ZO research in LLMs has
mostly focused on classification, overlooking more complex generative tasks. In
this paper, we introduce ZOPrO, a novel ZO algorithm designed for
\textit{Preference Optimisation} in LLMs. We begin by analysing the interplay
between policy and reward models during traditional (first-order) Preference
Optimisation, uncovering patterns in their relative updates. Guided by these
insights, we adapt Simultaneous Perturbation Stochastic Approximation (SPSA)
with a targeted sampling strategy to accelerate convergence. Through
experiments on summarisation, machine translation, and conversational
assistants, we demonstrate that our method consistently enhances reward signals
while achieving convergence times comparable to first-order methods. While it
falls short of some state-of-the-art methods, our work is the first to apply
Zeroth-Order methods to Preference Optimisation in LLMs, going beyond
classification tasks and paving the way for a largely unexplored research
direction. Code and visualisations are available at
https://github.com/alessioGalatolo/VisZOPrO


## Skeletonisation Scale-Spaces

>Authors: Julia Gierke, Pascal Peter

>2025-03-05

> http://arxiv.org/abs/2503.03450v1

The medial axis transform is a well-known tool for shape recognition. Instead
of the object contour, it equivalently describes a binary object in terms of a
skeleton containing all centres of maximal inscribed discs. While this shape
descriptor is useful for many applications, it is also sensitive to noise:
Small boundary perturbations can result in large unwanted expansions of the
skeleton. Pruning offers a remedy by removing unwanted skeleton parts. In our
contribution, we generalise this principle to skeleton sparsification: We show
that subsequently removing parts of the skeleton simplifies the associated
shape in a hierarchical manner that obeys scale-space properties.
  To this end, we provide both a continuous and discrete theory that
incorporates architectural and simplification statements as well as
invariances. We illustrate how our skeletonisation scale-spaces can be employed
for practical applications with two proof-of-concept implementations for
**pruning** and compression.


## JamMa Ultra-lightweight Local Feature Matching with Joint Mamba

>Authors: Xiaoyong Lu, Songlin Du

>2025-03-05

> http://arxiv.org/abs/2503.03437v1

Existing state-of-the-art feature matchers capture long-range dependencies
with Transformers but are hindered by high spatial complexity, leading to
demanding training and highlatency inference. Striking a better balance between
performance and efficiency remains a challenge in feature matching. Inspired by
the linear complexity O(N) of Mamba, we propose an ultra-lightweight
Mamba-based matcher, named JamMa, which converges on a single GPU and achieves
an impressive performance-efficiency balance in inference. To unlock the
potential of Mamba for feature matching, we propose Joint Mamba with a
scan-merge strategy named JEGO, which enables: (1) Joint scan of two images to
achieve high-frequency mutual interaction, (2) Efficient scan with skip steps
to reduce sequence length, (3) Global receptive field, and (4) Omnidirectional
feature representation. With the above properties, the JEGO strategy
significantly outperforms the scan-merge strategies proposed in VMamba and
EVMamba in the feature matching task. Compared to attention-based **sparse** and
semi-dense matchers, JamMa demonstrates a superior balance between performance
and efficiency, delivering better performance with less than 50% of the
parameters and FLOPs.


## RASD Retrieval-Augmented Speculative Decoding

>Authors: Guofeng Quan, Wenfeng Feng, Chuzhan Hao, Guochao Jiang, Yuewei Zhang, Hao Wang

>2025-03-05

> http://arxiv.org/abs/2503.03434v1

Speculative decoding accelerates inference in large language models (LLMs) by
generating draft tokens for target model verification. Current approaches for
obtaining draft tokens rely on lightweight draft models or additional model
structures to generate draft tokens and retrieve context from databases. Due to
the draft model's small size and limited training data, model-based speculative
decoding frequently becomes less effective in out-of-domain scenarios.
Additionally, the time cost of the drafting phase results in a low upper limit
on acceptance length during the verification step, limiting overall efficiency.
This paper proposes RASD (Retrieval-Augmented Speculative Decoding), which
adopts retrieval methods to enhance model-based speculative decoding. We
introduce tree **pruning** and tree fusion to achieve this. Specifically, we
develop a **pruning** method based on the draft model's probability distribution to
construct the optimal retrieval tree. Second, we employ the longest prefix
matching algorithm to merge the tree generated by the draft model with the
retrieval tree, resulting in a unified tree for verification. Experimental
results demonstrate that RASD achieves state-of-the-art inference **acceleration**
across tasks such as DocQA, Summary, Code, and In-Domain QA. Moreover, RASD
exhibits strong scalability, seamlessly integrating with various speculative
decoding approaches, including both generation-based and retrieval-based
methods.


## The Gaia Astrometric Catalogue and Secular Aberration Drift in Proper Motions

>Authors: Anthony G. A. Brown, Ulrich Bastian, Sergei Klioner

>2025-03-05

> http://arxiv.org/abs/2503.03389v1

A recent paper demonstrated the existence of a secular aberration drift term
in stellar proper motions that arises when transforming an astrometric
catalogue defined for an observer at rest with respect to the solar system
barycentre to some other reference frame in which, for example, the observer is
at rest with respect to the Galactic centre. Such a transformation requires an
accurate and precise estimate of the velocity of the solar system barycentre.
It was argued that the Gaia catalogue construction should account for this
effect and also for the aberrational effect due to **acceleration** of the solar
system barycentre. We argue that these two effects should not be accounted for
in the construction of the Gaia astrometric catalogue. We briefly review the
Gaia catalogue reference frame, the concepts of stellar aberration and secular
aberration drift, and their observable consequences. The Gaia catalogue is (and
should be) constructed in the Barycentric Celestial Reference System: the
reference system with the origin at the solar system barycentre as defined by
the underlying solar system ephemerides. We explain that the Gaia catalogue is
consistent with the International Celestial Reference System despite the
presence of proper motion terms due to the **acceleration** of the solar system
barycentre. We also explain why transformation of the astrometry to a frame in
which the observer is at rest with respect to the Galactic centre or distant
universe is not needed for the interpretation of stellar kinematics, and that
there are practical concerns with such a transformation. The estimation of the
velocity and **acceleration** of the solar system barycentre, although important as
a matter of scientific investigation, are not needed for the construction of
the Gaia astrometric catalogue.


## On the Relation Between Speech Quality and Quantized Latent Representations of Neural Codecs

>Authors: Mhd Modar Halimeh, Matteo Torcoli, Philipp Grundhuber, EmanuÃ«l A. P. Habets

>2025-03-05

> http://arxiv.org/abs/2503.03304v1

Neural audio signal codecs have attracted significant attention in recent
years. In essence, the impressive low bitrate achieved by such encoders is
enabled by learning an abstract representation that captures the properties of
encoded signals, e.g., speech. In this work, we investigate the relation
between the latent representation of the input signal learned by a neural codec
and the quality of speech signals. To do so, we introduce
Latent-representation-to-Quantization error Ratio (LQR) measures, which
quantify the distance from the idealized neural codec's speech signal model for
a given speech signal. We compare the proposed metrics to intrusive measures as
well as data-driven supervised methods using two subjective speech quality
datasets. This analysis shows that the proposed LQR correlates strongly (up to
0.9 Pearson's correlation) with the subjective quality of speech. Despite being
a non-intrusive metric, this yields a competitive performance with, or even
better than, other pre-trained and intrusive measures. These results show that
LQR is a promising basis for more sophisticated speech quality measures.


## A 262 TOPS Hyperdimensional Photonic AI Accelerator powered by a Si3N4 microcomb laser

>Authors: Christos Pappas, Antonios Prapas, Theodoros Moschos, Manos Kirtas, Odysseas Asimopoulos, Apostolos Tsakyridis, Miltiadis Moralis-Pegios, Chris Vagionas, Nikolaos Passalis, Cagri Ozdilek, Timofey Shpakovsky, Alain Yuji Takabayashi, John D. Jost, Maxim Karpov, Anastasios Tefas, Nikos Pleros

>2025-03-05

> http://arxiv.org/abs/2503.03263v1

The ever-increasing volume of data has necessitated a new computing paradigm,
embodied through Artificial Intelligence (AI) and Large Language Models (LLMs).
Digital electronic AI computing systems, however, are gradually reaching their
physical plateaus, stimulating extensive research towards next-generation AI
accelerators. Photonic Neural Networks (PNNs), with their unique ability to
capitalize on the interplay of multiple physical dimensions including time,
wavelength, and space, have been brought forward with a credible promise for
boosting computational power and energy efficiency in AI processors. In this
article, we experimentally demonstrate a novel multidimensional arrayed
waveguide grating router (AWGR)-based photonic AI accelerator that can execute
tensor multiplications at a record-high total computational power of 262 TOPS,
offering a ~24x improvement over the existing waveguide-based optical
accelerators. It consists of a 16x16 AWGR that exploits the time-, wavelength-
and space- division multiplexing (T-WSDM) for weight and input encoding
together with an integrated Si3N4-based frequency comb for multi-wavelength
generation. The photonic AI accelerator has been experimentally validated in
both Fully-Connected (FC) and Convolutional NN (NNs) models, with the FC and
CNN being trained for DDoS attack identification and MNIST classification,
respectively. The experimental inference at 32 Gbaud achieved a Cohen's kappa
score of 0.867 for DDoS detection and an accuracy of 92.14% for MNIST
classification, respectively, closely matching the software performance.


## BAT Learning Event-based Optical Flow with Bidirectional Adaptive Temporal Correlation

>Authors: Gangwei Xu, Haotong Lin, Zhaoxing Zhang, Hongcheng Luo, Haiyang Sun, Xin Yang

>2025-03-05

> http://arxiv.org/abs/2503.03256v1

Event cameras deliver visual information characterized by a high dynamic
range and high temporal resolution, offering significant advantages in
estimating optical flow for complex lighting conditions and fast-moving
objects. Current advanced optical flow methods for event cameras largely adopt
established image-based frameworks. However, the spatial **sparsity** of event data
limits their performance. In this paper, we present BAT, an innovative
framework that estimates event-based optical flow using bidirectional adaptive
temporal correlation. BAT includes three novel designs: 1) a bidirectional
temporal correlation that transforms bidirectional temporally dense motion cues
into spatially dense ones, enabling accurate and spatially dense optical flow
estimation; 2) an adaptive temporal sampling strategy for maintaining temporal
consistency in correlation; 3) spatially adaptive temporal motion aggregation
to efficiently and adaptively aggregate consistent target motion features into
adjacent motion features while suppressing inconsistent ones. Our results rank
$1^{st}$ on the DSEC-Flow benchmark, outperforming existing state-of-the-art
methods by a large margin while also exhibiting sharp edges and high-quality
details. Notably, our BAT can accurately predict future optical flow using only
past events, significantly outperforming E-RAFT's warm-start approach. Code:
\textcolor{magenta}{https://github.com/gangweiX/BAT}.


## A Multimodal Framework for Topic Propagation Classification in Social Networks

>Authors: Yuchuan Jiang, Chaolong Jia, Yunyi Qin, Wei Cai, Yongsen Qian

>2025-03-05

> http://arxiv.org/abs/2503.03112v1

The rapid proliferation of the Internet and the widespread adoption of social
networks have significantly accelerated information dissemination. However,
this transformation has introduced complexities in information capture and
processing, posing substantial challenges for researchers and practitioners.
Predicting the dissemination of topic-related information within social
networks has thus become a critical research focus. This paper proposes a
predictive model for topic dissemination in social networks by integrating
multidimensional features derived from key dissemination characteristics.
Specifically, we introduce two novel indicators, user relationship breadth and
user authority, into the PageRank algorithm to quantify user influence more
effectively. Additionally, we employ a Text-CNN model for sentiment
classification, extracting sentiment features from textual content. Temporal
embeddings of nodes are encoded using a Bi-LSTM model to capture temporal
dynamics. Furthermore, we refine the measurement of user interaction traces
with topics, replacing traditional topic view metrics with a more precise
communication characteristics measure. Finally, we integrate the extracted
multidimensional features using a Transformer model, significantly enhancing
predictive performance. Experimental results demonstrate that our proposed
model outperforms traditional machine learning and unimodal deep learning
models in terms of FI-Score, AUC, and Recall, validating its effectiveness in
predicting topic propagation within social networks.


## SAFE A Sparse Autoencoder-Based Framework for Robust Query Enrichment and Hallucination Mitigation in LLMs

>Authors: Samir Abdaljalil, Filippo Pallucchini, Andrea Seveso, Hasan Kurban, Fabio Mercorio, Erchin Serpedin

>2025-03-04

> http://arxiv.org/abs/2503.03032v1

Despite the state-of-the-art performance of Large Language Models (LLMs),
these models often suffer from hallucinations, which can undermine their
performance in critical applications. In this work, we propose SAFE, a novel
method for detecting and mitigating hallucinations by leveraging Sparse
Autoencoders (SAEs). While hallucination detection techniques and SAEs have
been explored independently, their synergistic application in a comprehensive
system, particularly for hallucination-aware query enrichment, has not been
fully investigated. To validate the effectiveness of SAFE, we evaluate it on
two models with available SAEs across three diverse cross-domain datasets
designed to assess hallucination problems. Empirical results demonstrate that
SAFE consistently improves query generation accuracy and mitigates
hallucinations across all datasets, achieving accuracy improvements of up to
29.45%.


## FairSense-AI Responsible AI Meets Sustainability

>Authors: Shaina Raza, Mukund Sayeeganesh Chettiar, Matin Yousefabadi, Tahniat Khan, Marcelo Lotif

>2025-03-04

> http://arxiv.org/abs/2503.02865v2

In this paper, we introduce FairSense-AI: a multimodal framework designed to
detect and mitigate bias in both text and images. By leveraging Large Language
Models (LLMs) and Vision-Language Models (VLMs), FairSense-AI uncovers subtle
forms of prejudice or stereotyping that can appear in content, providing users
with bias scores, explanatory highlights, and automated recommendations for
fairness enhancements. In addition, FairSense-AI integrates an AI risk
assessment component that aligns with frameworks like the MIT AI Risk
Repository and NIST AI Risk Management Framework, enabling structured
identification of ethical and safety concerns. The platform is optimized for
energy efficiency via techniques such as model **pruning** and mixed-precision
computation, thereby reducing its environmental footprint. Through a series of
case studies and applications, we demonstrate how FairSense-AI promotes
responsible AI use by addressing both the social dimension of fairness and the
pressing need for sustainability in large-scale AI deployments.
https://vectorinstitute.github.io/FairSense-AI,
https://pypi.org/project/fair-sense-ai/ (Sustainability , Responsible AI ,
Large Language Models , Vision Language Models , Ethical AI , Green AI)


## Privacy and Accuracy-Aware AI/ML Model Deduplication

>Authors: Hong Guan, Lei Yu, Lixi Zhou, Li Xiong, Kanchan Chowdhury, Lulu Xie, Xusheng Xiao, Jia Zou

>2025-03-04

> http://arxiv.org/abs/2503.02862v1

With the growing adoption of privacy-preserving machine learning algorithms,
such as Differentially Private Stochastic Gradient Descent (DP-SGD), training
or fine-tuning models on private datasets has become increasingly prevalent.
This shift has led to the need for models offering varying privacy guarantees
and utility levels to satisfy diverse user requirements. However, managing
numerous versions of large models introduces significant operational
challenges, including increased inference latency, higher resource consumption,
and elevated costs. Model deduplication is a technique widely used by many
model serving and database systems to support high-performance and low-cost
inference queries and model diagnosis queries. However, none of the existing
model deduplication works has considered privacy, leading to unbounded
aggregation of privacy costs for certain deduplicated models and inefficiencies
when applied to deduplicate DP-trained models. We formalize the problems of
deduplicating DP-trained models for the first time and propose a novel privacy-
and accuracy-aware deduplication mechanism to address the problems. We
developed a greedy strategy to select and assign base models to target models
to minimize storage and privacy costs. When deduplicating a target model, we
dynamically schedule accuracy validations and apply the Sparse Vector Technique
to reduce the privacy costs associated with private validation data. Compared
to baselines that do not provide privacy guarantees, our approach improved the
compression ratio by up to $35\times$ for individual models (including large
language models and vision transformers). We also observed up to $43\times$
inference speedup due to the reduction of I/O operations.


## (How) Do Language Models Track State?

>Authors: Belinda Z. Li, Zifan Carl Guo, Jacob Andreas

>2025-03-04

> http://arxiv.org/abs/2503.02854v1

Transformer language models (LMs) exhibit behaviors -- from storytelling to
code generation -- that appear to require tracking the unobserved state of an
evolving world. How do they do so? We study state tracking in LMs trained or
fine-tuned to compose permutations (i.e., to compute the order of a set of
objects after a sequence of swaps). Despite the simple algebraic structure of
this problem, many other tasks (e.g., simulation of finite automata and
evaluation of boolean expressions) can be reduced to permutation composition,
making it a natural model for state tracking in general. We show that LMs
consistently learn one of two state tracking mechanisms for this task. The
first closely resembles the "associative scan" construction used in recent
theoretical work by Liu et al. (2023) and Merrill et al. (2024). The second
uses an easy-to-compute feature (permutation parity) to partially prune the
space of outputs, then refines this with an associative scan. The two
mechanisms exhibit markedly different robustness properties, and we show how to
steer LMs toward one or the other with intermediate training tasks that
encourage or suppress the heuristics. Our results demonstrate that transformer
LMs, whether pretrained or fine-tuned, can learn to implement efficient and
interpretable state tracking mechanisms, and the emergence of these mechanisms
can be predicted and controlled.


## Boltzmann Attention Sampling for Image Analysis with Small Objects

>Authors: Theodore Zhao, Sid Kiblawi, Naoto Usuyama, Ho Hin Lee, Sam Preston, Hoifung Poon, Mu Wei

>2025-03-04

> http://arxiv.org/abs/2503.02841v1

Detecting and segmenting small objects, such as lung nodules and tumor
lesions, remains a critical challenge in image analysis. These objects often
occupy less than 0.1% of an image, making traditional transformer architectures
inefficient and prone to performance degradation due to redundant attention
computations on irrelevant regions. Existing **sparse** attention mechanisms rely
on rigid hierarchical structures, which are poorly suited for detecting small,
variable, and uncertain object locations. In this paper, we propose
BoltzFormer, a novel transformer-based architecture designed to address these
challenges through dynamic **sparse** attention. BoltzFormer identifies and focuses
attention on relevant areas by modeling uncertainty using a Boltzmann
distribution with an annealing schedule. Initially, a higher temperature allows
broader area sampling in early layers, when object location uncertainty is
greatest. As the temperature decreases in later layers, attention becomes more
focused, enhancing efficiency and accuracy. BoltzFormer seamlessly integrates
into existing transformer architectures via a modular Boltzmann attention
sampling mechanism. Comprehensive evaluations on benchmark datasets demonstrate
that BoltzFormer significantly improves segmentation performance for small
objects while reducing attention computation by an order of magnitude compared
to previous state-of-the-art methods.


## AlignDistil Token-Level Language Model Alignment as Adaptive Policy Distillation

>Authors: Songming Zhang, Xue Zhang, Tong Zhang, Bojie Hu, Yufeng Chen, Jinan Xu

>2025-03-04

> http://arxiv.org/abs/2503.02832v1

In modern large language models (LLMs), LLM alignment is of crucial
importance and is typically achieved through methods such as reinforcement
learning from human feedback (RLHF) and direct preference optimization (DPO).
However, in most existing methods for LLM alignment, all tokens in the response
are optimized using a **sparse**, response-level reward or preference annotation.
The ignorance of token-level rewards may erroneously punish high-quality tokens
or encourage low-quality tokens, resulting in suboptimal performance and slow
convergence speed. To address this issue, we propose AlignDistil, an
RLHF-equivalent distillation method for token-level reward optimization.
Specifically, we introduce the reward learned by DPO into the RLHF objective
and theoretically prove the equivalence between this objective and a
token-level distillation process, where the teacher distribution linearly
combines the logits from the DPO model and a reference model. On this basis, we
further bridge the accuracy gap between the reward from the DPO model and the
pure reward model, by building a contrastive DPO reward with a normal and a
reverse DPO model. Moreover, to avoid under- and over-optimization on different
tokens, we design a token adaptive logit extrapolation mechanism to construct
an appropriate teacher distribution for each token. Experimental results
demonstrate the superiority of our AlignDistil over existing methods and
showcase fast convergence due to its token-level distributional reward
optimization.


## Q-Filters Leveraging QK Geometry for Efficient KV Cache Compression

>Authors: Nathan Godey, Alessio Devoto, Yu Zhao, Simone Scardapane, Pasquale Minervini, Ãric de la Clergerie, BenoÃ®t Sagot

>2025-03-04

> http://arxiv.org/abs/2503.02812v1

Autoregressive language models rely on a Key-Value (**KV**) Cache, which avoids
re-computing past hidden states during generation, making it faster. As model
sizes and context lengths grow, the **KV** Cache becomes a significant memory
bottleneck, which calls for compression methods that limit its size during
generation. In this paper, we discover surprising properties of Query (Q) and
Key (K) vectors that allow us to efficiently approximate attention scores
without computing the attention maps. We propose Q-Filters, a training-free **KV**
Cache compression method that filters out less crucial Key-Value pairs based on
a single context-agnostic projection. Contrarily to many alternatives,
Q-Filters is compatible with FlashAttention, as it does not require direct
access to attention weights. Experimental results in long-context settings
demonstrate that Q-Filters is competitive with attention-based compression
methods such as Snap**KV** in retrieval tasks while consistently outperforming
efficient compression schemes such as Streaming-LLM in generation setups.
Notably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task
with a x32 compression level while reducing the generation perplexity drop by
up to 65% in text generation compared to Streaming-LLM.


## RAAD-LLM Adaptive Anomaly Detection Using LLMs and RAG Integration

>Authors: Alicia Russell-Gilbert, Sudip Mittal, Shahram Rahimi, Maria Seale, Joseph Jabour, Thomas Arnold, Joshua Church

>2025-03-04

> http://arxiv.org/abs/2503.02800v2

Anomaly detection in complex industrial environments poses unique challenges,
particularly in contexts characterized by data **sparsity** and evolving
operational conditions. Predictive maintenance (PdM) in such settings demands
methodologies that are adaptive, transferable, and capable of integrating
domain-specific knowledge. In this paper, we present RAAD-LLM, a novel
framework for adaptive anomaly detection, leveraging large language models
(LLMs) integrated with Retrieval-Augmented Generation (RAG). This approach
addresses the aforementioned PdM challenges. By effectively utilizing
domain-specific knowledge, RAAD-LLM enhances the detection of anomalies in time
series data without requiring fine-tuning on specific datasets. The framework's
adaptability mechanism enables it to adjust its understanding of normal
operating conditions dynamically, thus increasing detection accuracy. We
validate this methodology through a real-world application for a plastics
manufacturing plant and the Skoltech Anomaly Benchmark (SKAB). Results show
significant improvements over our previous model with an accuracy increase from
70.7% to 89.1% on the real-world dataset. By allowing for the enriching of
input series data with semantics, RAAD-LLM incorporates multimodal capabilities
that facilitate more collaborative decision-making between the model and plant
operators. Overall, our findings support RAAD-LLM's ability to revolutionize
anomaly detection methodologies in PdM, potentially leading to a paradigm shift
in how anomaly detection is implemented across various industries.


## ArcPro Architectural Programs for Structured 3D Abstraction of Sparse Points

>Authors: Qirui Huang, Runze Zhang, Kangjun Liu, Minglun Gong, Hao Zhang, Hui Huang

>2025-03-04

> http://arxiv.org/abs/2503.02745v2

We introduce ArcPro, a novel learning framework built on architectural
programs to recover structured 3D abstractions from highly **sparse** and
low-quality point clouds. Specifically, we design a domain-specific language
(DSL) to hierarchically represent building structures as a program, which can
be efficiently converted into a mesh. We bridge feedforward and inverse
procedural modeling by using a feedforward process for training data synthesis,
allowing the network to make reverse predictions. We train an encoder-decoder
on the points-program pairs to establish a mapping from unstructured point
clouds to architectural programs, where a 3D convolutional encoder extracts
point cloud features and a transformer decoder autoregressively predicts the
programs in a tokenized form. Inference by our method is highly efficient and
produces plausible and faithful 3D abstractions. Comprehensive experiments
demonstrate that ArcPro outperforms both traditional architectural proxy
reconstruction and learning-based abstraction methods. We further explore its
potential to work with multi-view image and natural language inputs.


## TReND Transformer derived features and Regularized NMF for neonatal functional network Delineation

>Authors: Sovesh Mohapatra, Minhui Ouyang, Shufang Tan, Jianlin Guo, Lianglong Sun, Yong He, Hao Huang

>2025-03-04

> http://arxiv.org/abs/2503.02685v1

Precise parcellation of functional networks (FNs) of early developing human
brain is the fundamental basis for identifying biomarker of developmental
disorders and understanding functional development. Resting-state fMRI
(rs-fMRI) enables in vivo exploration of functional changes, but adult FN
parcellations cannot be directly applied to the neonates due to incomplete
network maturation. No standardized neonatal functional atlas is currently
available. To solve this fundamental issue, we propose TReND, a novel and fully
automated self-supervised transformer-autoencoder framework that integrates
regularized nonnegative matrix factorization (RNMF) to unveil the FNs in
neonates. TReND effectively disentangles spatiotemporal features in voxel-wise
rs-fMRI data. The framework integrates confidence-adaptive masks into
transformer self-attention layers to mitigate noise influence. A self
supervised decoder acts as a regulator to refine the encoder's latent
embeddings, which serve as reliable temporal features. For spatial coherence,
we incorporate brain surface-based geodesic distances as spatial encodings
along with functional connectivity from temporal features. The TReND clustering
approach processes these features under **sparsity** and smoothness constraints,
producing robust and biologically plausible parcellations. We extensively
validated our TReND framework on three different rs-fMRI datasets: simulated,
dHCP and HCP-YA against comparable traditional feature extraction and
clustering techniques. Our results demonstrated the superiority of the TReND
framework in the delineation of neonate FNs with significantly better spatial
contiguity and functional homogeneity. Collectively, we established TReND, a
novel and robust framework, for neonatal FN delineation. TReND-derived neonatal
FNs could serve as a neonatal functional atlas for perinatal populations in
health and disease.


## TeTRA-VPR A Ternary Transformer Approach for Compact Visual Place Recognition

>Authors: Oliver Grainge, Michael Milford, Indu Bodala, Sarvapali D. Ramchurn, Shoaib Ehsan

>2025-03-04

> http://arxiv.org/abs/2503.02511v1

Visual Place Recognition (VPR) localizes a query image by matching it against
a database of geo-tagged reference images, making it essential for navigation
and mapping in robotics. Although Vision Transformer (ViT) solutions deliver
high accuracy, their large models often exceed the memory and compute budgets
of resource-constrained platforms such as drones and mobile robots. To address
this issue, we propose TeTRA, a ternary transformer approach that progressively
**quantize**s the ViT backbone to 2-bit precision and binarizes its final embedding
layer, offering substantial reductions in model size and latency. A carefully
designed progressive distillation strategy preserves the representational power
of a full-precision teacher, allowing TeTRA to retain or even surpass the
accuracy of uncompressed convolutional counterparts, despite using fewer
resources. Experiments on standard VPR benchmarks demonstrate that TeTRA
reduces memory consumption by up to 69% compared to efficient baselines, while
lowering inference latency by 35%, with either no loss or a slight improvement
in recall@1. These gains enable high-accuracy VPR on power-constrained,
memory-limited robotic platforms, making TeTRA an appealing solution for
real-world deployment.


## Q&C When Quantization Meets Cache in Efficient Image Generation

>Authors: Xin Ding, Xin Li, Haotong Qin, Zhibo Chen

>2025-03-04

> http://arxiv.org/abs/2503.02508v1

Quantization and cache mechanisms are typically applied individually for
efficient Diffusion Transformers (DiTs), each demonstrating notable potential
for **acceleration**. However, the promoting effect of combining the two mechanisms
on efficient generation remains under-explored. Through empirical
investigation, we find that the combination of **quantization** and cache
mechanisms for DiT is not straightforward, and two key challenges lead to
severe catastrophic performance degradation: (i) the sample efficacy of
calibration datasets in post-training **quantization** (PTQ) is significantly
eliminated by cache operation; (ii) the combination of the above mechanisms
introduces more severe exposure bias within sampling distribution, resulting in
amplified error accumulation in the image generation process. In this work, we
take advantage of these two **acceleration** mechanisms and propose a hybrid
**acceleration** method by tackling the above challenges, aiming to further improve
the efficiency of DiTs while maintaining excellent generation capability.
Concretely, a temporal-aware parallel clustering (TAP) is designed to
dynamically improve the sample selection efficacy for the calibration within
PTQ for different diffusion steps. A variance compensation (VC) strategy is
derived to correct the sampling distribution. It mitigates exposure bias
through an adaptive correction factor generation. Extensive experiments have
shown that our method has accelerated DiTs by 12.7x while preserving
competitive generation capability. The code will be available at
https://github.com/xinding-sys/Quant-Cache.


## The Distributionally Robust Optimization Model of Sparse Principal Component Analysis

>Authors: Lei Wang, Xin Liu, Xiaojun Chen

>2025-03-04

> http://arxiv.org/abs/2503.02494v1

We consider **sparse** principal component analysis (PCA) under a stochastic
setting where the underlying probability distribution of the random parameter
is uncertain. This problem is formulated as a distributionally robust
optimization (DRO) model based on a constructive approach to capturing
uncertainty in the covariance matrix, which constitutes a nonsmooth constrained
min-max optimization problem. We further prove that the inner maximization
problem admits a closed-form solution, reformulating the original DRO model
into an equivalent minimization problem on the Stiefel manifold. This
transformation leads to a Riemannian optimization problem with intricate
nonsmooth terms, a challenging formulation beyond the reach of existing
algorithms. To address this issue, we devise an efficient smoothing manifold
proximal gradient algorithm. We prove the Riemannian gradient consistency and
global convergence of our algorithm to a stationary point of the nonsmooth
minimization problem. Moreover, we establish the iteration complexity of our
algorithm. Finally, numerical experiments are conducted to validate the
effectiveness and scalability of our algorithm, as well as to highlight the
necessity and rationality of adopting the DRO model for **sparse** PCA.


## Sparse Meets Dense Unified Generative Recommendations with Cascaded Sparse-Dense Representations

>Authors: Yuhao Yang, Zhi Ji, Zhaopeng Li, Yi Li, Zhonglin Mo, Yue Ding, Kai Chen, Zijian Zhang, Jie Li, Shuanglong Li, Lin Liu

>2025-03-04

> http://arxiv.org/abs/2503.02453v1

Generative models have recently gained attention in recommendation systems by
directly predicting item identifiers from user interaction sequences. However,
existing methods suffer from significant information loss due to the separation
of stages such as **quantization** and sequence modeling, hindering their ability
to achieve the modeling precision and accuracy of sequential dense retrieval
techniques. Integrating generative and dense retrieval methods remains a
critical challenge. To address this, we introduce the Cascaded Organized
Bi-Represented generAtive retrieval (COBRA) framework, which innovatively
integrates **sparse** semantic IDs and dense vectors through a cascading process.
Our method alternates between generating these representations by first
generating **sparse** IDs, which serve as conditions to aid in the generation of
dense vectors. End-to-end training enables dynamic refinement of dense
representations, capturing both semantic insights and collaborative signals
from user-item interactions. During inference, COBRA employs a coarse-to-fine
strategy, starting with **sparse** ID generation and refining them into dense
vectors via the generative model. We further propose BeamFusion, an innovative
approach combining beam search with nearest neighbor scores to enhance
inference flexibility and recommendation diversity. Extensive experiments on
public datasets and offline tests validate our method's robustness. Online A/B
tests on a real-world advertising platform with over 200 million daily users
demonstrate substantial improvements in key metrics, highlighting COBRA's
practical advantages.


## Joint Tensor and Inter-View Low-Rank Recovery for Incomplete Multiview Clustering

>Authors: Jianyu Wang, Zhengqiao Zhao, Nicolas Dobigeon, Jingdong Chen

>2025-03-04

> http://arxiv.org/abs/2503.02449v1

Incomplete multiview clustering (IMVC) has gained significant attention for
its effectiveness in handling missing sample challenges across various views in
real-world multiview clustering applications. Most IMVC approaches tackle this
problem by either learning consensus representations from available views or
reconstructing missing samples using the underlying manifold structure.
However, the reconstruction of learned similarity graph tensor in prior studies
only exploits the low-tubal-rank information, neglecting the exploration of
inter-view correlations. This paper propose a novel joint tensor and inter-view
low-rank Recovery (JTIV-LRR), framing IMVC as a joint optimization problem that
integrates incomplete similarity graph learning and tensor representation
recovery. By leveraging both intra-view and inter-view low rank information,
the method achieves robust estimation of the complete similarity graph tensor
through **sparse** noise removal and low-tubal-rank constraints along different
modes. Extensive experiments on both synthetic and real-world datasets
demonstrate the superiority of the proposed approach, achieving significant
improvements in clustering accuracy and robustness compared to state-of-the-art
methods.


## NodeNAS Node-Specific Graph Neural Architecture Search for Out-of-Distribution Generalization

>Authors: Qiyi Wang, Yinning Shao, Yunlong Ma, Min Liu

>2025-03-04

> http://arxiv.org/abs/2503.02448v2

Graph neural architecture search (GraphNAS) has demonstrated advantages in
mitigating performance degradation of graph neural networks (GNNs) due to
distribution shifts. Recent approaches introduce weight sharing across tailored
architectures, generating unique GNN architectures for each graph end-to-end.
However, existing GraphNAS methods do not account for distribution patterns
across different graphs and heavily rely on extensive training data. With
**sparse** or single training graphs, these methods struggle to discover optimal
mappings between graphs and architectures, failing to generalize to
out-of-distribution (OOD) data. In this paper, we propose node-specific graph
neural architecture search(NodeNAS), which aims to tailor distinct aggregation
methods for different nodes through disentangling node topology and graph
distribution with limited datasets. We further propose adaptive aggregation
attention based Multi-dim NodeNAS method(MNNAS), which learns an node-specific
architecture customizer with good generalizability. Specifically, we extend the
vertical depth of the search space, supporting simultaneous node-specific
architecture customization across multiple dimensions. Moreover, we model the
power-law distribution of node degrees under varying assortativity, encoding
structure invariant information to guide architecture customization across each
dimension. Extensive experiments across supervised and unsupervised tasks
demonstrate that MNNAS surpasses state-of-the-art algorithms and achieves
excellent OOD generalization.


## BHViT Binarized Hybrid Vision Transformer

>Authors: Tian Gao, Zhiyuan Zhang, Yu Zhang, Huajun Liu, Kaijie Yin, Chengzhong Xu, Hui Kong

>2025-03-04

> http://arxiv.org/abs/2503.02394v3

Model binarization has made significant progress in enabling real-time and
energy-efficient computation for convolutional neural networks (CNN), offering
a potential solution to the deployment challenges faced by Vision Transformers
(ViTs) on edge devices. However, due to the structural differences between CNN
and Transformer architectures, simply applying binary CNN strategies to the ViT
models will lead to a significant performance drop. To tackle this challenge,
we propose BHViT, a binarization-friendly hybrid ViT architecture and its full
binarization model with the guidance of three important observations.
Initially, BHViT utilizes the local information interaction and hierarchical
feature aggregation technique from coarse to fine levels to address redundant
computations stemming from excessive tokens. Then, a novel module based on
shift operations is proposed to enhance the performance of the binary
Multilayer Perceptron (MLP) module without significantly increasing
computational overhead. In addition, an innovative attention matrix
binarization method based on **quantization** decomposition is proposed to evaluate
the token's importance in the binarized attention matrix. Finally, we propose a
regularization loss to address the inadequate optimization caused by the
incompatibility between the weight oscillation in the binary layers and the
Adam Optimizer. Extensive experimental results demonstrate that our proposed
algorithm achieves SOTA performance among binary ViT methods.


## Cosmic Acceleration from Nothing

>Authors: Michael R. R. Good, Eric V. Linder

>2025-03-04

> http://arxiv.org/abs/2503.02380v1

We demonstrate that if the universe started as a vacuum fluctuation rather
than from a singular Big Bang state, the universe must have a late-time cosmic
**acceleration**. This is required by a ``cosmological sum rule'' derived using the
Schwarzian form of the Friedmann equations. We discuss possible connections to
conformal and M\"obius transformations, and also compute that the best fit
present cosmic data is consistent with the necessary crossing of the Schwarzian
through zero having occurred (while it would not yet have happened in a
$\Lambda$CDM cosmology).


## Teaching Metric Distance to Autoregressive Multimodal Foundational Models

>Authors: Jiwan Chung, Saejin Kim, Yongrae Jo, Jaewoo Park, Dongjun Min, Youngjae Yu

>2025-03-04

> http://arxiv.org/abs/2503.02379v1

As large language models expand beyond natural language to domains such as
mathematics, multimodal understanding, and embodied agents, tokens increasingly
reflect metric relationships rather than purely linguistic meaning. We
introduce DIST2Loss, a distance-aware framework designed to train
autoregressive discrete models by leveraging predefined distance relationships
among output tokens. At its core, DIST2Loss transforms continuous exponential
family distributions derived from inherent distance metrics into discrete,
categorical optimization targets compatible with the models' architectures.
This approach enables the models to learn and preserve meaningful distance
relationships during token generation while maintaining compatibility with
existing architectures. Empirical evaluations show consistent performance gains
in diverse multimodal applications, including visual grounding, robotic
manipulation, generative reward modeling, and image generation using
vector-**quantize**d features. These improvements are pronounced in cases of
limited training data, highlighting DIST2Loss's effectiveness in
resource-constrained settings.


## BdSLW401 Transformer-Based Word-Level Bangla Sign Language Recognition Using Relative Quantization Encoding (RQE)

>Authors: Husne Ara Rubaiyeat, Njayou Youssouf, Md Kamrul Hasan, Hasan Mahmud

>2025-03-04

> http://arxiv.org/abs/2503.02360v1

Sign language recognition (SLR) for low-resource languages like Bangla
suffers from signer variability, viewpoint variations, and limited annotated
datasets. In this paper, we present BdSLW401, a large-scale, multi-view,
word-level Bangla Sign Language (BdSL) dataset with 401 signs and 102,176 video
samples from 18 signers in front and lateral views. To improve
transformer-based SLR, we introduce Relative Quantization Encoding (RQE), a
structured embedding approach anchoring landmarks to physiological reference
points and **quantize** motion trajectories. RQE improves attention allocation by
decreasing spatial variability, resulting in 44.3% WER reduction in WLASL100,
21.0% in SignBD-200, and significant gains in BdSLW60 and SignBD-90. However,
fixed **quantization** becomes insufficient on large-scale datasets (e.g.,
WLASL2000), indicating the need for adaptive encoding strategies. Further,
RQE-SF, an extended variant that stabilizes shoulder landmarks, achieves
improvements in pose consistency at the cost of small trade-offs in lateral
view recognition. The attention graphs prove that RQE improves model
interpretability by focusing on the major articulatory features (fingers,
wrists) and the more distinctive frames instead of global pose changes.
Introducing BdSLW401 and demonstrating the effectiveness of RQE-enhanced
structured embeddings, this work advances transformer-based SLR for
low-resource languages and sets a benchmark for future research in this area.


## VQ-LLM High-performance Code Generation for Vector Quantization Augmented LLM Inference

>Authors: Zihan Liu, Xinhao Luo, Junxian Guo, Wentao Ni, Yangjie Zhou, Yue Guan, Cong Guo, Weihao Cui, Yu Feng, Minyi Guo, Yuhao Zhu, Minjia Zhang, Jingwen Leng, Chen Jin

>2025-03-04

> http://arxiv.org/abs/2503.02236v1

In this work, we design and implement VQ-LLM, an efficient fused Vector
Quantization (VQ) kernel generation framework. We first introduce a software
abstraction called codebook cache to optimize codebook access efficiency and
support the integration of VQ with various computations. The codebook cache
adaptively stores different entries across the GPU's memory hierarchy,
including off-chip global memory, on-chip shared memory, and registers.
Centered around the codebook cache, we design an efficient computation engine
that optimizes memory traffic during computations involving codebooks. This
compute engine adopts the codebook-centric dataflow and fusion optimizations.
Additionally, we provide adaptive heuristics to tailor parameter selection in
our optimizations to diverse VQ configurations. Our optimizations achieve an
average latency reduction of 46.13% compared to unoptimized versions. Compared
to existing open-source implementations, our methods decrease latency by 64.36%
to 99.1%. A final comparison with state-of-the-art element-wise **quantization**
methods like AWQ and **KV**Quant shows that our VQ-LLM is practically viable,
achieving latencies close or even better latencies to those at equivalent
bit-widths, potentially offering greater accuracy.


## Empowering Sparse-Input Neural Radiance Fields with Dual-Level Semantic Guidance from Dense Novel Views

>Authors: Yingji Zhong, Kaichen Zhou, Zhihao Li, Lanqing Hong, Zhenguo Li, Dan Xu

>2025-03-04

> http://arxiv.org/abs/2503.02230v1

Neural Radiance Fields (NeRF) have shown remarkable capabilities for
photorealistic novel view synthesis. One major deficiency of NeRF is that dense
inputs are typically required, and the rendering quality will drop drastically
given **sparse** inputs. In this paper, we highlight the effectiveness of rendered
semantics from dense novel views, and show that rendered semantics can be
treated as a more robust form of augmented data than rendered RGB. Our method
enhances NeRF's performance by incorporating guidance derived from the rendered
semantics. The rendered semantic guidance encompasses two levels: the
supervision level and the feature level. The supervision-level guidance
incorporates a bi-directional verification module that decides the validity of
each rendered semantic label, while the feature-level guidance integrates a
learnable codebook that encodes semantic-aware information, which is queried by
each point via the attention mechanism to obtain semantic-relevant predictions.
The overall semantic guidance is embedded into a self-improved pipeline. We
also introduce a more challenging **sparse**-input indoor benchmark, where the
number of inputs is limited to as few as 6. Experiments demonstrate the
effectiveness of our method and it exhibits superior performance compared to
existing approaches.


## DivPrune Diversity-based Visual Token Pruning for Large Multimodal Models

>Authors: Saeed Ranjbar Alvar, Gursimran Singh, Mohammad Akbari, Yong Zhang

>2025-03-04

> http://arxiv.org/abs/2503.02175v1

Large Multimodal Models (LMMs) have emerged as powerful models capable of
understanding various data modalities, including text, images, and videos. LMMs
encode both text and visual data into tokens that are then combined and
processed by an integrated Large Language Model (LLM). Including visual tokens
substantially increases the total token count, often by thousands. The
increased input length for LLM significantly raises the complexity of
inference, resulting in high latency in LMMs. To address this issue, token
**pruning** methods, which remove part of the visual tokens, are proposed. The
existing token **pruning** methods either require extensive calibration and
fine-tuning or rely on suboptimal importance metrics which results in increased
redundancy among the retained tokens. In this paper, we first formulate token
**pruning** as Max-Min Diversity Problem (MMDP) where the goal is to select a
subset such that the diversity among the selected {tokens} is maximized. Then,
we solve the MMDP to obtain the selected subset and prune the rest. The
proposed method, DivPrune, reduces redundancy and achieves the highest
diversity of the selected tokens. By ensuring high diversity, the selected
tokens better represent the original tokens, enabling effective performance
even at high **pruning** ratios without requiring fine-tuning. Extensive
experiments with various LMMs show that DivPrune achieves state-of-the-art
accuracy over 16 image- and video-language datasets. Additionally, DivPrune
reduces both the end-to-end latency and GPU memory usage for the tested models.
The code is available $\href{https://github.com/vbdi/divprune}{\text{here}}$.


## Leveraging Large Language Models for Enhanced Digital Twin Modeling Trends, Methods, and Challenges

>Authors: Linyao Yang, Shi Luo, Xi Cheng, Lei Yu

>2025-03-04

> http://arxiv.org/abs/2503.02167v1

Digital twin technology is a transformative innovation driving the digital
transformation and intelligent optimization of manufacturing systems. By
integrating real-time data with computational models, digital twins enable
continuous monitoring, simulation, prediction, and optimization, effectively
bridging the gap between the physical and digital worlds. Recent advancements
in communication, computing, and control technologies have accelerated the
development and adoption of digital twins across various industries. However,
significant challenges remain, including limited data for accurate system
modeling, inefficiencies in system analysis, and a lack of explainability in
the interactions between physical and digital systems. The rise of large
language models (LLMs) offers new avenues to address these challenges. LLMs
have shown exceptional capabilities across diverse domains, exhibiting strong
generalization and emergent abilities that hold great potential for enhancing
digital twins. This paper provides a comprehensive review of recent
developments in LLMs and their applications to digital twin modeling. We
propose a unified description-prediction-prescription framework to integrate
digital twin modeling technologies and introduce a structured taxonomy to
categorize LLM functionalities in these contexts. For each stage of
application, we summarize the methodologies, identify key challenges, and
explore potential future directions. To demonstrate the effectiveness of
LLM-enhanced digital twins, we present an LLM-enhanced enterprise digital twin
system, which enables automatic modeling and optimization of an enterprise.
Finally, we discuss future opportunities and challenges in advancing
LLM-enhanced digital twins, offering valuable insights for researchers and
practitioners in related fields.


## A broadband solid impedance transformer for acoustic transmission between water and air

>Authors: Hesam Bakhtiary Yekta, Andrew N. Norris

>2025-03-03

> http://arxiv.org/abs/2503.02095v1

Total acoustic transmission between air and water was shown in our recent
paper to be attainable with a solid interface comprising two parallel thin
elastic plates connected by rigid ribs, although the transmissivity is a
narrow-band effect. We demonstrate here that broadband transmission can be
obtained by introducing a third, central plate. A theoretical analysis combined
with numerical optimization shows that the optimal 3-plate impedance
transformer has a central plate far thicker than the others. This implies a
simpler interpretation of the optimal 3-plate impedance transformer as two
elastic plates separated by a mass-like impedance. The characteristics of the
broadband transformer may then be understood using results for the previously
studied 2-plate system and asymptotic approximations using the small
air-to-water impedance ratio. Optimal systems with water and air-side plates of
similar material have relative thicknesses of approximately three to one,
respectively, with the central mass having areal density approximately 17 times
the water side plate. Further identities relate the frequency of total
transmission to the plate thicknesses and to the rib separation length. The
impedance transformer is compared to an ideal two layer quarter wavelength
model, allowing us to identify a minimal attainable Q-factor of about 5.5,
which is achieved in examples presented. The formulas for approximately
optimized parameters also serve as the initial population for numerical
optimization, greatly accelerating the process. Together, the theoretical and
numerical results point to a remarkably simple class of purely solid impedance
transformers, with system parameters well defined by the asymptotically small
parameter: the ratio of air-to-water acoustic impedances.


## Analyzing the Safety of Japanese Large Language Models in Stereotype-Triggering Prompts

>Authors: Akito Nakanishi, Yukie Sano, Geng Liu, Francesco Pierri

>2025-03-03

> http://arxiv.org/abs/2503.01947v2

In recent years, Large Language Models have attracted growing interest for
their significant potential, though concerns have rapidly emerged regarding
unsafe behaviors stemming from inherent stereotypes and biases. Most research
on stereotypes in LLMs has primarily relied on indirect evaluation setups, in
which models are prompted to select between pairs of sentences associated with
particular social groups. Recently, direct evaluation methods have emerged,
examining open-ended model responses to overcome limitations of previous
approaches, such as annotator biases. Most existing studies have focused on
English-centric LLMs, whereas research on non-English models, particularly
Japanese, remains **sparse**, despite the growing development and adoption of these
models. This study examines the safety of Japanese LLMs when responding to
stereotype-triggering prompts in direct setups. We constructed 3,612 prompts by
combining 301 social group terms, categorized by age, gender, and other
attributes, with 12 stereotype-inducing templates in Japanese. Responses were
analyzed from three foundational models trained respectively on Japanese,
English, and Chinese language. Our findings reveal that LLM-jp, a Japanese
native model, exhibits the lowest refusal rate and is more likely to generate
toxic and negative responses compared to other models. Additionally, prompt
format significantly influence the output of all models, and the generated
responses include exaggerated reactions toward specific social groups, varying
across models. These findings underscore the insufficient ethical safety
mechanisms in Japanese LLMs and demonstrate that even high-accuracy models can
produce biased outputs when processing Japanese-language prompts. We advocate
for improving safety mechanisms and bias mitigation strategies in Japanese
LLMs, contributing to ongoing discussions on AI ethics beyond linguistic
boundaries.


## From superposition to sparse codes interpretable representations in neural networks

>Authors: David Klindt, Charles O'Neill, Patrik Reizinger, Harald Maurer, Nina Miolane

>2025-03-03

> http://arxiv.org/abs/2503.01824v1

Understanding how information is represented in neural networks is a
fundamental challenge in both neuroscience and artificial intelligence. Despite
their nonlinear architectures, recent evidence suggests that neural networks
encode features in superposition, meaning that input concepts are linearly
overlaid within the network's representations. We present a perspective that
explains this phenomenon and provides a foundation for extracting interpretable
representations from neural activations. Our theoretical framework consists of
three steps: (1) Identifiability theory shows that neural networks trained for
classification recover latent features up to a linear transformation. (2)
Sparse coding methods can extract disentangled features from these
representations by leveraging principles from compressed sensing. (3)
Quantitative interpretability metrics provide a means to assess the success of
these methods, ensuring that extracted features align with human-interpretable
concepts. By bridging insights from theoretical neuroscience, representation
learning, and interpretability research, we propose an emerging perspective on
understanding neural representations in both artificial and biological systems.
Our arguments have implications for neural coding theories, AI transparency,
and the broader goal of making deep learning models more interpretable.


## RSQ Learning from Important Tokens Leads to Better Quantized LLMs

>Authors: Yi-Lin Sung, Prateek Yadav, Jialu Li, Jaehong Yoon, Mohit Bansal

>2025-03-03

> http://arxiv.org/abs/2503.01820v1

Layer-wise **quantization** is a key technique for efficiently compressing large
models without expensive retraining. Previous methods typically **quantize** the
weights of each layer by "uniformly" optimizing the layer reconstruction loss
across all output tokens. However, in this paper, we demonstrate that
better-**quantize**d models can be obtained by prioritizing learning from important
tokens (e.g. which have large attention scores). Building on this finding, we
propose RSQ (Rotate, Scale, then Quantize), which (1) applies rotations
(orthogonal transformation) to the model to mitigate outliers (those with
exceptionally large magnitude), (2) scales the token feature based on its
importance, and (3) **quantize**s the model using the GPTQ framework with the
second-order statistics computed by scaled tokens. To compute token importance,
we explore both heuristic and dynamic strategies. Based on a thorough analysis
of all approaches, we adopt attention concentration, which uses attention
scores of each token as its importance, as the best approach. We demonstrate
that RSQ consistently outperforms baseline methods across multiple downstream
tasks and three model families: LLaMA3, Mistral, and Qwen2.5. Additionally,
models **quantize**d with RSQ achieve superior performance on long-context tasks,
further highlighting its effectiveness. Lastly, RSQ demonstrates
generalizability across various setups, including different model sizes,
calibration datasets, bit precisions, and **quantization** methods.


## LLMInit A Free Lunch from Large Language Models for Selective Initialization of Recommendation

>Authors: Weizhi Zhang, Liangwei Yang, Wooseong Yang, Henry Peng Zou, Yuqing Liu, Ke Xu, Sourav Medya, Philip S. Yu

>2025-03-03

> http://arxiv.org/abs/2503.01814v1

Collaborative filtering models, particularly graph-based approaches, have
demonstrated strong performance in capturing user-item interactions for
recommendation systems. However, they continue to struggle in cold-start and
data-**sparse** scenarios. The emergence of large language models (LLMs) like GPT
and LLaMA presents new possibilities for enhancing recommendation performance,
especially in cold-start settings. Despite their promise, LLMs pose challenges
related to scalability and efficiency due to their high computational demands
and limited ability to model complex user-item relationships effectively. In
this work, we introduce a novel perspective on leveraging LLMs for CF model
initialization. Through experiments, we uncover an embedding collapse issue
when scaling CF models to larger embedding dimensions. To effectively harness
large-scale LLM embeddings, we propose innovative selective initialization
strategies utilizing random, uniform, and variance-based index sampling. Our
comprehensive evaluation on multiple real-world datasets demonstrates
significant performance gains across various CF models while maintaining a
lower computational cost compared to existing LLM-based recommendation
approaches.


## DILEMMA Joint LLM Quantization and Distributed LLM Inference Over Edge Computing Systems

>Authors: Minoo Hosseinzadeh, Hana Khamfroush

>2025-03-03

> http://arxiv.org/abs/2503.01704v1

With a recent trend of using Large Language Models (LLMs) for different
applications within smart cities, there is a need for pushing these models
toward the edge of network while still preserving their performance. Edge
Computing (EC) as a physically closer computing resource to the end users can
help to reduce the communication delay for serving end users' tasks for
LLM-dependent services. However, EC servers have limited capacity in terms of
communication, computation, and storage capacity. This paper introduces
DILEMMA, a novel framework addressing the challenges of deploying LLMs in EC
systems by jointly optimizing layer placement and layer **quantization** in EC
systems. DILEMMA formulates an Integer Linear Programming problem to minimize
total inference delay while ensuring acceptable LLM performance levels,
leveraging layer-wise **quantization** and knowledge distillation for LLM
performance control. Experimental evaluations on OPT-350 model using the SQuAD
dataset demonstrate that DILEMMA achieves a **quantization** ratio of up to 12.75%
while preserving model loss, highlighting its effectiveness in
resource-constrained environments.


## Galaxy populations of ProtoClusters in cosmological hydrodynamical simulations

>Authors: Michela Esposito, Stefano Borgani, Veronica Strazzullo, Maurilio Pannella, Gian Luigi Granato, Cinthia Ragone-Figueroa, Alex Saro, Mario Nonino, Milena Valentini

>2025-03-03

> http://arxiv.org/abs/2503.01674v1

The study of protoclusters at cosmic noon is essential to understand the
impact on galaxies of the environment and of the transformational processes
occurring in this epoch. This work tests the predictions of the DIANOGA
cosmological hydrodynamical simulations of cluster progenitors at z=2.2,
comparing them with observations, and investigates the environmental effects on
galaxies by comparing protoclusters with an average volume of the Universe. We
analyze 14 protoclusters and a cosmological box of 49 cMpc/h per side. We
compare predictions and observations of the galaxy properties, including colors
of galaxies obtained with radiative transfer, to analyze UVJ diagrams. We
showed that the DIANOGA simulations produce a galaxy stellar mass function in
broad agreement with observations, with a higher fraction of high-mass galaxies
($M_{\ast}>10^{10} \ M_{\odot}$) in massive halos in protoclusters, compared to
the box. The same signal, with lower significance, is also observed in the
wide-field protocluster structures, indicating an accelerated evolution of
galaxies before their infall into massive halos. Our simulations underestimate
SFRs of galaxies both in protoclusters and in the box, compared to
observations, due to low gas reservoirs. We find a weak suppression of SFRs in
protocluster galaxies (~0.05 dex), compared to the box, increasing up to ~0.25
dex in massive halos. The quenched galaxy fraction varies significantly across
different protocluster halos, consistent with observations. The simulations
show a strong dependence of quenched fractions on halo mass and an excess of
quenched galaxies in the wide-field protocluster region, compared to the
cosmological box. UVJ diagram analysis shows qualitative agreement with
observed color distributions of star-forming and quenched galaxies, except for
few massive galaxies with steeper reddening vectors than typically assumed in
observations.


## CoPL Collaborative Preference Learning for Personalizing LLMs

>Authors: Youngbin Choi, Seunghyuk Cho, Minjong Lee, MoonJeong Park, Yesong Ko, Jungseul Ok, Dongwoo Kim

>2025-03-03

> http://arxiv.org/abs/2503.01658v1

Personalizing large language models (LLMs) is important for aligning outputs
with diverse user preferences, yet existing methods struggle with flexibility
and generalization. We propose CoPL (Collaborative Preference Learning), a
graph-based collaborative filtering framework that models user-response
relationships to enhance preference estimation, particularly in **sparse**
annotation settings. By integrating a mixture of LoRA experts, CoPL efficiently
fine-tunes LLMs while dynamically balancing shared and user-specific
preferences. Additionally, an optimization-free adaptation strategy enables
generalization to unseen users without fine-tuning. Experiments on
UltraFeedback-P demonstrate that CoPL outperforms existing personalized reward
models, effectively capturing both common and controversial preferences, making
it a scalable solution for personalized LLM alignment.


## An Efficient Approach to Detecting Lung Nodules Using Swin Transformer

>Authors: Saeed Shakuri, Alireza Rezvanian

>2025-03-03

> http://arxiv.org/abs/2503.01592v1

Lung cancer has the highest rate of cancer-caused deaths, and early-stage
diagnosis could increase the survival rate. Lung nodules are common indicators
of lung cancer, making their detection crucial. Various lung nodule detection
models exist, but many lack efficiency. Hence, we propose a more efficient
approach by leveraging 2D CT slices, reducing computational load and complexity
in training and inference. We employ the tiny version of Swin Transformer to
benefit from Vision Transformers (ViT) while maintaining low computational
cost. A Feature Pyramid Network is added to enhance detection, particularly for
small nodules. Additionally, Transfer Learning is used to accelerate training.
Our experimental results show that the proposed model outperforms
state-of-the-art methods, achieving higher mAP and mAR for small nodules by
1.3% and 1.6%, respectively. Overall, our model achieves the highest mAP of
94.7% and mAR of 94.9%.


## EliteKV Scalable KV Cache Compression via RoPE Frequency Selection and Joint Low-Rank Projection

>Authors: Yuhao Zhou, Sirui Song, Boyang Liu, Zhiheng Xi, Senjie Jin, Xiaoran Fan, Zhihao Zhang, Wei Li, Xuanjing Huang

>2025-03-03

> http://arxiv.org/abs/2503.01586v1

Rotary Position Embedding (RoPE) enables each attention head to capture
multi-frequency information along the sequence dimension and is widely applied
in foundation models. However, the nonlinearity introduced by RoPE complicates
optimization of the key state in the Key-Value (**KV**) cache for RoPE-based
attention. Existing **KV** cache compression methods typically store key state
before rotation and apply the transformation during decoding, introducing
additional computational overhead. This paper introduces Elite**KV**, a flexible
modification framework for RoPE-based models supporting variable **KV** cache
compression ratios. Elite**KV** first identifies the intrinsic frequency preference
of each head using RoPElite, selectively restoring linearity to certain
dimensions of key within attention computation. Building on this, joint
low-rank compression of key and value enables partial cache sharing.
Experimental results show that with minimal uptraining on only $0.6\%$ of the
original training data, RoPE-based models achieve a $75\%$ reduction in **KV**
cache size while preserving performance within a negligible margin.
Furthermore, Elite**KV** consistently performs well across models of different
scales within the same family.


## MRI super-resolution reconstruction using efficient diffusion probabilistic model with residual shifting

>Authors: Mojtaba Safari, Shansong Wang, Zach Eidex, Qiang Li, Erik H. Middlebrooks, David S. Yu, Xiaofeng Yang

>2025-03-03

> http://arxiv.org/abs/2503.01576v1

Objective:This study introduces a residual error-shifting mechanism that
drastically reduces sampling steps while preserving critical anatomical
details, thus accelerating MRI reconstruction. Approach:We propose a novel
diffusion-based SR framework called Res-SRDiff, which integrates residual error
shifting into the forward diffusion process. This enables efficient HR image
reconstruction by aligning the degraded HR and LR distributions.We evaluated
Res-SRDiff on ultra-high-field brain T1 MP2RAGE maps and T2-weighted prostate
images, comparing it with Bicubic, Pix2pix, CycleGAN, and a conventional
denoising diffusion probabilistic model with vision transformer backbone
(TM-DDPM), using quantitative metrics such as peak signal-to-noise ratio
(PSNR), structural similarity index (SSIM), gradient magnitude similarity
deviation (GMSD), and learned perceptual image patch similarity (LPIPS). Main
results: Res-SRDiff significantly outperformed all comparative methods in terms
of PSNR, SSIM, and GMSD across both datasets, with statistically significant
improvements (p-values<<0.05). The model achieved high-fidelity image
restoration with only four sampling steps, drastically reducing computational
time to under one second per slice, which is substantially faster than
conventional TM-DDPM with around 20 seconds per slice. Qualitative analyses
further demonstrated that Res-SRDiff effectively preserved fine anatomical
details and lesion morphology in both brain and pelvic MRI images.
Significance: Our findings show that Res-SRDiff is an efficient and accurate
MRI SR method, markedly improving computational efficiency and image quality.
Integrating residual error shifting into the diffusion process allows for rapid
and robust HR image reconstruction, enhancing clinical MRI workflows and
advancing medical imaging research. The source
at:https://github.com/mosaf/Res-SRDiff


## Attention Condensation via Sparsity Induced Regularized Training

>Authors: Eli Sason, Darya Frolova, Boris Nazarov, Felix Goldberd

>2025-03-03

> http://arxiv.org/abs/2503.01564v1

As the context window expands, self-attention increasingly dominates the
transformer's inference time. Therefore, accelerating attention computation
while minimizing performance degradation is essential for the efficient
deployment of Large Language Models (LLMs). In this study we extend a
theoretical framework of attention **sparsity** in LLMs. A customized loss function
is designed to enforce the **sparsity** by restricting the number of top elements
in the attention matrix. We perform an initial set of evaluations with GPT-2 to
show the effectiveness of our sparsification approach. The attention matrices
of the models trained with the proposed loss are both **sparse** and effective in
capturing relevant input dependencies. We now continue working to demonstrate
the value of our approach on larger models and different architectures.


## Revisiting Large Language Model Pruning using Neuron Semantic Attribution

>Authors: Yizhuo Ding, Xinwei Sun, Yanwei Fu, Guosheng Hu

>2025-03-03

> http://arxiv.org/abs/2503.01542v1

Model **pruning** technique is vital for accelerating large language models by
reducing their size and computational requirements. However, the
generalizability of existing **pruning** methods across diverse datasets and tasks
remains unclear. Thus, we conduct extensive evaluations on 24 datasets and 4
tasks using popular **pruning** methods. Based on these evaluations, we find and
then investigate that calibration set greatly affect the performance of **pruning**
methods. In addition, we surprisingly find a significant performance drop of
existing **pruning** methods in sentiment classification tasks. To understand the
link between performance drop and pruned neurons, we propose Neuron Semantic
Attribution, which learns to associate each neuron with specific semantics.
This method first makes the unpruned neurons of LLMs explainable.


## KurTail  Kurtosis-based LLM Quantization

>Authors: Mohammad Sadegh Akhondzadeh, Aleksandar Bojchevski, Evangelos Eleftheriou, Martino Dazzi

>2025-03-03

> http://arxiv.org/abs/2503.01483v1

One of the challenges of quantizing a large language model (LLM) is the
presence of outliers. Outliers often make uniform **quantization** schemes less
effective, particularly in extreme cases such as 4-bit **quantization**. We
introduce KurTail, a new post-training **quantization** (PTQ) scheme that leverages
Kurtosis-based rotation to mitigate outliers in the activations of LLMs. Our
method optimizes Kurtosis as a measure of tailedness. This approach enables the
**quantization** of weights, activations, and the **KV** cache in 4 bits. We utilize
layer-wise optimization, ensuring memory efficiency. KurTail outperforms
existing **quantization** methods, offering a 13.3\% boost in MMLU accuracy and a
15.5\% drop in Wiki perplexity compared to QuaRot. It also outperforms
SpinQuant with a 2.6\% MMLU gain and reduces perplexity by 2.9\%, all while
reducing the training cost. For comparison, learning the rotation using
SpinQuant for Llama3-70B requires at least four NVIDIA H100 80GB GPUs, whereas
our method requires only a single GPU, making it a more accessible solution for
consumer GPU.


## Transferring between sparse and dense matching via probabilistic reweighting

>Authors: Ya Fan, Rongling Lang

>2025-03-03

> http://arxiv.org/abs/2503.01472v1

Detector-based and detector-free matchers are only applicable within their
respective **sparsity** ranges. To improve adaptability of existing matchers, this
paper introduces a novel probabilistic reweighting method. Our method is
applicable to Transformer-based matching networks and adapts them to different
**sparsity** levels without altering network parameters. The reweighting approach
adjusts attention weights and matching scores using detection probabilities of
features. And we prove that the reweighted matching network is the asymptotic
limit of detector-based matching network. Furthermore, we propose a **sparse**
training and **pruning** pipeline for detector-free networks based on reweighting.
Reweighted versions of SuperGlue, LightGlue, and LoFTR are implemented and
evaluated across different levels of **sparsity**. Experiments show that the
reweighting method improves pose accuracy of detector-based matchers on dense
features. And the performance of reweighted **sparse** LoFTR is comparable to
detector-based matchers, demonstrating good flexibility in balancing accuracy
and computational complexity.


## Structural Deep Encoding for Table Question Answering

>Authors: RaphaÃ«l Mouravieff, Benjamin Piwowarski, Sylvain Lamprier

>2025-03-03

> http://arxiv.org/abs/2503.01457v1

Although Transformers-based architectures excel at processing textual
information, their naive adaptation for tabular data often involves flattening
the table structure. This simplification can lead to the loss of essential
inter-dependencies between rows, columns, and cells, while also posing
scalability challenges for large tables. To address these issues, prior works
have explored special tokens, structured embeddings, and **sparse** attention
patterns. In this paper, we conduct a comprehensive analysis of tabular
encoding techniques, which highlights the crucial role of attention **sparsity** in
preserving structural information of tables. We also introduce a set of novel
**sparse** attention mask designs for tabular data, that not only enhance
computational efficiency but also preserve structural integrity, leading to
better overall performance.


## MeshPad Interactive Sketch Conditioned Artistic-designed Mesh Generation and Editing

>Authors: Haoxuan Li, Ziya Erkoc, Lei Li, Daniele Sirigatti, Vladyslav Rozov, Angela Dai, Matthias NieÃner

>2025-03-03

> http://arxiv.org/abs/2503.01425v1

We introduce MeshPad, a generative approach that creates 3D meshes from
sketch inputs. Building on recent advances in artistic-designed triangle mesh
generation, our approach addresses the need for interactive artistic mesh
creation. To this end, we focus on enabling consistent edits by decomposing
editing into 'deletion' of regions of a mesh, followed by 'addition' of new
mesh geometry. Both operations are invoked by simple user edits of a sketch
image, facilitating an iterative content creation process and enabling the
construction of complex 3D meshes. Our approach is based on a triangle
sequence-based mesh representation, exploiting a large Transformer model for
mesh triangle addition and deletion. In order to perform edits interactively,
we introduce a vertex-aligned speculative prediction strategy on top of our
additive mesh generator. This speculator predicts multiple output tokens
corresponding to a vertex, thus significantly reducing the computational cost
of inference and accelerating the editing process, making it possible to
execute each editing step in only a few seconds. Comprehensive experiments
demonstrate that MeshPad outperforms state-of-the-art sketch-conditioned mesh
generation methods, achieving more than 22% mesh quality improvement in Chamfer
distance, and being preferred by 90% of participants in perceptual evaluations.


## Enhancing Social Media Rumor Detection A Semantic and Graph Neural Network Approach for the 2024 Global Election

>Authors: Liu Yan, Liu Yunpeng, Zhao Liang

>2025-03-03

> http://arxiv.org/abs/2503.01394v1

The development of social media platforms has revolutionized the speed and
manner in which information is disseminated, leading to both beneficial and
detrimental effects on society. While these platforms facilitate rapid
communication, they also accelerate the spread of rumors and extremist speech,
impacting public perception and behavior significantly. This issue is
particularly pronounced during election periods, where the influence of social
media on election outcomes has become a matter of global concern. With the
unprecedented number of elections in 2024, against this backdrop, the election
ecosystem has encountered unprecedented challenges. This study addresses the
urgent need for effective rumor detection on social media by proposing a novel
method that combines semantic analysis with graph neural networks. We have
meticulously collected a dataset from PolitiFact and Twitter, focusing on
politically relevant rumors. Our approach involves semantic analysis using a
fine-tuned BERT model to vectorize text content and construct a directed graph
where tweets and comments are nodes, and interactions are edges. The core of
our method is a graph neural network, SAGEWithEdgeAttention, which extends the
GraphSAGE model by incorporating first-order differences as edge attributes and
applying an attention mechanism to enhance feature aggregation. This innovative
approach allows for the fine-grained analysis of the complex social network
structure, improving rumor detection accuracy. The study concludes that our
method significantly outperforms traditional content analysis and time-based
models, offering a theoretically sound and practically efficient solution.


## Wavelet-Enhanced Desnowing A Novel Single Image Restoration Approach for Traffic Surveillance under Adverse Weather Conditions

>Authors: Zihan Shen, Yu Xuan, Qingyu Yang

>2025-03-03

> http://arxiv.org/abs/2503.01339v1

Image restoration under adverse weather conditions refers to the process of
removing degradation caused by weather particles while improving visual
quality. Most existing deweathering methods rely on increasing the network
scale and data volume to achieve better performance which requires more
expensive computing power. Also, many methods lack generalization for specific
applications. In the traffic surveillance screener, the main challenges are
snow removal and veil effect elimination. In this paper, we propose a
wavelet-enhanced snow removal method that use a Dual-Tree Complex Wavelet
Transform feature enhancement module and a dynamic convolution **acceleration**
module to address snow degradation in surveillance images. We also use a
residual learning restoration module to remove veil effects caused by rain,
snow, and fog. The proposed architecture extracts and analyzes information from
snow-covered regions, significantly improving snow removal performance. And the
residual learning restoration module removes veiling effects in images,
enhancing clarity and detail. Experiments show that it performs better than
some popular desnowing methods. Our approach also demonstrates effectiveness
and accuracy when applied to real traffic surveillance images.


## WeightedKV Attention Scores Weighted Key-Value Cache Merging for Large Language Models

>Authors: Jian Yuan, Ziwei He, Haoli Bai, Jingwen Leng, Bo Jiang

>2025-03-03

> http://arxiv.org/abs/2503.01330v1

Large Language Models (LLMs) use key-value (**KV**) cache to reduce redundant
computation in autoregressive generation. However, the **KV** cache size increases
linearly during generation, leading to excessive memory usage, especially for
long texts. Most **KV** cache compression methods evict the unimportant **KV** pairs to
maintain a fixed cache size, which leads to the permanent loss of tokens during
generation. However, singular value decomposition shows that \textit{values} do
not exhibit a strong low-rank property as \textit{keys} do, suggesting that
information is distributed more evenly across \textit{values}, in contrast to
its more redundant distribution within \textit{keys}. Therefore, methods that
evict both \textit{keys} and \textit{values} risk losing crucial information
and compromise context integrity, ultimately degrading the output quality. To
address this problem, we propose Weighted**KV**, a novel, training-free approach
that discards the \textit{keys} of less important tokens, while merging their
\textit{values} into neighboring tokens via a convex combination weighted by
their average attention scores. In this way, the retained \textit{keys} serve
as anchors that guide the generation process, while the merged \textit{values}
provide a rich contextual backdrop. We assess our method on four widely used
language modeling datasets, demonstrating superior performance compared to all
baseline methods, particularly with a lower budget ratio.


## PipeOffload Improving Scalability of Pipeline Parallelism with Memory Optimization

>Authors: Xinyi Wan, Penghui Qi, Guangxing Huang, Jialin Li, Min Lin

>2025-03-03

> http://arxiv.org/abs/2503.01328v1

Pipeline parallelism (PP) is widely used for training large language models
(LLMs), yet its scalability is often constrained by high activation memory
consumption as the number of in-flight microbatches grows with the degree of
PP. In this paper, we focus on addressing this challenge by leveraging the
under-explored memory offload strategy in PP. With empirical study, we discover
that in the majority of standard configurations, at least half, and potentially
all, of the activations can be offloaded with negligible overhead. In the cases
where full overload is not possible, we introduce a novel selective offload
strategy that decreases peak activation memory in a better-than-linear manner.
Furthermore, we integrate memory offload with other techniques to jointly
consider overall throughput and memory limitation. Our experiments proves that
the per-device activation memory effectively reduces with the total number of
stages, making PP a stronger alternative than TP, offering up to a 19\%
**acceleration** with even lower memory consumption. The implementation is
open-sourced at
\href{https://github.com/sail-sg/zero-bubble-pipeline-parallelism}{this url}.


## PROPER A Progressive Learning Framework for Personalized Large Language Models with Group-Level Adaptation

>Authors: Linhai Zhang, Jialong Wu, Deyu Zhou, Yulan He

>2025-03-03

> http://arxiv.org/abs/2503.01303v1

Personalized large language models (LLMs) aim to tailor their outputs to user
preferences. Recent advances in parameter-efficient fine-tuning (PEFT) methods
have highlighted the effectiveness of adapting population-level LLMs to
personalized LLMs by fine-tuning user-specific parameters with user history.
However, user data is typically **sparse**, making it challenging to adapt LLMs to
specific user patterns. To address this challenge, we propose PROgressive
PERsonalization (PROPER), a novel progressive learning framework inspired by
meso-level theory in social science. PROPER bridges population-level and
user-level models by grouping users based on preferences and adapting LLMs in
stages. It combines a Mixture-of-Experts (MoE) structure with Low Ranked
Adaptation (LoRA), using a user-aware router to assign users to appropriate
groups automatically. Additionally, a LoRA-aware router is proposed to
facilitate the integration of individual user LoRAs with group-level LoRAs.
Experimental results show that PROPER significantly outperforms SOTA models
across multiple tasks, demonstrating the effectiveness of our approach.


## SVDC Consistent Direct Time-of-Flight Video Depth Completion with Frequency Selective Fusion

>Authors: Xuan Zhu, Jijun Xiang, Xianqi Wang, Longliang Liu, Yu Wang, Hong Zhang, Fei Guo, Xin Yang

>2025-03-03

> http://arxiv.org/abs/2503.01257v1

Lightweight direct Time-of-Flight (dToF) sensors are ideal for 3D sensing on
mobile devices. However, due to the manufacturing constraints of compact
devices and the inherent physical principles of imaging, dToF depth maps are
**sparse** and noisy. In this paper, we propose a novel video depth completion
method, called SVDC, by fusing the **sparse** dToF data with the corresponding RGB
guidance. Our method employs a multi-frame fusion scheme to mitigate the
spatial ambiguity resulting from the **sparse** dToF imaging. Misalignment between
consecutive frames during multi-frame fusion could cause blending between
object edges and the background, which results in a loss of detail. To address
this, we introduce an adaptive frequency selective fusion (AFSF) module, which
automatically selects convolution kernel sizes to fuse multi-frame features.
Our AFSF utilizes a channel-spatial enhancement attention (CSEA) module to
enhance features and generates an attention map as fusion weights. The AFSF
ensures edge detail recovery while suppressing high-frequency noise in smooth
regions. To further enhance temporal consistency, We propose a cross-window
consistency loss to ensure consistent predictions across different windows,
effectively reducing flickering. Our proposed SVDC achieves optimal accuracy
and consistency on the TartanAir and Dynamic Replica datasets. Code is
available at https://github.com/Lan1eve/SVDC.


## NM-SpMM Accelerating Matrix Multiplication Using NM Sparsity with GPGPU

>Authors: Cong Ma, Du Wu, Zhelang Deng, Jiang Chen, Xiaowen Huang, Jintao Meng, Wenxi Zhu, Bingqiang Wang, Amelie Chi Zhou, Peng Chen, Minwen Deng, Yanjie Wei, Shengzhong Feng, Yi Pan

>2025-03-03

> http://arxiv.org/abs/2503.01253v2

Deep learning demonstrates effectiveness across a wide range of tasks.
However, the dense and over-parameterized nature of these models results in
significant resource consumption during deployment. In response to this issue,
weight **pruning**, particularly through N:M **sparsity** matrix multiplication, offers
an efficient solution by transforming dense operations into semi-**sparse** ones.
N:M **sparsity** provides an option for balancing performance and model accuracy,
but introduces more complex programming and optimization challenges. To address
these issues, we design a systematic top-down performance analysis model for
N:M **sparsity**. Meanwhile, NM-SpMM is proposed as an efficient general N:M
**sparsity** implementation. Based on our performance analysis, NM-SpMM employs a
hierarchical blocking mechanism as a general optimization to enhance data
locality, while memory access optimization and pipeline design are introduced
as **sparsity**-aware optimization, allowing it to achieve close-to-theoretical
peak performance across different **sparsity** levels. Experimental results show
that NM-SpMM is 2.1x faster than nmSPARSE (the state-of-the-art for general N:M
**sparsity**) and 1.4x to 6.3x faster than cuBLAS's dense GEMM operations, closely
approaching the theoretical maximum speedup resulting from the reduction in
computation due to **sparsity**. NM-SpMM is open source and publicly available at
https://github.com/M-H482/NM-SpMM.


## STGAN Spatial-temporal Graph Autoregression Network for Pavement Distress Deterioration Prediction

>Authors: Shilin Tong, Difei Wu, Xiaona Liu, Le Zheng, Yuchuan Du, Difan Zou

>2025-03-03

> http://arxiv.org/abs/2503.01152v1

Pavement distress significantly compromises road integrity and poses risks to
drivers. Accurate prediction of pavement distress deterioration is essential
for effective road management, cost reduction in maintenance, and improvement
of traffic safety. However, real-world data on pavement distress is usually
collected irregularly, resulting in uneven, asynchronous, and **sparse**
spatial-temporal datasets. This hinders the application of existing
spatial-temporal models, such as DCRNN, since they are only applicable to
regularly and synchronously collected data. To overcome these challenges, we
propose the Spatial-Temporal Graph Autoregression Network (STGAN), a novel
graph neural network model designed for accurately predicting irregular
pavement distress deterioration using complex spatial-temporal data.
Specifically, STGAN integrates the temporal domain into the spatial domain,
creating a larger graph where nodes are represented by spatial-temporal tuples
and edges are formed based on a similarity-based connection mechanism.
Furthermore, based on the constructed spatiotemporal graph, we formulate
pavement distress deterioration prediction as a graph autoregression task,
i.e., the graph size increases incrementally and the prediction is performed
sequentially. This is accomplished by a novel spatial-temporal attention
mechanism deployed by STGAN. Utilizing the ConTrack dataset, which contains
pavement distress records collected from different locations in Shanghai, we
demonstrate the superior performance of STGAN in capturing spatial-temporal
correlations and addressing the aforementioned challenges. Experimental results
further show that STGAN outperforms baseline models, and ablation studies
confirm the effectiveness of its novel modules. Our findings contribute to
promoting proactive road maintenance decision-making and ultimately enhancing
road safety and resilience.


## Large AI Model for Delay-Doppler Domain Channel Prediction in 6G OTFS-Based Vehicular Networks

>Authors: Jianzhe Xue, Dongcheng Yuan, Zhanxi Ma, Tiankai Jiang, Yu Sun, Haibo Zhou, Xuemin Shen

>2025-03-03

> http://arxiv.org/abs/2503.01116v1

Channel prediction is crucial for high-mobility vehicular networks, as it
enables the anticipation of future channel conditions and the proactive
adjustment of communication strategies. However, achieving accurate vehicular
channel prediction is challenging due to significant Doppler effects and rapid
channel variations resulting from high-speed vehicle movement and complex
propagation environments. In this paper, we propose a novel delay-Doppler (DD)
domain channel prediction framework tailored for high-mobility vehicular
networks. By transforming the channel representation into the DD domain, we
obtain an intuitive, **sparse**, and stable depiction that closely aligns with the
underlying physical propagation processes, effectively reducing the complex
vehicular channel to a set of time-series parameters with enhanced
predictability. Furthermore, we leverage the large artificial intelligence (AI)
model to predict these DD-domain time-series parameters, capitalizing on their
advanced ability to model temporal correlations. The zero-shot capability of
the pre-trained large AI model facilitates accurate channel predictions without
requiring task-specific training, while subsequent fine-tuning on specific
vehicular channel data further improves prediction accuracy. Extensive
simulation results demonstrate the effectiveness of our DD-domain channel
prediction framework and the superior accuracy of the large AI model in
predicting time-series channel parameters, thereby highlighting the potential
of our approach for robust vehicular communication systems.


## MAPS Multi-Fidelity AI-Augmented Photonic Simulation and Inverse Design Infrastructure

>Authors: Pingchuan Ma, Zhengqi Gao, Meng Zhang, Haoyu Yang, Mark Ren, Rena Huang, Duane S. Boning, Jiaqi Gu

>2025-03-02

> http://arxiv.org/abs/2503.01046v1

Inverse design has emerged as a transformative approach for photonic device
optimization, enabling the exploration of high-dimensional, non-intuitive
design spaces to create ultra-compact devices and advance photonic integrated
circuits (PICs) in computing and interconnects. However, practical challenges,
such as suboptimal device performance, limited manufacturability, high
sensitivity to variations, computational inefficiency, and lack of
interpretability, have hindered its adoption in commercial hardware. Recent
advancements in AI-assisted photonic simulation and design offer transformative
potential, accelerating simulations and design generation by orders of
magnitude over traditional numerical methods. Despite these breakthroughs, the
lack of an open-source, standardized infrastructure and evaluation benchmark
limits accessibility and cross-disciplinary collaboration. To address this, we
introduce MAPS, a multi-fidelity AI-augmented photonic simulation and inverse
design infrastructure designed to bridge this gap. MAPS features three
synergistic components: (1) MAPS-Data: A dataset acquisition framework for
generating multi-fidelity, richly labeled devices, providing high-quality data
for AI-for-optics research. (2) MAPS-Train: A flexible AI-for-photonics
training framework offering a hierarchical data loading pipeline, customizable
model construction, support for data- and physics-driven losses, and
comprehensive evaluations. (3) MAPS-InvDes: An advanced adjoint inverse design
toolkit that abstracts complex physics but exposes flexible optimization steps,
integrates pre-trained AI models, and incorporates fabrication variation
models. This infrastructure MAPS provides a unified, open-source platform for
developing, benchmarking, and advancing AI-assisted photonic design workflows,
accelerating innovation in photonic hardware optimization and scientific
machine learning.


## MedUnifier Unifying Vision-and-Language Pre-training on Medical Data with Vision Generation Task using Discrete Visual Representations

>Authors: Ziyang Zhang, Yang Yu, Yucheng Chen, Xulei Yang, Si Yong Yeo

>2025-03-02

> http://arxiv.org/abs/2503.01019v2

Despite significant progress in Vision-Language Pre-training (VLP), current
approaches predominantly emphasize feature extraction and cross-modal
comprehension, with limited attention to generating or transforming visual
content. This gap hinders the model's ability to synthesize coherent and novel
visual representations from textual prompts, thereby reducing the effectiveness
of multi-modal learning. In this work, we propose MedUnifier, a unified VLP
framework tailored for medical data. MedUnifier seamlessly integrates
text-grounded image generation capabilities with multi-modal learning
strategies, including image-text contrastive alignment, image-text matching and
image-grounded text generation. Unlike traditional methods that reply on
continuous visual representations, our approach employs visual vector
**quantization**, which not only facilitates a more cohesive learning strategy for
cross-modal understanding but also enhances multi-modal generation quality by
effectively leveraging discrete representations. Our framework's effectiveness
is evidenced by the experiments on established benchmarks, including uni-modal
tasks (supervised fine-tuning), cross-modal tasks (image-text retrieval and
zero-shot image classification), and multi-modal tasks (medical report
generation, image synthesis), where it achieves state-of-the-art performance
across various tasks. MedUnifier also offers a highly adaptable tool for a wide
range of language and vision tasks in healthcare, marking advancement toward
the development of a generalizable AI model for medical applications.


## Dialogue Without Limits Constant-Sized KV Caches for Extended Responses in LLMs

>Authors: Ravi Ghadia, Avinash Kumar, Gaurav Jain, Prashant Nair, Poulami Das

>2025-03-02

> http://arxiv.org/abs/2503.00979v1

Autoregressive Transformers rely on Key-Value (**KV**) caching to accelerate
inference. However, the linear growth of the **KV** cache with context length leads
to excessive memory consumption and bandwidth constraints. This bottleneck is
particularly problematic in real-time applications -- such as chatbots and
interactive assistants -- where low latency and high memory efficiency are
critical. Existing methods drop distant tokens or compress states in a lossy
manner, sacrificing accuracy by discarding vital context or introducing bias.
  We propose Morph**KV**, an inference-time technique that maintains a
constant-sized **KV** cache while preserving accuracy. Morph**KV** balances long-range
dependencies and local coherence during text generation. It eliminates
early-token bias while retaining high-fidelity context by adaptively ranking
tokens through correlation-aware selection. Unlike heuristic retention or lossy
compression, Morph**KV** iteratively refines the **KV** cache via lightweight updates
guided by attention patterns of recent tokens. This approach captures
inter-token correlation with greater accuracy, crucial for tasks like content
creation and code generation. Our studies on long-response tasks show 52.9$\%$
memory savings and 18.2$\%$ higher accuracy on average compared to
state-of-the-art prior works, enabling efficient real-world deployment.


## Training-Free Dataset Pruning for Instance Segmentation

>Authors: Yalun Dai, Lingao Xiao, Ivor W. Tsang, Yang He

>2025-03-02

> http://arxiv.org/abs/2503.00828v1

Existing dataset **pruning** techniques primarily focus on classification tasks,
limiting their applicability to more complex and practical tasks like instance
segmentation. Instance segmentation presents three key challenges: pixel-level
annotations, instance area variations, and class imbalances, which
significantly complicate dataset **pruning** efforts. Directly adapting existing
classification-based **pruning** methods proves ineffective due to their reliance
on time-consuming model training process. To address this, we propose a novel
Training-Free Dataset Pruning (TFDP) method for instance segmentation.
Specifically, we leverage shape and class information from image annotations to
design a Shape Complexity Score (SCS), refining it into a Scale-Invariant
(SI-SCS) and Class-Balanced (CB-SCS) versions to address instance area
variations and class imbalances, all without requiring model training. We
achieve state-of-the-art results on VOC 2012, Cityscapes, and COCO datasets,
generalizing well across CNN and Transformer architectures. Remarkably, our
approach accelerates the **pruning** process by an average of 1349$\times$ on COCO
compared to the adapted baselines. Source code is available at:
https://github.com/he-y/dataset-**pruning**-for-instance-segmentation


## STAR-Edge Structure-aware Local Spherical Curve Representation for Thin-walled Edge Extraction from Unstructured Point Clouds

>Authors: Zikuan Li, Honghua Chen, Yuecheng Wang, Sibo Wu, Mingqiang Wei, Jun Wang

>2025-03-02

> http://arxiv.org/abs/2503.00801v1

Extracting geometric edges from unstructured point clouds remains a
significant challenge, particularly in thin-walled structures that are commonly
found in everyday objects. Traditional geometric methods and recent
learning-based approaches frequently struggle with these structures, as both
rely heavily on sufficient contextual information from local point
neighborhoods. However, 3D measurement data of thin-walled structures often
lack the accurate, dense, and regular neighborhood sampling required for
reliable edge extraction, resulting in degraded performance.
  In this work, we introduce STAR-Edge, a novel approach designed for detecting
and refining edge points in thin-walled structures. Our method leverages a
unique representation-the local spherical curve-to create structure-aware
neighborhoods that emphasize co-planar points while reducing interference from
close-by, non-co-planar surfaces. This representation is transformed into a
rotation-invariant descriptor, which, combined with a lightweight multi-layer
perceptron, enables robust edge point classification even in the presence of
noise and **sparse** or irregular sampling. Besides, we also use the local
spherical curve representation to estimate more precise normals and introduce
an optimization function to project initially identified edge points exactly on
the true edges. Experiments conducted on the ABC dataset and thin-walled
structure-specific datasets demonstrate that STAR-Edge outperforms existing
edge detection methods, showcasing better robustness under various challenging
conditions.


## Pruning Deep Neural Networks via a Combination of the Marchenko-Pastur Distribution and Regularization

>Authors: Leonid Berlyand, Theo Bourdais, Houman Owhadi, Yitzchak Shmalo

>2025-03-02

> http://arxiv.org/abs/2503.01922v2

Deep neural networks (DNNs) have brought significant advancements in various
applications in recent years, such as image recognition, speech recognition,
and natural language processing. In particular, Vision Transformers (ViTs) have
emerged as a powerful class of models in the field of deep learning for image
classification. In this work, we propose a novel Random Matrix Theory
(RMT)-based method for **pruning** pre-trained DNNs, based on the sparsification of
weights and singular vectors, and apply it to ViTs. RMT provides a robust
framework to analyze the statistical properties of large matrices, which has
been shown to be crucial for understanding and optimizing the performance of
DNNs. We demonstrate that our RMT-based **pruning** can be used to reduce the
number of parameters of ViT models (trained on ImageNet) by 30-50\% with less
than 1\% loss in accuracy. To our knowledge, this represents the
state-of-the-art in **pruning** for these ViT models. Furthermore, we provide a
rigorous mathematical underpinning of the above numerical studies, namely we
proved a theorem for fully connected DNNs, and other more general DNN
structures, describing how the randomness in the weight matrices of a DNN
decreases as the weights approach a local or global minimum (during training).
We verify this theorem through numerical experiments on fully connected DNNs,
providing empirical support for our theoretical findings. Moreover, we prove a
theorem that describes how DNN loss decreases as we remove randomness in the
weight layers, and show a monotone dependence of the decrease in loss with the
amount of randomness that we remove. Our results also provide significant
RMT-based insights into the role of regularization during training and **pruning**.


## Streaming Video Question-Answering with In-context Video KV-Cache Retrieval

>Authors: Shangzhe Di, Zhelun Yu, Guanghao Zhang, Haoyuan Li, Tao Zhong, Hao Cheng, Bolin Li, Wanggui He, Fangxun Shu, Hao Jiang

>2025-03-01

> http://arxiv.org/abs/2503.00540v1

We propose Re**KV**, a novel training-free approach that enables efficient
streaming video question-answering (StreamingVQA), by seamlessly integrating
with existing Video Large Language Models (Video-LLMs). Traditional VideoQA
systems struggle with long videos, as they must process entire videos before
responding to queries, and repeat this process for each new question. In
contrast, our approach analyzes long videos in a streaming manner, allowing for
prompt responses as soon as user queries are received. Building on a common
Video-LLM, we first incorporate a sliding-window attention mechanism, ensuring
that input frames attend to a limited number of preceding frames, thereby
reducing computational overhead. To prevent information loss, we store
processed video key-value caches (**KV**-Caches) in RAM and disk, reloading them
into GPU memory as needed. Additionally, we introduce a retrieval method that
leverages an external retriever or the parameters within Video-LLMs to retrieve
only query-relevant **KV**-Caches, ensuring both efficiency and accuracy in
question answering. Re**KV** enables the separation of video encoding and
question-answering across different processes and GPUs, significantly enhancing
the efficiency of StreamingVQA. Through comprehensive experimentation, we
validate the efficacy and practicality of our approach, which significantly
boosts efficiency and enhances applicability over existing VideoQA models.


## Tutorial Proposal Speculative Decoding for Efficient LLM Inference

>Authors: Heming Xia, Cunxiao Du, Yongqi Li, Qian Liu, Wenjie Li

>2025-03-01

> http://arxiv.org/abs/2503.00491v1

This tutorial presents a comprehensive introduction to Speculative Decoding
(SD), an advanced technique for LLM inference **acceleration** that has garnered
significant research interest in recent years. SD is introduced as an
innovative decoding paradigm to mitigate the high inference latency stemming
from autoregressive decoding in LLMs. At each decoding step, SD efficiently
drafts several future tokens and then verifies them in parallel. This approach,
unlike traditional autoregressive decoding, facilitates the simultaneous
decoding of multiple tokens per step, thereby achieving promising 2x-4x
speedups in LLM inference while maintaining original distributions. This
tutorial delves into the latest techniques in SD, including draft model
architectures and verification strategies. Additionally, it explores the
**acceleration** potential and future research directions in this promising field.
We aim for this tutorial to elucidate the current research landscape and offer
insights for researchers interested in Speculative Decoding, ultimately
contributing to more efficient LLM inference.


## Leveraging Compute-in-Memory for Efficient Generative Model Inference in TPUs

>Authors: Zhantong Zhu, Hongou Li, Wenjie Ren, Meng Wu, Le Ye, Ru Huang, Tianyu Jia

>2025-03-01

> http://arxiv.org/abs/2503.00461v1

With the rapid advent of generative models, efficiently deploying these
models on specialized hardware has become critical. Tensor Processing Units
(TPUs) are designed to accelerate AI workloads, but their high power
consumption necessitates innovations for improving efficiency.
Compute-in-memory (CIM) has emerged as a promising paradigm with superior area
and energy efficiency. In this work, we present a TPU architecture that
integrates digital CIM to replace conventional digital systolic arrays in
matrix multiply units (MXUs). We first establish a CIM-based TPU architecture
model and simulator to evaluate the benefits of CIM for diverse generative
model inference. Building upon the observed design insights, we further explore
various CIM-based TPU architectural design choices. Up to 44.2% and 33.8%
performance improvement for large language model and diffusion transformer
inference, and 27.3x reduction in MXU energy consumption can be achieved with
different design choices, compared to the baseline TPUv4i architecture.


## Split Adaptation for Pre-trained Vision Transformers

>Authors: Lixu Wang, Bingqi Shang, Yi Li, Payal Mohapatra, Wei Dong, Xiao Wang, Qi Zhu

>2025-03-01

> http://arxiv.org/abs/2503.00441v1

Vision Transformers (ViTs), extensively pre-trained on large-scale datasets,
have become essential to foundation models, allowing excellent performance on
diverse downstream tasks with minimal adaptation. Consequently, there is
growing interest in adapting pre-trained ViTs across various fields, including
privacy-sensitive domains where clients are often reluctant to share their
data. Existing adaptation methods typically require direct data access,
rendering them infeasible under these constraints. A straightforward solution
may be sending the pre-trained ViT to clients for local adaptation, which poses
issues of model intellectual property protection and incurs heavy client
computation overhead. To address these issues, we propose a novel split
adaptation (SA) method that enables effective downstream adaptation while
protecting data and models. SA, inspired by split learning (SL), segments the
pre-trained ViT into a frontend and a backend, with only the frontend shared
with the client for data representation extraction. But unlike regular SL, SA
replaces frontend parameters with **low-bit** **quantize**d values, preventing direct
exposure of the model. SA allows the client to add bi-level noise to the
frontend and the extracted data representations, ensuring data protection.
Accordingly, SA incorporates data-level and model-level out-of-distribution
enhancements to mitigate noise injection's impact on adaptation performance.
Our SA focuses on the challenging few-shot adaptation and adopts patch
retrieval augmentation for overfitting alleviation. Extensive experiments on
multiple datasets validate SA's superiority over state-of-the-art methods and
demonstrate its defense against advanced data reconstruction attacks while
preventing model leakage with minimal computation cost on the client side. The
source codes can be found at https://github.com/conditionWang/Split_Adaptation.


## Progressive Sparse Attention Algorithm and System Co-design for Efficient Attention in LLM Serving

>Authors: Qihui Zhou, Peiqi Yin, Pengfei Zuo, James Cheng

>2025-03-01

> http://arxiv.org/abs/2503.00392v1

Processing long contexts has become a critical capability for modern large
language models (LLMs). However, serving long-context LLMs comes with
significant inference costs due to the high memory overhead of the key-value
(**KV**) cache. Existing work leverages dynamic **sparse** attention algorithms (DSAes)
to mitigate the **KV** cache overhead, but these algorithms rely on top-$k$ **KV**
cache selection, which results in a trade-off between accuracy and efficiency.
A larger $k$ improves accuracy but decreases efficiency, while a smaller $k$
boosts efficiency but compromises accuracy. To overcome this trade-off, this
paper presents PSA, a $\underline{P}$rogressive $\underline{S}$parse
$\underline{A}$ttention mechanism that integrates algorithmic innovations with
system co-design to achieve both high inference accuracy and improved
efficiency in LLM serving. The PSA algorithm adaptively adjusts the **KV** cache
budget of different tokens and layers according to their real attention weight
distributions, rather than relying on a fixed budget $k$. This enables high
accuracy while minimizing **KV** cache usage. To further enhance execution
efficiency, we introduce a pipelined iteration scheme that reduces CPU-GPU
interleaving and synchronization overhead during PSA computation. Additionally,
we implement unified GPU memory management that optimizes PSA's memory
utilization by accounting for uneven memory requirements across different model
layers. Extensive experimental results demonstrate that PSA reduces **KV** cache
usage for attention computation by up to 2.4$\times$ and 8.8$\times$, and
increases end-to-end serving throughput by up to 1.4$\times$ and 2.0$\times$,
compared to state-of-the-art DSAes and systems without **sparse** attention,
respectively.


## Strong Solutions and Quantization-Based Numerical Schemes for a Class of Non-Markovian Volatility Models

>Authors: Martino Grasselli, Gilles PagÃ¨s

>2025-02-28

> http://arxiv.org/abs/2503.00243v1

We investigate a class of non-Markovian processes that hold particular
relevance in the realm of mathematical finance. This family encompasses
path-dependent volatility models, including those pioneered by [Platen and
Rendek, 2018] and, more recently, by [Guyon and Lekeufack, 2023], as well as an
extension of the framework proposed by [Blanc et al., 2017]. Our study unfolds
in two principal phases. In the first phase, we introduce a functional
**quantization** scheme based on an extended version of the Lamperti transformation
that we propose to handle the presence of a memory term incorporated into the
diffusion coefficient. For scenarios involving a Brownian integral in the
diffusion term, we propose alternative numerical schemes that leverage the
power of marginal recursive **quantization**. In the second phase, we study the
problem of existence and uniqueness of a strong solution for the SDEs related
to the examples that motivate our study, in order to provide a theoretical
basis to correctly apply the proposed numerical schemes.


## AnalogGenie A Generative Engine for Automatic Discovery of Analog Circuit Topologies

>Authors: Jian Gao, Weidong Cao, Junyi Yang, Xuan Zhang

>2025-02-28

> http://arxiv.org/abs/2503.00205v1

The massive and large-scale design of foundational semiconductor integrated
circuits (ICs) is crucial to sustaining the advancement of many emerging and
future technologies, such as generative AI, 5G/6G, and quantum computing.
Excitingly, recent studies have shown the great capabilities of foundational
models in expediting the design of digital ICs. Yet, applying generative AI
techniques to accelerate the design of analog ICs remains a significant
challenge due to critical domain-specific issues, such as the lack of a
comprehensive dataset and effective representation methods for analog circuits.
This paper proposes, $\textbf{AnalogGenie}$, a
$\underline{\textbf{Gen}}$erat$\underline{\textbf{i}}$ve
$\underline{\textbf{e}}$ngine for automatic design/discovery of
$\underline{\textbf{Analog}}$ circuit topologies--the most challenging and
creative task in the conventional manual design flow of analog ICs. AnalogGenie
addresses two key gaps in the field: building a foundational comprehensive
dataset of analog circuit topology and developing a scalable sequence-based
graph representation universal to analog circuits. Experimental results show
the remarkable generation performance of AnalogGenie in broadening the variety
of analog ICs, increasing the number of devices within a single design, and
discovering unseen circuit topologies far beyond any prior arts. Our work paves
the way to transform the longstanding time-consuming manual design flow of
analog ICs to an automatic and massive manner powered by generative AI. Our
source code is available at https://github.com/xz-group/AnalogGenie.


## Steering Large Language Model Activations in Sparse Spaces

>Authors: Reza Bayat, Ali Rahimi-Kalahroudi, Mohammad Pezeshki, Sarath Chandar, Pascal Vincent

>2025-02-28

> http://arxiv.org/abs/2503.00177v1

A key challenge in AI alignment is guiding large language models (LLMs) to
follow desired behaviors at test time. Activation steering, which modifies
internal model activations during inference, offers a potential solution.
However, prior work in dense activation spaces struggles with superposition,
wherein multiple features become entangled, limiting interpretability and
precise control. In contrast, **sparse** representations provide an untapped
opportunity for more interpretable behavior modulation. In this work, we
introduce **sparse** activation steering (SAS), a method that leverages **sparse**
autoencoders (SAEs) to steer LLM behavior in **sparse** spaces. By isolating
behavior-specific features through a contrastive prompt-pairing approach, we
define a set of features that can selectively reinforce or suppress behaviors.
Experiments on Gemma 2 LLMs show that SAS vectors enable nuanced behavioral
modulation and finer-grained control. Furthermore, scaling SAEs improves
monosemanticity of SAS vectors, suggesting more reliable and interpretable
interventions.


## BixBench a Comprehensive Benchmark for LLM-based Agents in Computational Biology

>Authors: Ludovico Mitchener, Jon M Laurent, Benjamin Tenmann, Siddharth Narayanan, Geemi P Wellawatte, Andrew White, Lorenzo Sani, Samuel G Rodriques

>2025-02-28

> http://arxiv.org/abs/2503.00096v1

Large Language Models (LLMs) and LLM-based agents show great promise in
accelerating scientific research. Existing benchmarks for measuring this
potential and guiding future development continue to evolve from pure recall
and rote knowledge tasks, towards more practical work such as literature review
and experimental planning. Bioinformatics is a domain where fully autonomous
AI-driven discovery may be near, but no extensive benchmarks for measuring
progress have been introduced to date. We therefore present the Bioinformatics
Benchmark (BixBench), a dataset comprising over 50 real-world scenarios of
practical biological data analysis with nearly 300 associated open-answer
questions designed to measure the ability of LLM-based agents to explore
biological datasets, perform long, multi-step analytical trajectories, and
interpret the nuanced results of those analyses. We evaluate the performance of
two frontier LLMs (GPT-4o and Claude 3.5 Sonnet) using a custom agent framework
we open source. We find that even the latest frontier models only achieve 17%
accuracy in the open-answer regime, and no better than random in a
multiple-choice setting. By exposing the current limitations of frontier
models, we hope BixBench can spur the development of agents capable of
conducting rigorous bioinformatic analysis and accelerate scientific discovery.


## Oscillatory finite-time singularities in rockbursts

>Authors: Qinghua Lei, Didier Sornette

>2025-02-28

> http://arxiv.org/abs/2502.21296v1

Forecasting violent rockbursts remains a formidable challenge due to
significant uncertainties involved. One major uncertainty arises from the
intermittency of rock failure processes, typically characterised by a series of
progressively shorter quiescent phases punctuated by sudden **acceleration**s,
rather than a smooth continuous progression towards the final breakdown. This
non-monotonic evolution of rock mass deformation complicates rockburst
prediction, challenging conventional time-to-failure models that often assume a
smooth power law accelerating behaviour. Here, we introduce a generalised
time-to-failure model called log-periodic power law singularity (LPPLS) model
to effectively capture the intermittent dynamics of damage and rupture
processes in rock leading up to violent rockbursts. We perform parametric and
nonparametric tests on 11 historical rockburst events at three underground
mines, documenting empirical evidence and providing theoretical arguments to
demonstrate the significance of log-periodic oscillatory power law finite-time
singularities. Log-periodicity in these rockburst events is likely driven by
the interaction of subparallel propagating cracks, the diffusion of
stress-triggering processes, or the interplay between stress drop and stress
corrosion. Our results and insights obtained have significant implications for
not only understanding but also forecasting rockbursts, as recognising and
characterising log-periodicity can help transform intermittency from
traditionally perceived noise into valuable predictive information.


## A Method of Selective Attention for Reservoir Based Agents

>Authors: Kevin McKee

>2025-02-28

> http://arxiv.org/abs/2502.21229v1

Training of deep reinforcement learning agents is slowed considerably by the
presence of input dimensions that do not usefully condition the reward
function. Existing modules such as layer normalization can be trained with
weight decay to act as a form of selective attention, i.e. an input mask, that
shrinks the scale of unnecessary inputs, which in turn accelerates training of
the policy. However, we find a surprising result that adding numerous
parameters to the computation of the input mask results in much faster
training. A simple, high dimensional masking module is compared with layer
normalization and a model without any input suppression. The high dimensional
mask resulted in a four-fold speedup in training over the null hypothesis and a
two-fold speedup in training over the layer normalization method.


## AMPLE Event-Driven Accelerator for Mixed-Precision Inference of Graph Neural Networks

>Authors: Pedro Gimenes, Yiren Zhao, George Constantinides

>2025-02-28

> http://arxiv.org/abs/2502.21196v1

Graph Neural Networks (GNNs) have recently gained attention due to their
performance on non-Euclidean data. The use of custom hardware architectures
proves particularly beneficial for GNNs due to their irregular memory access
patterns, resulting from the **sparse** structure of graphs. However, existing FPGA
accelerators are limited by their double buffering mechanism, which doesn't
account for the irregular node distribution in typical graph datasets. To
address this, we introduce \textbf{AMPLE} (Accelerated Message Passing Logic
Engine), an FPGA accelerator leveraging a new event-driven programming flow. We
develop a mixed-arithmetic architecture, enabling GNN inference to be **quantize**d
at a node-level granularity. Finally, prefetcher for data and instructions is
implemented to optimize off-chip memory access and maximize node parallelism.
Evaluation on citation and social media graph datasets ranging from $2$K to
$700$K nodes showed a mean speedup of $243\times$ and $7.2\times$ against CPU
and GPU counterparts, respectively.


## Tracks to Modernity Railroads, Growth, and Social Movements in Denmark

>Authors: Tom GÃ¶rges, Magnus Ãrberg Rove, Paul Sharp, Christian Vedel

>2025-02-28

> http://arxiv.org/abs/2502.21141v1

How do transport infrastructures shape economic transformation and social
change? We examine the impact of railway expansion in nineteenth-century
Denmark on local population growth, occupational shifts, and the diffusion of
ideas. Using a historical panel dataset and a difference-in-differences
approach, we document that railway access significantly increased population
growth and accelerated structural change. Moreover, railway-connected areas
were more likely to establish key institutions linked to civic engagement and
the cooperative movement. These findings suggest that improved market access
was not only a driver of economic modernization but also a catalyst for
institutional and cultural transformation.


## Training-free and Adaptive Sparse Attention for Efficient Long Video Generation

>Authors: Yifei Xia, Suhan Ling, Fangcheng Fu, Yujie Wang, Huixia Li, Xuefeng Xiao, Bin Cui

>2025-02-28

> http://arxiv.org/abs/2502.21079v1

Generating high-fidelity long videos with Diffusion Transformers (DiTs) is
often hindered by significant latency, primarily due to the computational
demands of attention mechanisms. For instance, generating an 8-second 720p
video (110K tokens) with HunyuanVideo takes about 600 PFLOPs, with around 500
PFLOPs consumed by attention computations. To address this issue, we propose
AdaSpa, the first Dynamic Pattern and Online Precise Search **sparse** attention
method. Firstly, to realize the Dynamic Pattern, we introduce a blockified
pattern to efficiently capture the hierarchical **sparsity** inherent in DiTs. This
is based on our observation that **sparse** characteristics of DiTs exhibit
hierarchical and blockified structures between and within different modalities.
This blockified approach significantly reduces the complexity of attention
computation while maintaining high fidelity in the generated videos. Secondly,
to enable Online Precise Search, we propose the Fused LSE-Cached Search with
Head-adaptive Hierarchical Block Sparse Attention. This method is motivated by
our finding that DiTs' **sparse** pattern and LSE vary w.r.t. inputs, layers, and
heads, but remain invariant across denoising steps. By leveraging this
invariance across denoising steps, it adapts to the dynamic nature of DiTs and
allows for precise, real-time identification of **sparse** indices with minimal
overhead. AdaSpa is implemented as an adaptive, plug-and-play solution and can
be integrated seamlessly with existing DiTs, requiring neither additional
fine-tuning nor a dataset-dependent profiling. Extensive experiments validate
that AdaSpa delivers substantial **acceleration** across various models while
preserving video quality, establishing itself as a robust and scalable approach
to efficient video generation.


## Fast 3D point clouds retrieval for Large-scale 3D Place Recognition

>Authors: Chahine-Nicolas Zede, Laurent Carrafa, ValÃ©rie Gouet-Brunet

>2025-02-28

> http://arxiv.org/abs/2502.21067v1

Retrieval in 3D point clouds is a challenging task that consists in
retrieving the most similar point clouds to a given query within a reference of
3D points. Current methods focus on comparing descriptors of point clouds in
order to identify similar ones. Due to the complexity of this latter step, here
we focus on the **acceleration** of the retrieval by adapting the Differentiable
Search Index (DSI), a transformer-based approach initially designed for text
information retrieval, for 3D point clouds retrieval. Our approach generates 1D
identifiers based on the point descriptors, enabling direct retrieval in
constant time. To adapt DSI to 3D data, we integrate Vision Transformers to map
descriptors to these identifiers while incorporating positional and semantic
encoding. The approach is evaluated for place recognition on a public benchmark
comparing its retrieval capabilities against state-of-the-art methods, in terms
of quality and speed of returned point clouds.


## The amplifier effect of artificial agents in social contagion

>Authors: Eric Hitz, Mingmin Feng, Radu Tanase, RenÃ© Algesheimer, Manuel S. Mariani

>2025-02-28

> http://arxiv.org/abs/2502.21037v1

Recent advances in artificial intelligence have led to the proliferation of
artificial agents in social contexts, ranging from education to online social
media and financial markets, among many others. The increasing rate at which
artificial and human agents interact makes it urgent to understand the
consequences of human-machine interactions for the propagation of new ideas,
products, and behaviors in society. Across two distinct empirical contexts, we
find here that artificial agents lead to significantly faster and wider social
contagion. To this end, we replicate a choice experiment previously conducted
with human subjects by using artificial agents powered by large language models
(LLMs). We use the experiment's results to measure the adoption thresholds of
artificial agents and their impact on the spread of social contagion. We find
that artificial agents tend to exhibit lower adoption thresholds than humans,
which leads to wider network-based social contagions. Our findings suggest that
the increased presence of artificial agents in real-world networks may
accelerate behavioral shifts, potentially in unforeseen ways.


## Preconditioned Block Encodings for Quantum Linear Systems

>Authors: Leigh Lapworth, Christoph SÃ¼nderhauf

>2025-02-28

> http://arxiv.org/abs/2502.20908v1

Quantum linear system solvers like the Quantum Singular Value Transformation
(QSVT) require a block encoding of the system matrix $A$ within a unitary
operator $U_A$. Unfortunately, block encoding often results in significant
subnormalisation and increase in the matrix's effective condition number
$\kappa$, affecting the efficiency of solvers. Matrix preconditioning is a
well-established classical technique to reduce $\kappa$ by multiplying $A$ by a
preconditioner $P$. Here, we study quantum preconditioning for block encodings.
We consider four preconditioners and two encoding approaches: (a) separately
encoding $A$ and its preconditioner $P$, followed by quantum multiplication,
and (b) classically multiplying $A$ and $P$ before encoding the product in
$U_{PA}$. Their impact on subnormalisation factors and condition number
$\kappa$ are analysed using practical matrices from Computational Fluid
Dynamics (CFD). Our results show that (a) quantum multiplication introduces
excessive subnormalisation factors, negating improvements in $\kappa$. We
introduce preamplified quantum multiplication to reduce subnormalisation, which
is of independent interest. Conversely, we see that (b) encoding of the
classical product can significantly improve the effective condition number
using the Sparse Approximate Inverse preconditioner with infill. Further, we
introduce a new matrix filtering technique that reduces the circuit depth
without adversely affecting the matrix solution. We apply these methods to
reduce the number of QSVT phase factors by a factor of 25 for an example CFD
matrix of size 1024x1024.


## Oscillation-Reduced MXFP4 Training for Vision Transformers

>Authors: Yuxiang Chen, Haocheng Xi, Jun Zhu, Jianfei Chen

>2025-02-28

> http://arxiv.org/abs/2502.20853v1

Pre-training Transformers in FP4 precision is becoming a promising approach
to gain substantial speedup, but it comes with a considerable loss of accuracy.
Microscaling (MX) data format provides a fine-grained per-group **quantization**
method to improve the representation ability of the FP4 format and is supported
by the next-generation Blackwell GPU architecture. However, training with MXFP4
data format still results in significant degradation and there is a lack of
systematic research on the reason.
  In this work, we propose a novel training method TetraJet for a more accurate
FP4 training. We comprehensively evaluate all of the **quantize**rs involved in the
training, and identify the weight oscillation problem in the forward pass as
the main source of the degradation in MXFP4 training. Therefore, we introduce
two novel methods, EMA Quantizer (Q-EMA) and Adaptive Ramping Optimizer
(Q-Ramping), to resolve the oscillation problem. Extensive experiments on
Vision Transformers demonstrate that TetraJet consistently outperforms the
existing 4-bit training methods, and Q-EMA & Q-Ramping can provide additional
enhancement by effectively reducing oscillation. We decreased the accuracy
degradation by more than $50\%$ compared to the baseline, and can even achieve
competitive performance compared to full precision training. The codes are
available at https://github.com/thu-ml/TetraJet-MXFP4Training


## Identifying Sensitive Weights via Post-quantization Integral

>Authors: Yuezhou Hu, Weiyu Huang, Zichen Liang, Chang Chen, Jintao Zhang, Jun Zhu, Jianfei Chen

>2025-02-28

> http://arxiv.org/abs/2503.01901v1

Serving Large Language Models (LLMs) is costly. However, post-training weight
**quantization** can address this problem by both compressing their sizes for
limited memory and saving bandwidth for **acceleration**. As not all weight
dimensions are equally important, those methods typically rely on a sensitivity
metric, which indicates the element-wise influence of weights on loss function
and is used to preprocess original weights for better **quantization**. In this
work, we conduct an empirical study on the accuracy of the sensitivity metric,
and find that existing gradient and Hessian based metrics are very inaccurate:
they underestimate **quantization**'s impact on the loss function by orders of
magnitude, mainly due to the small convergence radius of local 2nd order
approximation, \ie, gradient and Hessian term in Taylor's formula. To tackle
this problem, we propose Post-**quantization** Integral (PQI), an accurate metric
to estimate posterior sensitivity in a fine-grained manner. To leverage this
accurate metric, we further propose ReQuant, a simple yet powerful framework
that mainly consists of two Dense-and-Sparse detach components: self-adaptive
outlier selection and step-wise significant weights detach. Results show that
ReQuant boosts state-of-the-art post-training **quantization** methods, with a
pronounced improvement of 2.66 perplexity gain on Llama 3.2 1B with QTIP.


## FlexPrefill A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference

>Authors: Xunhao Lai, Jianqiao Lu, Yao Luo, Yiyuan Ma, Xun Zhou

>2025-02-28

> http://arxiv.org/abs/2502.20766v1

Large language models (LLMs) encounter computational challenges during
long-sequence inference, especially in the attention pre-filling phase, where
the complexity grows quadratically with the prompt length. Previous efforts to
mitigate these challenges have relied on fixed **sparse** attention patterns or
identifying **sparse** attention patterns based on limited cases. However, these
methods lacked the flexibility to efficiently adapt to varying input demands.
In this paper, we introduce FlexPrefill, a Flexible **sparse** Pre-filling
mechanism that dynamically adjusts **sparse** attention patterns and computational
budget in real-time to meet the specific requirements of each input and
attention head. The flexibility of our method is demonstrated through two key
innovations: 1) Query-Aware Sparse Pattern Determination: By measuring
Jensen-Shannon divergence, this component adaptively switches between
query-specific diverse attention patterns and predefined attention patterns. 2)
Cumulative-Attention Based Index Selection: This component dynamically selects
query-key indexes to be computed based on different attention patterns,
ensuring the sum of attention scores meets a predefined threshold. FlexPrefill
adaptively optimizes the **sparse** pattern and **sparse** ratio of each attention head
based on the prompt, enhancing efficiency in long-sequence inference tasks.
Experimental results show significant improvements in both speed and accuracy
over prior methods, providing a more flexible and efficient solution for LLM
inference.


## Enhanced Performance and Stability of Perovskite Solar Cells with Ag-Cu-Zn Alloy Electrodes

>Authors: Keshav Kumar Sharma, Ashutosh Ujjwal, Rohit Saini, Ramesh Karuppannan

>2025-02-28

> http://arxiv.org/abs/2502.20765v1

Though the common metal electrode-based perovskite solar cells have achieved
a power conversion efficiency of >25%, they also play a crucial role in
accelerating the degradation of the cells. In this study, we investigated phase
transition engineering in Ag electrodes via Cu and Zn alloying, transforming
from a cubic to a tetragonal phase. These alloyed electrodes are then thermally
deposited as back electrodes in perovskite solar cells. We conducted a
comprehensive analysis of the pure Ag and Ag-Cu-Zn alloys deposited atop a
hole-transport layer for use in Cs0.05(FA0.83MA0.17)0.95Pb(I0.83Br0.17)3-based
solar cells. Our findings reveal that solar cells developed with pure Ag
electrodes demonstrate a power conversion efficiency (PCE) of 18.71%,
characterized by a fill factor (FF) of 74.8%, an open-circuit voltage (VOC) of
1.08 V, and a short-circuit current density (JSC) of 23.17 mA/cm2. Conversely,
solar cells fabricated with optimized Ag0.875Cu0.120Zn0.005 electrodes exhibit
enhanced performance metrics, with an FF of 72.5%, VOC of 1.12 V, and JSC of
23.39 mA/cm2, culminating in an elevated PCE of 19.02%. Moreover, this
electrode demonstrates remarkable durability, sustaining operational integrity
for 460 hours for the PSCs stored in the N2 glove box, in contrast to the 320
hours for cells with Ag electrodes. The Ag-Cu-Zn alloys exhibited high
resistance to corrosion and good adhesion on the hole-transport material layer
compared to a layer of Ag. These advancements may lead to the realization of
cost-effective, durable, and efficient solar energy conversion systems.


## Generating Clinically Realistic EHR Data via a Hierarchy- and Semantics-Guided Transformer

>Authors: Guanglin Zhou, Sebastiano Barbieri

>2025-02-28

> http://arxiv.org/abs/2502.20719v1

Generating realistic synthetic electronic health records (EHRs) holds
tremendous promise for accelerating healthcare research, facilitating AI model
development and enhancing patient privacy. However, existing generative methods
typically treat EHRs as flat sequences of discrete medical codes. This approach
overlooks two critical aspects: the inherent hierarchical organization of
clinical coding systems and the rich semantic context provided by code
descriptions. Consequently, synthetic patient sequences often lack high
clinical fidelity and have limited utility in downstream clinical tasks. In
this paper, we propose the Hierarchy- and Semantics-Guided Transformer (HiSGT),
a novel framework that leverages both hierarchical and semantic information for
the generative process. HiSGT constructs a hierarchical graph to encode
parent-child and sibling relationships among clinical codes and employs a graph
neural network to derive hierarchy-aware embeddings. These are then fused with
semantic embeddings extracted from a pre-trained clinical language model (e.g.,
ClinicalBERT), enabling the Transformer-based generator to more accurately
model the nuanced clinical patterns inherent in real EHRs. Extensive
experiments on the MIMIC-III and MIMIC-IV datasets demonstrate that HiSGT
significantly improves the statistical alignment of synthetic data with real
patient records, as well as supports robust downstream applications such as
chronic disease classification. By addressing the limitations of conventional
raw code-based generative models, HiSGT represents a significant step toward
clinically high-fidelity synthetic data generation and a general paradigm
suitable for interpretable medical code representation, offering valuable
applications in data augmentation and privacy-preserving healthcare analytics.


## EDENet Echo Direction Encoding Network for Place Recognition Based on Ground Penetrating Radar

>Authors: Pengyu Zhang, Xieyuanli Chen, Yuwei Chen, Beizhen Bi, Zhuo Xu, Tian Jin, Xiaotao Huang, Liang Shen

>2025-02-28

> http://arxiv.org/abs/2502.20643v1

Ground penetrating radar (GPR) based localization has gained significant
recognition in robotics due to its ability to detect stable subsurface
features, offering advantages in environments where traditional sensors like
cameras and LiDAR may struggle. However, existing methods are primarily focused
on small-scale place recognition (PR), leaving the challenges of PR in
large-scale maps unaddressed. These challenges include the inherent **sparsity** of
underground features and the variability in underground dielectric constants,
which complicate robust localization. In this work, we investigate the
geometric relationship between GPR echo sequences and underground scenes,
leveraging the robustness of directional features to inform our network design.
We introduce learnable Gabor filters for the precise extraction of directional
responses, coupled with a direction-aware attention mechanism for effective
geometric encoding. To further enhance performance, we incorporate a
shift-invariant unit and a multi-scale aggregation strategy to better
accommodate variations in di-electric constants. Experiments conducted on
public datasets demonstrate that our proposed EDENet not only surpasses
existing solutions in terms of PR performance but also offers advantages in
model size and computational efficiency.

