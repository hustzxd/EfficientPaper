# 2025-01-13

# Table of Contents
* [Deriving Coding-Specific Sub-Models from LLMs using Resource-Efficient Pruning](#Deriving-Coding-Specific-Sub-Models-from-LLMs-using-Resource-Efficient-Pruning)
* [Optimized Sampling for Non-Line-of-Sight Imaging Using Modified Fast Fourier Transforms](#Optimized-Sampling-for-Non-Line-of-Sight-Imaging-Using-Modified-Fast-Fourier-Transforms)
* [TreeKV Smooth Key-Value Cache Compression with Tree Structures](#TreeKV-Smooth-Key-Value-Cache-Compression-with-Tree-Structures)
* [V2C-CBM Building Concept Bottlenecks with Vision-to-Concept Tokenizer](#V2C-CBM-Building-Concept-Bottlenecks-with-Vision-to-Concept-Tokenizer)
* [MyESL Sparse learning in molecular evolution and phylogenetic analysis](#MyESL-Sparse-learning-in-molecular-evolution-and-phylogenetic-analysis)
* [Intelligent experiments through real-time AI Fast Data Processing and Autonomous Detector Control for sPHENIX and future EIC detectors](#Intelligent-experiments-through-real-time-AI-Fast-Data-Processing-and-Autonomous-Detector-Control-for-sPHENIX-and-future-EIC-detectors)
* [ActPC-Geom Towards Scalable Online Neural-Symbolic Learning via Accelerating Active Predictive Coding with Information Geometry & Diverse Cognitive Mechanisms](#ActPC-Geom-Towards-Scalable-Online-Neural-Symbolic-Learning-via-Accelerating-Active-Predictive-Coding-with-Information-Geometry-&-Diverse-Cognitive-Mechanisms)
* [Entanglement in cyclic sign invariant quantum states](#Entanglement-in-cyclic-sign-invariant-quantum-states)
* [Discrete Wavelet Transform-Based Capsule Network for Hyperspectral Image Classification](#Discrete-Wavelet-Transform-Based-Capsule-Network-for-Hyperspectral-Image-Classification)
* [Learnable Scaled Gradient Descent for Guaranteed Robust Tensor PCA](#Learnable-Scaled-Gradient-Descent-for-Guaranteed-Robust-Tensor-PCA)
* [Multivariate Exploration of Metric Dilation](#Multivariate-Exploration-of-Metric-Dilation)
* [Exact recovery in the double sparse model sufficient and necessary signal conditions](#Exact-recovery-in-the-double-sparse-model-sufficient-and-necessary-signal-conditions)
* [The Impostor is Among Us Can Large Language Models Capture the Complexity of Human Personas?](#The-Impostor-is-Among-Us-Can-Large-Language-Models-Capture-the-Complexity-of-Human-Personas?)
* [A Plug-and-Play Bregman ADMM Module for Inferring Event Branches in Temporal Point Processes](#A-Plug-and-Play-Bregman-ADMM-Module-for-Inferring-Event-Branches-in-Temporal-Point-Processes)
* [Integrating remote sensing data assimilation, deep learning and large language model for interactive wheat breeding yield prediction](#Integrating-remote-sensing-data-assimilation,-deep-learning-and-large-language-model-for-interactive-wheat-breeding-yield-prediction)
* [MB-TaylorFormer V2 Improved Multi-branch Linear Transformer Expanded by Taylor Formula for Image Restoration](#MB-TaylorFormer-V2-Improved-Multi-branch-Linear-Transformer-Expanded-by-Taylor-Formula-for-Image-Restoration)
* [RoRA Efficient Fine-Tuning of LLM with Reliability Optimization for Rank Adaptation](#RoRA-Efficient-Fine-Tuning-of-LLM-with-Reliability-Optimization-for-Rank-Adaptation)
* [FSC-loss A Frequency-domain Structure Consistency Learning Approach for Signal Data Recovery and Reconstruction](#FSC-loss-A-Frequency-domain-Structure-Consistency-Learning-Approach-for-Signal-Data-Recovery-and-Reconstruction)
* [DGQ Distribution-Aware Group Quantization for Text-to-Image Diffusion Models](#DGQ-Distribution-Aware-Group-Quantization-for-Text-to-Image-Diffusion-Models)
* [Dynamic Localisation of Spatial-Temporal Graph Neural Network](#Dynamic-Localisation-of-Spatial-Temporal-Graph-Neural-Network)
* [Agent Laboratory Using LLM Agents as Research Assistants](#Agent-Laboratory-Using-LLM-Agents-as-Research-Assistants)
* [Graph classes through the lens of logic](#Graph-classes-through-the-lens-of-logic)
* [KGIF Optimizing Relation-Aware Recommendations with Knowledge Graph Information Fusion](#KGIF-Optimizing-Relation-Aware-Recommendations-with-Knowledge-Graph-Information-Fusion)
* [A GPU Implementation of Multi-Guiding Spark Fireworks Algorithm for Efficient Black-Box Neural Network Optimization](#A-GPU-Implementation-of-Multi-Guiding-Spark-Fireworks-Algorithm-for-Efficient-Black-Box-Neural-Network-Optimization)
* [Dolphin Closed-loop Open-ended Auto-research through Thinking, Practice, and Feedback](#Dolphin-Closed-loop-Open-ended-Auto-research-through-Thinking,-Practice,-and-Feedback)
* [Computational Astrophysics, Data Science & AI/ML in Astronomy A Perspective from Indian Community](#Computational-Astrophysics,-Data-Science-&-AI/ML-in-Astronomy-A-Perspective-from-Indian-Community)
* [A Sequential Optimal Learning Approach to Automated Prompt Engineering in Large Language Models](#A-Sequential-Optimal-Learning-Approach-to-Automated-Prompt-Engineering-in-Large-Language-Models)
* [The Power of Negative Zero Datatype Customization for Quantized Large Language Models](#The-Power-of-Negative-Zero-Datatype-Customization-for-Quantized-Large-Language-Models)
* [The Artificial Scientist -- in-transit Machine Learning of Plasma Simulations](#The-Artificial-Scientist----in-transit-Machine-Learning-of-Plasma-Simulations)
* [Rethinking Byzantine Robustness in Federated Recommendation from Sparse Aggregation Perspective](#Rethinking-Byzantine-Robustness-in-Federated-Recommendation-from-Sparse-Aggregation-Perspective)
* [Gas bubble dynamics](#Gas-bubble-dynamics)
* [Ultra-fast, high-power MUTC Photodiodes with bandwidth-efficiency product over 130 GHz * 100%](#Ultra-fast,-high-power-MUTC-Photodiodes-with-bandwidth-efficiency-product-over-130-GHz-*-100%)
* [Adaptive Pruning of Pretrained Transformer via Differential Inclusions](#Adaptive-Pruning-of-Pretrained-Transformer-via-Differential-Inclusions)
* [Artificial Intelligence in Creative Industries Advances Prior to 2025](#Artificial-Intelligence-in-Creative-Industries-Advances-Prior-to-2025)
* [GS-DiT Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking](#GS-DiT-Advancing-Video-Generation-with-Pseudo-4D-Gaussian-Fields-through-Efficient-Dense-3D-Point-Tracking)
* [HALO Hadamard-Assisted Lossless Optimization for Efficient Low-Precision LLM Training and Fine-Tuning](#HALO-Hadamard-Assisted-Lossless-Optimization-for-Efficient-Low-Precision-LLM-Training-and-Fine-Tuning)
* [LeetDecoding A PyTorch Library for Exponentially Decaying Causal Linear Attention with CUDA Implementations](#LeetDecoding-A-PyTorch-Library-for-Exponentially-Decaying-Causal-Linear-Attention-with-CUDA-Implementations)
* [Rethinking Hard Thresholding Pursuit Full Adaptation and Sharp Estimation](#Rethinking-Hard-Thresholding-Pursuit-Full-Adaptation-and-Sharp-Estimation)
* [Predicting IoT Device Vulnerability Fix Times with Survival and Failure Time Models](#Predicting-IoT-Device-Vulnerability-Fix-Times-with-Survival-and-Failure-Time-Models)
* [Strategic Fusion Optimizes Transformer Compression](#Strategic-Fusion-Optimizes-Transformer-Compression)
* [Efficient Deployment of Large Language Models on Resource-constrained Devices](#Efficient-Deployment-of-Large-Language-Models-on-Resource-constrained-Devices)
* [Scaling Laws for Floating Point Quantization Training](#Scaling-Laws-for-Floating-Point-Quantization-Training)
* [Graph-Aware Isomorphic Attention for Adaptive Dynamics in Transformers](#Graph-Aware-Isomorphic-Attention-for-Adaptive-Dynamics-in-Transformers)
* [Optimizing Small Language Models for In-Vehicle Function-Calling](#Optimizing-Small-Language-Models-for-In-Vehicle-Function-Calling)
* [AdaSkip Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference](#AdaSkip-Adaptive-Sublayer-Skipping-for-Accelerating-Long-Context-LLM-Inference)
* [Nuclear quantum effects slow down the energy transfer in biological light-harvesting complexes](#Nuclear-quantum-effects-slow-down-the-energy-transfer-in-biological-light-harvesting-complexes)
* [The Application of Large Language Models in Recommendation Systems](#The-Application-of-Large-Language-Models-in-Recommendation-Systems)
* [Personalized Graph-Based Retrieval for Large Language Models](#Personalized-Graph-Based-Retrieval-for-Large-Language-Models)
* [Instruction-Following Pruning for Large Language Models](#Instruction-Following-Pruning-for-Large-Language-Models)
* [VITA-1.5 Towards GPT-4o Level Real-Time Vision and Speech Interaction](#VITA-1.5-Towards-GPT-4o-Level-Real-Time-Vision-and-Speech-Interaction)
* [Compressed sensing for inverse problems II applications to deconvolution, source recovery, and MRI](#Compressed-sensing-for-inverse-problems-II-applications-to-deconvolution,-source-recovery,-and-MRI)
* [EnerVerse Envisioning Embodied Future Space for Robotics Manipulation](#EnerVerse-Envisioning-Embodied-Future-Space-for-Robotics-Manipulation)
* [Multi-Agent Conversational Online Learning for Adaptive LLM Response Identification](#Multi-Agent-Conversational-Online-Learning-for-Adaptive-LLM-Response-Identification)
* [Auto-RT Automatic Jailbreak Strategy Exploration for Red-Teaming Large Language Models](#Auto-RT-Automatic-Jailbreak-Strategy-Exploration-for-Red-Teaming-Large-Language-Models)
* [Efficient LLM Inference with Activation Checkpointing and Hybrid Caching](#Efficient-LLM-Inference-with-Activation-Checkpointing-and-Hybrid-Caching)
* [Compressed Domain Prior-Guided Video Super-Resolution for Cloud Gaming Content](#Compressed-Domain-Prior-Guided-Video-Super-Resolution-for-Cloud-Gaming-Content)
* [Spot Risks Before Speaking! Unraveling Safety Attention Heads in Large Vision-Language Models](#Spot-Risks-Before-Speaking!-Unraveling-Safety-Attention-Heads-in-Large-Vision-Language-Models)
* [PSYCHE A Multi-faceted Patient Simulation Framework for Evaluation of Psychiatric Assessment Conversational Agents](#PSYCHE-A-Multi-faceted-Patient-Simulation-Framework-for-Evaluation-of-Psychiatric-Assessment-Conversational-Agents)


## Deriving Coding-Specific Sub-Models from LLMs using Resource-Efficient Pruning

>Authors: Laura Puccioni, Alireza Farshin, Mariano Scazzariello, Changjie Wang, Marco Chiesa, Dejan Kostic

>2025-01-09

> http://arxiv.org/abs/2501.05248v1

Large Language Models (LLMs) have demonstrated their exceptional performance
in various complex code generation tasks. However, their broader adoption is
limited by significant computational demands and high resource requirements,
particularly memory and processing power. To mitigate such requirements, model
**pruning** techniques are used to create more compact models with significantly
fewer parameters. However, current approaches do not focus on the efficient
extraction of programming-language-specific sub-models. In this work, we
explore the idea of efficiently deriving coding-specific sub-models through
unstructured **pruning** (i.e., Wanda). We investigate the impact of different
domain-specific calibration datasets on **pruning** outcomes across three distinct
domains and extend our analysis to extracting four language-specific
sub-models: Python, Java, C++, and JavaScript. We are the first to efficiently
extract programming-language-specific sub-models using appropriate calibration
datasets while maintaining acceptable accuracy w.r.t. full models. We are also
the first to provide analytical evidence that domain-specific tasks activate
distinct regions within LLMs, supporting the creation of specialized sub-models
through unstructured **pruning**. We believe that this work has significant
potential to enhance LLM accessibility for coding by reducing computational
requirements to enable local execution on consumer-grade hardware, and
supporting faster inference times critical for real-time development feedback.


## Optimized Sampling for Non-Line-of-Sight Imaging Using Modified Fast Fourier Transforms

>Authors: Talha Sultan, Alex Bocchieri, Chaoying Gu, Xiaochun Liu, Pavel Polynkin, Andreas Velten

>2025-01-09

> http://arxiv.org/abs/2501.05244v1

Non-line-of-Sight (NLOS) imaging systems collect light at a diffuse relay
surface and input this measurement into computational algorithms that output a
3D volumetric reconstruction. These algorithms utilize the Fast Fourier
Transform (FFT) to accelerate the reconstruction process but require both input
and output to be sampled spatially with uniform grids. However, the geometry of
NLOS imaging inherently results in non-uniform sampling on the relay surface
when using multi-pixel detector arrays, even though such arrays significantly
reduce acquisition times. Furthermore, using these arrays increases the data
rate required for sensor readout, posing challenges for real-world deployment.
In this work, we utilize the phasor field framework to demonstrate that
existing NLOS imaging setups typically oversample the relay surface spatially,
explaining why the measurement can be compressed without significantly
sacrificing reconstruction quality. This enables us to utilize the Non-Uniform
Fast Fourier Transform (NUFFT) to reconstruct from **sparse** measurements acquired
from irregularly sampled relay surfaces of arbitrary shapes. Furthermore, we
utilize the NUFFT to reconstruct at arbitrary locations in the hidden volume,
ensuring flexible sampling schemes for both the input and output. Finally, we
utilize the Scaled Fast Fourier Transform (SFFT) to reconstruct larger volumes
without increasing the number of samples stored in memory. All algorithms
introduced in this paper preserve the computational complexity of FFT-based
methods, ensuring scalability for practical NLOS imaging applications.


## TreeKV Smooth Key-Value Cache Compression with Tree Structures

>Authors: Ziwei He, Jian Yuan, Haoli Bai, Jingwen Leng, Bo Jiang

>2025-01-09

> http://arxiv.org/abs/2501.04987v1

Efficient key-value (**KV**) cache compression is critical for scaling
transformer-based Large Language Models (LLMs) in long sequences and
resource-limited settings. Existing methods evict tokens based on their
positions or importance scores, but position-based strategies can miss crucial
information outside predefined regions, while those relying on global
importance scores resulting in strong regional biases, limiting the **KV** cache's
overall context retention and potentially impairing the performance of LLMs on
complex tasks. Our wavelet analysis reveals that as tokens approach the end of
sequence, their contributions to generation gradually increase and tends to
diverge more from neighboring tokens, indicating a smooth transition with
increasing complexity and variability from distant to nearby context. Motivated
by this observation, we propose Tree**KV**, an intuitive, training-free method that
employs a tree structure for smooth cache compression. Tree**KV** maintains a fixed
cache size, allowing LLMs to deliver high-quality output even in long text
scenarios. Unlike most compression methods, Tree**KV** is applicable to both the
generation and prefilling stages. It consistently surpasses all baseline models
in language modeling tasks on PG19 and OpenWebText2, allowing LLMs trained with
short context window to generalize to longer window with a 16x cache reduction.
On the Longbench benchmark, Tree**KV** achieves the best performance with only 6\%
of the budget at optimal efficiency.


## V2C-CBM Building Concept Bottlenecks with Vision-to-Concept Tokenizer

>Authors: Hangzhou He, Lei Zhu, Xinliang Zhang, Shuang Zeng, Qian Chen, Yanye Lu

>2025-01-09

> http://arxiv.org/abs/2501.04975v1

Concept Bottleneck Models (CBMs) offer inherent interpretability by initially
translating images into human-comprehensible concepts, followed by a linear
combination of these concepts for classification. However, the annotation of
concepts for visual recognition tasks requires extensive expert knowledge and
labor, constraining the broad adoption of CBMs. Recent approaches have
leveraged the knowledge of large language models to construct concept
bottlenecks, with multimodal models like CLIP subsequently mapping image
features into the concept feature space for classification. Despite this, the
concepts produced by language models can be verbose and may introduce
non-visual attributes, which hurts accuracy and interpretability. In this
study, we investigate to avoid these issues by constructing CBMs directly from
multimodal models. To this end, we adopt common words as base concept
vocabulary and leverage auxiliary unlabeled images to construct a
Vision-to-Concept (V2C) tokenizer that can explicitly **quantize** images into
their most relevant visual concepts, thus creating a vision-oriented concept
bottleneck tightly coupled with the multimodal model. This leads to our V2C-CBM
which is training efficient and interpretable with high accuracy. Our V2C-CBM
has matched or outperformed LLM-supervised CBMs on various visual
classification benchmarks, validating the efficacy of our approach.


## MyESL Sparse learning in molecular evolution and phylogenetic analysis

>Authors: Maxwell Sanderford, Sudip Sharma, Glen Stecher, Jun Liu, Jieping Ye, Sudhir Kumar

>2025-01-09

> http://arxiv.org/abs/2501.04941v1

Evolutionary **sparse** learning (ESL) uses a supervised machine learning
approach, Least Absolute Shrinkage and Selection Operator (LASSO), to build
models explaining the relationship between a hypothesis and the variation
across genomic features (e.g., sites) in sequence alignments. ESL employs
**sparsity** between and within the groups of genomic features (e.g., genomic loci)
by using **sparse**-group LASSO. Although some software packages are available for
performing **sparse** group LASSO, we found them less well-suited for processing
and analyzing genome-scale data containing millions of features, such as bases.
MyESL software fills the need for open-source software for conducting ESL
analyses with facilities to pre-process the input hypotheses and large
alignments, make LASSO flexible and computationally efficient, and post-process
the output model to produce different metrics useful in functional or
evolutionary genomics. MyESL can take phylogenetic trees and sequence
alignments as input and transform them into numeric responses and features,
respecetively. The model outputs are processed into user-friendly text and
graphical files. The computational core of MyESL is written in C++, which
offers model building with or without group **sparsity**, while the pre- and
post-processing of inputs and model outputs is performed using customized
functions written in Python. One of its applications in phylogenomics showcases
the utility of MyESL. Our analysis of empirical genome-scale datasets shows
that MyESL can build evolutionary models quickly and efficiently on a personal
desktop, while other computational packages were unable due to their
prohibitive requirements of computational resources and time. MyESL is
available for Python environments on Linux and distributed as a standalone
application for Windows and macOS. It is available from
https://github.com/kumarlabgit/MyESL.


## Intelligent experiments through real-time AI Fast Data Processing and Autonomous Detector Control for sPHENIX and future EIC detectors

>Authors: J. Kvapil, G. Borca-Tasciuc, H. Bossi, K. Chen, Y. Chen, Y. Corrales Morales, H. Da Costa, C. Da Silva, C. Dean, J. Durham, S. Fu, C. Hao, P. Harris, O. Hen, H. Jheng, Y. Lee, P. Li, X. Li, Y. Lin, M. X. Liu, V. Loncar, J. P. Mitrevski, A. Olvera, M. L. Purschke, J. S. Renck, G. Roland, J. Schambach, Z. Shi, N. Tran, N. Wuerfel, B. Xu, D. Yu, H. Zhang

>2025-01-08

> http://arxiv.org/abs/2501.04845v1

This R\&D project, initiated by the DOE Nuclear Physics AI-Machine Learning
initiative in 2022, leverages AI to address data processing challenges in
high-energy nuclear experiments (RHIC, LHC, and future EIC). Our focus is on
developing a demonstrator for real-time processing of high-rate data streams
from sPHENIX experiment tracking detectors. The limitations of a 15 kHz maximum
trigger rate imposed by the calorimeters can be negated by intelligent use of
streaming technology in the tracking system. The approach efficiently
identifies low momentum rare heavy flavor events in high-rate p+p collisions
(3MHz), using Graph Neural Network (GNN) and High Level Synthesis for Machine
Learning (hls4ml). Success at sPHENIX promises immediate benefits, minimizing
resources and accelerating the heavy-flavor measurements. The approach is
transferable to other fields. For the EIC, we develop a DIS-electron tagger
using Artificial Intelligence - Machine Learning (AI-ML) algorithms for
real-time identification, showcasing the transformative potential of AI and
FPGA technologies in high-energy nuclear and particle experiments real-time
data processing pipelines.


## ActPC-Geom Towards Scalable Online Neural-Symbolic Learning via Accelerating Active Predictive Coding with Information Geometry & Diverse Cognitive Mechanisms

>Authors: Ben Goertzel

>2025-01-08

> http://arxiv.org/abs/2501.04832v1

This paper introduces ActPC-Geom, an approach to accelerate Active Predictive
Coding (ActPC) in neural networks by integrating information geometry,
specifically using Wasserstein-metric-based methods for measure-dependent
gradient flows. We propose replacing KL-divergence in ActPC's predictive error
assessment with the Wasserstein metric, suggesting this may enhance network
robustness.
  To make this computationally feasible, we present strategies including: (1)
neural approximators for inverse measure-dependent Laplacians, (2) approximate
kernel PCA embeddings for low-rank approximations feeding into these
approximators, and (3) compositional hypervector embeddings derived from kPCA
outputs, with algebra optimized for fuzzy FCA lattices learned through neural
architectures analyzing network states.
  This results in an ActPC architecture capable of real-time online learning
and integrating continuous (e.g., transformer-like or Hopfield-net-like) and
discrete symbolic ActPC networks, including frameworks like OpenCog Hyperon or
ActPC-Chem for algorithmic chemistry evolution. Shared probabilistic,
concept-lattice, and hypervector models enable symbolic-subsymbolic
integration.
  Key features include (1) compositional reasoning via hypervector embeddings
in transformer-like architectures for tasks like commonsense reasoning, and (2)
Hopfield-net dynamics enabling associative long-term memory and
attractor-driven cognitive features.
  We outline how ActPC-Geom combines few-shot learning with online weight
updates, enabling deliberative thinking and seamless symbolic-subsymbolic
reasoning. Ideas from Galois connections are explored for efficient hybrid
ActPC/ActPC-Chem processing. Finally, we propose a specialized HPC design
optimized for real-time focused attention and deliberative reasoning tailored
to ActPC-Geom's demands.


## Entanglement in cyclic sign invariant quantum states

>Authors: Aabhas Gulati, Ion Nechita, Satvik Singh

>2025-01-08

> http://arxiv.org/abs/2501.04786v1

We introduce and study bipartite quantum states that are invariant under the
local action of the cyclic sign group. Due to symmetry, these states are **sparse**
and can be parameterized by a triple of vectors. Their important semi-definite
properties, such as positivity and positivity under partial transpose (PPT),
can be simply characterized in terms of these vectors and their discrete
Fourier transforms. We study in detail the entanglement properties of this
family of symmetric states, showing in particular that it contains PPT
entangled states. For states that are diagonal in the Dicke basis, deciding
separability is equivalent to a circulant version of the complete positivity
problem. We provide some geometric results for the PPT cone, showing in
particular that it is polyhedral. In local dimension less than 5, we completely
characterize these sets and construct entanglement witnesses; some partial
results are also obtained for d = 6, 7. Finally, we initiate the study of
cyclic sign covariant quantum channels, showing in particular that the PPT
squared conjecture holds for some of these maps.


## Discrete Wavelet Transform-Based Capsule Network for Hyperspectral Image Classification

>Authors: Zhiqiang Gao, Jiaqi Wang, Hangchi Shen, Zhihao Dou, Xiangbo Zhang, Kaizhu Huang

>2025-01-08

> http://arxiv.org/abs/2501.04643v1

Hyperspectral image (HSI) classification is a crucial technique for remote
sensing to build large-scale earth monitoring systems. HSI contains much more
information than traditional visual images for identifying the categories of
land covers. One recent feasible solution for HSI is to leverage CapsNets for
capturing spectral-spatial information. However, these methods require high
computational requirements due to the full connection architecture between
stacked capsule layers. To solve this problem, a DWT-CapsNet is proposed to
identify partial but important connections in CapsNet for a effective and
efficient HSI classification. Specifically, we integrate a tailored attention
mechanism into a Discrete Wavelet Transform (DWT)-based downsampling layer,
alleviating the information loss problem of conventional downsampling operation
in feature extractors. Moreover, we propose a novel multi-scale routing
algorithm that prunes a large proportion of connections in CapsNet. A capsule
pyramid fusion mechanism is designed to aggregate the spectral-spatial
relationships in multiple levels of granularity, and then a self-attention
mechanism is further conducted in a partially and locally connected
architecture to emphasize the meaningful relationships. As shown in the
experimental results, our method achieves state-of-the-art accuracy while
keeping lower computational demand regarding running time, flops, and the
number of parameters, rendering it an appealing choice for practical
implementation in HSI classification.


## Learnable Scaled Gradient Descent for Guaranteed Robust Tensor PCA

>Authors: Lanlan Feng, Ce Zhu, Yipeng Liu, Saiprasad Ravishankar, Longxiu Huang

>2025-01-08

> http://arxiv.org/abs/2501.04565v1

Robust tensor principal component analysis (RTPCA) aims to separate the
low-rank and **sparse** components from multi-dimensional data, making it an
essential technique in the signal processing and computer vision fields.
Recently emerging tensor singular value decomposition (t-SVD) has gained
considerable attention for its ability to better capture the low-rank structure
of tensors compared to traditional matrix SVD. However, existing methods often
rely on the computationally expensive tensor nuclear norm (TNN), which limits
their scalability for real-world tensors. To address this issue, we explore an
efficient scaled gradient descent (SGD) approach within the t-SVD framework for
the first time, and propose the RTPCA-SGD method. Theoretically, we rigorously
establish the recovery guarantees of RTPCA-SGD under mild assumptions,
demonstrating that with appropriate parameter selection, it achieves linear
convergence to the true low-rank tensor at a constant rate, independent of the
condition number. To enhance its practical applicability, we further propose a
learnable self-supervised deep unfolding model, which enables effective
parameter learning. Numerical experiments on both synthetic and real-world
datasets demonstrate the superior performance of the proposed methods while
maintaining competitive computational efficiency, especially consuming less
time than RTPCA-TNN.


## Multivariate Exploration of Metric Dilation

>Authors: Aritra Banik, Fedor V. Fomin, Petr A. Golovach, Tanmay Inamdar, Satyabrata Jana, Saket Saurabh

>2025-01-08

> http://arxiv.org/abs/2501.04555v1

Let $G$ be a weighted graph embedded in a metric space $(M, d_M )$. The
vertices of $G$ correspond to the points in $M$ , with the weight of each edge
$uv$ being the distance $d_M (u, v)$ between their respective points in $M$ .
The dilation (or stretch) of $G$ is defined as the minimum factor $t$ such
that, for any pair of vertices $u, v$, the distance between $u$ and
$v$-represented by the weight of a shortest $u$, $v$-path is at most $ t \cdot
d_M (u, v)$. We study Dilation t-Augmentation, where the objective is, given a
metric $M $, a graph $G$, and numerical values $k$ and $t$, to determine
whether $G$ can be transformed into a graph with dilation $t$ by adding at most
$k$ edges.
  Our primary focus is on the scenario where the metric $M$ is the shortest
path metric of an unweighted graph $\Gamma$. Even in this specific case,
Dilation $t$-Augmentation remains computationally challenging. In particular,
the problem is W[2]-hard parameterized by $k$ when $\Gamma$ is a complete
graph, already for $t=2$. Our main contribution lies in providing new insights
into the impact of combinations of various parameters on the computational
complexity of the problem. We establish the following.
  -- The parameterized dichotomy of the problem with respect to dilation $t$,
when the graph $G$ is **sparse**: Parameterized by $k$, the problem is FPT for
graphs excluding a biclique $K_{d,d}$ as a subgraph for $t\leq 2$ and the
problem is W[1]-hard for $t\geq 3$ even if $G$ is a forest consisting of
disjoint stars.
  -- The problem is FPT parameterized by the combined parameter $k+t+\Delta$,
where $\Delta$ is the maximum degree of the graph $G$ or $\Gamma$.


## Exact recovery in the double sparse model sufficient and necessary signal conditions

>Authors: Shixiang Liu, Zhifan Li, Yanhang Zhang, Jianxin Yin

>2025-01-08

> http://arxiv.org/abs/2501.04551v1

The double **sparse** linear model, which has both group-wise and element-wise
**sparsity** in regression coefficients, has attracted lots of attention recently.
This paper establishes the sufficient and necessary relationship between the
exact support recovery and the optimal minimum signal conditions in the double
**sparse** model. Specifically, sharply under the proposed signal conditions, a
two-stage double **sparse** iterative hard thresholding procedure achieves exact
support recovery with a suitably chosen threshold parameter. Also, this
procedure maintains asymptotic normality aligning with an OLS estimator given
true support, hence holding the oracle properties. Conversely, we prove that no
method can achieve exact support recovery if these signal conditions are
violated. This fills a critical gap in the minimax optimality theory on support
recovery of the double **sparse** model. Finally, numerical experiments are
provided to support our theoretical findings.


## The Impostor is Among Us Can Large Language Models Capture the Complexity of Human Personas?

>Authors: Christopher Lazik, Christopher Katins, Charlotte Kauter, Jonas Jakob, Caroline Jay, Lars Grunske, Thomas Kosch

>2025-01-08

> http://arxiv.org/abs/2501.04543v1

Large Language Models (LLMs) created new opportunities for generating
personas, which are expected to streamline and accelerate the human-centered
design process. Yet, AI-generated personas may not accurately represent actual
user experiences, as they can miss contextual and emotional insights critical
to understanding real users' needs and behaviors. This paper examines the
differences in how users perceive personas created by LLMs compared to those
crafted by humans regarding their credibility for design. We gathered ten
human-crafted personas developed by HCI experts according to relevant
attributes established in related work. Then, we systematically generated ten
personas and compared them with human-crafted ones in a survey. The results
showed that participants differentiated between human-created and AI-generated
personas, with the latter being perceived as more informative and consistent.
However, participants noted that the AI-generated personas tended to follow
stereotypes, highlighting the need for a greater emphasis on diversity when
utilizing LLMs for persona creation.


## A Plug-and-Play Bregman ADMM Module for Inferring Event Branches in Temporal Point Processes

>Authors: Qingmei Wang, Yuxin Wu, Yujie Long, Jing Huang, Fengyuan Ran, Bing Su, Hongteng Xu

>2025-01-08

> http://arxiv.org/abs/2501.04529v1

An event sequence generated by a temporal point process is often associated
with a hidden and structured event branching process that captures the
triggering relations between its historical and current events. In this study,
we design a new plug-and-play module based on the Bregman ADMM (BADMM)
algorithm, which infers event branches associated with event sequences in the
maximum likelihood estimation framework of temporal point processes (TPPs).
Specifically, we formulate the inference of event branches as an optimization
problem for the event transition matrix under **sparse** and low-rank constraints,
which is embedded in existing TPP models or their learning paradigms. We can
implement this optimization problem based on subspace clustering and **sparse**
group-lasso, respectively, and solve it using the Bregman ADMM algorithm, whose
unrolling leads to the proposed BADMM module. When learning a classic TPP
(e.g., Hawkes process) by the expectation-maximization algorithm, the BADMM
module helps derive structured responsibility matrices in the E-step.
Similarly, the BADMM module helps derive low-rank and **sparse** attention maps for
the neural TPPs with self-attention layers. The structured responsibility
matrices and attention maps, which work as learned event transition matrices,
indicate event branches, e.g., inferring isolated events and those key events
triggering many subsequent events. Experiments on both synthetic and real-world
data show that plugging our BADMM module into existing TPP models and learning
paradigms can improve model performance and provide us with interpretable
structured event branches. The code is available at
\url{https://github.com/qingmeiwangdaily/BADMM_TPP}.


## Integrating remote sensing data assimilation, deep learning and large language model for interactive wheat breeding yield prediction

>Authors: Guofeng Yang, Nanfei Jin, Wenjie Ai, Zhonghua Zheng, Yuhong He, Yong He

>2025-01-08

> http://arxiv.org/abs/2501.04487v1

Yield is one of the core goals of crop breeding. By predicting the potential
yield of different breeding materials, breeders can screen these materials at
various growth stages to select the best performing. Based on unmanned aerial
vehicle remote sensing technology, high-throughput crop phenotyping data in
breeding areas is collected to provide data support for the breeding decisions
of breeders. However, the accuracy of current yield predictions still requires
improvement, and the usability and user-friendliness of yield forecasting tools
remain suboptimal. To address these challenges, this study introduces a hybrid
method and tool for crop yield prediction, designed to allow breeders to
interactively and accurately predict wheat yield by chatting with a large
language model (LLM). First, the newly designed data assimilation algorithm is
used to assimilate the leaf area index into the WOFOST model. Then, selected
outputs from the assimilation process, along with remote sensing inversion
results, are used to drive the time-series temporal fusion transformer model
for wheat yield prediction. Finally, based on this hybrid method and leveraging
an LLM with retrieval augmented generation technology, we developed an
interactive yield prediction Web tool that is user-friendly and supports
sustainable data updates. This tool integrates multi-source data to assist
breeding decision-making. This study aims to accelerate the identification of
high-yield materials in the breeding process, enhance breeding efficiency, and
enable more scientific and smart breeding decisions.


## MB-TaylorFormer V2 Improved Multi-branch Linear Transformer Expanded by Taylor Formula for Image Restoration

>Authors: Zhi Jin, Yuwei Qiu, Kaihao Zhang, Hongdong Li, Wenhan Luo

>2025-01-08

> http://arxiv.org/abs/2501.04486v1

Recently, Transformer networks have demonstrated outstanding performance in
the field of image restoration due to the global receptive field and
adaptability to input. However, the quadratic computational complexity of
Softmax-attention poses a significant limitation on its extensive application
in image restoration tasks, particularly for high-resolution images. To tackle
this challenge, we propose a novel variant of the Transformer. This variant
leverages the Taylor expansion to approximate the Softmax-attention and
utilizes the concept of norm-preserving mapping to approximate the remainder of
the first-order Taylor expansion, resulting in a linear computational
complexity. Moreover, we introduce a multi-branch architecture featuring
multi-scale patch embedding into the proposed Transformer, which has four
distinct advantages: 1) various sizes of the receptive field; 2) multi-level
semantic information; 3) flexible shapes of the receptive field; 4) accelerated
training and inference speed. Hence, the proposed model, named the second
version of Taylor formula expansion-based Transformer (for short
MB-TaylorFormer V2) has the capability to concurrently process coarse-to-fine
features, capture long-distance pixel interactions with limited computational
cost, and improve the approximation of the Taylor expansion remainder.
Experimental results across diverse image restoration benchmarks demonstrate
that MB-TaylorFormer V2 achieves state-of-the-art performance in multiple image
restoration tasks, such as image dehazing, deraining, desnowing, motion
deblurring, and denoising, with very little computational overhead. The source
code is available at https://github.com/FVL2020/MB-TaylorFormerV2.


## RoRA Efficient Fine-Tuning of LLM with Reliability Optimization for Rank Adaptation

>Authors: Jun Liu, Zhenglun Kong, Peiyan Dong, Xuan Shen, Pu Zhao, Hao Tang, Geng Yuan, Wei Niu, Wenbin Zhang, Xue Lin, Dong Huang, Yanzhi Wang

>2025-01-08

> http://arxiv.org/abs/2501.04315v1

Fine-tuning helps large language models (LLM) recover degraded information
and enhance task performance.Although Low-Rank Adaptation (LoRA) is widely used
and effective for fine-tuning, we have observed that its scaling factor can
limit or even reduce performance as the rank size increases. To address this
issue, we propose RoRA (Rank-adaptive Reliability Optimization), a simple yet
effective method for optimizing LoRA's scaling factor. By replacing $\alpha/r$
with $\alpha/\sqrt{r}$, RoRA ensures improved performance as rank size
increases. Moreover, RoRA enhances low-rank adaptation in fine-tuning
uncompressed models and excels in the more challenging task of accuracy
recovery when fine-tuning pruned models. Extensive experiments demonstrate the
effectiveness of RoRA in fine-tuning both uncompressed and pruned models. RoRA
surpasses the state-of-the-art (SOTA) in average accuracy and robustness on
LLaMA-7B/13B, LLaMA2-7B, and LLaMA3-8B, specifically outperforming LoRA and
DoRA by 6.5% and 2.9% on LLaMA-7B, respectively. In pruned model fine-tuning,
RoRA shows significant advantages; for SHEARED-LLAMA-1.3, a LLaMA-7B with 81.4%
**pruning**, RoRA achieves 5.7% higher average accuracy than LoRA and 3.9% higher
than DoRA.


## FSC-loss A Frequency-domain Structure Consistency Learning Approach for Signal Data Recovery and Reconstruction

>Authors: Liwen Zhang, Zhaoji Miao, Fan Yang, Gen Shi, Jie He, Yu An, Hui Hui, Jie Tian

>2025-01-08

> http://arxiv.org/abs/2501.04308v1

A core challenge for signal data recovery is to model the distribution of
signal matrix (SM) data based on measured low-quality data in biomedical
engineering of magnetic particle imaging (MPI). For acquiring the
high-resolution (high-quality) SM, the number of meticulous measurements at
numerous positions in the field-of-view proves time-consuming (measurement of a
37x37x37 SM takes about 32 hours). To improve reconstructed signal quality and
shorten SM measurement time, existing methods explore to generating
high-resolution SM based on time-saving measured low-resolution SM (a 9x9x9 SM
just takes about 0.5 hours). However, previous methods show poor performance
for high-frequency signal recovery in SM. To achieve a high-resolution SM
recovery and shorten its acquisition time, we propose a frequency-domain
structure consistency loss function and data component embedding strategy to
model global and local structural information of SM. We adopt a
transformer-based network to evaluate this function and the strategy. We
evaluate our methods and state-of-the-art (SOTA) methods on the two simulation
datasets and four public measured SMs in Open MPI Data. The results show that
our method outperforms the SOTA methods in high-frequency structural signal
recovery. Additionally, our method can recover a high-resolution SM with clear
high-frequency structure based on a down-sampling factor of 16 less than 15
seconds, which accelerates the acquisition time over 60 times faster than the
measurement-based HR SM with the minimum error (nRMSE=0.041). Moreover, our
method is applied in our three in-house MPI systems, and boost their
performance for signal reconstruction.


## DGQ Distribution-Aware Group Quantization for Text-to-Image Diffusion Models

>Authors: Hyogon Ryu, NaHyeon Park, Hyunjung Shim

>2025-01-08

> http://arxiv.org/abs/2501.04304v1

Despite the widespread use of text-to-image diffusion models across various
tasks, their computational and memory demands limit practical applications. To
mitigate this issue, **quantization** of diffusion models has been explored. It
reduces memory usage and computational costs by compressing weights and
activations into lower-bit formats. However, existing methods often struggle to
preserve both image quality and text-image alignment, particularly in
lower-bit($<$ 8bits) **quantization**. In this paper, we analyze the challenges
associated with quantizing text-to-image diffusion models from a distributional
perspective. Our analysis reveals that activation outliers play a crucial role
in determining image quality. Additionally, we identify distinctive patterns in
cross-attention scores, which significantly affects text-image alignment. To
address these challenges, we propose Distribution-aware Group Quantization
(DGQ), a method that identifies and adaptively handles pixel-wise and
channel-wise outliers to preserve image quality. Furthermore, DGQ applies
prompt-specific logarithmic **quantization** scales to maintain text-image
alignment. Our method demonstrates remarkable performance on datasets such as
MS-COCO and PartiPrompts. We are the first to successfully achieve **low-bit**
**quantization** of text-to-image diffusion models without requiring additional
fine-tuning of weight **quantization** parameters.


## Dynamic Localisation of Spatial-Temporal Graph Neural Network

>Authors: Wenying Duan, Shujun Guo, Wei huang, Hong Rao, Xiaoxi He

>2025-01-08

> http://arxiv.org/abs/2501.04239v2

Spatial-temporal data, fundamental to many intelligent applications, reveals
dependencies indicating causal links between present measurements at specific
locations and historical data at the same or other locations. Within this
context, adaptive spatial-temporal graph neural networks (ASTGNNs) have emerged
as valuable tools for modelling these dependencies, especially through a
data-driven approach rather than pre-defined spatial graphs. While this
approach offers higher accuracy, it presents increased computational demands.
Addressing this challenge, this paper delves into the concept of localisation
within ASTGNNs, introducing an innovative perspective that spatial dependencies
should be dynamically evolving over time. We introduce \textit{DynAGS}, a
localised ASTGNN framework aimed at maximising efficiency and accuracy in
distributed deployment. This framework integrates dynamic localisation,
time-evolving spatial graphs, and personalised localisation, all orchestrated
around the Dynamic Graph Generator, a light-weighted central module leveraging
cross attention. The central module can integrate historical information in a
node-independent manner to enhance the feature representation of nodes at the
current moment. This improved feature representation is then used to generate a
dynamic **sparse** graph without the need for costly data exchanges, and it
supports personalised localisation. Performance assessments across two core
ASTGNN architectures and nine real-world datasets from various applications
reveal that \textit{DynAGS} outshines current benchmarks, underscoring that the
dynamic modelling of spatial dependencies can drastically improve model
expressibility, flexibility, and system efficiency, especially in distributed
settings.


## Agent Laboratory Using LLM Agents as Research Assistants

>Authors: Samuel Schmidgall, Yusheng Su, Ze Wang, Ximeng Sun, Jialian Wu, Xiaodong Yu, Jiang Liu, Zicheng Liu, Emad Barsoum

>2025-01-08

> http://arxiv.org/abs/2501.04227v1

Historically, scientific discovery has been a lengthy and costly process,
demanding substantial time and resources from initial conception to final
results. To accelerate scientific discovery, reduce research costs, and improve
research quality, we introduce Agent Laboratory, an autonomous LLM-based
framework capable of completing the entire research process. This framework
accepts a human-provided research idea and progresses through three
stages--literature review, experimentation, and report writing to produce
comprehensive research outputs, including a code repository and a research
report, while enabling users to provide feedback and guidance at each stage. We
deploy Agent Laboratory with various state-of-the-art LLMs and invite multiple
researchers to assess its quality by participating in a survey, providing human
feedback to guide the research process, and then evaluate the final paper. We
found that: (1) Agent Laboratory driven by o1-preview generates the best
research outcomes; (2) The generated machine learning code is able to achieve
state-of-the-art performance compared to existing methods; (3) Human
involvement, providing feedback at each stage, significantly improves the
overall quality of research; (4) Agent Laboratory significantly reduces
research expenses, achieving an 84% decrease compared to previous autonomous
research methods. We hope Agent Laboratory enables researchers to allocate more
effort toward creative ideation rather than low-level coding and writing,
ultimately accelerating scientific discovery.


## Graph classes through the lens of logic

>Authors: Michał Pilipczuk

>2025-01-07

> http://arxiv.org/abs/2501.04166v1

Graph transformations definable in logic can be described using the notion of
transductions. By understanding transductions as a basic embedding mechanism,
which captures the possibility of encoding one graph in another graph by means
of logical formulas, we obtain a new perspective on the landscape of graph
classes and of their properties. The aim of this survey is to give a
comprehensive presentation of this angle on structural graph theory.
  We first give a logic-focused overview of classic graph-theoretic concepts,
such as treedepth, shrubdepth, treewidth, cliquewidth, twin-width, bounded
expansion, and nowhere denseness. Then, we present recent developments related
to notions defined purely through transductions, such as monadic stability,
monadic dependence, and classes of structurally **sparse** graphs.


## KGIF Optimizing Relation-Aware Recommendations with Knowledge Graph Information Fusion

>Authors: Dong Hyun Jeon, Wenbo Sun, Houbing Herbert Song, Dongfang Liu, Velasquez Alvaro, Yixin Chloe Xie, Shuteng Niu

>2025-01-07

> http://arxiv.org/abs/2501.04161v1

While deep-learning-enabled recommender systems demonstrate strong
performance benchmarks, many struggle to adapt effectively in real-world
environments due to limited use of user-item relationship data and insufficient
transparency in recommendation generation. Traditional collaborative filtering
approaches fail to integrate multifaceted item attributes, and although
Factorization Machines account for item-specific details, they overlook broader
relational patterns. Collaborative knowledge graph-based models have progressed
by embedding user-item interactions with item-attribute relationships, offering
a holistic perspective on interconnected entities. However, these models
frequently aggregate attribute and interaction data in an implicit manner,
leaving valuable relational nuances underutilized.
  This study introduces the Knowledge Graph Attention Network with Information
Fusion (KGIF), a specialized framework designed to merge entity and relation
embeddings explicitly through a tailored self-attention mechanism. The KGIF
framework integrates reparameterization via dynamic projection vectors,
enabling embeddings to adaptively represent intricate relationships within
knowledge graphs. This explicit fusion enhances the interplay between user-item
interactions and item-attribute relationships, providing a nuanced balance
between user-centric and item-centric representations. An attentive propagation
mechanism further optimizes knowledge graph embeddings, capturing multi-layered
interaction patterns. The contributions of this work include an innovative
method for explicit information fusion, improved robustness for **sparse**
knowledge graphs, and the ability to generate explainable recommendations
through interpretable path visualization.


## A GPU Implementation of Multi-Guiding Spark Fireworks Algorithm for Efficient Black-Box Neural Network Optimization

>Authors: Xiangrui Meng, Ying Tan

>2025-01-07

> http://arxiv.org/abs/2501.03944v1

Swarm intelligence optimization algorithms have gained significant attention
due to their ability to solve complex optimization problems. However, the
efficiency of optimization in large-scale problems limits the use of related
methods. This paper presents a GPU-accelerated version of the Multi-Guiding
Spark Fireworks Algorithm (MGFWA), which significantly improves the
computational efficiency compared to its traditional CPU-based counterpart. We
benchmark the GPU-MGFWA on several neural network black-box optimization
problems and demonstrate its superior performance in terms of both speed and
solution quality. By leveraging the parallel processing power of modern GPUs,
the proposed GPU-MGFWA results in faster convergence and reduced computation
time for large-scale optimization tasks. The proposed implementation offers a
promising approach to accelerate swarm intelligence algorithms, making them
more suitable for real-time applications and large-scale industrial problems.
Source code is released at https://github.com/mxxxr/MGFWA.


## Dolphin Closed-loop Open-ended Auto-research through Thinking, Practice, and Feedback

>Authors: Jiakang Yuan, Xiangchao Yan, Botian Shi, Tao Chen, Wanli Ouyang, Bo Zhang, Lei Bai, Yu Qiao, Bowen Zhou

>2025-01-07

> http://arxiv.org/abs/2501.03916v2

The scientific research paradigm is undergoing a profound transformation
owing to the development of Artificial Intelligence (AI). Recent works
demonstrate that various AI-assisted research methods can largely improve
research efficiency by improving data analysis, accelerating computation, and
fostering novel idea generation. To further move towards the ultimate goal
(i.e., automatic scientific research), in this paper, we propose Dolphin, the
first closed-loop open-ended auto-research framework to further build the
entire process of human scientific research. Dolphin can generate research
ideas, perform experiments, and get feedback from experimental results to
generate higher-quality ideas. More specifically, Dolphin first generates novel
ideas based on relevant papers which are ranked by the topic and task
attributes. Then, the codes are automatically generated and debugged with the
exception-traceback-guided local code structure. Finally, Dolphin automatically
analyzes the results of each idea and feeds the results back to the next round
of idea generation. Experiments are conducted on the benchmark datasets of
different topics and results show that Dolphin can generate novel ideas
continuously and complete the experiment in a loop. We highlight that Dolphin
can automatically propose methods that are comparable to the state-of-the-art
in some tasks such as 2D image classification and 3D point classification.


## Computational Astrophysics, Data Science & AI/ML in Astronomy A Perspective from Indian Community

>Authors: Prateek Sharma, Bhargav Vaidya, Yogesh Wadadekar, Jasjeet Bagla, Piyali Chatterjee, Shravan Hanasoge, Prayush Kumar, Dipanjan Mukherjee, Ninan Sajeeth Philip, Nishant Singh

>2025-01-07

> http://arxiv.org/abs/2501.03876v1

In contemporary astronomy and astrophysics (A&A), the integration of
high-performance computing (HPC), big data analytics, and artificial
intelligence/machine learning (AI/ML) has become essential for advancing
research across a wide range of scientific domains. These tools are playing an
increasingly pivotal role in accelerating discoveries, simulating complex
astrophysical phenomena, and analyzing vast amounts of observational data. For
India to maintain and enhance its competitive edge in the global landscape of
computational astrophysics and data science, it is crucial for the Indian A&A
community to fully embrace these transformative technologies. Despite limited
resources, the expanding Indian community has already made significant
scientific contributions. However, to remain globally competitive in the coming
years, it is vital to establish a robust national framework that provides
researchers with reliable access to state-of-the-art computational resources.
This system should involve the regular solicitation of computational proposals,
which can be assessed by domain experts and HPC specialists, ensuring that
high-impact research receives the necessary support. By building such a system,
India can cultivate the talent, infrastructure, and collaborative environment
necessary to foster world-class research in computational astrophysics and data
science.


## A Sequential Optimal Learning Approach to Automated Prompt Engineering in Large Language Models

>Authors: Shuyang Wang, Somayeh Moazeni, Diego Klabjan

>2025-01-07

> http://arxiv.org/abs/2501.03508v1

Designing effective prompts is essential to guiding large language models
(LLMs) toward desired responses. Automated prompt engineering aims to reduce
reliance on manual effort by streamlining the design, refinement, and
optimization of natural language prompts. This paper proposes an optimal
learning framework for automated prompt engineering, designed to sequentially
identify effective prompt features while efficiently allocating a limited
evaluation budget. We introduce a feature-based method to express prompts,
which significantly broadens the search space. Bayesian regression is employed
to utilize correlations among similar prompts, accelerating the learning
process. To efficiently explore the large space of prompt features for a high
quality prompt, we adopt the forward-looking Knowledge-Gradient (KG) policy for
sequential optimal learning. The KG policy is computed efficiently by solving
mixed-integer second-order cone optimization problems, making it scalable and
capable of accommodating prompts characterized only through constraints. We
demonstrate that our method significantly outperforms a set of benchmark
strategies assessed on instruction induction tasks. The results highlight the
advantages of using the KG policy for prompt learning given a limited
evaluation budget. Our framework provides a solution to deploying automated
prompt engineering in a wider range applications where prompt evaluation is
costly.


## The Power of Negative Zero Datatype Customization for Quantized Large Language Models

>Authors: Yuzong Chen, Xilai Dai, Chi-chih Chang, Yash Akhauri, Mohamed S. Abdelfattah

>2025-01-06

> http://arxiv.org/abs/2501.04052v1

Large language models (LLMs) have demonstrated remarkable performance across
various machine learning tasks, quickly becoming one of the most prevalent AI
workloads. Yet the substantial memory requirement of LLMs significantly hinders
their deployment for end users. Post-training **quantization** (PTQ) serves as one
of the most hardware-efficient methods to mitigate the memory and computational
demands of LLMs. Although the traditional integer (INT) datatype has received
widespread adoption in PTQ methods, floating-point (FP) **quantization** has
emerged as a viable alternative thanks to its effectiveness in fitting LLM
numerical distributions. However, the FP datatype in sign-magnitude binary
representation contains both positive and negative zero, which constrains its
representation capability, particularly under low precision (3 and 4 bits). In
this paper, we extend the basic FP datatype to perform Redundant Zero Remapping
(RaZeR), which remaps the negative zero FP encoding to a set of pre-defined
special values to maximally utilize FP **quantization** encodings and to better fit
LLM numerical distributions. Through careful selection of special values, RaZeR
outperforms conventional asymmetric INT **quantization** while achieving high
computational efficiency. We demonstrate that RaZeR can be seamlessly
integrated with **quantization** algorithms for both weights and **KV**-cache,
including advanced methods with clipping and transformations, and consistently
achieve better model accuracy. Additionally, we implement a fast GEMV kernel
with fused de**quantization** that efficiently converts the 4-bit RaZeR value to
FP16 through novel bit-level manipulation. On modern GPUs, our evaluation shows
that RaZeR improves the GEMV speed by up to 7.56$\times$ compared to the FP16
implementation, while achieving up to 2.72$\times$ speedup in the LLM decoding
throughput.


## The Artificial Scientist -- in-transit Machine Learning of Plasma Simulations

>Authors: Jeffrey Kelling, Vicente Bolea, Michael Bussmann, Ankush Checkervarty, Alexander Debus, Jan Ebert, Greg Eisenhauer, Vineeth Gutta, Stefan Kesselheim, Scott Klasky, Richard Pausch, Norbert Podhorszki, Franz Poschel, David Rogers, Jeyhun Rustamov, Steve Schmerler, Ulrich Schramm, Klaus Steiniger, Rene Widera, Anna Willmann, Sunita Chandrasekaran

>2025-01-06

> http://arxiv.org/abs/2501.03383v1

Increasing HPC cluster sizes and large-scale simulations that produce
petabytes of data per run, create massive IO and storage challenges for
analysis. Deep learning-based techniques, in particular, make use of these
amounts of domain data to extract patterns that help build scientific
understanding. Here, we demonstrate a streaming workflow in which simulation
data is streamed directly to a machine-learning (ML) framework, circumventing
the file system bottleneck. Data is transformed in transit, asynchronously to
the simulation and the training of the model. With the presented workflow, data
operations can be performed in common and easy-to-use programming languages,
freeing the application user from adapting the application output routines. As
a proof-of-concept we consider a GPU accelerated particle-in-cell (PIConGPU)
simulation of the Kelvin- Helmholtz instability (KHI). We employ experience
replay to avoid catastrophic forgetting in learning from this non-steady
process in a continual manner. We detail challenges addressed while porting and
scaling to Frontier exascale system.


## Rethinking Byzantine Robustness in Federated Recommendation from Sparse Aggregation Perspective

>Authors: Zhongjian Zhang, Mengmei Zhang, Xiao Wang, Lingjuan Lyu, Bo Yan, Junping Du, Chuan Shi

>2025-01-06

> http://arxiv.org/abs/2501.03301v2

To preserve user privacy in recommender systems, federated recommendation
(FR) based on federated learning (FL) emerges, keeping the personal data on the
local client and updating a model collaboratively. Unlike FL, FR has a unique
**sparse** aggregation mechanism, where the embedding of each item is updated by
only partial clients, instead of full clients in a dense aggregation of general
FL. Recently, as an essential principle of FL, model security has received
increasing attention, especially for Byzantine attacks, where malicious clients
can send arbitrary updates. The problem of exploring the Byzantine robustness
of FR is particularly critical since in the domains applying FR, e.g.,
e-commerce, malicious clients can be injected easily by registering new
accounts. However, existing Byzantine works neglect the unique **sparse**
aggregation of FR, making them unsuitable for our problem. Thus, we make the
first effort to investigate Byzantine attacks on FR from the perspective of
**sparse** aggregation, which is non-trivial: it is not clear how to define
Byzantine robustness under **sparse** aggregations and design Byzantine attacks
under limited knowledge/capability. In this paper, we reformulate the Byzantine
robustness under **sparse** aggregation by defining the aggregation for a single
item as the smallest execution unit. Then we propose a family of effective
attack strategies, named Spattack, which exploit the vulnerability in **sparse**
aggregation and are categorized along the adversary's knowledge and capability.
Extensive experimental results demonstrate that Spattack can effectively
prevent convergence and even break down defenses under a few malicious clients,
raising alarms for securing FR systems.


## Gas bubble dynamics

>Authors: Dominique Legendre, Roberto Zenit

>2025-01-06

> http://arxiv.org/abs/2501.02988v1

The study of gas bubble dynamics in liquids is justified by the numerous
applications and natural phenomena where this two-phase flow is encountered.
Gas bubbles move as forces are applied to them; their dynamics are full of
nuances that need to be addressed carefully. Since the mass of gas bubbles is
practically negligible, in comparison to that of the surrounding liquid, their
reaction to the fluid is controlled by the added mass **acceleration** and is thus
impacted by all the forces arising from the fluid action. Furthermore, since
their surface can be deformed by the same forces acting on them, their shape
may change leading to changes in their resistance to move, the drag force, and
therefore affecting their speed and their interaction with the surrounding flow
which is often turbulent. The liquid rheology, as well as its surfactant
content can also affect the bubble shape and motion as well. Understanding
these issues, in addition to the effect of interactions with other bubbles,
walls, and non-uniform flows, provides sufficient elements to model and predict
bubble behavior through the solution of dynamic equations. In this review, we
cover the key aspects of non-condensable gas bubble dynamics. We survey
classical references on the subject and provide an overview of the main
findings in the past 20 years. We conclude with a scope and suggestions for
future research directions, with special attention to the dynamics of bubble in
turbulence, in non-Newtonian fluid and/or in the presence of electrolytes.


## Ultra-fast, high-power MUTC Photodiodes with bandwidth-efficiency product over 130 GHz * 100%

>Authors: Linze Li, Tianyu Long, Xiongwei Yang, Zhouze Zhang, Luyu Wang, Jingyi Wang, Mingxu Wang, Juanjuan Lu, Jianjun Yu, Baile Chen

>2025-01-06

> http://arxiv.org/abs/2501.02812v1

The accelerating demand for wireless communication necessitates wideband,
energy-efficient photonic sub-terahertz (sub-THz) sources to enable ultra-fast
data transfer. However, as critical components for THz photonic mixing,
photodiodes (PDs) face a fundamental trade-off between quantum efficiency and
bandwidth, presenting a major obstacle to achieving high-speed performance with
high optoelectronic conversion efficiency. Here, we overcome this challenge by
demonstrating an InP-based, waveguide-integrated modified uni-traveling carrier
photodiode (MUTC-PD) with a terahertz bandwidth exceeding 200 GHz and a
bandwidth-efficiency product (BEP) surpassing 130 GHz * 100%. Through the
integration of a spot-size converter (SSC) to enhance external responsivity,
alongside optimized electric field distribution, balanced carrier transport,
and minimized parasitic capacitance, the device achieves a 3-dB bandwidth of
206 GHz and an external responsivity of 0.8 A/W, setting a new benchmark for
BEP. Packaged with WR-5.1 waveguide output, it delivers radio-frequency (RF)
power exceeding -5 dBm across the 127-185 GHz frequency range. As a proof of
concept, we achieved a wireless transmission of 54 meters with a single-line
rate of up to 120 Gbps, leveraging photonics-aided technology without requiring
a low-noise amplifier (LNA). This work establishes a pathway to significantly
enhance optical power budgets and reduce energy consumption, presenting a
transformative step toward high-bandwidth, high-efficiency sub-THz
communication systems and next-generation wireless networks.


## Adaptive Pruning of Pretrained Transformer via Differential Inclusions

>Authors: Yizhuo Ding, Ke Fan, Yikai Wang, Xinwei Sun, Yanwei Fu

>2025-01-06

> http://arxiv.org/abs/2501.03289v1

Large transformers have demonstrated remarkable success, making it necessary
to compress these models to reduce inference costs while preserving their
perfor-mance. Current compression algorithms prune transformers at fixed
compression ratios, requiring a unique **pruning** process for each ratio, which
results in high computational costs. In contrast, we propose **pruning** of
pretrained transformers at any desired ratio within a single **pruning** stage,
based on a differential inclusion for a mask parameter. This dynamic can
generate the whole regularization solution path of the mask parameter, whose
support set identifies the network structure. Therefore, the solution path
identifies a Transformer weight family with various **sparsity** levels, offering
greater flexibility and customization. In this paper, we introduce such an
effective **pruning** method, termed SPP (Solution Path Pruning). To achieve
effective **pruning**, we segment the transformers into paired modules, including
query-key pairs, value-projection pairs, and sequential linear layers, and
apply low-rank compression to these pairs, maintaining the output structure
while enabling structural compression within the inner states. Extensive
experiments conducted on various well-known transformer backbones have
demonstrated the efficacy of SPP.


## Artificial Intelligence in Creative Industries Advances Prior to 2025

>Authors: Nantheera Anantrasirichai, Fan Zhang, David Bull

>2025-01-06

> http://arxiv.org/abs/2501.02725v1

The rapid advancements in artificial intelligence (AI), particularly in
generative AI and large language models (LLMs), have profoundly impacted the
creative industries by enabling innovative content creation, enhancing
workflows, and democratizing access to creative tools. This paper explores the
significant technological shifts since our previous review in 2022,
highlighting how these developments have expanded creative opportunities and
efficiency. These technological advancements have enhanced the capabilities of
text-to-image, text-to-video, and multimodal generation technologies. In
particular, key breakthroughs in LLMs have established new benchmarks in
conversational AI, while advancements in image generators have revolutionized
content creation. We also discuss AI integration into post-production
workflows, which has significantly accelerated and refined traditional
processes. Despite these innovations, challenges remain, particularly for the
media industry, due to the demands on communication traffic from creative
content. We therefore include data compression and quality assessment in this
paper. Furthermore, we highlight the trend toward unified AI frameworks capable
of addressing multiple creative tasks and underscore the importance of human
oversight to mitigate AI-generated inaccuracies. Finally, we explore AI's
future potential in the creative sector, stressing the need to navigate
emerging challenges to maximize its benefits while addressing associated risks.


## GS-DiT Advancing Video Generation with Pseudo 4D Gaussian Fields through Efficient Dense 3D Point Tracking

>Authors: Weikang Bian, Zhaoyang Huang, Xiaoyu Shi, Yijin Li, Fu-Yun Wang, Hongsheng Li

>2025-01-05

> http://arxiv.org/abs/2501.02690v1

4D video control is essential in video generation as it enables the use of
sophisticated lens techniques, such as multi-camera shooting and dolly zoom,
which are currently unsupported by existing methods. Training a video Diffusion
Transformer (DiT) directly to control 4D content requires expensive multi-view
videos. Inspired by Monocular Dynamic novel View Synthesis (MDVS) that
optimizes a 4D representation and renders videos according to different 4D
elements, such as camera pose and object motion editing, we bring pseudo 4D
Gaussian fields to video generation. Specifically, we propose a novel framework
that constructs a pseudo 4D Gaussian field with dense 3D point tracking and
renders the Gaussian field for all video frames. Then we finetune a pretrained
DiT to generate videos following the guidance of the rendered video, dubbed as
GS-DiT. To boost the training of the GS-DiT, we also propose an efficient Dense
3D Point Tracking (D3D-PT) method for the pseudo 4D Gaussian field
construction. Our D3D-PT outperforms SpatialTracker, the state-of-the-art
**sparse** 3D point tracking method, in accuracy and accelerates the inference
speed by two orders of magnitude. During the inference stage, GS-DiT can
generate videos with the same dynamic content while adhering to different
camera parameters, addressing a significant limitation of current video
generation models. GS-DiT demonstrates strong generalization capabilities and
extends the 4D controllability of Gaussian splatting to video generation beyond
just camera poses. It supports advanced cinematic effects through the
manipulation of the Gaussian field and camera intrinsics, making it a powerful
tool for creative video production. Demos are available at
https://wkbian.github.io/Projects/GS-DiT/.


## HALO Hadamard-Assisted Lossless Optimization for Efficient Low-Precision LLM Training and Fine-Tuning

>Authors: Saleh Ashkboos, Mahdi Nikdan, Soroush Tabesh, Roberto L. Castro, Torsten Hoefler, Dan Alistarh

>2025-01-05

> http://arxiv.org/abs/2501.02625v1

Quantized training of Large Language Models (LLMs) remains an open challenge,
as maintaining accuracy while performing all matrix multiplications in low
precision has proven difficult. This is particularly the case when fine-tuning
pre-trained models, which often already have large weight and activation
outlier values that render **quantize**d optimization difficult. We present HALO, a
novel **quantization**-aware training approach for Transformers that enables
accurate and efficient low-precision training by combining 1) strategic
placement of Hadamard rotations in both forward and backward passes, to
mitigate outliers during the low-precision computation, 2) FSDP integration for
low-precision communication, and 3) high-performance kernel support. Our
approach ensures that all large matrix multiplications during the forward and
backward passes are executed in lower precision. Applied to LLAMA-family
models, HALO achieves near-full-precision-equivalent results during fine-tuning
on various tasks, while delivering up to 1.31x end-to-end speedup for full
fine-tuning on RTX 4090 GPUs. Our method supports both standard and
parameter-efficient fine-tuning (PEFT) methods, both backed by efficient kernel
implementations. Our results demonstrate the first practical approach to fully
**quantize**d LLM fine-tuning that maintains accuracy in FP8 precision, while
delivering performance benefits.


## LeetDecoding A PyTorch Library for Exponentially Decaying Causal Linear Attention with CUDA Implementations

>Authors: Jiaping Wang, Simiao Zhang, Qiao-Chu He, Yifan Chen

>2025-01-05

> http://arxiv.org/abs/2501.02573v1

The machine learning and data science community has made significant while
dispersive progress in accelerating transformer-based large language models
(LLMs), and one promising approach is to replace the original causal attention
in a generative pre-trained transformer (GPT) with \emph{exponentially decaying
causal linear attention}. In this paper, we present LeetDecoding, which is the
first Python package that provides a large set of computation routines for this
fundamental operator. The launch of LeetDecoding was motivated by the current
lack of (1) clear understanding of the complexity regarding this operator, (2)
a comprehensive collection of existing computation methods (usually spread in
seemingly unrelated fields), and (3) CUDA implementations for fast inference on
GPU. LeetDecoding's design is easy to integrate with existing linear-attention
LLMs, and allows for researchers to benchmark and evaluate new computation
methods for exponentially decaying causal linear attention. The usage of
LeetDecoding does not require any knowledge of GPU programming and the
underlying complexity analysis, intentionally making LeetDecoding accessible to
LLM practitioners. The source code of LeetDecoding is provided at
\href{https://github.com/Computational-Machine-Intelligence/LeetDecoding}{this
GitHub repository}, and users can simply install LeetDecoding by the command
\texttt{pip install leet-decoding}.


## Rethinking Hard Thresholding Pursuit Full Adaptation and Sharp Estimation

>Authors: Yanhang Zhang, Zhifan Li, Shixiang Liu, Xueqin Wang, Jianxin Yin

>2025-01-05

> http://arxiv.org/abs/2501.02554v1

Hard Thresholding Pursuit (HTP) has aroused increasing attention for its
robust theoretical guarantees and impressive numerical performance in
non-convex optimization. In this paper, we introduce a novel tuning-free
procedure, named Full-Adaptive HTP (FAHTP), that simultaneously adapts to both
the unknown **sparsity** and signal strength of the underlying model. We provide an
in-depth analysis of the iterative thresholding dynamics of FAHTP, offering
refined theoretical insights. In specific, under the beta-min condition
$\min_{i \in S^*}|{\boldsymbol{\beta}}^*_i| \ge C\sigma (\log p/n)^{1/2}$, we
show that the FAHTP achieves oracle estimation rate $\sigma (s^*/n)^{1/2}$,
highlighting its theoretical superiority over convex competitors such as LASSO
and SLOPE, and recovers the true support set exactly. More importantly, even
without the beta-min condition, our method achieves a tighter error bound than
the classical minimax rate with high probability. The comprehensive numerical
experiments substantiate our theoretical findings, underscoring the
effectiveness and robustness of the proposed FAHTP.


## Predicting IoT Device Vulnerability Fix Times with Survival and Failure Time Models

>Authors: Carlos A Rivera A, Xinzhang Chen, Arash Shaghaghi, Gustavo Batista, Salil Kanhere

>2025-01-05

> http://arxiv.org/abs/2501.02520v1

The rapid integration of Internet of Things (IoT) devices into enterprise
environments presents significant security challenges. Many IoT devices are
released to the market with minimal security measures, often harbouring an
average of 25 vulnerabilities per device. To enhance cybersecurity measures and
aid system administrators in managing IoT patches more effectively, we propose
an innovative framework that predicts the time it will take for a vulnerable
IoT device to receive a fix or patch. We developed a survival analysis model
based on the Accelerated Failure Time (AFT) approach, implemented using the
XGBoost ensemble regression model, to predict when vulnerable IoT devices will
receive fixes or patches. By constructing a comprehensive IoT vulnerabilities
database that combines public and private sources, we provide insights into
affected devices, vulnerability detection dates, published CVEs, patch release
dates, and associated Twitter activity trends. We conducted thorough
experiments evaluating different combinations of features, including
fundamental device and vulnerability data, National Vulnerability Database
(NVD) information such as CVE, CWE, and CVSS scores, transformed textual
descriptions into sentence vectors, and the frequency of Twitter trends related
to CVEs. Our experiments demonstrate that the proposed model accurately
predicts the time to fix for IoT vulnerabilities, with data from VulDB and NVD
proving particularly effective. Incorporating Twitter trend data offered
minimal additional benefit. This framework provides a practical tool for
organisations to anticipate vulnerability resolutions, improve IoT patch
management, and strengthen their cybersecurity posture against potential
threats.


## Strategic Fusion Optimizes Transformer Compression

>Authors: Md Shoaibur Rahman

>2025-01-05

> http://arxiv.org/abs/2501.03273v1

This study investigates transformer model compression by systematically
**pruning** its layers. We evaluated 14 **pruning** strategies across nine diverse
datasets, including 12 strategies based on different signals obtained from
layer activations, mutual information, gradients, weights, and attention. To
address the limitations of single-signal strategies, we introduced two fusion
strategies, linear regression and random forest, which combine individual
strategies (i.e., strategic fusion), for more informed **pruning** decisions.
Additionally, we applied knowledge distillation to mitigate any accuracy loss
during layer **pruning**. Our results reveal that random forest strategic fusion
outperforms individual strategies in seven out of nine datasets and achieves
near-optimal performance in the other two. The distilled random forest
surpasses the original accuracy in six datasets and mitigates accuracy drops in
the remaining three. Knowledge distillation also improves the accuracy-to-size
ratio by an average factor of 18.84 across all datasets. Supported by
mathematical foundations and biological analogies, our findings suggest that
strategically combining multiple signals can lead to efficient, high-performing
transformer models for resource-constrained applications.


## Efficient Deployment of Large Language Models on Resource-constrained Devices

>Authors: Zhiwei Yao, Yang Xu, Hongli Xu, Yunming Liao, Zuan Xie

>2025-01-05

> http://arxiv.org/abs/2501.02438v1

Deploying Large Language Models (LLMs) on resource-constrained (or weak)
devices presents significant challenges due to limited resources and
heterogeneous data distribution. To address the data concern, it is necessary
to fine-tune LLMs using on-device private data for various downstream tasks.
While Federated Learning (FL) offers a promising privacy-preserving solution,
existing fine-tuning methods retain the original LLM size, leaving issues of
high inference latency and excessive memory demands unresolved. Hence, we
design FedSpine, an FL framework that combines Parameter- Efficient Fine-Tuning
(PEFT) with structured **pruning** for efficient deployment of LLMs on
resource-constrained devices. Specifically, FedSpine introduces an iterative
process to prune and tune the parameters of LLMs. To mitigate the impact of
device heterogeneity, an online Multi-Armed Bandit (MAB) algorithm is employed
to adaptively determine different **pruning** ratios and LoRA ranks for
heterogeneous devices without any prior knowledge of their computing and
communication capabilities. As a result, FedSpine maintains higher inference
accuracy while improving fine-tuning efficiency. Experimental results conducted
on a physical platform with 80 devices demonstrate that FedSpine can speed up
fine-tuning by 1.4$\times$-6.9$\times$ and improve final accuracy by 0.4%-4.5%
under the same **sparsity** level compared to other baselines.


## Scaling Laws for Floating Point Quantization Training

>Authors: Xingwu Sun, Shuaipeng Li, Ruobing Xie, Weidong Han, Kan Wu, Zhen Yang, Yixing Li, An Wang, Shuai Li, Jinbao Xue, Yu Cheng, Yangyu Tao, Zhanhui Kang, Chengzhong Xu, Di Wang, Jie Jiang

>2025-01-05

> http://arxiv.org/abs/2501.02423v1

Low-precision training is considered an effective strategy for reducing both
training and downstream inference costs. Previous scaling laws for precision
mainly focus on integer **quantization**, which pay less attention to the
constituents in floating-point **quantization** and thus cannot well fit the LLM
losses in this scenario. In contrast, while floating-point **quantization**
training is more commonly implemented in production, the research on it has
been relatively superficial. In this paper, we thoroughly explore the effects
of floating-point **quantization** targets, exponent bits, mantissa bits, and the
calculation granularity of the scaling factor in floating-point **quantization**
training performance of LLM models. While presenting an accurate floating-point
**quantization** unified scaling law, we also provide valuable suggestions for the
community: (1) Exponent bits contribute slightly more to the model performance
than mantissa bits. We provide the optimal exponent-mantissa bit ratio for
different bit numbers, which is available for future reference by hardware
manufacturers; (2) We discover the formation of the critical data size in
low-precision LLM training. Too much training data exceeding the critical data
size will inversely bring in degradation of LLM performance; (3) The optimal
floating-point **quantization** precision is directly proportional to the
computational power, but within a wide computational power range, we estimate
that the best cost-performance precision lies between 4-8 bits.


## Graph-Aware Isomorphic Attention for Adaptive Dynamics in Transformers

>Authors: Markus J. Buehler

>2025-01-04

> http://arxiv.org/abs/2501.02393v2

We present an approach to modifying Transformer architectures by integrating
graph-aware relational reasoning into the attention mechanism, merging concepts
from graph neural networks and language modeling. Building on the inherent
connection between attention and graph theory, we reformulate the Transformer's
attention mechanism as a graph operation and propose Graph-Aware Isomorphic
Attention. This method leverages advanced graph modeling strategies, including
Graph Isomorphism Networks (GIN) and Principal Neighborhood Aggregation (PNA),
to enrich the representation of relational structures. Our approach captures
complex dependencies and generalizes across tasks, as evidenced by a reduced
generalization gap and improved learning performance. Additionally, we expand
the concept of graph-aware attention to introduce Sparse GIN-Attention, a
fine-tuning approach that employs **sparse** GINs. By interpreting attention
matrices as **sparse** adjacency graphs, this technique enhances the adaptability
of pre-trained foundational models with minimal computational overhead,
endowing them with graph-aware capabilities. Sparse GIN-Attention fine-tuning
achieves improved training dynamics and better generalization compared to
alternative methods like low-rank adaption (LoRA). We discuss latent graph-like
structures within traditional attention mechanisms, offering a new lens through
which Transformers can be understood. By evolving Transformers as hierarchical
GIN models for relational reasoning. This perspective suggests profound
implications for foundational model development, enabling the design of
architectures that dynamically adapt to both local and global dependencies.
Applications in bioinformatics, materials science, language modeling, and
beyond could benefit from this synthesis of relational and sequential data
modeling, setting the stage for interpretable and generalizable modeling
strategies.


## Optimizing Small Language Models for In-Vehicle Function-Calling

>Authors: Yahya Sowti Khiabani, Farris Atif, Chieh Hsu, Sven Stahlmann, Tobias Michels, Sebastian Kramer, Benedikt Heidrich, M. Saquib Sarfraz, Julian Merten, Faezeh Tafazzoli

>2025-01-04

> http://arxiv.org/abs/2501.02342v1

We propose a holistic approach for deploying Small Language Models (SLMs) as
function-calling agents within vehicles as edge devices, offering a more
flexible and robust alternative to traditional rule-based systems. By
leveraging SLMs, we simplify vehicle control mechanisms and enhance the user
experience. Given the in-vehicle hardware constraints, we apply
state-of-the-art model compression techniques, including structured **pruning**,
healing, and **quantization**, ensuring that the model fits within the resource
limitations while maintaining acceptable performance. Our work focuses on
optimizing a representative SLM, Microsoft's Phi-3 mini, and outlines best
practices for enabling embedded models, including compression, task-specific
fine-tuning, and vehicle integration. We demonstrate that, despite significant
reduction in model size which removes up to 2 billion parameters from the
original model, our approach preserves the model's ability to handle complex
in-vehicle tasks accurately and efficiently. Furthermore, by executing the
model in a lightweight runtime environment, we achieve a generation speed of 11
tokens per second, making real-time, on-device inference feasible without
hardware **acceleration**. Our results demonstrate the potential of SLMs to
transform vehicle control systems, enabling more intuitive interactions between
users and their vehicles for an enhanced driving experience.


## AdaSkip Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference

>Authors: Zhuomin He, Yizhen Yao, Pengfei Zuo, Bin Gao, Qinya Li, Zhenzhe Zheng, Fan Wu

>2025-01-04

> http://arxiv.org/abs/2501.02336v1

Long-context large language models (LLMs) inference is increasingly critical,
motivating a number of studies devoted to alleviating the substantial storage
and computational costs in such scenarios. Layer-wise skipping methods are
promising optimizations but rarely explored in long-context inference. We
observe that existing layer-wise skipping strategies have several limitations
when applied in long-context inference, including the inability to adapt to
model and context variability, disregard for sublayer significance, and
inapplicability for the prefilling phase. This paper proposes \sysname, an
adaptive sublayer skipping method specifically designed for long-context
inference. \sysname adaptively identifies less important layers by leveraging
on-the-fly similarity information, enables sublayer-wise skipping, and
accelerates both the prefilling and decoding phases. The effectiveness of
\sysname is demonstrated through extensive experiments on various long-context
benchmarks and models, showcasing its superior inference performance over
existing baselines.


## Nuclear quantum effects slow down the energy transfer in biological light-harvesting complexes

>Authors: Johan E. Runeson, David E. Manolopoulos

>2025-01-04

> http://arxiv.org/abs/2501.02212v1

We assess how quantum-mechanical effects associated with high-frequency
chromophore vibrations influence excitation energy transfer in biological
light-harvesting complexes. We begin with a mixed quantum-classical theory that
combines a quantum description of the electronic motion with a classical
description of the nuclear motion in a way that is consistent with the
quantum-classical equilibrium distribution. We then include nuclear quantum
effects in this theory with a variational polaron transformation of the high
frequency vibrational modes. This approach is validated by comparison with
fully quantum mechanical benchmark calculations and then applied to three
prototypical biological light-harvesting complexes. We find that high-frequency
vibrations delay the energy transfer in the quantum treatment, but accelerate
it in the classical treatment. For the inter-ring transfer in the
light-harvesting complex 2 of purple bacteria, the transfer rate is a factor of
1.5 times slower in the quantum treatment than the classical. The transfer
timescale in the Fenna--Matthews--Olson complex is essentially the same in both
cases, whereas the transfer in light-harvesting complex II of spinach is 1.7
times slower in the quantum treatment. In all cases, the quantum mechanical
long-time equilibrium populations of the chromophores are well reproduced by
the classical treatment, suggesting that nuclear quantum effects are generally
unimportant for the directionality of energy transfer. Nuclear quantum effects
do however reduce the transfer rate in systems with large excitonic energy gaps
and strong vibronic coupling to high-frequency vibrational modes.


## The Application of Large Language Models in Recommendation Systems

>Authors: Peiyang Yu, Zeqiu Xu, Jiani Wang, Xiaochuan Xu

>2025-01-04

> http://arxiv.org/abs/2501.02178v1

The integration of Large Language Models into recommendation frameworks
presents key advantages for personalization and adaptability of experiences to
the users. Classic methods of recommendations, such as collaborative filtering
and content-based filtering, are seriously limited in the solution of
cold-start problems, **sparsity** of data, and lack of diversity in information
considered. LLMs, of which GPT-4 is a good example, have emerged as powerful
tools that enable recommendation frameworks to tap into unstructured data
sources such as user reviews, social interactions, and text-based content. By
analyzing these data sources, LLMs improve the accuracy and relevance of
recommendations, thereby overcoming some of the limitations of traditional
approaches. This work discusses applications of LLMs in recommendation systems,
especially in electronic commerce, social media platforms, streaming services,
and educational technologies. This showcases how LLMs enrich recommendation
diversity, user engagement, and the system's adaptability; yet it also looks
into the challenges connected to their technical implementation. This can also
be presented as a study that shows the potential of LLMs for changing user
experiences and making innovation possible in industries.


## Personalized Graph-Based Retrieval for Large Language Models

>Authors: Steven Au, Cameron J. Dimacali, Ojasmitha Pedirappagari, Namyong Park, Franck Dernoncourt, Yu Wang, Nikos Kanakaris, Hanieh Deilamsalehy, Ryan A. Rossi, Nesreen K. Ahmed

>2025-01-04

> http://arxiv.org/abs/2501.02157v1

As large language models (LLMs) evolve, their ability to deliver personalized
and context-aware responses offers transformative potential for improving user
experiences. Existing personalization approaches, however, often rely solely on
user history to augment the prompt, limiting their effectiveness in generating
tailored outputs, especially in cold-start scenarios with **sparse** data. To
address these limitations, we propose Personalized Graph-based
Retrieval-Augmented Generation (PGraphRAG), a framework that leverages
user-centric knowledge graphs to enrich personalization. By directly
integrating structured user knowledge into the retrieval process and augmenting
prompts with user-relevant context, PGraphRAG enhances contextual understanding
and output quality. We also introduce the Personalized Graph-based Benchmark
for Text Generation, designed to evaluate personalized text generation tasks in
real-world settings where user history is **sparse** or unavailable. Experimental
results show that PGraphRAG significantly outperforms state-of-the-art
personalization methods across diverse tasks, demonstrating the unique
advantages of graph-based retrieval for personalization.


## Instruction-Following Pruning for Large Language Models

>Authors: Bairu Hou, Qibin Chen, Jianyu Wang, Guoli Yin, Chong Wang, Nan Du, Ruoming Pang, Shiyu Chang, Tao Lei

>2025-01-03

> http://arxiv.org/abs/2501.02086v2

With the rapid scaling of large language models (LLMs), structured **pruning**
has become a widely used technique to learn efficient, smaller models from
larger ones, delivering superior performance compared to training similarly
sized models from scratch. In this paper, we move beyond the traditional static
**pruning** approach of determining a fixed **pruning** mask for a model, and propose a
dynamic approach to structured **pruning**. In our method, the **pruning** mask is
input-dependent and adapts dynamically based on the information described in a
user instruction. Our approach, termed "instruction-following **pruning**",
introduces a **sparse** mask predictor that takes the user instruction as input and
dynamically selects the most relevant model parameters for the given task. To
identify and activate effective parameters, we jointly optimize the **sparse** mask
predictor and the LLM, leveraging both instruction-following data and the
pre-training corpus. Experimental results demonstrate the effectiveness of our
approach on a wide range of evaluation benchmarks. For example, our 3B
activated model improves over the 3B dense model by 5-8 points of absolute
margin on domains such as math and coding, and rivals the performance of a 9B
model.


## VITA-1.5 Towards GPT-4o Level Real-Time Vision and Speech Interaction

>Authors: Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Yangze Li, Zuwei Long, Heting Gao, Ke Li, Xiawu Zheng, Rongrong Ji, Xing Sun, Caifeng Shan, Ran He

>2025-01-03

> http://arxiv.org/abs/2501.01957v1

Recent Multimodal Large Language Models (MLLMs) have typically focused on
integrating visual and textual modalities, with less emphasis placed on the
role of speech in enhancing interaction. However, speech plays a crucial role
in multimodal dialogue systems, and implementing high-performance in both
vision and speech tasks remains a significant challenge due to the fundamental
modality differences. In this paper, we propose a carefully designed
multi-stage training methodology that progressively trains LLM to understand
both visual and speech information, ultimately enabling fluent vision and
speech interaction. Our approach not only preserves strong vision-language
capacity, but also enables efficient speech-to-speech dialogue capabilities
without separate ASR and TTS modules, significantly accelerating multimodal
end-to-end response speed. By comparing our method against state-of-the-art
counterparts across benchmarks for image, video, and speech tasks, we
demonstrate that our model is equipped with both strong visual and speech
capabilities, making near real-time vision and speech interaction.


## Compressed sensing for inverse problems II applications to deconvolution, source recovery, and MRI

>Authors: Giovanni S. Alberti, Alessandro Felisi, Matteo Santacesaria, S. Ivan Trapasso

>2025-01-03

> http://arxiv.org/abs/2501.01929v1

This paper extends the sample complexity theory for ill-posed inverse
problems developed in a recent work by the authors [`Compressed sensing for
inverse problems and the sample complexity of the **sparse** Radon transform', J.
Eur. Math. Soc., to appear], which was originally focused on the **sparse** Radon
transform. We demonstrate that the underlying abstract framework, based on
infinite-dimensional compressed sensing and generalized sampling techniques,
can effectively handle a variety of practical applications. Specifically, we
analyze three case studies: (1) The reconstruction of a **sparse** signal from a
finite number of pointwise blurred samples; (2) The recovery of the (**sparse**)
source term of an elliptic partial differential equation from finite samples of
the solution; and (3) A moderately ill-posed variation of the classical sensing
problem of recovering a wavelet-**sparse** signal from finite Fourier samples,
motivated by magnetic resonance imaging. For each application, we establish
rigorous recovery guarantees by verifying the key theoretical requirements,
including quasi-diagonalization and coherence bounds. Our analysis reveals that
careful consideration of balancing properties and optimized sampling strategies
can lead to improved reconstruction performance. The results provide a unified
theoretical foundation for compressed sensing approaches to inverse problems
while yielding practical insights for specific applications.


## EnerVerse Envisioning Embodied Future Space for Robotics Manipulation

>Authors: Siyuan Huang, Liliang Chen, Pengfei Zhou, Shengcong Chen, Zhengkai Jiang, Yue Hu, Peng Gao, Hongsheng Li, Maoqing Yao, Guanghui Ren

>2025-01-03

> http://arxiv.org/abs/2501.01895v1

We introduce EnerVerse, a comprehensive framework for embodied future space
generation specifically designed for robotic manipulation tasks. EnerVerse
seamlessly integrates convolutional and bidirectional attention mechanisms for
inner-chunk space modeling, ensuring low-level consistency and continuity.
Recognizing the inherent redundancy in video data, we propose a **sparse** memory
context combined with a chunkwise unidirectional generative paradigm to enable
the generation of infinitely long sequences. To further augment robotic
capabilities, we introduce the Free Anchor View (FAV) space, which provides
flexible perspectives to enhance observation and analysis. The FAV space
mitigates motion modeling ambiguity, removes physical constraints in confined
environments, and significantly improves the robot's generalization and
adaptability across various tasks and settings. To address the prohibitive
costs and labor intensity of acquiring multi-camera observations, we present a
data engine pipeline that integrates a generative model with 4D Gaussian
Splatting (4DGS). This pipeline leverages the generative model's robust
generalization capabilities and the spatial constraints provided by 4DGS,
enabling an iterative enhancement of data quality and diversity, thus creating
a data flywheel effect that effectively narrows the sim-to-real gap. Finally,
our experiments demonstrate that the embodied future space generation prior
substantially enhances policy predictive capabilities, resulting in improved
overall performance, particularly in long-range robotic manipulation tasks.


## Multi-Agent Conversational Online Learning for Adaptive LLM Response Identification

>Authors: Xiangxiang Dai, Yuejin Xie, Maoli Liu, Xuchuang Wang, Zhuohua Li, Huanyu Wang, John C. S. Lui

>2025-01-03

> http://arxiv.org/abs/2501.01849v1

The remarkable generative capability of large language models (LLMs) has
sparked a growing interest in automatically generating responses for different
applications. Given the dynamic nature of user preferences and the uncertainty
of LLM response performance, it is crucial to design efficient online learning
algorithms to identify optimal LLM responses (i.e., high-quality responses that
also meet user preferences). Most existing online algorithms adopt a
centralized approach and fail to leverage explicit user preferences for more
efficient and personalized LLM response identification. In contrast, this paper
introduces \textit{MACO} (\underline{M}ulti-\underline{A}gent
\underline{C}onversational \underline{O}nline Learning for Adaptive LLM
Response Identification): 1) The online LLM response identification process is
accelerated by multiple local agents (such as smartphones), while enhancing
data privacy; 2) A novel conversational mechanism is proposed to adaptively
conduct conversations for soliciting user preferences (e.g., a preference for a
humorous tone over a serious one in generated responses), so to minimize
uncertainty in preference estimation. Our theoretical analysis demonstrates
that \cadi\ is near-optimal regarding cumulative regret. Additionally, \cadi\
offers reduced communication costs and computational complexity by eliminating
the traditional, computing-intensive ``G-optimal design" found in previous
works. Extensive experiments with the open LLM \textit{Llama}, coupled with two
different embedding models from Google and OpenAI for text vector
representation, demonstrate that \cadi\ significantly outperforms the current
state-of-the-art in online LLM response identification.


## Auto-RT Automatic Jailbreak Strategy Exploration for Red-Teaming Large Language Models

>Authors: Yanjiang Liu, Shuhen Zhou, Yaojie Lu, Huijia Zhu, Weiqiang Wang, Hongyu Lin, Ben He, Xianpei Han, Le Sun

>2025-01-03

> http://arxiv.org/abs/2501.01830v1

Automated red-teaming has become a crucial approach for uncovering
vulnerabilities in large language models (LLMs). However, most existing methods
focus on isolated safety flaws, limiting their ability to adapt to dynamic
defenses and uncover complex vulnerabilities efficiently. To address this
challenge, we propose Auto-RT, a reinforcement learning framework that
automatically explores and optimizes complex attack strategies to effectively
uncover security vulnerabilities through malicious queries. Specifically, we
introduce two key mechanisms to reduce exploration complexity and improve
strategy optimization: 1) Early-terminated Exploration, which accelerate
exploration by focusing on high-potential attack strategies; and 2) Progressive
Reward Tracking algorithm with intermediate downgrade models, which dynamically
refine the search trajectory toward successful vulnerability exploitation.
Extensive experiments across diverse LLMs demonstrate that, by significantly
improving exploration efficiency and automatically optimizing attack
strategies, Auto-RT detects a boarder range of vulnerabilities, achieving a
faster detection speed and 16.63\% higher success rates compared to existing
methods.


## Efficient LLM Inference with Activation Checkpointing and Hybrid Caching

>Authors: Sanghyeon Lee, Hongbeen Kim, Soojin Hwang, Guseul Heo, Minwoo Noh, Jaehyuk Huh

>2025-01-03

> http://arxiv.org/abs/2501.01792v1

Recent large language models (LLMs) with enormous model sizes use many GPUs
to meet memory capacity requirements incurring substantial costs for token
generation. To provide cost-effective LLM inference with relaxed latency
constraints, extensive research has focused on expanding GPU memory by
leveraging the host memory. However, LLM inference engines that utilize the
host memory often face underutilization of GPU compute units, as a considerable
portion of inference time is spent in loading the model onto the GPU via
host-GPU interconnect. To tackle these challenges of the host memory offloading
for LLM, we introduce HybridServe, an LLM inference system with activation
checkpointing based on activation caching. The activation cache stores
activation checkpoints generated during intermediate inference stages, allowing
the fast recomputation of **KV** cache while model parameters are transferred to
GPU from host memory. Unlike conventional methods that recompute the **KV** cache
from scratch using token IDs, the activation cache allows bypassing projection
and FFN operations. To balance between the activation recomputation and
parameter loading overhead, this study proposes a **KV**-activation hybrid caching
scheme which finds the best ratio of the key-value and activation caches to
adjust the recomputation time. Our system achieves 2.19x throughput improvement
over the state-of-the-art prior work for offloading both model weights and **KV**
cache.


## Compressed Domain Prior-Guided Video Super-Resolution for Cloud Gaming Content

>Authors: Qizhe Wang, Qian Yin, Zhimeng Huang, Weijia Jiang, Yi Su, Siwei Ma, Jiaqi Zhang

>2025-01-03

> http://arxiv.org/abs/2501.01773v1

Cloud gaming is an advanced form of Internet service that necessitates local
terminals to decode within limited resources and time latency. Super-Resolution
(SR) techniques are often employed on these terminals as an efficient way to
reduce the required bit-rate bandwidth for cloud gaming. However, insufficient
attention has been paid to SR of compressed game video content. Most SR
networks amplify block artifacts and ringing effects in decoded frames while
ignoring edge details of game content, leading to unsatisfactory reconstruction
results. In this paper, we propose a novel lightweight network called Coding
Prior-Guided Super-Resolution (CPGSR) to address the SR challenges in
compressed game video content. First, we design a Compressed Domain Guided
Block (CDGB) to extract features of different depths from coding priors, which
are subsequently integrated with features from the U-net backbone. Then, a
series of re-parameterization blocks are utilized for reconstruction.
Ultimately, inspired by the **quantization** in video coding, we propose a
partitioned focal frequency loss to effectively guide the model's focus on
preserving high-frequency information. Extensive experiments demonstrate the
advancement of our approach.


## Spot Risks Before Speaking! Unraveling Safety Attention Heads in Large Vision-Language Models

>Authors: Ziwei Zheng, Junyao Zhao, Le Yang, Lijun He, Fan Li

>2025-01-03

> http://arxiv.org/abs/2501.02029v1

With the integration of an additional modality, large vision-language models
(LVLMs) exhibit greater vulnerability to safety risks (e.g., jailbreaking)
compared to their language-only predecessors. Although recent studies have
devoted considerable effort to the post-hoc alignment of LVLMs, the inner
safety mechanisms remain largely unexplored. In this paper, we discover that
internal activations of LVLMs during the first token generation can effectively
identify malicious prompts across different attacks. This inherent safety
perception is governed by **sparse** attention heads, which we term ``safety
heads." Further analysis reveals that these heads act as specialized shields
against malicious prompts; ablating them leads to higher attack success rates,
while the model's utility remains unaffected. By locating these safety heads
and concatenating their activations, we construct a straightforward but
powerful malicious prompt detector that integrates seamlessly into the
generation process with minimal extra inference overhead. Despite its simple
structure of a logistic regression model, the detector surprisingly exhibits
strong zero-shot generalization capabilities. Experiments across various
prompt-based attacks confirm the effectiveness of leveraging safety heads to
protect LVLMs. Code is available at \url{https://github.com/Ziwei-Zheng/SAHs}.


## PSYCHE A Multi-faceted Patient Simulation Framework for Evaluation of Psychiatric Assessment Conversational Agents

>Authors: Jingoo Lee, Kyungho Lim, Young-Chul Jung, Byung-Hoon Kim

>2025-01-03

> http://arxiv.org/abs/2501.01594v1

Recent advances in large language models (LLMs) have accelerated the
development of conversational agents capable of generating human-like
responses. Since psychiatric assessments typically involve complex
conversational interactions between psychiatrists and patients, there is
growing interest in developing LLM-based psychiatric assessment conversational
agents (PACAs) that aim to simulate the role of psychiatrists in clinical
evaluations. However, standardized methods for benchmarking the clinical
appropriateness of PACAs' interaction with patients still remain underexplored.
Here, we propose PSYCHE, a novel framework designed to enable the 1) clinically
relevant, 2) ethically safe, 3) cost-efficient, and 4) quantitative evaluation
of PACAs. This is achieved by simulating psychiatric patients based on a
multi-faceted psychiatric construct that defines the simulated patients'
profiles, histories, and behaviors, which PACAs are expected to assess. We
validate the effectiveness of PSYCHE through a study with 10 board-certified
psychiatrists, supported by an in-depth analysis of the simulated patient
utterances.

