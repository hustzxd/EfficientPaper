# 2025-02-14

# Table of Contents
* [Scalable Thermodynamic Second-order Optimization](#Scalable-Thermodynamic-Second-order-Optimization)
* [The MoE-Empowered Edge LLMs Deployment Architecture, Challenges, and Opportunities](#The-MoE-Empowered-Edge-LLMs-Deployment-Architecture,-Challenges,-and-Opportunities)
* [Top-Theta Attention Sparsifying Transformers by Compensated Thresholding](#Top-Theta-Attention-Sparsifying-Transformers-by-Compensated-Thresholding)
* [Exploiting Non-uniform Quantization for Enhanced ILC in Wideband Digital Pre-distortion](#Exploiting-Non-uniform-Quantization-for-Enhanced-ILC-in-Wideband-Digital-Pre-distortion)
* [Contextual Compression Encoding for Large Language Models A Novel Framework for Multi-Layered Parameter Space Pruning](#Contextual-Compression-Encoding-for-Large-Language-Models-A-Novel-Framework-for-Multi-Layered-Parameter-Space-Pruning)
* [HDT Hierarchical Discrete Transformer for Multivariate Time Series Forecasting](#HDT-Hierarchical-Discrete-Transformer-for-Multivariate-Time-Series-Forecasting)
* [Inference-time sparse attention with asymmetric indexing](#Inference-time-sparse-attention-with-asymmetric-indexing)
* [Intention is All You Need Refining Your Code from Your Intention](#Intention-is-All-You-Need-Refining-Your-Code-from-Your-Intention)
* [LowRA Accurate and Efficient LoRA Fine-Tuning of LLMs under 2 Bits](#LowRA-Accurate-and-Efficient-LoRA-Fine-Tuning-of-LLMs-under-2-Bits)
* [HexGen-2 Disaggregated Generative Inference of LLMs in Heterogeneous Environment](#HexGen-2-Disaggregated-Generative-Inference-of-LLMs-in-Heterogeneous-Environment)
* [DarwinLM Evolutionary Structured Pruning of Large Language Models](#DarwinLM-Evolutionary-Structured-Pruning-of-Large-Language-Models)
* [Breaking Down Bias On The Limits of Generalizable Pruning Strategies](#Breaking-Down-Bias-On-The-Limits-of-Generalizable-Pruning-Strategies)
* [TransMLA Multi-Head Latent Attention Is All You Need](#TransMLA-Multi-Head-Latent-Attention-Is-All-You-Need)
* [BalanceKV KV Cache Compression through Discrepancy Theory](#BalanceKV-KV-Cache-Compression-through-Discrepancy-Theory)
* [MAAT Mamba Adaptive Anomaly Transformer with association discrepancy for time series](#MAAT-Mamba-Adaptive-Anomaly-Transformer-with-association-discrepancy-for-time-series)
* [Tractable Transformers for Flexible Conditional Generation](#Tractable-Transformers-for-Flexible-Conditional-Generation)
* [DSV Exploiting Dynamic Sparsity to Accelerate Large-Scale Video DiT Training](#DSV-Exploiting-Dynamic-Sparsity-to-Accelerate-Large-Scale-Video-DiT-Training)
* [5D Neural Surrogates for Nonlinear Gyrokinetic Simulations of Plasma Turbulence](#5D-Neural-Surrogates-for-Nonlinear-Gyrokinetic-Simulations-of-Plasma-Turbulence)
* [DEG Efficient Hybrid Vector Search Using the Dynamic Edge Navigation Graph](#DEG-Efficient-Hybrid-Vector-Search-Using-the-Dynamic-Edge-Navigation-Graph)
* [CodeI/O Condensing Reasoning Patterns via Code Input-Output Prediction](#CodeI/O-Condensing-Reasoning-Patterns-via-Code-Input-Output-Prediction)
* [Learning Inverse Laplacian Pyramid for Progressive Depth Completion](#Learning-Inverse-Laplacian-Pyramid-for-Progressive-Depth-Completion)
* [Physics-Informed Recurrent Network for Gas Pipeline Network Parameters Identification](#Physics-Informed-Recurrent-Network-for-Gas-Pipeline-Network-Parameters-Identification)
* [SparseFormer Detecting Objects in HRW Shots via Sparse Vision Transformer](#SparseFormer-Detecting-Objects-in-HRW-Shots-via-Sparse-Vision-Transformer)
* [Exploring Neural Network Pruning with Screening Methods](#Exploring-Neural-Network-Pruning-with-Screening-Methods)
* [Foreign-Object Detection in High-Voltage Transmission Line Based on Improved YOLOv8m](#Foreign-Object-Detection-in-High-Voltage-Transmission-Line-Based-on-Improved-YOLOv8m)
* [SHARP Accelerating Language Model Inference by SHaring Adjacent layers with Recovery Parameters](#SHARP-Accelerating-Language-Model-Inference-by-SHaring-Adjacent-layers-with-Recovery-Parameters)
* [Cardiverse Harnessing LLMs for Novel Card Game Prototyping](#Cardiverse-Harnessing-LLMs-for-Novel-Card-Game-Prototyping)
* [Online Scheduling for LLM Inference with KV Cache Constraints](#Online-Scheduling-for-LLM-Inference-with-KV-Cache-Constraints)
* [Specializing Large Language Models to Simulate Survey Response Distributions for Global Populations](#Specializing-Large-Language-Models-to-Simulate-Survey-Response-Distributions-for-Global-Populations)
* [Federated Sinkhorn](#Federated-Sinkhorn)
* [Demystifying Singular Defects in Large Language Models](#Demystifying-Singular-Defects-in-Large-Language-Models)
* [Exploiting Sparsity for Long Context Inference Million Token Contexts on Commodity GPUs](#Exploiting-Sparsity-for-Long-Context-Inference-Million-Token-Contexts-on-Commodity-GPUs)
* [Gradient Multi-Normalization for Stateless and Scalable LLM Training](#Gradient-Multi-Normalization-for-Stateless-and-Scalable-LLM-Training)
* [XAMBA Enabling Efficient State Space Models on Resource-Constrained Neural Processing Units](#XAMBA-Enabling-Efficient-State-Space-Models-on-Resource-Constrained-Neural-Processing-Units)
* [GraNNite Enabling High-Performance Execution of Graph Neural Networks on Resource-Constrained Neural Processing Units](#GraNNite-Enabling-High-Performance-Execution-of-Graph-Neural-Networks-on-Resource-Constrained-Neural-Processing-Units)
* [EfficientLLM Scalable Pruning-Aware Pretraining for Architecture-Agnostic Edge Language Models](#EfficientLLM-Scalable-Pruning-Aware-Pretraining-for-Architecture-Agnostic-Edge-Language-Models)
* [MoETuner Optimized Mixture of Expert Serving with Balanced Expert Placement and Token Routing](#MoETuner-Optimized-Mixture-of-Expert-Serving-with-Balanced-Expert-Placement-and-Token-Routing)
* [TripoSG High-Fidelity 3D Shape Synthesis using Large-Scale Rectified Flow Models](#TripoSG-High-Fidelity-3D-Shape-Synthesis-using-Large-Scale-Rectified-Flow-Models)
* [UniMoD Efficient Unified Multimodal Transformers with Mixture-of-Depths](#UniMoD-Efficient-Unified-Multimodal-Transformers-with-Mixture-of-Depths)
* [Rethinking Large-scale Dataset Compression Shifting Focus From Labels to Images](#Rethinking-Large-scale-Dataset-Compression-Shifting-Focus-From-Labels-to-Images)
* [Systematic Outliers in Large Language Models](#Systematic-Outliers-in-Large-Language-Models)
* [Utilizing Novelty-based Evolution Strategies to Train Transformers in Reinforcement Learning](#Utilizing-Novelty-based-Evolution-Strategies-to-Train-Transformers-in-Reinforcement-Learning)
* [Progressive Collaborative and Semantic Knowledge Fusion for Generative Recommendation](#Progressive-Collaborative-and-Semantic-Knowledge-Fusion-for-Generative-Recommendation)
* [Efficient-vDiT Efficient Video Diffusion Transformers With Attention Tile](#Efficient-vDiT-Efficient-Video-Diffusion-Transformers-With-Attention-Tile)
* [A CT Geometry With Multiple Centers Of Rotation For Solving Sparse View Problem](#A-CT-Geometry-With-Multiple-Centers-Of-Rotation-For-Solving-Sparse-View-Problem)
* [Real-Time LiDAR Point Cloud Compression and Transmission for Resource-constrained Robots](#Real-Time-LiDAR-Point-Cloud-Compression-and-Transmission-for-Resource-constrained-Robots)
* [Enabling Autoregressive Models to Fill In Masked Tokens](#Enabling-Autoregressive-Models-to-Fill-In-Masked-Tokens)
* [Electric field control of nonlinear Hall effect in Weyl semimetal TaIrTe4](#Electric-field-control-of-nonlinear-Hall-effect-in-Weyl-semimetal-TaIrTe4)
* [Acceleration Multiple Heads Decoding for LLM via Dynamic Tree Attention](#Acceleration-Multiple-Heads-Decoding-for-LLM-via-Dynamic-Tree-Attention)
* [A Universal Transformer-Based Coarse-Grained Molecular Dynamics Framework for Protein Dynamics](#A-Universal-Transformer-Based-Coarse-Grained-Molecular-Dynamics-Framework-for-Protein-Dynamics)
* [Zak-Transform-Induced Optimal Sequences and Their Applications in OTFS](#Zak-Transform-Induced-Optimal-Sequences-and-Their-Applications-in-OTFS)
* [MetaML-Pro Cross-Stage Design Flow Automation for Efficient Deep Learning Acceleration](#MetaML-Pro-Cross-Stage-Design-Flow-Automation-for-Efficient-Deep-Learning-Acceleration)
* [StreamDCIM A Tile-based Streaming Digital CIM Accelerator with Mixed-stationary Cross-forwarding Dataflow for Multimodal Transformer](#StreamDCIM-A-Tile-based-Streaming-Digital-CIM-Accelerator-with-Mixed-stationary-Cross-forwarding-Dataflow-for-Multimodal-Transformer)
* [Explainable and Class-Revealing Signal Feature Extraction via Scattering Transform and Constrained Zeroth-Order Optimization](#Explainable-and-Class-Revealing-Signal-Feature-Extraction-via-Scattering-Transform-and-Constrained-Zeroth-Order-Optimization)
* [Flowing Through Layers A Continuous Dynamical Systems Perspective on Transformers](#Flowing-Through-Layers-A-Continuous-Dynamical-Systems-Perspective-on-Transformers)
* [Quantization of Carrollian Fermions](#Quantization-of-Carrollian-Fermions)
* [Towards Sustainable NLP Insights from Benchmarking Inference Energy in Large Language Models](#Towards-Sustainable-NLP-Insights-from-Benchmarking-Inference-Energy-in-Large-Language-Models)
* [Lossless Acceleration of Large Language Models with Hierarchical Drafting based on Temporal Locality in Speculative Decoding](#Lossless-Acceleration-of-Large-Language-Models-with-Hierarchical-Drafting-based-on-Temporal-Locality-in-Speculative-Decoding)
* [UbiMoE A Ubiquitous Mixture-of-Experts Vision Transformer Accelerator With Hybrid Computation Pattern on FPGA](#UbiMoE-A-Ubiquitous-Mixture-of-Experts-Vision-Transformer-Accelerator-With-Hybrid-Computation-Pattern-on-FPGA)
* [Human-AI collaboration for modeling heat conduction in nanostructures](#Human-AI-collaboration-for-modeling-heat-conduction-in-nanostructures)
* [Diffusion Model for Interest Refinement in Multi-Interest Recommendation](#Diffusion-Model-for-Interest-Refinement-in-Multi-Interest-Recommendation)
* [SSH Sparse Spectrum Adaptation via Discrete Hartley Transformation](#SSH-Sparse-Spectrum-Adaptation-via-Discrete-Hartley-Transformation)
* [Mix Data or Merge Models? Balancing the Helpfulness, Honesty, and Harmlessness of Large Language Model via Model Merging](#Mix-Data-or-Merge-Models?-Balancing-the-Helpfulness,-Honesty,-and-Harmlessness-of-Large-Language-Model-via-Model-Merging)
* [IndexTTS An Industrial-Level Controllable and Efficient Zero-Shot Text-To-Speech System](#IndexTTS-An-Industrial-Level-Controllable-and-Efficient-Zero-Shot-Text-To-Speech-System)
* [AdaFlow Efficient Long Video Editing via Adaptive Attention Slimming And Keyframe Selection](#AdaFlow-Efficient-Long-Video-Editing-via-Adaptive-Attention-Slimming-And-Keyframe-Selection)
* [APE Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding](#APE-Faster-and-Longer-Context-Augmented-Generation-via-Adaptive-Parallel-Encoding)
* [The Complexity of Learning Sparse Superposed Features with Feedback](#The-Complexity-of-Learning-Sparse-Superposed-Features-with-Feedback)
* [BCQ Block Clustered Quantization for 4-bit (W4A4) LLM Inference](#BCQ-Block-Clustered-Quantization-for-4-bit-(W4A4)-LLM-Inference)
* [fMoE Fine-Grained Expert Offloading for Large Mixture-of-Experts Serving](#fMoE-Fine-Grained-Expert-Offloading-for-Large-Mixture-of-Experts-Serving)
* [Fillerbuster Multi-View Scene Completion for Casual Captures](#Fillerbuster-Multi-View-Scene-Completion-for-Casual-Captures)
* [Multiphoton, multimode state classification for nonlinear optical circuits](#Multiphoton,-multimode-state-classification-for-nonlinear-optical-circuits)
* [QuEST Stable Training of LLMs with 1-Bit Weights and Activations](#QuEST-Stable-Training-of-LLMs-with-1-Bit-Weights-and-Activations)
* [Gaussian Models to Non-Gaussian Realms of Quantum Photonic Simulators](#Gaussian-Models-to-Non-Gaussian-Realms-of-Quantum-Photonic-Simulators)
* [Discovery of a large magnetic nonlinear Hall effect in an altermagnet](#Discovery-of-a-large-magnetic-nonlinear-Hall-effect-in-an-altermagnet)
* [Sparse Autoencoders Do Not Find Canonical Units of Analysis](#Sparse-Autoencoders-Do-Not-Find-Canonical-Units-of-Analysis)
* [AIQViT Architecture-Informed Post-Training Quantization for Vision Transformers](#AIQViT-Architecture-Informed-Post-Training-Quantization-for-Vision-Transformers)


## Scalable Thermodynamic Second-order Optimization

>Authors: Kaelan Donatella, Samuel Duffield, Denis Melanson, Maxwell Aifer, Phoebe Klett, Rajath Salegame, Zach Belateche, Gavin Crooks, Antonio J. Martinez, Patrick J. Coles

>2025-02-12

> http://arxiv.org/abs/2502.08603v1

Many hardware proposals have aimed to accelerate inference in AI workloads.
Less attention has been paid to hardware **acceleration** of training, despite the
enormous societal impact of rapid training of AI models. Physics-based
computers, such as thermodynamic computers, offer an efficient means to solve
key primitives in AI training algorithms. Optimizers that normally would be
computationally out-of-reach (e.g., due to expensive matrix inversions) on
digital hardware could be unlocked with physics-based hardware. In this work,
we propose a scalable algorithm for employing thermodynamic computers to
accelerate a popular second-order optimizer called Kronecker-factored
approximate curvature (K-FAC). Our asymptotic complexity analysis predicts
increasing advantage with our algorithm as $n$, the number of neurons per
layer, increases. Numerical experiments show that even under significant
**quantization** noise, the benefits of second-order optimization can be preserved.
Finally, we predict substantial speedups for large-scale vision and graph
problems based on realistic hardware characteristics.


## The MoE-Empowered Edge LLMs Deployment Architecture, Challenges, and Opportunities

>Authors: Ning Li, Song Guo, Tuo Zhang, Muqing Li, Zicong Hong, Qihua Zhou, Xin Yuan, Haijun Zhang

>2025-02-12

> http://arxiv.org/abs/2502.08381v1

The powerfulness of LLMs indicates that deploying various LLMs with different
scales and architectures on end, edge, and cloud to satisfy different
requirements and adaptive heterogeneous hardware is the critical way to achieve
ubiquitous intelligence for 6G. However, the massive parameter scale of LLMs
poses significant challenges in deploying them on edge devices due to high
computational and storage demands. Considering that the **sparse** activation in
Mixture of Experts (MoE) is effective on scalable and dynamic allocation of
computational and communications resources at the edge, this paper proposes a
novel MoE-empowered collaborative deployment framework for edge LLMs, denoted
as CoEL. This framework fully leverages the properties of MoE architecture and
encompasses four key aspects: Perception, Deployment, Compression, and
Updating. Edge servers broadcast their resource status and the specific
resource requirements of LLMs to their neighbors. Then, utilizing this data,
two sophisticated deployment strategies are proposed for satisfying varying
model scales, ensuring that each model is deployed effectively. One for
deploying LLMs on a single edge device through intra-device resource
collaboration, and another for a distributed deployment across multiple edge
devices via inter-device resource collaboration. Furthermore, both the models
and the intermediate data are compressed for reducing memory footprint by
**quantization** and reducing the volume of intermediate data by token fusion and
**pruning**. Finally, given the dynamic of network topology, resource status, and
user requirements, the deployment strategies are regularly updated to maintain
its relevance and effectiveness. This paper also delineates the challenges and
potential research directions for the deployment of edge LLMs.


## Top-Theta Attention Sparsifying Transformers by Compensated Thresholding

>Authors: Konstantin Berestizshevsky, Renzo Andri, Lukas Cavigelli

>2025-02-12

> http://arxiv.org/abs/2502.08363v1

The attention mechanism is essential for the impressive capabilities of
transformer-based Large Language Models (LLMs). However, calculating attention
is computationally intensive due to its quadratic dependency on the sequence
length. We introduce a novel approach called Top-Theta Attention, or simply
Top-$\theta$, which selectively prunes less essential attention elements by
comparing them against carefully calibrated thresholds. This method greatly
improves the efficiency of self-attention matrix multiplication while
preserving model accuracy, reducing the number of required V cache rows by 3x
during generative decoding and the number of attention elements by 10x during
the prefill phase. Our method does not require model retraining; instead, it
requires only a brief calibration phase to be resilient to distribution shifts,
thus not requiring the thresholds for different datasets to be recalibrated.
Unlike top-k attention, Top-$\theta$ eliminates full-vector dependency, making
it suitable for tiling and scale-out and avoiding costly top-k search. A key
innovation of our approach is the development of efficient numerical
compensation techniques, which help preserve model accuracy even under
aggressive **pruning** of attention scores.


## Exploiting Non-uniform Quantization for Enhanced ILC in Wideband Digital Pre-distortion

>Authors: Jinfei Wang, Yi Ma, Fei Tong, Ziming He

>2025-02-12

> http://arxiv.org/abs/2502.08360v1

In this paper, it is identified that lowering the reference level at the
vector signal analyzer can significantly improve the performance of iterative
learning control (ILC). We present a mathematical explanation for this
phenomenon, where the signals experience logarithmic transform prior to
analogue-to-digital conversion, resulting in non-uniform **quantization**. This
process reduces the **quantization** noise of low-amplitude signals that constitute
a substantial portion of orthogonal frequency division multiplexing (OFDM)
signals, thereby improving ILC performance. Measurement results show that
compared to setting the reference level to the peak amplitude, lowering the
reference level achieves 3 dB improvement on error vector magnitude (EVM) and
15 dB improvement on normalized mean square error (NMSE) for 320 MHz WiFi OFDM
signals.


## Contextual Compression Encoding for Large Language Models A Novel Framework for Multi-Layered Parameter Space Pruning

>Authors: Barnaby Schmitt, Alistair Grosvenor, Matthias Cunningham, Clementine Walsh, Julius Pembrokeshire, Jonathan Teel

>2025-02-12

> http://arxiv.org/abs/2502.08323v1

Context-aware compression techniques have gained increasing attention as
model sizes continue to grow, introducing computational bottlenecks that hinder
efficient deployment. A structured encoding approach was proposed to
selectively eliminate redundant parameter groups while ensuring that
representational fidelity was preserved across multiple layers. Contextual
Compression Encoding (CCE) introduced a multi-stage encoding mechanism that
dynamically restructured parameter distributions, allowing for significant
reductions in memory footprint and computational complexity. Experimental
evaluations demonstrated that models compressed through CCE retained linguistic
expressivity and coherence, maintaining accuracy across a range of text
generation and classification tasks. Layer-wise analysis revealed that
middle-network layers exhibited higher compression ratios, aligning with the
observation that self-attention and feed-forward transformations contained
redundancies that could be reorganized without impairing functional capacity.
Comparisons against conventional **quantization** and **pruning** methods confirmed
that CCE provided a more balanced trade-off between efficiency and model
retention, achieving reductions in energy consumption and inference latency
without requiring extensive retraining. Computational efficiency improvements
were particularly evident in deployment scenarios involving
resource-constrained environments, where reductions in memory usage enabled
more scalable implementations. Further analyses of internal network behavior
showed that compressed models exhibited stable activation distributions and
adapted dynamically to input variations, reinforcing the viability of
structured compression strategies for optimizing large-scale architectures.


## HDT Hierarchical Discrete Transformer for Multivariate Time Series Forecasting

>Authors: Shibo Feng, Peilin Zhao, Liu Liu, Pengcheng Wu, Zhiqi Shen

>2025-02-12

> http://arxiv.org/abs/2502.08302v1

Generative models have gained significant attention in multivariate time
series forecasting (MTS), particularly due to their ability to generate
high-fidelity samples. Forecasting the probability distribution of multivariate
time series is a challenging yet practical task. Although some recent attempts
have been made to handle this task, two major challenges persist: 1) some
existing generative methods underperform in high-dimensional multivariate time
series forecasting, which is hard to scale to higher dimensions; 2) the
inherent high-dimensional multivariate attributes constrain the forecasting
lengths of existing generative models. In this paper, we point out that
discrete token representations can model high-dimensional MTS with faster
inference time, and forecasting the target with long-term trends of itself can
extend the forecasting length with high accuracy. Motivated by this, we propose
a vector **quantize**d framework called Hierarchical Discrete Transformer (HDT)
that models time series into discrete token representations with l2
normalization enhanced vector **quantize**d strategy, in which we transform the MTS
forecasting into discrete tokens generation. To address the limitations of
generative models in long-term forecasting, we propose a hierarchical discrete
Transformer. This model captures the discrete long-term trend of the target at
the low level and leverages this trend as a condition to generate the discrete
representation of the target at the high level that introduces the features of
the target itself to extend the forecasting length in high-dimensional MTS.
Extensive experiments on five popular MTS datasets verify the effectiveness of
our proposed method.


## Inference-time sparse attention with asymmetric indexing

>Authors: Pierre-Emmanuel Mazaré, Gergely Szilvasy, Maria Lomeli, Francisco Massa, Naila Murray, Hervé Jégou, Matthijs Douze

>2025-02-12

> http://arxiv.org/abs/2502.08246v1

Self-attention in transformer models is an incremental associative memory
that maps key vectors to value vectors. One way to speed up self-attention is
to employ GPU-compliant vector search algorithms, yet the standard partitioning
methods yield poor results in this context, because (1) keys and queries follow
different distributions and (2) the effect of RoPE positional encoding.
  In this paper, we introduce SAAP (Self-Attention with Asymmetric Partitions),
which overcomes these problems. It is an asymmetrical indexing technique that
employs distinct partitions for keys and queries, thereby approximating
self-attention with a data-adaptive **sparsity** pattern.
  It works on pretrained language models without finetuning, as it only
requires to train (offline) a small query classifier. On a long context Llama
3.1-8b model, with sequences ranging from 100k to 500k tokens, our method
typically reduces by a factor 20 the fraction of memory that needs to be
looked-up, which translates to a time saving of 60\% when compared to
FlashAttention-v2.


## Intention is All You Need Refining Your Code from Your Intention

>Authors: Qi Guo, Xiaofei Xie, Shangqing Liu, Ming Hu, Xiaohong Li, Lei Bu

>2025-02-12

> http://arxiv.org/abs/2502.08172v1

Code refinement aims to enhance existing code by addressing issues,
refactoring, and optimizing to improve quality and meet specific requirements.
As software projects scale in size and complexity, the traditional iterative
exchange between reviewers and developers becomes increasingly burdensome.
While recent deep learning techniques have been explored to accelerate this
process, their performance remains limited, primarily due to challenges in
accurately understanding reviewers' intents.
  This paper proposes an intention-based code refinement technique that
enhances the conventional comment-to-code process by explicitly extracting
reviewer intentions from the comments. Our approach consists of two key phases:
Intention Extraction and Intention Guided Revision Generation. Intention
Extraction categorizes comments using predefined templates, while Intention
Guided Revision Generation employs large language models (LLMs) to generate
revised code based on these defined intentions. Three categories with eight
subcategories are designed for comment transformation, which is followed by a
hybrid approach that combines rule-based and LLM-based classifiers for accurate
classification. Extensive experiments with five LLMs (GPT4o, GPT3.5,
DeepSeekV2, DeepSeek7B, CodeQwen7B) under different prompting settings
demonstrate that our approach achieves 79% accuracy in intention extraction and
up to 66% in code refinement generation. Our results highlight the potential of
our approach in enhancing data quality and improving the efficiency of code
refinement.


## LowRA Accurate and Efficient LoRA Fine-Tuning of LLMs under 2 Bits

>Authors: Zikai Zhou, Qizheng Zhang, Hermann Kumbong, Kunle Olukotun

>2025-02-12

> http://arxiv.org/abs/2502.08141v1

Fine-tuning large language models (LLMs) is increasingly costly as models
scale to hundreds of billions of parameters, and even parameter-efficient
fine-tuning (PEFT) methods like LoRA remain resource-intensive. We introduce
LowRA, the first framework to enable LoRA fine-tuning below 2 bits per
parameter with minimal performance loss. LowRA optimizes fine-grained
**quantization** - mapping, threshold selection, and precision assignment - while
leveraging efficient CUDA kernels for scalable deployment. Extensive
evaluations across 4 LLMs and 4 datasets show that LowRA achieves a superior
performance-precision trade-off above 2 bits and remains accurate down to 1.15
bits, reducing memory usage by up to 50%. Our results highlight the potential
of ultra-**low-bit** LoRA fine-tuning for resource-constrained environments.


## HexGen-2 Disaggregated Generative Inference of LLMs in Heterogeneous Environment

>Authors: Youhe Jiang, Ran Yan, Binhang Yuan

>2025-02-11

> http://arxiv.org/abs/2502.07903v1

Disaggregating the prefill and decoding phases represents an effective new
paradigm for generative inference of large language models (LLM), which
eliminates prefill-decoding interference and optimizes resource allocation.
However, it is still an open problem about how to deploy the disaggregated
inference paradigm across a group of heterogeneous GPUs, which can be an
economical alternative to deployment over homogeneous high-performance GPUs.
Towards this end, we introduce HexGen-2, a distributed system for efficient and
economical LLM serving on heterogeneous GPUs following the disaggregated
paradigm. Built on top of HexGen, the core component of HexGen-2 is a
scheduling algorithm that formalizes the allocation of disaggregated LLM
inference computations and communications over heterogeneous GPUs and network
connections as a constraint optimization problem. We leverage the graph
partitioning and max-flow algorithms to co-optimize resource allocation,
parallel strategies for distinct inference phases, and the efficiency of
inter-phase key-value (**KV**) cache communications. We conduct extensive
experiments to evaluate HexGen-2, i.e., on OPT (30B) and Llama-2 (70B) models
in various real-world settings, the results reveal that HexGen-2 delivers up to
a 2.0 times and on average a 1.3 times improvement in serving throughput,
reduces the average inference latency by 1.5 times compared with
state-of-the-art systems given the same price budget, and achieves comparable
inference performance with a 30% lower price budget.


## DarwinLM Evolutionary Structured Pruning of Large Language Models

>Authors: Shengkun Tang, Oliver Sieberling, Eldar Kurtic, Zhiqiang Shen, Dan Alistarh

>2025-02-11

> http://arxiv.org/abs/2502.07780v1

Large Language Models (LLMs) have achieved significant success across various
NLP tasks. However, their massive computational costs limit their widespread
use, particularly in real-time applications. Structured **pruning** offers an
effective solution by compressing models and directly providing end-to-end
speed improvements, regardless of the hardware environment. Meanwhile,
different components of the model exhibit varying sensitivities towards
**pruning**, calling for \emph{non-uniform} model compression. However, a **pruning**
method should not only identify a capable substructure, but also account for
post-compression training. To this end, we propose \sysname, a method for
\emph{training-aware} structured **pruning**. \sysname builds upon an evolutionary
search process, generating multiple offspring models in each generation through
mutation, and selecting the fittest for survival. To assess the effect of
post-training, we incorporate a lightweight, multistep training process within
the offspring population, progressively increasing the number of tokens and
eliminating poorly performing models in each selection stage. We validate our
method through extensive experiments on Llama-2-7B, Llama-3.1-8B and
Qwen-2.5-14B-Instruct, achieving state-of-the-art performance for structured
**pruning**. For instance, \sysname surpasses ShearedLlama while requiring
$5\times$ less training data during post-compression training.


## Breaking Down Bias On The Limits of Generalizable Pruning Strategies

>Authors: Sibo Ma, Alejandro Salinas, Peter Henderson, Julian Nyarko

>2025-02-11

> http://arxiv.org/abs/2502.07771v1

We employ model **pruning** to examine how LLMs conceptualize racial biases, and
whether a generalizable mitigation strategy for such biases appears feasible.
Our analysis yields several novel insights. We find that **pruning** can be an
effective method to reduce bias without significantly increasing anomalous
model behavior. Neuron-based **pruning** strategies generally yield better results
than approaches **pruning** entire attention heads. However, our results also show
that the effectiveness of either approach quickly deteriorates as **pruning**
strategies become more generalized. For instance, a model that is trained on
removing racial biases in the context of financial decision-making poorly
generalizes to biases in commercial transactions. Overall, our analysis
suggests that racial biases are only partially represented as a general concept
within language models. The other part of these biases is highly
context-specific, suggesting that generalizable mitigation strategies may be of
limited effectiveness. Our findings have important implications for legal
frameworks surrounding AI. In particular, they suggest that an effective
mitigation strategy should include the allocation of legal responsibility on
those that deploy models in a specific use case.


## TransMLA Multi-Head Latent Attention Is All You Need

>Authors: Fanxu Meng, Zengwei Yao, Muhan Zhang

>2025-02-11

> http://arxiv.org/abs/2502.07864v2

Modern large language models (LLMs) often encounter communication bottlenecks
on current hardware, rather than purely computational constraints. Multi-head
Latent Attention (MLA) tackles this challenge by using low-rank matrices in the
key-value (**KV**) layers, thereby allowing compressed latent **KV** states to be
cached. This approach significantly reduces the **KV** cache size relative to
traditional multi-head attention, leading to faster inference. Moreover, MLA
employs an up-projection matrix to increase expressiveness, trading additional
computation for reduced communication overhead. Although MLA has demonstrated
efficiency and effectiveness in Deepseek V2/V3/R1, many major model providers
still rely on Group Query Attention (GQA) and have not announced any plans to
adopt MLA. In this paper, we show that GQA can always be represented by MLA
while maintaining the same **KV** cache overhead, but the converse does not hold.
To encourage broader use of MLA, we introduce TransMLA, a post-training method
that converts widely used GQA-based pre-trained models (e.g., LLaMA, Qwen,
Mixtral) into MLA-based models. After conversion, the model can undergo
additional training to boost expressiveness without increasing the **KV** cache
size. Furthermore, we plan to develop MLA-specific inference **acceleration**
techniques to preserve low latency in transformed models, thus enabling more
efficient distillation of Deepseek R1.


## BalanceKV KV Cache Compression through Discrepancy Theory

>Authors: Insu Han, Michael Kapralov, Ekaterina Kochetkova, Kshiteej Sheth, Amir Zandieh

>2025-02-11

> http://arxiv.org/abs/2502.07861v1

Large language models (LLMs) have achieved impressive success, but their high
memory requirements present challenges for long-context token generation. The
memory complexity of long-context LLMs is primarily due to the need to store
Key-Value (**KV**) embeddings in their **KV** cache. We present Balance**KV**, a **KV** cache
compression method based on geometric sampling process stemming from
Banaszczyk's vector balancing theory, which introduces dependencies informed by
the geometry of keys and value tokens, and improves precision. Balance**KV** offers
both theoretically proven and empirically validated performance improvements
over existing methods.


## MAAT Mamba Adaptive Anomaly Transformer with association discrepancy for time series

>Authors: Abdellah Zakaria Sellam, Ilyes Benaissa, Abdelmalik Taleb-Ahmed, Luigi Patrono, Cosimo Distante

>2025-02-11

> http://arxiv.org/abs/2502.07858v1

Anomaly detection in time series is essential for industrial monitoring and
environmental sensing, yet distinguishing anomalies from complex patterns
remains challenging. Existing methods like the Anomaly Transformer and
DCdetector have progressed, but they face limitations such as sensitivity to
short-term contexts and inefficiency in noisy, non-stationary environments.
  To overcome these issues, we introduce MAAT, an improved architecture that
enhances association discrepancy modeling and reconstruction quality. MAAT
features Sparse Attention, efficiently capturing long-range dependencies by
focusing on relevant time steps, thereby reducing computational redundancy.
Additionally, a Mamba-Selective State Space Model is incorporated into the
reconstruction module, utilizing a skip connection and Gated Attention to
improve anomaly localization and detection performance.
  Extensive experiments show that MAAT significantly outperforms previous
methods, achieving better anomaly distinguishability and generalization across
various time series applications, setting a new standard for unsupervised time
series anomaly detection in real-world scenarios.


## Tractable Transformers for Flexible Conditional Generation

>Authors: Anji Liu, Xuejie Liu, Dayuan Zhao, Mathias Niepert, Yitao Liang, Guy Van den Broeck

>2025-02-11

> http://arxiv.org/abs/2502.07616v1

Non-autoregressive (NAR) generative models are valuable because they can
handle diverse conditional generation tasks in a more principled way than their
autoregressive (AR) counterparts, which are constrained by sequential
dependency requirements. Recent advancements in NAR models, such as diffusion
language models, have demonstrated superior performance in unconditional
generation compared to AR models (e.g., GPTs) of similar sizes. However, such
improvements do not always lead to improved conditional generation performance.
We show that a key reason for this gap is the difficulty in generalizing to
conditional probability queries unseen during training. As a result, strong
unconditional generation performance does not guarantee high-quality
conditional generation. This paper proposes Tractable Transformers
(Tracformer), a Transformer-based generative model that is more robust to
different conditional generation tasks. Unlike existing models that rely solely
on global contextual features derived from full inputs, Tracformers incorporate
a **sparse** Transformer encoder to capture both local and global contextual
information. This information is routed through a decoder for conditional
generation. Empirical results demonstrate that Tracformers achieve
state-of-the-art conditional generation performance on text modeling compared
to recent diffusion and AR model baselines.


## DSV Exploiting Dynamic Sparsity to Accelerate Large-Scale Video DiT Training

>Authors: Xin Tan, Yuetao Chen, Yimin Jiang, Xing Chen, Kun Yan, Nan Duan, Yibo Zhu, Daxin Jiang, Hong Xu

>2025-02-11

> http://arxiv.org/abs/2502.07590v1

Diffusion Transformers (DiTs) have shown remarkable performance in modeling
and generating high-quality videos. However, the quadratic computational
complexity of 3D full attention mechanism presents significant challenges in
scaling video DiT training, especially for high-definition and lengthy videos,
where attention can dominate up to 95% of the end-to-end time and necessitate
specialized communication paradigms to handle large input sizes.
  This paper introduces DSV, a novel framework designed to accelerate and scale
the training of video DiTs by leveraging the inherent dynamic attention
**sparsity** throughout the training process. DSV employs a two-stage training
algorithm that exploits **sparsity** patterns, focusing on critical elements
supported by efficient, tailored kernels. To accommodate the new **sparsity**
dimension, we develop a hybrid **sparsity**-aware context parallelism that
effectively scales to large inputs by addressing the heterogeneity of **sparsity**
across attention heads and blocks, resulting in optimized **sparse** computation
and communication. Extensive evaluations demonstrate that DSV achieves up to
3.02x gain in training throughput with nearly no quality degradation.


## 5D Neural Surrogates for Nonlinear Gyrokinetic Simulations of Plasma Turbulence

>Authors: Gianluca Galletti, Fabian Paischer, Paul Setinek, William Hornsby, Lorenzo Zanisi, Naomi Carey, Stanislas Pamela, Johannes Brandstetter

>2025-02-11

> http://arxiv.org/abs/2502.07469v1

Nuclear fusion plays a pivotal role in the quest for reliable and sustainable
energy production. A major roadblock to achieving commercially viable fusion
power is understanding plasma turbulence, which can significantly degrade
plasma confinement. Modelling turbulence is crucial to design performing plasma
scenarios for next-generation reactor-class devices and current experimental
machines. The nonlinear gyrokinetic equation underpinning turbulence modelling
evolves a 5D distribution function over time. Solving this equation numerically
is extremely expensive, requiring up to weeks for a single run to converge,
making it unfeasible for iterative optimisation and control studies. In this
work, we propose a method for training neural surrogates for 5D gyrokinetic
simulations. Our method extends a hierarchical vision transformer to five
dimensions and is trained on the 5D distribution function for the adiabatic
electron approximation. We demonstrate that our model can accurately infer
downstream physical quantities such as heat flux time trace and electrostatic
potentials for single-step predictions two orders of magnitude faster than
numerical codes. Our work paves the way towards neural surrogates for plasma
turbulence simulations to accelerate deployment of commercial energy production
via nuclear fusion.


## DEG Efficient Hybrid Vector Search Using the Dynamic Edge Navigation Graph

>Authors: Ziqi Yin, Jianyang Gao, Pasquale Balsebre, Gao Cong, Cheng Long

>2025-02-11

> http://arxiv.org/abs/2502.07343v1

Bimodal data, such as image-text pairs, has become increasingly prevalent in
the digital era. The Hybrid Vector Query (HVQ) is an effective approach for
querying such data and has recently garnered considerable attention from
researchers. It calculates similarity scores for objects represented by two
vectors using a weighted sum of each individual vector's similarity, with a
query-specific parameter $\alpha$ to determine the weight. Existing methods for
HVQ typically construct Approximate Nearest Neighbors Search (ANNS) indexes
with a fixed $\alpha$ value. This leads to significant performance degradation
when the query's $\alpha$ dynamically changes based on the different scenarios
and needs.
  In this study, we introduce the Dynamic Edge Navigation Graph (DEG), a
graph-based ANNS index that maintains efficiency and accuracy with changing
$\alpha$ values. It includes three novel components: (1) a greedy Pareto
frontier search algorithm to compute a candidate neighbor set for each node,
which comprises the node's approximate nearest neighbors for all possible
$\alpha$ values; (2) a dynamic edge **pruning** strategy to determine the final
edges from the candidate set and assign each edge an active range. This active
range enables the dynamic use of the Relative Neighborhood Graph's **pruning**
strategy based on the query's $\alpha$ values, skipping redundant edges at
query time and achieving a better accuracy-efficiency trade-off; and (3) an
edge seed method that accelerates the querying process. Extensive experiments
on real-world datasets show that DEG demonstrates superior performance compared
to existing methods under varying $\alpha$ values.


## CodeI/O Condensing Reasoning Patterns via Code Input-Output Prediction

>Authors: Junlong Li, Daya Guo, Dejian Yang, Runxin Xu, Yu Wu, Junxian He

>2025-02-11

> http://arxiv.org/abs/2502.07316v2

Reasoning is a fundamental capability of Large Language Models. While prior
research predominantly focuses on enhancing narrow skills like math or code
generation, improving performance on many other reasoning tasks remains
challenging due to **sparse** and fragmented training data. To address this issue,
we propose CodeI/O, a novel approach that systematically condenses diverse
reasoning patterns inherently embedded in contextually-grounded codes, through
transforming the original code into a code input-output prediction format. By
training models to predict inputs/outputs given code and test cases entirely in
natural language as Chain-of-Thought (CoT) rationales, we expose them to
universal reasoning primitives -- like logic flow planning, state-space
searching, decision tree traversal, and modular decomposition -- while
decoupling structured reasoning from code-specific syntax and preserving
procedural rigor. Experimental results demonstrate CodeI/O leads to consistent
improvements across symbolic, scientific, logic, math & numerical, and
commonsense reasoning tasks. By matching the existing ground-truth outputs or
re-executing the code with predicted inputs, we can verify each prediction and
further enhance the CoTs through multi-turn revision, resulting in CodeI/O++
and achieving higher performance. Our data and models are available at
https://github.com/hkust-nlp/CodeIO.


## Learning Inverse Laplacian Pyramid for Progressive Depth Completion

>Authors: Kun Wang, Zhiqiang Yan, Junkai Fan, Jun Li, Jian Yang

>2025-02-11

> http://arxiv.org/abs/2502.07289v1

Depth completion endeavors to reconstruct a dense depth map from **sparse** depth
measurements, leveraging the information provided by a corresponding color
image. Existing approaches mostly hinge on single-scale propagation strategies
that iteratively ameliorate initial coarse depth estimates through pixel-level
message passing. Despite their commendable outcomes, these techniques are
frequently hampered by computational inefficiencies and a limited grasp of
scene context. To circumvent these challenges, we introduce LP-Net, an
innovative framework that implements a multi-scale, progressive prediction
paradigm based on Laplacian Pyramid decomposition. Diverging from
propagation-based approaches, LP-Net initiates with a rudimentary,
low-resolution depth prediction to encapsulate the global scene context,
subsequently refining this through successive upsampling and the reinstatement
of high-frequency details at incremental scales. We have developed two novel
modules to bolster this strategy: 1) the Multi-path Feature Pyramid module,
which segregates feature maps into discrete pathways, employing multi-scale
transformations to amalgamate comprehensive spatial information, and 2) the
Selective Depth Filtering module, which dynamically learns to apply both
smoothness and sharpness filters to judiciously mitigate noise while
accentuating intricate details. By integrating these advancements, LP-Net not
only secures state-of-the-art (SOTA) performance across both outdoor and indoor
benchmarks such as KITTI, NYUv2, and TOFDC, but also demonstrates superior
computational efficiency. At the time of submission, LP-Net ranks 1st among all
peer-reviewed methods on the official KITTI leaderboard.


## Physics-Informed Recurrent Network for Gas Pipeline Network Parameters Identification

>Authors: Siyuan Wang, Wenchuan Wu, Chenhui Lin, Qi Wang, Shuwei Xu, Binbin Chen

>2025-02-11

> http://arxiv.org/abs/2502.07230v1

As a part of the integrated energy system (IES), gas pipeline networks can
provide additional flexibility to power systems through coordinated optimal
dispatch. An accurate pipeline network model is critical for the optimal
operation and control of IESs. However, inaccuracies or unavailability of
accurate pipeline parameters often introduce errors in the mathematical models
of such networks. This paper proposes a physics-informed recurrent network
(PIRN) model to identify the state-space model of gas pipelines. The approach
combines data-driven learning from measurement data with the fluid dynamics
described by partial differential equations. By embedding the physical
state-space model within the recurrent network, parameter identification is
transformed into a training process for a PIRN. Similar to standard recurrent
neural networks, this model can be implemented using the PyTorch framework and
trained via backpropagation. Case studies demonstrate that our method
accurately estimates gas pipeline models from **sparse** terminal node
measurements, providing robust performance and significantly higher parameter
efficiency. Furthermore, the identified models can be seamlessly integrated
into optimization frameworks.


## SparseFormer Detecting Objects in HRW Shots via Sparse Vision Transformer

>Authors: Wenxi Li, Yuchen Guo, Jilai Zheng, Haozhe Lin, Chao Ma, Lu Fang, Xiaokang Yang

>2025-02-11

> http://arxiv.org/abs/2502.07216v1

Recent years have seen an increase in the use of gigapixel-level image and
video capture systems and benchmarks with high-resolution wide (HRW) shots.
However, unlike close-up shots in the MS COCO dataset, the higher resolution
and wider field of view raise unique challenges, such as extreme **sparsity** and
huge scale changes, causing existing close-up detectors inaccuracy and
inefficiency. In this paper, we present a novel model-agnostic **sparse** vision
transformer, dubbed SparseFormer, to bridge the gap of object detection between
close-up and HRW shots. The proposed SparseFormer selectively uses attentive
tokens to scrutinize the **sparse**ly distributed windows that may contain objects.
In this way, it can jointly explore global and local attention by fusing
coarse- and fine-grained features to handle huge scale changes. SparseFormer
also benefits from a novel Cross-slice non-maximum suppression (C-NMS)
algorithm to precisely localize objects from noisy windows and a simple yet
effective multi-scale strategy to improve accuracy. Extensive experiments on
two HRW benchmarks, PANDA and DOTA-v1.0, demonstrate that the proposed
SparseFormer significantly improves detection accuracy (up to 5.8%) and speed
(up to 3x) over the state-of-the-art approaches.


## Exploring Neural Network Pruning with Screening Methods

>Authors: Mingyuan Wang, Yangzi Guo, Sida Liu, Yanwen Xiao

>2025-02-11

> http://arxiv.org/abs/2502.07189v1

Deep neural networks (DNNs) such as convolutional neural networks (CNNs) for
visual tasks, recurrent neural networks (RNNs) for sequence data, and
transformer models for rich linguistic or multimodal tasks, achieved
unprecedented performance on a wide range of tasks. The impressive performance
of modern DNNs is partially attributed to their sheer scale. The latest deep
learning models have tens to hundreds of millions of parameters which makes the
inference processes resource-intensive. The high computational complexity of
these networks prevents their deployment on resource-limited devices such as
mobile platforms, IoT devices, and edge computing systems because these devices
require energy-efficient and real-time processing capabilities. This paper
proposes and evaluates a network **pruning** framework that eliminates
non-essential parameters based on a statistical analysis of network component
significance across classification categories. The proposed method uses
screening methods coupled with a weighted scheme to assess connection and
channel contributions for unstructured and structured **pruning** which allows for
the elimination of unnecessary network elements without significantly degrading
model performance. Extensive experimental validation on real-world vision
datasets for both fully connected neural networks (FNNs) and CNNs has shown
that the proposed framework produces competitive lean networks compared to the
original networks. Moreover, the proposed framework outperforms state-of-art
network **pruning** methods in two out of three cases.


## Foreign-Object Detection in High-Voltage Transmission Line Based on Improved YOLOv8m

>Authors: Zhenyue Wang, Guowu Yuan, Hao Zhou, Yi Ma, Yutang Ma

>2025-02-11

> http://arxiv.org/abs/2502.07175v1

The safe operation of high-voltage transmission lines ensures the power
grid's security. Various foreign objects attached to the transmission lines,
such as balloons, kites and nesting birds, can significantly affect the safe
and stable operation of high-voltage transmission lines. With the advancement
of computer vision technology, periodic automatic inspection of foreign objects
is efficient and necessary. Existing detection methods have low accuracy
because foreign objects at-tached to the transmission lines are complex,
including occlusions, diverse object types, significant scale variations, and
complex backgrounds. In response to the practical needs of the Yunnan Branch of
China Southern Power Grid Co., Ltd., this paper proposes an improved
YOLOv8m-based model for detecting foreign objects on transmission lines.
Experiments are conducted on a dataset collected from Yunnan Power Grid. The
proposed model enhances the original YOLOv8m by in-corporating a Global
Attention Module (GAM) into the backbone to focus on occluded foreign objects,
replacing the SPPF module with the SPPCSPC module to augment the model's
multiscale feature extraction capability, and introducing the Focal-EIoU loss
function to address the issue of high- and low-quality sample imbalances. These
improvements accelerate model convergence and enhance detection accuracy. The
experimental results demonstrate that our proposed model achieves a 2.7%
increase in mAP_0.5, a 4% increase in mAP_0.5:0.95, and a 6% increase in
recall.


## SHARP Accelerating Language Model Inference by SHaring Adjacent layers with Recovery Parameters

>Authors: Yiping Wang, Hanxian Huang, Yifang Chen, Jishen Zhao, Simon Shaolei Du, Yuandong Tian

>2025-02-11

> http://arxiv.org/abs/2502.07832v1

While Large language models (LLMs) have advanced natural language processing
tasks, their growing computational and memory demands make deployment on
resource-constrained devices like mobile phones increasingly challenging. In
this paper, we propose SHARP (SHaring Adjacent Layers with Recovery
Parameters), a novel approach to accelerate LLM inference by sharing parameters
across adjacent layers, thus reducing memory load overhead, while introducing
low-rank recovery parameters to maintain performance. Inspired by observations
that consecutive layers have similar outputs, SHARP employs a two-stage
recovery process: Single Layer Warmup (SLW), and Supervised Fine-Tuning (SFT).
The SLW stage aligns the outputs of the shared layers using L_2 loss, providing
a good initialization for the following SFT stage to further restore the model
performance. Extensive experiments demonstrate that SHARP can recover the
model's perplexity on various in-distribution tasks using no more than 50k
fine-tuning data while reducing the number of stored MLP parameters by 38% to
65%. We also conduct several ablation studies of SHARP and show that replacing
layers towards the later parts of the model yields better performance
retention, and that different recovery parameterizations perform similarly when
parameter counts are matched. Furthermore, SHARP saves 42.8% in model storage
and reduces the total inference time by 42.2% compared to the original
Llama2-7b model on mobile devices. Our results highlight SHARP as an efficient
solution for reducing inference costs in deploying LLMs without the need for
pretraining-scale resources.


## Cardiverse Harnessing LLMs for Novel Card Game Prototyping

>Authors: Danrui Li, Sen Zhang, Sam S. Sohn, Kaidong Hu, Muhammad Usman, Mubbasir Kapadia

>2025-02-10

> http://arxiv.org/abs/2502.07128v1

The prototyping of computer games, particularly card games, requires
extensive human effort in creative ideation and gameplay evaluation. Recent
advances in Large Language Models (LLMs) offer opportunities to automate and
streamline these processes. However, it remains challenging for LLMs to design
novel game mechanics beyond existing databases, generate consistent gameplay
environments, and develop scalable gameplay AI for large-scale evaluations.
This paper addresses these challenges by introducing a comprehensive automated
card game prototyping framework. The approach highlights a graph-based indexing
method for generating novel game designs, an LLM-driven system for consistent
game code generation validated by gameplay records, and a gameplay AI
constructing method that uses an ensemble of LLM-generated action-value
functions optimized through self-play. These contributions aim to accelerate
card game prototyping, reduce human labor, and lower barriers to entry for game
developers.


## Online Scheduling for LLM Inference with KV Cache Constraints

>Authors: Patrick Jaillet, Jiashuo Jiang, Chara Podimata, Zijie Zhou

>2025-02-10

> http://arxiv.org/abs/2502.07115v2

Large Language Model (LLM) inference, where a trained model generates text
one word at a time in response to user prompts, is a computationally intensive
process requiring efficient scheduling to optimize latency and resource
utilization. A key challenge in LLM inference is the management of the
Key-Value (**KV**) cache, which reduces redundant computations but introduces
memory constraints. In this work, we model LLM inference with **KV** cache
constraints theoretically and propose novel batching and scheduling algorithms
that minimize inference latency while effectively managing the **KV** cache's
memory.
  We analyze both semi-online and fully online scheduling models, and our
results are threefold. First, we provide a polynomial-time algorithm that
achieves exact optimality in terms of average latency in the semi-online prompt
arrival model. Second, in the fully online case with a stochastic prompt
arrival, we introduce an efficient online scheduling algorithm with constant
regret. Third, we prove that no algorithm (deterministic or randomized) can
achieve a constant competitive ratio in fully online adversarial settings. Our
empirical evaluations on a public LLM inference dataset, using the Llama-70B
model on A100 GPUs, show that our approach significantly outperforms benchmark
algorithms used currently in practice, achieving lower latency while reducing
energy consumption. Overall, our results offer a path toward more sustainable
and cost-effective LLM deployment.


## Specializing Large Language Models to Simulate Survey Response Distributions for Global Populations

>Authors: Yong Cao, Haijiang Liu, Arnav Arora, Isabelle Augenstein, Paul Röttger, Daniel Hershcovich

>2025-02-10

> http://arxiv.org/abs/2502.07068v1

Large-scale surveys are essential tools for informing social science research
and policy, but running surveys is costly and time-intensive. If we could
accurately simulate group-level survey results, this would therefore be very
valuable to social science research. Prior work has explored the use of large
language models (LLMs) for simulating human behaviors, mostly through
prompting. In this paper, we are the first to specialize LLMs for the task of
simulating survey response distributions. As a testbed, we use country-level
results from two global cultural surveys. We devise a fine-tuning method based
on first-token probabilities to minimize divergence between predicted and
actual response distributions for a given question. Then, we show that this
method substantially outperforms other methods and zero-shot classifiers, even
on unseen questions, countries, and a completely unseen survey. While even our
best models struggle with the task, especially on unseen questions, our results
demonstrate the benefits of specialization for simulation, which may accelerate
progress towards sufficiently accurate simulation in the future.


## Federated Sinkhorn

>Authors: Jeremy Kulcsar, Vyacheslav Kungurtsev, Georgios Korpas, Giulio Giaconi, William Shoosmith

>2025-02-10

> http://arxiv.org/abs/2502.07021v1

In this work we investigate the potential of solving the discrete Optimal
Transport (OT) problem with entropy regularization in a federated learning
setting. Recall that the celebrated Sinkhorn algorithm transforms the classical
OT linear program into strongly convex constrained optimization, facilitating
first order methods for otherwise intractably large problems. A common
contemporary setting that remains an open problem as far as the application of
Sinkhorn is the presence of data spread across clients with distributed
inter-communication, either due to clients whose privacy is a concern, or
simply by necessity of processing and memory hardware limitations. In this work
we investigate various natural procedures, which we refer to as Federated
Sinkhorn, that handle distributed environments where data is partitioned across
multiple clients. We formulate the problem as minimizing the transport cost
with an entropy regularization term, subject to marginal constraints, where
block components of the source and target distribution vectors are locally
known to clients corresponding to each block. We consider both synchronous and
asynchronous variants as well as all-to-all and server-client communication
topology protocols. Each procedure allows clients to compute local operations
on their data partition while periodically exchanging information with others.
We provide theoretical guarantees on convergence for the different variants
under different possible conditions. We empirically demonstrate the algorithms
performance on synthetic datasets and a real-world financial risk assessment
application. The investigation highlights the subtle tradeoffs associated with
computation and communication time in different settings and how they depend on
problem size and **sparsity**.


## Demystifying Singular Defects in Large Language Models

>Authors: Haoqi Wang, Tong Zhang, Mathieu Salzmann

>2025-02-10

> http://arxiv.org/abs/2502.07004v1

Large transformer models are known to produce high-norm tokens. In vision
transformers (ViTs), such tokens have been mathematically modeled through the
singular vectors of the linear approximations of layers. However, in large
language models (LLMs), the underlying causes of high-norm tokens remain
largely unexplored, and their different properties from those of ViTs require a
new analysis framework. In this paper, we provide both theoretical insights and
empirical validation across a range of recent models, leading to the following
observations: i) The layer-wise singular direction predicts the abrupt
explosion of token norms in LLMs. ii) The negative eigenvalues of a layer
explain its sudden decay. iii) The computational pathways leading to high-norm
tokens differ between initial and noninitial tokens. iv) High-norm tokens are
triggered by the right leading singular vector of the matrix approximating the
corresponding modules. We showcase two practical applications of these
findings: the improvement of **quantization** schemes and the design of LLM
signatures. Our findings not only advance the understanding of singular defects
in LLMs but also open new avenues for their application. We expect that this
work will stimulate further research into the internal mechanisms of LLMs and
will therefore publicly release our code.


## Exploiting Sparsity for Long Context Inference Million Token Contexts on Commodity GPUs

>Authors: Ryan Synk, Monte Hoover, John Kirchenbauer, Neel Jain, Alex Stein, Manli Shu, Josue Melendez Sanchez, Ramani Duraiswami, Tom Goldstein

>2025-02-10

> http://arxiv.org/abs/2502.06766v2

There is growing demand for performing inference with hundreds of thousands
of input tokens on trained transformer models. Inference at this extreme scale
demands significant computational resources, hindering the application of
transformers at long contexts on commodity (i.e not data center scale)
hardware. To address the inference time costs associated with running
self-attention based transformer language models on long contexts and enable
their adoption on widely available hardware, we propose a tunable mechanism
that reduces the cost of the forward pass by attending to only the most
relevant tokens at every generation step using a top-k selection mechanism. We
showcase the efficiency gains afforded by our method by performing inference on
context windows up to 1M tokens using approximately 16GB of GPU RAM. Our
experiments reveal that models are capable of handling the **sparsity** induced by
the reduced number of keys and values. By attending to less than 2% of input
tokens, we achieve over 95% of model performance on common benchmarks (RULER,
AlpacaEval, and Open LLM Leaderboard).


## Gradient Multi-Normalization for Stateless and Scalable LLM Training

>Authors: Meyer Scetbon, Chao Ma, Wenbo Gong, Edward Meeds

>2025-02-10

> http://arxiv.org/abs/2502.06742v1

Training large language models (LLMs) typically relies on adaptive optimizers
like Adam (Kingma & Ba, 2015) which store additional state information to
accelerate convergence but incur significant memory overhead. Recent efforts,
such as SWAN (Ma et al., 2024) address this by eliminating the need for
optimizer states while achieving performance comparable to Adam via a
multi-step preprocessing procedure applied to instantaneous gradients.
Motivated by the success of SWAN, we introduce a novel framework for designing
stateless optimizers that normalizes stochastic gradients according to multiple
norms. To achieve this, we propose a simple alternating scheme to enforce the
normalization of gradients w.r.t these norms. We show that our procedure can
produce, up to an arbitrary precision, a fixed-point of the problem, and that
SWAN is a particular instance of our approach with carefully chosen norms,
providing a deeper understanding of its design. However, SWAN's computationally
expensive whitening/orthogonalization step limit its practicality for large
LMs. Using our principled perspective, we develop of a more efficient,
scalable, and practical stateless optimizer. Our algorithm relaxes the
properties of SWAN, significantly reducing its computational cost while
retaining its memory efficiency, making it applicable to training large-scale
models. Experiments on pre-training LLaMA models with up to 1 billion
parameters demonstrate a 3X speedup over Adam with significantly reduced memory
requirements, outperforming other memory-efficient baselines.


## XAMBA Enabling Efficient State Space Models on Resource-Constrained Neural Processing Units

>Authors: Arghadip Das, Arnab Raha, Shamik Kundu, Soumendu Kumar Ghosh, Deepak Mathaikutty, Vijay Raghunathan

>2025-02-10

> http://arxiv.org/abs/2502.06924v2

State-Space Models (SSMs) have emerged as efficient alternatives to
transformers for sequential data tasks, offering linear or near-linear
scalability with sequence length, making them ideal for long-sequence
applications in NLP, vision, and edge AI, including real-time transcription,
translation, and contextual search. These applications require lightweight,
high-performance models for deployment on resource-constrained devices like
laptops and PCs. Designing specialized accelerators for every emerging neural
network is costly and impractical; instead, optimizing models for existing NPUs
in AI PCs provides a scalable solution. To this end, we propose XAMBA, the
first framework to enable and optimize SSMs on commercial off-the-shelf (COTS)
state-of-the-art (SOTA) NPUs. XAMBA follows a three-step methodology: (1)
enabling SSMs on NPUs, (2) optimizing performance to meet KPI requirements, and
(3) trading accuracy for additional performance gains. After enabling SSMs on
NPUs, XAMBA mitigates key bottlenecks using CumBA and ReduBA, replacing
sequential CumSum and ReduceSum operations with matrix-based computations,
significantly improving execution speed and memory efficiency. Additionally,
ActiBA enhances performance by approximating expensive activation functions
(e.g., Swish, Softplus) using piecewise linear mappings, reducing latency with
minimal accuracy loss. Evaluations on an Intel Core Ultra Series 2 AI PC show
that XAMBA achieves up to 2.6X speed-up over the baseline. Our implementation
is available at https://github.com/arghadippurdue/XAMBA.


## GraNNite Enabling High-Performance Execution of Graph Neural Networks on Resource-Constrained Neural Processing Units

>Authors: Arghadip Das, Shamik Kundu, Arnab Raha, Soumendu Ghosh, Deepak Mathaikutty, Vijay Raghunathan

>2025-02-10

> http://arxiv.org/abs/2502.06921v2

Graph Neural Networks (GNNs) are vital for learning from graph-structured
data, enabling applications in network analysis, recommendation systems, and
speech analytics. Deploying them on edge devices like client PCs and laptops
enhances real-time processing, privacy, and cloud independence. GNNs aid
Retrieval-Augmented Generation (RAG) for Large Language Models (LLMs) and
enable event-based vision tasks. However, irregular memory access, **sparsity**,
and dynamic structures cause high latency and energy overhead on
resource-constrained devices. While modern edge processors integrate CPUs,
GPUs, and NPUs, NPUs designed for data-parallel tasks struggle with irregular
GNN computations. We introduce GraNNite, the first hardware-aware framework
optimizing GNN execution on commercial-off-the-shelf (COTS) SOTA DNN
accelerators via a structured three-step methodology: (1) enabling NPU
execution, (2) optimizing performance, and (3) trading accuracy for efficiency
gains. Step 1 employs GraphSplit for workload distribution and StaGr for static
aggregation, while GrAd and NodePad handle dynamic graphs. Step 2 boosts
performance using EffOp for control-heavy tasks and GraSp for **sparsity**
exploitation. Graph Convolution optimizations PreG, SymG, and CacheG reduce
redundancy and memory transfers. Step 3 balances quality versus efficiency,
where QuantGr applies INT8 **quantization**, and GrAx1, GrAx2, and GrAx3 accelerate
attention, broadcast-add, and SAGE-max aggregation. On Intel Core Ultra AI PCs,
GraNNite achieves 2.6X to 7.6X speedups over default NPU mappings and up to
8.6X energy gains over CPUs and GPUs, delivering 10.8X and 6.7X higher
performance than CPUs and GPUs, respectively, across GNN models.


## EfficientLLM Scalable Pruning-Aware Pretraining for Architecture-Agnostic Edge Language Models

>Authors: Xingrun Xing, Zheng Liu, Shitao Xiao, Boyan Gao, Yiming Liang, Wanpeng Zhang, Haokun Lin, Guoqi Li, Jiajun Zhang

>2025-02-10

> http://arxiv.org/abs/2502.06663v2

Modern large language models (LLMs) driven by scaling laws, achieve
intelligence emergency in large model sizes. Recently, the increasing concerns
about cloud costs, latency, and privacy make it an urgent requirement to
develop compact edge language models. Distinguished from direct pretraining
that bounded by the scaling law, this work proposes the **pruning**-aware
pretraining, focusing on retaining performance of much larger optimized models.
It features following characteristics: 1) Data-scalable: we introduce minimal
parameter groups in LLM and continuously optimize structural **pruning**, extending
post-training **pruning** methods like LLM-Pruner and SparseGPT into the
pretraining phase. 2) Architecture-agnostic: the LLM architecture is
auto-designed using saliency-driven **pruning**, which is the first time to exceed
SoTA human-designed LLMs in modern pretraining. We reveal that it achieves
top-quality edge language models, termed EfficientLLM, by scaling up LLM
compression and extending its boundary. EfficientLLM significantly outperforms
SoTA baselines with $100M \sim 1B$ parameters, such as MobileLLM, SmolLM,
Qwen2.5-0.5B, OLMo-1B, Llama3.2-1B in common sense benchmarks. As the first
attempt, EfficientLLM bridges the performance gap between traditional LLM
compression and direct pretraining methods, and we will fully open source at
https://github.com/Xingrun-Xing2/EfficientLLM.


## MoETuner Optimized Mixture of Expert Serving with Balanced Expert Placement and Token Routing

>Authors: Seokjin Go, Divya Mahajan

>2025-02-10

> http://arxiv.org/abs/2502.06643v1

Mixture-of-Experts (MoE) model architecture has emerged as a promising
solution for scaling transformer models efficiently, offering **sparse** activation
that reduces computational costs while increasing model capacity. However, as
MoE models scale, they need to be distributed across GPU devices, thus face
critical performance bottlenecks due to their large memory footprint. Expert
parallelism distributes experts across GPUs, however, faces key challenges
including an unbalanced token routing and expert activation, resulting in
communication tail latency and processing inefficiencies. While existing
solutions address some of these issues, they fail to resolve the dual
challenges of load imbalance and communication skew. The imbalance in token
processing load across experts causes uneven processing times on different
GPUs, while communication skew between GPUs leads to unbalanced inter-GPU data
transfers. These factors degrade the performance of MoE models by increasing
tail latency and reducing overall throughput. To address these limitations, we
propose an Integer Linear Programming (ILP) formulation to optimize expert
placement by jointly considering token load, communication, and computation
costs. We exploit the property that there is a token routing dependency across
layers, where tokens routed to a specific expert in one layer are likely to be
routed to a limited set of experts in the subsequent layer. Our solution,
MoETuner, offers an optimal expert-to-GPU assignment that minimizes inter-GPU
token routing costs and balances token processing across devices, thereby
reducing tail latency and end-to-end execution time. Experimental results
demonstrate 9.3% and 17.5% of end-to-end speedups for single-node and
multi-node inference respectively, showcasing the potential of our ILP-based
optimization for offering expert parallel solutions for next-generation MoEs.


## TripoSG High-Fidelity 3D Shape Synthesis using Large-Scale Rectified Flow Models

>Authors: Yangguang Li, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan Liang, Zhipeng Yu, Xingchao Liu, Yuan-Chen Guo, Ding Liang, Wanli Ouyang, Yan-Pei Cao

>2025-02-10

> http://arxiv.org/abs/2502.06608v1

Recent advancements in diffusion techniques have propelled image and video
generation to unprece- dented levels of quality, significantly accelerating the
deployment and application of generative AI. However, 3D shape generation
technology has so far lagged behind, constrained by limitations in 3D data
scale, complexity of 3D data process- ing, and insufficient exploration of
advanced tech- niques in the 3D domain. Current approaches to 3D shape
generation face substantial challenges in terms of output quality,
generalization capa- bility, and alignment with input conditions. We present
TripoSG, a new streamlined shape diffu- sion paradigm capable of generating
high-fidelity 3D meshes with precise correspondence to input images.
Specifically, we propose: 1) A large-scale rectified flow transformer for 3D
shape generation, achieving state-of-the-art fidelity through training on
extensive, high-quality data. 2) A hybrid supervised training strategy
combining SDF, normal, and eikonal losses for 3D VAE, achieving high- quality
3D reconstruction performance. 3) A data processing pipeline to generate 2
million high- quality 3D samples, highlighting the crucial rules for data
quality and quantity in training 3D gen- erative models. Through comprehensive
experi- ments, we have validated the effectiveness of each component in our new
framework. The seamless integration of these parts has enabled TripoSG to
achieve state-of-the-art performance in 3D shape generation. The resulting 3D
shapes exhibit en- hanced detail due to high-resolution capabilities and
demonstrate exceptional fidelity to input im- ages. Moreover, TripoSG
demonstrates improved versatility in generating 3D models from diverse image
styles and contents, showcasing strong gen- eralization capabilities. To foster
progress and innovation in the field of 3D generation, we will make our model
publicly available.


## UniMoD Efficient Unified Multimodal Transformers with Mixture-of-Depths

>Authors: Weijia Mao, Zhenheng Yang, Mike Zheng Shou

>2025-02-10

> http://arxiv.org/abs/2502.06474v1

Unified multimodal transformers, which handle both generation and
understanding tasks within a shared parameter space, have received increasing
attention in recent research. Although various unified transformers have been
proposed, training these models is costly due to redundant tokens and heavy
attention computation. In the past, studies on large language models have
demonstrated that token **pruning** methods, such as Mixture of Depths (MoD), can
significantly improve computational efficiency. MoD employs a router to select
the most important ones for processing within a transformer layer. However,
directly applying MoD-based token **pruning** to unified transformers will result
in suboptimal performance because different tasks exhibit varying levels of
token redundancy. In our work, we analyze the unified transformers by (1)
examining attention weight patterns, (2) evaluating the layer importance and
token redundancy, and (3) analyzing task interactions. Our findings reveal that
token redundancy is primarily influenced by different tasks and layers.
Building on these findings, we introduce UniMoD, a task-aware token **pruning**
method that employs a separate router for each task to determine which tokens
should be pruned. We apply our method to Show-o and Emu3, reducing training
FLOPs by approximately 15% in Show-o and 40% in Emu3, while maintaining or
improving performance on several benchmarks. Code will be released at
https://github.com/showlab/UniMoD.


## Rethinking Large-scale Dataset Compression Shifting Focus From Labels to Images

>Authors: Lingao Xiao, Songhua Liu, Yang He, Xinchao Wang

>2025-02-10

> http://arxiv.org/abs/2502.06434v1

Dataset distillation and dataset **pruning** are two prominent techniques for
compressing datasets to improve computational and storage efficiency. Despite
their overlapping objectives, these approaches are rarely compared directly.
Even within each field, the evaluation protocols are inconsistent across
various methods, which complicates fair comparisons and hinders
reproducibility. Considering these limitations, we introduce in this paper a
benchmark that equitably evaluates methodologies across both distillation and
**pruning** literatures. Notably, our benchmark reveals that in the mainstream
dataset distillation setting for large-scale datasets, which heavily rely on
soft labels from pre-trained models, even randomly selected subsets can achieve
surprisingly competitive performance. This finding suggests that an
overemphasis on soft labels may be diverting attention from the intrinsic value
of the image data, while also imposing additional burdens in terms of
generation, storage, and application. To address these issues, we propose a new
framework for dataset compression, termed Prune, Combine, and Augment (PCA),
which focuses on leveraging image data exclusively, relies solely on hard
labels for evaluation, and achieves state-of-the-art performance in this setup.
By shifting the emphasis back to the images, our benchmark and PCA framework
pave the way for more balanced and accessible techniques in dataset compression
research. Our code is available at:
https://github.com/ArmandXiao/Rethinking-Dataset-Compression


## Systematic Outliers in Large Language Models

>Authors: Yongqi An, Xu Zhao, Tao Yu, Ming Tang, Jinqiao Wang

>2025-02-10

> http://arxiv.org/abs/2502.06415v1

Outliers have been widely observed in Large Language Models (LLMs),
significantly impacting model performance and posing challenges for model
compression. Understanding the functionality and formation mechanisms of these
outliers is critically important. Existing works, however, largely focus on
reducing the impact of outliers from an algorithmic perspective, lacking an
in-depth investigation into their causes and roles. In this work, we provide a
detailed analysis of the formation process, underlying causes, and functions of
outliers in LLMs. We define and categorize three types of outliers-activation
outliers, weight outliers, and attention outliers-and analyze their
distributions across different dimensions, uncovering inherent connections
between their occurrences and their ultimate influence on the attention
mechanism. Based on these observations, we hypothesize and explore the
mechanisms by which these outliers arise and function, demonstrating through
theoretical derivations and experiments that they emerge due to the
self-attention mechanism's softmax operation. These outliers act as implicit
context-aware scaling factors within the attention mechanism. As these outliers
stem from systematic influences, we term them systematic outliers. Our study
not only enhances the understanding of Transformer-based LLMs but also shows
that structurally eliminating outliers can accelerate convergence and improve
model compression. The code is avilable at
https://github.com/an-yongqi/systematic-outliers.


## Utilizing Novelty-based Evolution Strategies to Train Transformers in Reinforcement Learning

>Authors: Matyáš Lorenc

>2025-02-10

> http://arxiv.org/abs/2502.06301v1

In this paper, we experiment with novelty-based variants of OpenAI-ES, the
NS-ES and NSR-ES algorithms, and evaluate their effectiveness in training
complex, transformer-based architectures designed for the problem of
reinforcement learning such as Decision Transformers. We also test if we can
accelerate the novelty-based training of these larger models by seeding the
training by a pretrained models. By this, we build on our previous work, where
we tested the ability of evolution strategies - specifically the aforementioned
OpenAI-ES - to train the Decision Transformer architecture. The results were
mixed. NS-ES showed progress, but it would clearly need many more iterations
for it to yield interesting results. NSR-ES, on the other hand, proved quite
capable of being straightforwardly used on larger models, since its performance
appears as similar between the feed-forward model and Decision Transformer, as
it was for the OpenAI-ES in our previous work.


## Progressive Collaborative and Semantic Knowledge Fusion for Generative Recommendation

>Authors: Longtao Xiao, Haozhao Wang, Cheng Wang, Linfei Ji, Yifan Wang, Jieming Zhu, Zhenhua Dong, Rui Zhang, Ruixuan Li

>2025-02-10

> http://arxiv.org/abs/2502.06269v1

With the recent surge in interest surrounding generative paradigms,
generative recommendation has increasingly attracted the attention of
researchers in the recommendation community. This paradigm generally consists
of two stages. In the first stage, pretrained semantic embeddings or
collaborative ID embeddings are **quantize**d to create item codes, aiming to
capture and preserve rich semantic or collaborative knowledge within these
codes. The second stage involves utilizing these discrete codes to perform an
autoregressive sequence generation task. Existing methods often either overlook
collaborative or semantic knowledge, or combine the two roughly. In this paper,
we observe that naively concatenating representations from semantic and
collaborative modality leads to a semantic domination issue, where the
resulting representation is overly influenced by semantic information,
effectively overshadowing the collaborative representation. Consequently,
downstream recommendation tasks fail to fully exploit the knowledge from both
modalities, resulting in suboptimal performance. To address this, we propose a
progressive collaborative and semantic knowledge fusion model for generative
recommendation, named PRORec, which integrates semantic and collaborative
knowledge with a unified code through a two-stage framework. Specifically, in
the first stage, we propose a cross-modality knowledge alignment task, which
integrates semantic knowledge into collaborative embeddings, enhancing their
representational capability. In the second stage, we propose an in-modality
knowledge distillation task, designed to effectively capture and integrate
knowledge from both semantic and collaborative modalities. Extensive
experiments on three widely used benchmarks validate the effectiveness of our
approach, demonstrating its superiority compared to existing methods.


## Efficient-vDiT Efficient Video Diffusion Transformers With Attention Tile

>Authors: Hangliang Ding, Dacheng Li, Runlong Su, Peiyuan Zhang, Zhijie Deng, Ion Stoica, Hao Zhang

>2025-02-10

> http://arxiv.org/abs/2502.06155v1

Despite the promise of synthesizing high-fidelity videos, Diffusion
Transformers (DiTs) with 3D full attention suffer from expensive inference due
to the complexity of attention computation and numerous sampling steps. For
example, the popular Open-Sora-Plan model consumes more than 9 minutes for
generating a single video of 29 frames. This paper addresses the inefficiency
issue from two aspects: 1) Prune the 3D full attention based on the redundancy
within video data; We identify a prevalent tile-style repetitive pattern in the
3D attention maps for video data, and advocate a new family of **sparse** 3D
attention that holds a linear complexity w.r.t. the number of video frames. 2)
Shorten the sampling process by adopting existing multi-step consistency
distillation; We split the entire sampling trajectory into several segments and
perform consistency distillation within each one to activate few-step
generation capacities. We further devise a three-stage training pipeline to
conjoin the low-complexity attention and few-step generation capacities.
Notably, with 0.1% pretraining data, we turn the Open-Sora-Plan-1.2 model into
an efficient one that is 7.4x -7.8x faster for 29 and 93 frames 720p video
generation with a marginal performance trade-off in VBench. In addition, we
demonstrate that our approach is amenable to distributed inference, achieving
an additional 3.91x speedup when running on 4 GPUs with sequence parallelism.


## A CT Geometry With Multiple Centers Of Rotation For Solving Sparse View Problem

>Authors: Jiayu Duan, Yang Li, Jianmei Cai, Xuanqin Mou

>2025-02-10

> http://arxiv.org/abs/2502.06125v1

With the emergence of CNT (Carbon nanotube), static and instant CT scanning
becomes possible. By transforming the traditionally rotated thermal source into
a static ring array source composed of multiple CNTs, the imaging system can
achieve high temporal resolution in scanning. However, due to the
non-negligible packaging size of CNTs, the static CT based on CNTs faces **sparse**
view problem, which affects the image quality by introducing streak artifacts.
In this study, we based on the local correlation equation (LCE) to address the
**sparse** view problem of static CT. The LCE is a series of partial differential
equations (PDEs) to describe the local correlation of Radon transform in a
neighborhood projection domain. Based on LCE, we analyze the characteristic of
**sparse** view projection and propose a scanning geometry with multiple rotation
centers, which is different from existing CT devices that acquires the
projection around one rotation center. Specifically, in the proposed scanning
geometry, the circular ring array X-ray sources is divided into several arcs
while the sources of each arc share one rotation center. All rotation centers
of the arcs are uniformly distributed on a small circle. The optimal
distribution of the rotation centers can be optimized by the radius of the
circle. Moreover, to elevate the image quality under the **sparse** view
reconstruction, we employed the LCE to interpolate unmeasured projections.
Compared to the single rotation center scheme used in existing CT geometries,
the multiple rotation centers scan contributes to a more even projection
distribution with same view number. The simulated results demonstrated the
efficiency and potential applications of the proposed method in static CT
reconstructions.


## Real-Time LiDAR Point Cloud Compression and Transmission for Resource-constrained Robots

>Authors: Yuhao Cao, Yu Wang, Haoyao Chen

>2025-02-10

> http://arxiv.org/abs/2502.06123v1

LiDARs are widely used in autonomous robots due to their ability to provide
accurate environment structural information. However, the large size of point
clouds poses challenges in terms of data storage and transmission. In this
paper, we propose a novel point cloud compression and transmission framework
for resource-constrained robotic applications, called RCPCC. We iteratively fit
the surface of point clouds with a similar range value and eliminate redundancy
through their spatial relationships. Then, we use Shape-adaptive DCT (SA-DCT)
to transform the unfit points and reduce the data volume by quantizing the
transformed coefficients. We design an adaptive bitrate control strategy based
on QoE as the optimization goal to control the quality of the transmitted point
cloud. Experiments show that our framework achieves compression rates of
40$\times$ to 80$\times$ while maintaining high accuracy for downstream
applications. our method significantly outperforms other baselines in terms of
accuracy when the compression rate exceeds 70$\times$. Furthermore, in
situations of reduced communication bandwidth, our adaptive bitrate control
strategy demonstrates significant QoE improvements. The code will be available
at https://github.com/HITSZ-NRSL/RCPCC.git.


## Enabling Autoregressive Models to Fill In Masked Tokens

>Authors: Daniel Israel, Aditya Grover, Guy Van den Broeck

>2025-02-09

> http://arxiv.org/abs/2502.06901v1

Historically, LLMs have been trained using either autoregressive (AR) or
masked language modeling (MLM) objectives, with AR models gaining dominance in
recent years. However, AR models are inherently incapable of masked infilling,
which is the ability to predict masked tokens between past and future context.
In contrast, MLM models suffer from intrinsic computational inefficiencies
during both training and inference that hinder their scalability. This work
introduces MARIA (Masked and Autoregressive Infilling Architecture), a novel
approach that leverages the strengths of both paradigms to achieve
state-of-the-art masked infilling performance. MARIA combines a pre-trained MLM
and AR model by training a linear decoder that takes their concatenated hidden
states as input. This minimal modification enables the AR model to perform
infilling while retaining its inherent advantages in terms of faster inference
with **KV** caching. Our results demonstrate that MARIA significantly outperforms
existing methods, namely discrete diffusion models, on masked infilling tasks.


## Electric field control of nonlinear Hall effect in Weyl semimetal TaIrTe4

>Authors: Jiaju Yang, Lujun Wei, Yanghui Li, Lina Chen, Wei Niu, Shuo Wang, Feng Li, Ping Liu, Shuang Zhou, Yong Pu

>2025-02-09

> http://arxiv.org/abs/2502.05960v1

The nonlinear Hall effect (NLHE), as an important probe to reveal the
symmetry breaking in topological properties of materials, opens up a new
dimension for exploring the energy band structure and electron transport
mechanism of quantum materials. Current studies mainly focus on the observation
of material intrinsic the NLHE or inducing the NLHE response by artificially
constructing corrugated/twisted twodimensionalmaterial systems. Notably, the
modulation of NLHE signal strength, a core parameter of device performance, has
attracted much attention, while theoretical predictions suggest that an applied
electric field can achieve the NLHE enhancement through modulation of the Berry
curvature dipole (BCD). Here we report effective modulation the magnitude and
sign of the NLHE by applying additional constant electric fields of different
directions and magnitudes in the semimetal TaIrTe4. The NLHE response strength
is enhanced by 168 times compared to the intrinsic one at 4 K when the
additional constant electric field of -0.5 kV/cm is applied to the b-axis of
TaIrTe4 and the through a.c. current is parallel to the TaIrTe4 a-axis. Scaling
law analysis suggests that the enhancement may be the result of the combined
effect of the electric field on the intrinsic BCD and disorder scattering
effect of TaIrTe4. This work provides a means to study the properties of
TaIrTe4, as well as a valuable reference for the study of novel electronic
devices.


## Acceleration Multiple Heads Decoding for LLM via Dynamic Tree Attention

>Authors: Zhendong Zhang

>2025-02-09

> http://arxiv.org/abs/2502.05947v1

Multiple heads decoding accelerates the inference of Large Language Models
(LLMs) by predicting next several tokens simultaneously. It generates and
verifies multiple candidate sequences in parallel via tree attention with a
fixed structure. In this paper, we replace the fixed tree attention with
dynamic tree attention on multiple head decoding, specifically in the context
of MEDUSA. We propose a simple and low complexity strategy to generate
candidates and construct the dynamic tree structure. Preliminary experiments
show that the proposed method improves the decoding efficiency of multiple head
decoding for LLMs while maintaining the generation quality. This result
demonstrates the potential for improvement of multiple head decoding in
candidate generation.


## A Universal Transformer-Based Coarse-Grained Molecular Dynamics Framework for Protein Dynamics

>Authors: Jinzhen Zhu

>2025-02-09

> http://arxiv.org/abs/2502.05909v1

We present a novel, universal, Transformer-based coarse-grained molecular
dynamics (CG-MD) framework for simulating protein dynamics. Our trained model
generalizes to all protein systems, regardless of sequence length or number of
chains. First, we extend a tree-structured protein representation to
accommodate multi-chain proteins, demonstrating sub-angstrom-level accuracy in
reconstructing a 169-amino-acid protein structure. Then, representing
collective variables as language-like sequences, we use a Transformer network
as a propagator for stochastic differential equations, generating MD
trajectories over 10,000 times faster than all-atom MD simulations. This single
trained model accurately simulates both single-chain and two-chain proteins,
and the generated trajectories closely resemble all-atom MD trajectories in
their RMSD profiles. With sufficient training data, we anticipate that our
model can achieve universality across all proteins, offering a ~10,000x
**acceleration** of MD simulations with high accuracy.


## Zak-Transform-Induced Optimal Sequences and Their Applications in OTFS

>Authors: Xiuping Peng, Congying Wu, Zilong Liu, Chunlei Li, Jianye Zhang, Pingzhi Fan

>2025-02-09

> http://arxiv.org/abs/2502.05853v1

This paper introduces a novel finite Zak transform (FZT)-aided framework for
constructing multiple zero-correlation zone (ZCZ) sequence sets with optimal
correlation properties. Specifically, each sequence is perfect with zero
auto-correlation sidelobes, each ZCZ sequence set meets the Tang-Fan-Matsufuji
bound with equality, and the maximum inter-set cross-correlation of multiple
sequence sets meets the Sarwate bound with equality. Our study shows that these
sequences can be **sparse**ly expressed in the Zak domain through properly selected
index and phase matrices. Particularly, it is found that the maximum inter-set
cross-correlation beats the Sarwate bound if every index matrix is a circular
Florentine array. Several construction methods of multiple ZCZ sequence sets
are proposed, demonstrating both the optimality and high flexibility.
Additionally, it is shown that excellent synchronization performance can be
achieved by the proposed sequences in orthogonal-time-frequency-space (OTFS)
systems.


## MetaML-Pro Cross-Stage Design Flow Automation for Efficient Deep Learning Acceleration

>Authors: Zhiqiang Que, Jose G. F. Coutinho, Ce Guo, Hongxiang Fan, Wayne Luk

>2025-02-09

> http://arxiv.org/abs/2502.05850v1

This paper presents a unified framework for codifying and automating
optimization strategies to efficiently deploy deep neural networks (DNNs) on
resource-constrained hardware, such as FPGAs, while maintaining high
performance, accuracy, and resource efficiency. Deploying DNNs on such
platforms involves addressing the significant challenge of balancing
performance, resource usage (e.g., DSPs and LUTs), and inference accuracy,
which often requires extensive manual effort and domain expertise. Our novel
approach addresses two key issues: cross-stage co-optimization and optimization
search. By seamlessly integrating programmatic DNN optimization techniques with
high-level synthesis (HLS)-based metaprogramming and leveraging advanced design
space exploration (DSE) strategies like Bayesian optimization, the framework
automates both top-down and bottom-up design flows, reducing the need for
manual intervention and domain expertise. The proposed framework introduces
customizable optimization, transformation, and control blocks to enhance DNN
accelerator performance and resource efficiency. Experimental results
demonstrate up to a 92\% DSP and 89\% LUT usage reduction for select networks,
while preserving accuracy, along with a 15.6-fold reduction in optimization
time compared to grid search. These results underscore the novelty and
potential of the proposed framework for automated, resource-efficient DNN
accelerator designs.


## StreamDCIM A Tile-based Streaming Digital CIM Accelerator with Mixed-stationary Cross-forwarding Dataflow for Multimodal Transformer

>Authors: Shantian Qin, Ziqing Qiang, Zhihua Fan, Wenming Li, Xuejun An, Xiaochun Ye, Dongrui Fan

>2025-02-09

> http://arxiv.org/abs/2502.05798v1

Multimodal Transformers are emerging artificial intelligence (AI) models
designed to process a mixture of signals from diverse modalities. Digital
computing-in-memory (CIM) architectures are considered promising for achieving
high efficiency while maintaining high accuracy. However, current digital
CIM-based accelerators exhibit inflexibility in microarchitecture, dataflow,
and pipeline to effectively accelerate multimodal Transformer. In this paper,
we propose StreamDCIM, a tile-based streaming digital CIM accelerator for
multimodal Transformers. It overcomes the above challenges with three features:
First, we present a tile-based reconfigurable CIM macro microarchitecture with
normal and hybrid reconfigurable modes to improve intra-macro CIM utilization.
Second, we implement a mixed-stationary cross-forwarding dataflow with
tile-based execution decoupling to exploit tile-level computation parallelism.
Third, we introduce a ping-pong-like fine-grained compute-rewriting pipeline to
overlap high-latency on-chip CIM rewriting. Experimental results show that
StreamDCIM outperforms non-streaming and layer-based streaming CIM-based
solutions by geomean 2.63$\times$ and 1.28$\times$ on typical multimodal
Transformer models.


## Explainable and Class-Revealing Signal Feature Extraction via Scattering Transform and Constrained Zeroth-Order Optimization

>Authors: Naoki Saito, David Weber

>2025-02-08

> http://arxiv.org/abs/2502.05722v2

We propose a new method to extract discriminant and explainable features from
a particular machine learning model, i.e., a combination of the scattering
transform and the multiclass logistic regression. Although this model is
well-known for its ability to learn various signal classes with high
classification rate, it remains elusive to understand why it can generate such
successful classification, mainly due to the nonlinearity of the scattering
transform. In order to uncover the meaning of the scattering transform
coefficients selected by the multiclass logistic regression (with the Lasso
penalty), we adopt zeroth-order optimization algorithms to search an input
pattern that maximizes the class probability of a class of interest given the
learned model. In order to do so, it turns out that imposing **sparsity** and
smoothness of input patterns is important. We demonstrate the effectiveness of
our proposed method using a couple of synthetic time-series classification
problems.


## Flowing Through Layers A Continuous Dynamical Systems Perspective on Transformers

>Authors: Jacob Fein-Ashley

>2025-02-08

> http://arxiv.org/abs/2502.05656v1

We show that the standard discrete update rule of transformer layers can be
naturally interpreted as a forward Euler discretization of a continuous
dynamical system. Our Transformer Flow Approximation Theorem demonstrates that,
under standard Lipschitz continuity assumptions, token representations converge
uniformly to the unique solution of an ODE as the number of layers grows.
Moreover, if the underlying mapping satisfies a one-sided Lipschitz condition
with a negative constant, the resulting dynamics are contractive, causing
perturbations to decay exponentially across layers. Beyond clarifying the
empirical stability and expressivity of transformer models, these insights link
transformer updates to a broader iterative reasoning framework, suggesting new
avenues for accelerated convergence and architectural innovations inspired by
dynamical systems theory.


## Quantization of Carrollian Fermions

>Authors: Ertuğrul Ekiz, Emre Onur Kahya, Utku Zorba

>2025-02-08

> http://arxiv.org/abs/2502.05645v2

We provide the first example of interacting **quantize**d Carrollian Dirac
fermions and investigate their discrete symmetries, including charge
conjugation (C), parity (P), and time reversal (T) transformations. As a toy
model, we couple these fermions to a Carrollian scalar field using Carrollian
Yukawa theory and compute the tree-level diagram, revealing an ultralocal
interaction between the Carrollian fermions and the scalar field. This
interaction, widely known as a Dirac delta interaction with time-dependent
factor, frequently appears in quantum physics. We then address the
renormalization of the theory by employing the Wilsonian procedure at one-loop
order. Furthermore, we analyze the fixed points and stability properties of
Carrollian Yukawa theory, comparing them with their relativistic counterparts.
Beyond the specific Yukawa model studied here, we expect that our framework
will have broader applications in Carrollian physics, particularly in
understanding ultralocal interactions and their role in condensed matter
systems, where similar phenomena arise in strongly correlated and
non-relativistic regimes.


## Towards Sustainable NLP Insights from Benchmarking Inference Energy in Large Language Models

>Authors: Soham Poddar, Paramita Koley, Janardan Misra, Niloy Ganguly, Saptarshi Ghosh

>2025-02-08

> http://arxiv.org/abs/2502.05610v1

Large language models (LLMs) are increasingly recognized for their
exceptional generative capabilities and versatility across various tasks.
However, the high inference costs associated with these models have not
received adequate attention, particularly when compared to the focus on
training costs in existing research. In response to this gap, our study
conducts a comprehensive benchmarking of LLM inference energy across a wide
range of NLP tasks, where we analyze the impact of different models, tasks,
prompts, and system-related factors on inference energy. Specifically, our
experiments reveal several interesting insights, including strong correlation
of inference energy with output token length and response time. Also, we find
that **quantization** and optimal batch sizes, along with targeted prompt phrases,
can significantly reduce energy usage. This study is the first to thoroughly
benchmark LLM inference across such a diverse range of aspects, providing
insights and offering several recommendations for improving energy efficiency
in model deployment.


## Lossless Acceleration of Large Language Models with Hierarchical Drafting based on Temporal Locality in Speculative Decoding

>Authors: Sukmin Cho, Sangjin Choi, Taeho Hwang, Jeongyeon Seo, Soyeong Jeong, Huije Lee, Hoyun Song, Jong C. Park, Youngjin Kwon

>2025-02-08

> http://arxiv.org/abs/2502.05609v1

Accelerating inference in Large Language Models (LLMs) is critical for
real-time interactions, as they have been widely incorporated into real-world
services. Speculative decoding, a fully algorithmic solution, has gained
attention for improving inference speed by drafting and verifying tokens,
thereby generating multiple tokens in a single forward pass. However, current
drafting strategies usually require significant fine-tuning or have
inconsistent performance across tasks. To address these challenges, we propose
Hierarchy Drafting (HD), a novel lossless drafting approach that organizes
various token sources into multiple databases in a hierarchical framework based
on temporal locality. In the drafting step, HD sequentially accesses multiple
databases to obtain draft tokens from the highest to the lowest locality,
ensuring consistent **acceleration** across diverse tasks and minimizing drafting
latency. Our experiments on Spec-Bench using LLMs with 7B and 13B parameters
demonstrate that HD outperforms existing database drafting methods, achieving
robust inference speedups across model sizes, tasks, and temperatures.


## UbiMoE A Ubiquitous Mixture-of-Experts Vision Transformer Accelerator With Hybrid Computation Pattern on FPGA

>Authors: Jiale Dong, Wenqi Lou, Zhendong Zheng, Yunji Qin, Lei Gong, Chao Wang, Xuehai Zhou

>2025-02-08

> http://arxiv.org/abs/2502.05602v2

Compared to traditional Vision Transformers (ViT), Mixture-of-Experts Vision
Transformers (MoE-ViT) are introduced to scale model size without a
proportional increase in computational complexity, making them a new research
focus. Given the high performance and reconfigurability, FPGA-based
accelerators for MoE-ViT emerge, delivering substantial gains over
general-purpose processors. However, existing accelerators often fall short of
fully exploring the design space, leading to suboptimal trade-offs between
resource utilization and performance. To overcome this problem, we introduce
UbiMoE, a novel end-to-end FPGA accelerator tailored for MoE-ViT. Leveraging
the unique computational and memory access patterns of MoE-ViTs, we develop a
latency-optimized streaming attention kernel and a resource-efficient reusable
linear kernel, effectively balancing performance and resource consumption. To
further enhance design efficiency, we propose a two-stage heuristic search
algorithm that optimally tunes hardware parameters for various FPGA resource
constraints. Compared to state-of-the-art (SOTA) FPGA designs, UbiMoE achieves
1.34x and 3.35x throughput improvements for MoE-ViT on Xilinx ZCU102 and Alveo
U280 platforms, respectively, while enhancing energy efficiency by 1.75x and
1.54x. Our implementation is available at https://github.com/DJ000011/UbiMoE.


## Human-AI collaboration for modeling heat conduction in nanostructures

>Authors: Wenyang Ding, Jiang Guo, Meng An, Koji Tsuda, Junichiro Shiomi

>2025-02-08

> http://arxiv.org/abs/2502.05576v1

In recent years, materials informatics, which combines data science and
artificial intelligence (AI), has garnered significant attention owing to its
ability to accelerate material development, reduce costs, and enhance product
design. However, despite the widespread use of AI, human involvement is often
limited to the initiation and oversight of machine learning processes and
rarely includes more substantial roles that capitalize on human intuition or
domain expertise. Consequently, true human-AI collaborations, where integrated
insights can be maximized, are scarce. This study considers the problem of heat
conduction in a two-dimensional nanostructure as a case study. An integrated
human-AI collaboration framework is designed and used to construct a model to
predict the thermal conductivity. This approach is used to determine the
parameters that govern phonon transmission over the full range of frequencies
and incidence angles. During operation, the self-learning entropic population
annealing technique, which combines entropic sampling with a surrogate machine
learning model, generates a small dataset that can be interpreted by a human.
Therefore, data-efficient and global modeling is achieved, and parameters with
physical interpretations are developed, which can guide nanostructural design
to produce materials with specific properties. The proposed framework can
leverage the complementary strengths of humans and AI, thereby enhancing the
understanding and control of materials.


## Diffusion Model for Interest Refinement in Multi-Interest Recommendation

>Authors: Yankun Le, Haoran Li, Baoyuan Ou, Yingjie Qin, Zhixuan Yang, Ruilong Su, Fu Zhang

>2025-02-08

> http://arxiv.org/abs/2502.05561v2

Multi-interest candidate matching plays a pivotal role in personalized
recommender systems, as it captures diverse user interests from their
historical behaviors. Most existing methods utilize attention mechanisms to
generate interest representations by aggregating historical item embeddings.
However, these methods only capture overall item-level relevance, leading to
coarse-grained interest representations that include irrelevant information. To
address this issue, we propose the Diffusion Multi-Interest model (DMI), a
novel framework for refining user interest representations at the dimension
level. Specifically, DMI first introduces controllable noise into
coarse-grained interest representations at the dimensional level. Then, in the
iterative reconstruction process, DMI combines a cross-attention mechanism and
an item **pruning** strategy to reconstruct the personalized interest vectors with
the guidance of tailored collaborative information. Extensive experiments
demonstrate the effectiveness of DMI, surpassing state-of-the-art methods on
offline evaluations and an online A/B test. Successfully deployed in the
real-world recommender system, DMI effectively enhances user satisfaction and
system performance at scale, serving the major traffic of hundreds of millions
of daily active users. \footnote{The code will be released for reproducibility
once the paper is accepted.}


## SSH Sparse Spectrum Adaptation via Discrete Hartley Transformation

>Authors: Yixian Shen, Qi Bi, Jia-Hong Huang, Hongyi Zhu, Andy D. Pimentel, Anuj Pathania

>2025-02-08

> http://arxiv.org/abs/2502.05539v1

Low-rank adaptation (LoRA) has been demonstrated effective in reducing the
trainable parameter number when fine-tuning a large foundation model (LLM).
However, it still encounters computational and memory challenges when scaling
to larger models or addressing more complex task adaptation.
  In this work, we introduce Sparse Spectrum Adaptation via Discrete Hartley
Transformation (SSH), a novel approach that significantly reduces the number of
trainable parameters while enhancing model performance. It selects the most
informative spectral components across all layers, under the guidance of the
initial weights after a discrete Hartley transformation (DHT). The lightweight
inverse DHT then projects the spectrum back into the spatial domain for
updates.
  Extensive experiments across both single-modality tasks such as language
understanding and generation and multi-modality tasks such as video-text
understanding demonstrate that SSH outperforms existing parameter-efficient
fine-tuning (PEFT) methods while achieving substantial reductions in
computational cost and memory requirements.


## Mix Data or Merge Models? Balancing the Helpfulness, Honesty, and Harmlessness of Large Language Model via Model Merging

>Authors: Jinluan Yang, Dingnan Jin, Anke Tang, Li Shen, Didi Zhu, Zhengyu Chen, Daixin Wang, Qing Cui, Zhiqiang Zhang, Jun Zhou, Fei Wu, Kun Kuang

>2025-02-08

> http://arxiv.org/abs/2502.06876v2

Achieving balanced alignment of large language models (LLMs) in terms of
Helpfulness, Honesty, and Harmlessness (3H optimization) constitutes a
cornerstone of responsible AI, with existing methods like data mixture
strategies facing limitations including reliance on expert knowledge and
conflicting optimization signals. While model merging offers a promising
alternative by integrating specialized models, its potential for 3H
optimization remains underexplored. This paper establishes the first
comprehensive benchmark for model merging in 3H-aligned LLMs, systematically
evaluating 15 methods (12 training-free merging and 3 data mixture techniques)
across 10 datasets associated with 5 annotation dimensions, 2 LLM families, and
2 training paradigms. Our analysis reveals three pivotal insights: (i)
previously overlooked collaborative/conflicting relationships among 3H
dimensions, (ii) the consistent superiority of model merging over data mixture
approaches in balancing alignment trade-offs, and (iii) the critical role of
parameter-level conflict resolution through redundant component **pruning** and
outlier mitigation. Building on these findings, we propose R-TSVM, a
Reweighting-enhanced Task Singular Vector Merging method that incorporates
outlier-aware parameter weighting and **sparsity**-adaptive rank selection
strategies adapted to the heavy-tailed parameter distribution and **sparsity** for
LLMs, further improving LLM alignment across multiple evaluations. We release
our trained models for further exploration.


## IndexTTS An Industrial-Level Controllable and Efficient Zero-Shot Text-To-Speech System

>Authors: Wei Deng, Siyi Zhou, Jingchen Shu, Jinchao Wang, Lu Wang

>2025-02-08

> http://arxiv.org/abs/2502.05512v1

Recently, large language model (LLM) based text-to-speech (TTS) systems have
gradually become the mainstream in the industry due to their high naturalness
and powerful zero-shot voice cloning capabilities.Here, we introduce the
IndexTTS system, which is mainly based on the XTTS and Tortoise model. We add
some novel improvements. Specifically, in Chinese scenarios, we adopt a hybrid
modeling method that combines characters and pinyin, making the pronunciations
of polyphonic characters and long-tail characters controllable. We also
performed a comparative analysis of the Vector Quantization (VQ) with
Finite-Scalar Quantization (FSQ) for codebook utilization of acoustic speech
tokens. To further enhance the effect and stability of voice cloning, we
introduce a conformer-based speech conditional encoder and replace the
speechcode decoder with BigVGAN2. Compared with XTTS, it has achieved
significant improvements in naturalness, content consistency, and zero-shot
voice cloning. As for the popular TTS systems in the open-source, such as
Fish-Speech, CosyVoice2, FireRedTTS and F5-TTS, IndexTTS has a relatively
simple training process, more controllable usage, and faster inference speed.
Moreover, its performance surpasses that of these systems. Our demos are
available at https://index-tts.github.io.


## AdaFlow Efficient Long Video Editing via Adaptive Attention Slimming And Keyframe Selection

>Authors: Shuheng Zhang, Yuqi Liu, Hongbo Zhou, Jun Peng, Yiyi Zhou, Xiaoshuai Sun, Rongrong Ji

>2025-02-08

> http://arxiv.org/abs/2502.05433v1

Despite great progress, text-driven long video editing is still notoriously
challenging mainly due to excessive memory overhead. Although recent efforts
have simplified this task into a two-step process of keyframe translation and
interpolation generation, the token-wise keyframe translation still plagues the
upper limit of video length. In this paper, we propose a novel and
training-free approach towards efficient and effective long video editing,
termed AdaFlow. We first reveal that not all tokens of video frames hold equal
importance for keyframe translation, based on which we propose an Adaptive
Attention Slimming scheme for AdaFlow to squeeze the $**KV**$ sequence, thus
increasing the number of keyframes for translations by an order of magnitude.
In addition, an Adaptive Keyframe Selection scheme is also equipped to select
the representative frames for joint editing, further improving generation
quality. With these innovative designs, AdaFlow achieves high-quality long
video editing of minutes in one inference, i.e., more than 1$k$ frames on one
A800 GPU, which is about ten times longer than the compared methods, e.g.,
TokenFlow. To validate AdaFlow, we also build a new benchmark for long video
editing with high-quality annotations, termed LongV-EVAL. Our code is released
at: https://github.com/jidantang55/AdaFlow.


## APE Faster and Longer Context-Augmented Generation via Adaptive Parallel Encoding

>Authors: Xinyu Yang, Tianqi Chen, Beidi Chen

>2025-02-08

> http://arxiv.org/abs/2502.05431v2

Context-augmented generation (CAG) techniques, including RAG and ICL, require
the efficient combination of multiple contexts to generate responses to user
queries. Directly inputting these contexts as a sequence introduces a
considerable computational burden by re-encoding the combined selection of
contexts for every request. To address this, we explore the promising potential
of parallel encoding to independently pre-compute and cache each context's **KV**
states. This approach enables the direct loading of cached states during
inference while accommodating more contexts through position reuse across
contexts. However, due to misalignments in attention distribution, directly
applying parallel encoding results in a significant performance drop. To enable
effective and efficient CAG, we propose Adaptive Parallel Encoding
($\textbf{APE}$), which brings shared prefix, attention temperature, and
scaling factor to align the distribution of parallel encoding with sequential
encoding. Results on RAG and ICL tasks demonstrate that APE can preserve 98%
and 93% sequential encoding performance using the same inputs while
outperforming parallel encoding by 3.6% and 7.9%, respectively. It also scales
to many-shot CAG, effectively encoding hundreds of contexts in parallel.
Efficiency evaluation shows that APE can achieve an end-to-end 4.5$\times$
speedup by reducing 28$\times$ prefilling time for a 128K-length context.


## The Complexity of Learning Sparse Superposed Features with Feedback

>Authors: Akash Kumar

>2025-02-08

> http://arxiv.org/abs/2502.05407v2

The success of deep networks is crucially attributed to their ability to
capture latent features within a representation space. In this work, we
investigate whether the underlying learned features of a model can be
efficiently retrieved through feedback from an agent, such as a large language
model (LLM), in the form of relative \textit{triplet comparisons}. These
features may represent various constructs, including dictionaries in LLMs or
components of a covariance matrix of Mahalanobis distances. We analyze the
feedback complexity associated with learning a feature matrix in **sparse**
settings. Our results establish tight bounds when the agent is permitted to
construct activations and demonstrate strong upper bounds in **sparse** scenarios
when the agent's feedback is limited to distributional information. We validate
our theoretical findings through experiments on two distinct applications:
feature recovery from Recursive Feature Machine-trained models and dictionary
extraction from **sparse** autoencoders trained on Large Language Models.


## BCQ Block Clustered Quantization for 4-bit (W4A4) LLM Inference

>Authors: Reena Elangovan, Charbel Sakr, Anand Raghunathan, Brucek Khailany

>2025-02-07

> http://arxiv.org/abs/2502.05376v1

Post-training **quantization** (PTQ) is a promising approach to reducing the
storage and computational requirements of large language models (LLMs) without
additional training cost. Recent PTQ studies have primarily focused on
quantizing only weights to sub-8-bits while maintaining activations at 8-bits
or higher. Accurate sub-8-bit **quantization** for both weights and activations
without relying on **quantization**-aware training remains a significant challenge.
We propose a novel **quantization** method called block clustered **quantization**
(BCQ) wherein each operand tensor is decomposed into blocks (a block is a group
of contiguous scalars), blocks are clustered based on their statistics, and a
dedicated optimal **quantization** codebook is designed for each cluster. As a
specific embodiment of this approach, we propose a PTQ algorithm called
Locally-Optimal BCQ (LO-BCQ) that iterates between the steps of block
clustering and codebook design to greedily minimize the **quantization** mean
squared error. When weight and activation scalars are encoded to W4A4 format
(with 0.5-bits of overhead for storing scaling factors and codebook selectors),
we advance the current state-of-the-art by demonstrating <1% loss in inference
accuracy across several LLMs and downstream tasks.


## fMoE Fine-Grained Expert Offloading for Large Mixture-of-Experts Serving

>Authors: Hanfei Yu, Xingqi Cui, Hong Zhang, Hao Wang, Hao Wang

>2025-02-07

> http://arxiv.org/abs/2502.05370v1

Large Language Models (LLMs) have gained immense success in revolutionizing
various applications, including content generation, search and recommendation,
and AI-assisted operation. To reduce high training costs, Mixture-of-Experts
(MoE) architecture has become a popular backbone for modern LLMs. However,
despite the benefits, serving MoE-based LLMs experience severe memory
inefficiency due to **sparse**ly activated experts. Recent studies propose to
offload inactive experts from GPU memory to CPU memory to improve the serving
efficiency of MoE models. However, they either incur high inference latency or
high model memory footprints due to coarse-grained designs. To tame the
latency-memory trade-off in MoE serving, we present fMoE, a fine-grained expert
offloading system for MoE serving that achieves low inference latency with
memory efficiency. We design fMoE to extract fine-grained expert selection
patterns from MoE models and semantic hints from input prompts to efficiently
guide expert prefetching, caching, and offloading decisions. fMoE is prototyped
on top of HuggingFace Transformers and deployed on a six-GPU testbed.
Experiments with open-source MoE models and real-world workloads show that fMoE
reduces inference latency by 47% and improves expert hit rate by 36% over
state-of-the-art solutions.


## Fillerbuster Multi-View Scene Completion for Casual Captures

>Authors: Ethan Weber, Norman Müller, Yash Kant, Vasu Agrawal, Michael Zollhöfer, Angjoo Kanazawa, Christian Richardt

>2025-02-07

> http://arxiv.org/abs/2502.05175v1

We present Fillerbuster, a method that completes unknown regions of a 3D
scene by utilizing a novel large-scale multi-view latent diffusion transformer.
Casual captures are often **sparse** and miss surrounding content behind objects or
above the scene. Existing methods are not suitable for handling this challenge
as they focus on making the known pixels look good with **sparse**-view priors, or
on creating the missing sides of objects from just one or two photos. In
reality, we often have hundreds of input frames and want to complete areas that
are missing and unobserved from the input frames. Additionally, the images
often do not have known camera parameters. Our solution is to train a
generative model that can consume a large context of input frames while
generating unknown target views and recovering image poses when desired. We
show results where we complete partial captures on two existing datasets. We
also present an uncalibrated scene completion task where our unified model
predicts both poses and creates new content. Our model is the first to predict
many images and poses together for scene completion.


## Multiphoton, multimode state classification for nonlinear optical circuits

>Authors: Denis A. Kopylov, Christian Offen, Laura Ares, Boris Wembe Moafo, Sina Ober-Blöbaum, Torsten Meier, Polina R. Sharapova, Jan Sperling

>2025-02-07

> http://arxiv.org/abs/2502.05123v1

We introduce a new classification of multimode states with a fixed number of
photons. This classification is based on the factorizability of homogeneous
multivariate polynomials and is invariant under unitary transformations. The
classes physically correspond to field excitations in terms of single and
multiple photons, each of which being in an arbitrary irreducible superposition
of **quantize**d modes. We further show how the transitions between classes are
rendered possible by photon addition, photon subtraction, and photon-projection
nonlinearities. We explicitly put forward a design for a multilayer
interferometer in which the states for different classes can be generated with
state-of-the-art experimental techniques. Limitations of the proposed designs
are analyzed using the introduced classification, providing a benchmark for the
robustness of certain states and classes.


## QuEST Stable Training of LLMs with 1-Bit Weights and Activations

>Authors: Andrei Panferov, Jiale Chen, Soroush Tabesh, Roberto L. Castro, Mahdi Nikdan, Dan Alistarh

>2025-02-07

> http://arxiv.org/abs/2502.05003v1

One approach to reducing the massive costs of large language models (LLMs) is
the use of **quantize**d or **sparse** representations for training or deployment.
While post-training compression methods are very popular, the question of
obtaining even more accurate compressed models by directly training over such
representations, i.e., Quantization-Aware Training (QAT), is still open: for
example, a recent study (arXiv:2411.04330v2) put the "optimal" bit-width at
which models can be trained using QAT, while staying accuracy-competitive with
standard FP16/BF16 precision, at 8-bits weights and activations.
  We advance this state-of-the-art via a new method called QuEST, which is
Pareto-competitive with FP16, i.e., it provides better accuracy at lower model
size, while training models with weights and activations in 4-bits or less.
Moreover, QuEST allows stable training with 1-bit weights and activations.
QuEST achieves this by improving two key aspects of QAT methods: (1) accurate
and fast **quantization** of the (continuous) distributions of weights and
activations via Hadamard normalization and MSE-optimal fitting; (2) a new trust
gradient estimator based on the idea of explicitly minimizing the error between
the noisy gradient computed over **quantize**d states and the "true" (but unknown)
full-precision gradient. Experiments on Llama-type architectures show that
QuEST induces stable scaling laws across the entire range of hardware-supported
precisions, and can be extended to **sparse** representations. We provide GPU
kernel support showing that models produced by QuEST can be executed
efficiently. Our code is available at https://github.com/IST-DASLab/QuEST.


## Gaussian Models to Non-Gaussian Realms of Quantum Photonic Simulators

>Authors: Dennis Delali Kwesi Wayo, Rodrigo Alves Dias, Masoud Darvish Ganji, Camila Martins Saporetti, Leonardo Goliatt

>2025-02-07

> http://arxiv.org/abs/2502.05245v1

Quantum photonic simulators have emerged as indispensable tools for modeling
and optimizing quantum photonic circuits, bridging the gap between theoretical
models and experimental implementations. This review explores the landscape of
photonic quantum simulation, focusing on the transition from Gaussian to
non-Gaussian models and the computational challenges associated with simulating
large-scale photonic systems. Gaussian states and operations, which enable
efficient simulations through covariance matrices and phase-space
representations, serve as the foundation for photonic quantum computing.
However, non-Gaussian states crucial for universal quantum computation
introduce significant computational complexity, requiring advanced numerical
techniques such as tensor networks and high-performance GPU **acceleration**. We
evaluate the leading photonic quantum simulators, including Strawberry Fields,
Piquasso, QuTiP SimulaQron, Perceval, and QuantumOPtics.jl analyzing their
capabilities in handling continuous-variable (CV) and discrete-variable (DV)
quantum systems. Special attention is given to hardware-accelerated methods,
including GPU-based tensor network approaches, machine learning integration,
and hybrid quantum-classical workflows. Furthermore, we investigate noise
modeling techniques, such as photon loss and dark counts, and their impact on
simulation accuracy. As photonic quantum computing moves toward practical
implementations, advancements in high-performance computing (HPC)
architectures, such as tensor processing units (TPUs) and system-on-a-chip
(SoC) solutions, are accelerating the field. This review highlights emerging
trends, challenges, and future directions for developing scalable and efficient
photonic quantum simulators.


## Discovery of a large magnetic nonlinear Hall effect in an altermagnet

>Authors: Lei Han, Xizhi Fu, Cheng Song, Yuxiang Zhu, Xiaokang Li, Zengwei Zhu, Hua Bai, Ruiyue Chu, Jiankun Dai, Shixuan Liang, Junwei Liu, Feng Pan

>2025-02-07

> http://arxiv.org/abs/2502.04920v1

Since Edwin Halls groundbreaking discovery of the Hall effect in 1879,
magnetism, spin, and **quantization** have been expanding the scope of Hall
effects, continuously driving transformative progress in science and
technology. Among them, the latest nonlinear Hall effect (NLHE), where
longitudinal electric field tunes quantum geometry to generate nonlinear Hall
voltage, attracts wide attention as a sensitive probe of topological phases
across a wide range of materials. Here, we report a new Hall effect member: the
magnetic nonlinear Hall effect (MNLHE), characterized by a quadratic Hall
conductivity dependence on magnetic field, rather than electric field as in
NLHE. This finding relies on an altermagnet, Mn5Si3 thin film, whose
alternating-sign Berry curvatures ensure higher-order MNLHE clearly
distinguishable from the first-order anomalous Hall effect. The observed
quadratic dependence originates from chiral next-nearest-neighbor hopping
processes that acquire magnetic-exchange-driven Zeeman energies and
Haldane-like chiral flux phases. Remarkably, this MNLHE is non-analytic, as
reversing the magnetic field flips the alternating spin-splitting bands and
reverses the hopping chirality, which is absent in traditional NLHE. Beyond
offering a distinctive transport fingerprint for altermagnet Mn5Si3 thin film,
this MNLHE is large and unsaturated up to 60 T, providing opportunities for
pulsed high-field sensing technologies in both fundamental researches and
engineering applications.


## Sparse Autoencoders Do Not Find Canonical Units of Analysis

>Authors: Patrick Leask, Bart Bussmann, Michael Pearce, Joseph Bloom, Curt Tigges, Noura Al Moubayed, Lee Sharkey, Neel Nanda

>2025-02-07

> http://arxiv.org/abs/2502.04878v1

A common goal of mechanistic interpretability is to decompose the activations
of neural networks into features: interpretable properties of the input
computed by the model. Sparse autoencoders (SAEs) are a popular method for
finding these features in LLMs, and it has been postulated that they can be
used to find a \textit{canonical} set of units: a unique and complete list of
atomic features. We cast doubt on this belief using two novel techniques: SAE
stitching to show they are incomplete, and meta-SAEs to show they are not
atomic. SAE stitching involves inserting or swapping latents from a larger SAE
into a smaller one. Latents from the larger SAE can be divided into two
categories: \emph{novel latents}, which improve performance when added to the
smaller SAE, indicating they capture novel information, and
\emph{reconstruction latents}, which can replace corresponding latents in the
smaller SAE that have similar behavior. The existence of novel features
indicates incompleteness of smaller SAEs. Using meta-SAEs -- SAEs trained on
the decoder matrix of another SAE -- we find that latents in SAEs often
decompose into combinations of latents from a smaller SAE, showing that larger
SAE latents are not atomic. The resulting decompositions are often
interpretable; e.g. a latent representing ``Einstein'' decomposes into
``scientist'', ``Germany'', and ``famous person''. Even if SAEs do not find
canonical units of analysis, they may still be useful tools. We suggest that
future research should either pursue different approaches for identifying such
units, or pragmatically choose the SAE size suited to their task. We provide an
interactive dashboard to explore meta-SAEs: https://metasaes.streamlit.app/


## AIQViT Architecture-Informed Post-Training Quantization for Vision Transformers

>Authors: Runqing Jiang, Ye Zhang, Longguang Wang, Pengpeng Yu, Yulan Guo

>2025-02-07

> http://arxiv.org/abs/2502.04628v1

Post-training **quantization** (PTQ) has emerged as a promising solution for
reducing the storage and computational cost of vision transformers (ViTs).
Recent advances primarily target at crafting **quantize**rs to deal with peculiar
activations characterized by ViTs. However, most existing methods underestimate
the information loss incurred by weight **quantization**, resulting in significant
performance deterioration, particularly in **low-bit** cases. Furthermore, a common
practice in quantizing post-Softmax activations of ViTs is to employ
logarithmic transformations, which unfortunately prioritize less informative
values around zero. This approach introduces additional redundancies,
ultimately leading to suboptimal **quantization** efficacy. To handle these, this
paper proposes an innovative PTQ method tailored for ViTs, termed AIQViT
(Architecture-Informed Post-training Quantization for ViTs). First, we design
an architecture-informed low rank compensation mechanism, wherein learnable
low-rank weights are introduced to compensate for the degradation caused by
weight **quantization**. Second, we design a dynamic focusing **quantize**r to
accommodate the unbalanced distribution of post-Softmax activations, which
dynamically selects the most valuable interval for higher **quantization**
resolution. Extensive experiments on five vision tasks, including image
classification, object detection, instance segmentation, point cloud
classification, and point cloud part segmentation, demonstrate the superiority
of AIQViT over state-of-the-art PTQ methods.

