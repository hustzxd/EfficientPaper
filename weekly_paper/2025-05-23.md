# 2025-05-23

# Table of Contents
* [An Effective Training Framework for Light-Weight Automatic Speech Recognition Models](#An-Effective-Training-Framework-for-Light-Weight-Automatic-Speech-Recognition-Models)
* [UFT Unifying Supervised and Reinforcement Fine-Tuning](#UFT-Unifying-Supervised-and-Reinforcement-Fine-Tuning)
* [Beyond Correlation Towards Causal Large Language Model Agents in Biomedicine](#Beyond-Correlation-Towards-Causal-Large-Language-Model-Agents-in-Biomedicine)
* [Fixing Data That Hurts Performance Cascading LLMs to Relabel Hard Negatives for Robust Information Retrieval](#Fixing-Data-That-Hurts-Performance-Cascading-LLMs-to-Relabel-Hard-Negatives-for-Robust-Information-Retrieval)
* [MedFrameQA A Multi-Image Medical VQA Benchmark for Clinical Reasoning](#MedFrameQA-A-Multi-Image-Medical-VQA-Benchmark-for-Clinical-Reasoning)
* [From Reality to Virtual Worlds The Role of Photogrammetry in Game Development](#From-Reality-to-Virtual-Worlds-The-Role-of-Photogrammetry-in-Game-Development)
* [Bottlenecked Transformers Periodic KV Cache Abstraction for Generalised Reasoning](#Bottlenecked-Transformers-Periodic-KV-Cache-Abstraction-for-Generalised-Reasoning)
* [MixAT Combining Continuous and Discrete Adversarial Training for LLMs](#MixAT-Combining-Continuous-and-Discrete-Adversarial-Training-for-LLMs)
* [NovelSeek When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification](#NovelSeek-When-Agent-Becomes-the-Scientist----Building-Closed-Loop-System-from-Hypothesis-to-Verification)
* [REPA Works Until It Doesn't Early-Stopped, Holistic Alignment Supercharges Diffusion Training](#REPA-Works-Until-It-Doesn't-Early-Stopped,-Holistic-Alignment-Supercharges-Diffusion-Training)
* [Mitigating Overfitting in Medical Imaging Self-Supervised Pretraining vs. ImageNet Transfer Learning for Dermatological Diagnosis](#Mitigating-Overfitting-in-Medical-Imaging-Self-Supervised-Pretraining-vs.-ImageNet-Transfer-Learning-for-Dermatological-Diagnosis)
* [TRIM Achieving Extreme Sparsity with Targeted Row-wise Iterative Metric-driven Pruning](#TRIM-Achieving-Extreme-Sparsity-with-Targeted-Row-wise-Iterative-Metric-driven-Pruning)
* [Training Long-Context LLMs Efficiently via Chunk-wise Optimization](#Training-Long-Context-LLMs-Efficiently-via-Chunk-wise-Optimization)
* [R1-ShareVL Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO](#R1-ShareVL-Incentivizing-Reasoning-Capability-of-Multimodal-Large-Language-Models-via-Share-GRPO)
* [Open and Sustainable AI challenges, opportunities and the road ahead in the life sciences](#Open-and-Sustainable-AI-challenges,-opportunities-and-the-road-ahead-in-the-life-sciences)
* [Steering Large Language Models for Machine Translation Personalization](#Steering-Large-Language-Models-for-Machine-Translation-Personalization)
* [URLs Help, Topics Guide Understanding Metadata Utility in LLM Training](#URLs-Help,-Topics-Guide-Understanding-Metadata-Utility-in-LLM-Training)
* [SHaDe Compact and Consistent Dynamic 3D Reconstruction via Tri-Plane Deformation and Latent Diffusion](#SHaDe-Compact-and-Consistent-Dynamic-3D-Reconstruction-via-Tri-Plane-Deformation-and-Latent-Diffusion)
* [Causal-Invariant Cross-Domain Out-of-Distribution Recommendation](#Causal-Invariant-Cross-Domain-Out-of-Distribution-Recommendation)
* [DuFFin A Dual-Level Fingerprinting Framework for LLMs IP Protection](#DuFFin-A-Dual-Level-Fingerprinting-Framework-for-LLMs-IP-Protection)
* [AnchorFormer Differentiable Anchor Attention for Efficient Vision Transformer](#AnchorFormer-Differentiable-Anchor-Attention-for-Efficient-Vision-Transformer)
* [TAT-VPR Ternary Adaptive Transformer for Dynamic and Efficient Visual Place Recognition](#TAT-VPR-Ternary-Adaptive-Transformer-for-Dynamic-and-Efficient-Visual-Place-Recognition)
* [FPQVAR Floating Point Quantization for Visual Autoregressive Model with FPGA Hardware Co-design](#FPQVAR-Floating-Point-Quantization-for-Visual-Autoregressive-Model-with-FPGA-Hardware-Co-design)
* [EquivPruner Boosting Efficiency and Quality in LLM-Based Search via Action Pruning](#EquivPruner-Boosting-Efficiency-and-Quality-in-LLM-Based-Search-via-Action-Pruning)
* [Artificial Intelligence for Direct Prediction of Molecular Dynamics Across Chemical Space](#Artificial-Intelligence-for-Direct-Prediction-of-Molecular-Dynamics-Across-Chemical-Space)
* [ARPOEnd-to-End Policy Optimization for GUI Agents with Experience Replay](#ARPOEnd-to-End-Policy-Optimization-for-GUI-Agents-with-Experience-Replay)
* [Align-GRAG Reasoning-Guided Dual Alignment for Graph Retrieval-Augmented Generation](#Align-GRAG-Reasoning-Guided-Dual-Alignment-for-Graph-Retrieval-Augmented-Generation)
* [NQKV A KV Cache Quantization Scheme Based on Normal Distribution Characteristics](#NQKV-A-KV-Cache-Quantization-Scheme-Based-on-Normal-Distribution-Characteristics)
* [SAE-SSV Supervised Steering in Sparse Representation Spaces for Reliable Control of Language Models](#SAE-SSV-Supervised-Steering-in-Sparse-Representation-Spaces-for-Reliable-Control-of-Language-Models)
* [Generative Latent Coding for Ultra-Low Bitrate Image and Video Compression](#Generative-Latent-Coding-for-Ultra-Low-Bitrate-Image-and-Video-Compression)
* [QuickVideo Real-Time Long Video Understanding with System Algorithm Co-Design](#QuickVideo-Real-Time-Long-Video-Understanding-with-System-Algorithm-Co-Design)
* [KNN-SSD Enabling Dynamic Self-Speculative Decoding via Nearest Neighbor Layer Set Optimization](#KNN-SSD-Enabling-Dynamic-Self-Speculative-Decoding-via-Nearest-Neighbor-Layer-Set-Optimization)
* [Breaking Complexity Barriers High-Resolution Image Restoration with Rank Enhanced Linear Attention](#Breaking-Complexity-Barriers-High-Resolution-Image-Restoration-with-Rank-Enhanced-Linear-Attention)
* [Hierarchical Safety Realignment Lightweight Restoration of Safety in Pruned Large Vision-Language Models](#Hierarchical-Safety-Realignment-Lightweight-Restoration-of-Safety-in-Pruned-Large-Vision-Language-Models)
* [Not All Models Suit Expert Offloading On Local Routing Consistency of Mixture-of-Expert Models](#Not-All-Models-Suit-Expert-Offloading-On-Local-Routing-Consistency-of-Mixture-of-Expert-Models)
* [Interpretability Illusions with Sparse Autoencoders Evaluating Robustness of Concept Representations](#Interpretability-Illusions-with-Sparse-Autoencoders-Evaluating-Robustness-of-Concept-Representations)
* [Analyzing Hierarchical Structure in Vision Models with Sparse Autoencoders](#Analyzing-Hierarchical-Structure-in-Vision-Models-with-Sparse-Autoencoders)
* [Directional Sparsity Based Statistical Channel Estimation for 6D Movable Antenna Communications](#Directional-Sparsity-Based-Statistical-Channel-Estimation-for-6D-Movable-Antenna-Communications)
* [Bloch oscillation with a diatomic tight-binding model on quantum computers](#Bloch-oscillation-with-a-diatomic-tight-binding-model-on-quantum-computers)
* [Is (Selective) Round-To-Nearest Quantization All You Need?](#Is-(Selective)-Round-To-Nearest-Quantization-All-You-Need?)
* [STAR-R1 Spacial TrAnsformation Reasoning by Reinforcing Multimodal LLMs](#STAR-R1-Spacial-TrAnsformation-Reasoning-by-Reinforcing-Multimodal-LLMs)
* [Adaptive Estimation and Learning under Temporal Distribution Shift](#Adaptive-Estimation-and-Learning-under-Temporal-Distribution-Shift)
* [dKV-Cache The Cache for Diffusion Language Models](#dKV-Cache-The-Cache-for-Diffusion-Language-Models)
* [ConvSearch-R1 Enhancing Query Reformulation for Conversational Search with Reasoning via Reinforcement Learning](#ConvSearch-R1-Enhancing-Query-Reformulation-for-Conversational-Search-with-Reasoning-via-Reinforcement-Learning)
* [VocalBench Benchmarking the Vocal Conversational Abilities for Speech Interaction Models](#VocalBench-Benchmarking-the-Vocal-Conversational-Abilities-for-Speech-Interaction-Models)
* [ThinkLess A Training-Free Inference-Efficient Method for Reducing Reasoning Redundancy](#ThinkLess-A-Training-Free-Inference-Efficient-Method-for-Reducing-Reasoning-Redundancy)
* [A Federated Splitting Framework for LLMs Security, Efficiency, and Adaptability](#A-Federated-Splitting-Framework-for-LLMs-Security,-Efficiency,-and-Adaptability)
* [Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models](#Feature-Extraction-and-Steering-for-Enhanced-Chain-of-Thought-Reasoning-in-Language-Models)
* [Guidelines for the Quality Assessment of Energy-Aware NAS Benchmarks](#Guidelines-for-the-Quality-Assessment-of-Energy-Aware-NAS-Benchmarks)
* [Mechanistic Insights into Grokking from the Embedding Layer](#Mechanistic-Insights-into-Grokking-from-the-Embedding-Layer)
* [Temporal Spectrum Cartography in Low-Altitude Economy Networks A Generative AI Framework with Multi-Agent Learning](#Temporal-Spectrum-Cartography-in-Low-Altitude-Economy-Networks-A-Generative-AI-Framework-with-Multi-Agent-Learning)
* [Plasma-state metasurfaces for ultra-intensive field manipulation](#Plasma-state-metasurfaces-for-ultra-intensive-field-manipulation)
* [Robo-DM Data Management For Large Robot Datasets](#Robo-DM-Data-Management-For-Large-Robot-Datasets)
* [Evaluate Bias without Manual Test Sets A Concept Representation Perspective for LLMs](#Evaluate-Bias-without-Manual-Test-Sets-A-Concept-Representation-Perspective-for-LLMs)
* [Hunyuan-TurboS Advancing Large Language Models through Mamba-Transformer Synergy and Adaptive Chain-of-Thought](#Hunyuan-TurboS-Advancing-Large-Language-Models-through-Mamba-Transformer-Synergy-and-Adaptive-Chain-of-Thought)
* [Efficient Data Driven Mixture-of-Expert Extraction from Trained Networks](#Efficient-Data-Driven-Mixture-of-Expert-Extraction-from-Trained-Networks)
* [FlowKV Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management](#FlowKV-Enhancing-Multi-Turn-Conversational-Coherence-in-LLMs-via-Isolated-Key-Value-Cache-Management)
* [SSR Speculative Parallel Scaling Reasoning in Test-time](#SSR-Speculative-Parallel-Scaling-Reasoning-in-Test-time)
* [SoftHGNN Soft Hypergraph Neural Networks for General Visual Recognition](#SoftHGNN-Soft-Hypergraph-Neural-Networks-for-General-Visual-Recognition)
* [LiveVLM Efficient Online Video Understanding via Streaming-Oriented KV Cache and Retrieval](#LiveVLM-Efficient-Online-Video-Understanding-via-Streaming-Oriented-KV-Cache-and-Retrieval)
* [An Efficient Private GPT Never Autoregressively Decodes](#An-Efficient-Private-GPT-Never-Autoregressively-Decodes)
* [X-GRM Large Gaussian Reconstruction Model for Sparse-view X-rays to Computed Tomography](#X-GRM-Large-Gaussian-Reconstruction-Model-for-Sparse-view-X-rays-to-Computed-Tomography)
* [A novel framework for detecting multiple change points in functional data sequences](#A-novel-framework-for-detecting-multiple-change-points-in-functional-data-sequences)
* [Time Tracker Mixture-of-Experts-Enhanced Foundation Time Series Forecasting Model with Decoupled Training Pipelines](#Time-Tracker-Mixture-of-Experts-Enhanced-Foundation-Time-Series-Forecasting-Model-with-Decoupled-Training-Pipelines)
* [BanditSpec Adaptive Speculative Decoding via Bandit Algorithms](#BanditSpec-Adaptive-Speculative-Decoding-via-Bandit-Algorithms)
* [Multicrossmodal Automated Agent for Integrating Diverse Materials Science Data](#Multicrossmodal-Automated-Agent-for-Integrating-Diverse-Materials-Science-Data)
* [Adaptive Inertial Method](#Adaptive-Inertial-Method)
* [StepSearch Igniting LLMs Search Ability via Step-Wise Proximal Policy Optimization](#StepSearch-Igniting-LLMs-Search-Ability-via-Step-Wise-Proximal-Policy-Optimization)
* [SUS backprop linear backpropagation algorithm for long inputs in transformers](#SUS-backprop-linear-backpropagation-algorithm-for-long-inputs-in-transformers)
* [Self-GIVE Associative Thinking from Limited Structured Knowledge for Enhanced Large Language Model Reasoning](#Self-GIVE-Associative-Thinking-from-Limited-Structured-Knowledge-for-Enhanced-Large-Language-Model-Reasoning)
* [PiFlow Principle-aware Scientific Discovery with Multi-Agent Collaboration](#PiFlow-Principle-aware-Scientific-Discovery-with-Multi-Agent-Collaboration)
* [Denoising Concept Vectors with Sparse Autoencoders for Improved Language Model Steering](#Denoising-Concept-Vectors-with-Sparse-Autoencoders-for-Improved-Language-Model-Steering)
* [Harnessing On-Device Large Language Model Empirical Results and Implications for AI PC](#Harnessing-On-Device-Large-Language-Model-Empirical-Results-and-Implications-for-AI-PC)
* [Rate-Distortion Optimization with Non-Reference Metrics for UGC Compression](#Rate-Distortion-Optimization-with-Non-Reference-Metrics-for-UGC-Compression)
* [Effective and Efficient Schema-aware Information Extraction Using On-Device Large Language Models](#Effective-and-Efficient-Schema-aware-Information-Extraction-Using-On-Device-Large-Language-Models)
* [CRAFT Training-Free Cascaded Retrieval for Tabular QA](#CRAFT-Training-Free-Cascaded-Retrieval-for-Tabular-QA)
* [Reinforcement Learning from User Feedback](#Reinforcement-Learning-from-User-Feedback)
* [Scaling Laws for State Dynamics in Large Language Models](#Scaling-Laws-for-State-Dynamics-in-Large-Language-Models)
* [Polar Sparsity High Throughput Batched LLM Inferencing with Scalable Contextual Sparsity](#Polar-Sparsity-High-Throughput-Batched-LLM-Inferencing-with-Scalable-Contextual-Sparsity)
* [Saten Sparse Augmented Tensor Networks for Post-Training Compression of Large Language Models](#Saten-Sparse-Augmented-Tensor-Networks-for-Post-Training-Compression-of-Large-Language-Models)
* [Balanced and Elastic End-to-end Training of Dynamic LLMs](#Balanced-and-Elastic-End-to-end-Training-of-Dynamic-LLMs)
* [Grouping First, Attending Smartly Training-Free Acceleration for Diffusion Transformers](#Grouping-First,-Attending-Smartly-Training-Free-Acceleration-for-Diffusion-Transformers)
* [A General Framework for Group Sparsity in Hyperspectral Unmixing Using Endmember Bundles](#A-General-Framework-for-Group-Sparsity-in-Hyperspectral-Unmixing-Using-Endmember-Bundles)
* [TinyV Reducing False Negatives in Verification Improves RL for LLM Reasoning](#TinyV-Reducing-False-Negatives-in-Verification-Improves-RL-for-LLM-Reasoning)
* [Toward Reliable Biomedical Hypothesis Generation Evaluating Truthfulness and Hallucination in Large Language Models](#Toward-Reliable-Biomedical-Hypothesis-Generation-Evaluating-Truthfulness-and-Hallucination-in-Large-Language-Models)
* [Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with Capability in Mind for Better Reasoning](#Can-Pruning-Improve-Reasoning?-Revisiting-Long-CoT-Compression-with-Capability-in-Mind-for-Better-Reasoning)
* [Breaking Bad Tokens Detoxification of LLMs Using Sparse Autoencoders](#Breaking-Bad-Tokens-Detoxification-of-LLMs-Using-Sparse-Autoencoders)
* [Interpretable Dual-Stream Learning for Local Wind Hazard Prediction in Vulnerable Communities](#Interpretable-Dual-Stream-Learning-for-Local-Wind-Hazard-Prediction-in-Vulnerable-Communities)
* [ModRWKV Transformer Multimodality in Linear Time](#ModRWKV-Transformer-Multimodality-in-Linear-Time)
* [Video Compression Commander Plug-and-Play Inference Acceleration for Video Large Language Models](#Video-Compression-Commander-Plug-and-Play-Inference-Acceleration-for-Video-Large-Language-Models)
* [SAE-FiRE Enhancing Earnings Surprise Predictions Through Sparse Autoencoder Feature Selection](#SAE-FiRE-Enhancing-Earnings-Surprise-Predictions-Through-Sparse-Autoencoder-Feature-Selection)
* [OmniGenBench A Modular Platform for Reproducible Genomic Foundation Models Benchmarking](#OmniGenBench-A-Modular-Platform-for-Reproducible-Genomic-Foundation-Models-Benchmarking)
* [Log-Augmented Generation Scaling Test-Time Reasoning with Reusable Computation](#Log-Augmented-Generation-Scaling-Test-Time-Reasoning-with-Reusable-Computation)
* [Layer-wise Quantization for Quantized Optimistic Dual Averaging](#Layer-wise-Quantization-for-Quantized-Optimistic-Dual-Averaging)
* [Scaling and Enhancing LLM-based AVSR A Sparse Mixture of Projectors Approach](#Scaling-and-Enhancing-LLM-based-AVSR-A-Sparse-Mixture-of-Projectors-Approach)
* [Low-Cost FlashAttention with Fused Exponential and Multiplication Hardware Operators](#Low-Cost-FlashAttention-with-Fused-Exponential-and-Multiplication-Hardware-Operators)
* [HausaNLP Current Status, Challenges and Future Directions for Hausa Natural Language Processing](#HausaNLP-Current-Status,-Challenges-and-Future-Directions-for-Hausa-Natural-Language-Processing)
* [Scaling Law for Quantization-Aware Training](#Scaling-Law-for-Quantization-Aware-Training)
* [Speculative Decoding Reimagined for Multimodal Large Language Models](#Speculative-Decoding-Reimagined-for-Multimodal-Large-Language-Models)
* [MatchDance Collaborative Mamba-Transformer Architecture Matching for High-Quality 3D Dance Synthesis](#MatchDance-Collaborative-Mamba-Transformer-Architecture-Matching-for-High-Quality-3D-Dance-Synthesis)
* [Embedded Mean Field Reinforcement Learning for Perimeter-defense Game](#Embedded-Mean-Field-Reinforcement-Learning-for-Perimeter-defense-Game)
* [MSDformer Multi-scale Discrete Transformer For Time Series Generation](#MSDformer-Multi-scale-Discrete-Transformer-For-Time-Series-Generation)
* [FLASH-D FlashAttention with Hidden Softmax Division](#FLASH-D-FlashAttention-with-Hidden-Softmax-Division)
* [Capturing the Effects of Quantization on Trojans in Code LLMs](#Capturing-the-Effects-of-Quantization-on-Trojans-in-Code-LLMs)
* [M3Depth Wavelet-Enhanced Depth Estimation on Mars via Mutual Boosting of Dual-Modal Data](#M3Depth-Wavelet-Enhanced-Depth-Estimation-on-Mars-via-Mutual-Boosting-of-Dual-Modal-Data)
* [MultiHal Multilingual Dataset for Knowledge-Graph Grounded Evaluation of LLM Hallucinations](#MultiHal-Multilingual-Dataset-for-Knowledge-Graph-Grounded-Evaluation-of-LLM-Hallucinations)
* [CE-LSLM Efficient Large-Small Language Model Inference and Communication via Cloud-Edge Collaboration](#CE-LSLM-Efficient-Large-Small-Language-Model-Inference-and-Communication-via-Cloud-Edge-Collaboration)
* [Quantum Internet, Governance, Trust, and the Promise of Secure Communication On building a Quantum Internet that will be used](#Quantum-Internet,-Governance,-Trust,-and-the-Promise-of-Secure-Communication-On-building-a-Quantum-Internet-that-will-be-used)
* [Textual Steering Vectors Can Improve Visual Understanding in Multimodal Large Language Models](#Textual-Steering-Vectors-Can-Improve-Visual-Understanding-in-Multimodal-Large-Language-Models)
* [Process vs. Outcome Reward Which is Better for Agentic RAG Reinforcement Learning](#Process-vs.-Outcome-Reward-Which-is-Better-for-Agentic-RAG-Reinforcement-Learning)
* [Unsupervised Graph Clustering with Deep Structural Entropy](#Unsupervised-Graph-Clustering-with-Deep-Structural-Entropy)
* [Towards Efficient Multi-Scale Deformable Attention on NPU](#Towards-Efficient-Multi-Scale-Deformable-Attention-on-NPU)
* [Quaff Quantized Parameter-Efficient Fine-Tuning under Outlier Spatial Stability Hypothesis](#Quaff-Quantized-Parameter-Efficient-Fine-Tuning-under-Outlier-Spatial-Stability-Hypothesis)
* [UHD Image Dehazing via anDehazeFormer with Atmospheric-aware KV Cache](#UHD-Image-Dehazing-via-anDehazeFormer-with-Atmospheric-aware-KV-Cache)
* [R&D-Agent Automating Data-Driven AI Solution Building Through LLM-Powered Automated Research, Development, and Evolution](#R&D-Agent-Automating-Data-Driven-AI-Solution-Building-Through-LLM-Powered-Automated-Research,-Development,-and-Evolution)
* [Through a Compressed Lens Investigating the Impact of Quantization on LLM Explainability and Interpretability](#Through-a-Compressed-Lens-Investigating-the-Impact-of-Quantization-on-LLM-Explainability-and-Interpretability)
* [Leveraging Multivariate Long-Term History Representation for Time Series Forecasting](#Leveraging-Multivariate-Long-Term-History-Representation-for-Time-Series-Forecasting)
* [Reasoning Path Compression Compressing Generation Trajectories for Efficient LLM Reasoning](#Reasoning-Path-Compression-Compressing-Generation-Trajectories-for-Efficient-LLM-Reasoning)
* [Learning Spatio-Temporal Dynamics for Trajectory Recovery via Time-Aware Transformer](#Learning-Spatio-Temporal-Dynamics-for-Trajectory-Recovery-via-Time-Aware-Transformer)
* [EfficientLLM Efficiency in Large Language Models](#EfficientLLM-Efficiency-in-Large-Language-Models)
* [Predicting Neo-Adjuvant Chemotherapy Response in Triple-Negative Breast Cancer Using Pre-Treatment Histopathologic Images](#Predicting-Neo-Adjuvant-Chemotherapy-Response-in-Triple-Negative-Breast-Cancer-Using-Pre-Treatment-Histopathologic-Images)
* [Lorentz force on superconducting vortices near line defects](#Lorentz-force-on-superconducting-vortices-near-line-defects)
* [Causal Head Gating A Framework for Interpreting Roles of Attention Heads in Transformers](#Causal-Head-Gating-A-Framework-for-Interpreting-Roles-of-Attention-Heads-in-Transformers)
* [Nonlinear Nonlocal Comparing A. O. Barut's Theory to Mine with special emphasis on That Dot on the Screen](#Nonlinear-Nonlocal-Comparing-A.-O.-Barut's-Theory-to-Mine-with-special-emphasis-on-That-Dot-on-the-Screen)
* [Fine-tuning Quantized Neural Networks with Zeroth-order Optimization](#Fine-tuning-Quantized-Neural-Networks-with-Zeroth-order-Optimization)
* [Faster Video Diffusion with Trainable Sparse Attention](#Faster-Video-Diffusion-with-Trainable-Sparse-Attention)
* [Occult Optimizing Collaborative Communication across Experts for Accelerated Parallel MoE Training and Inference](#Occult-Optimizing-Collaborative-Communication-across-Experts-for-Accelerated-Parallel-MoE-Training-and-Inference)
* [Thinking Short and Right Over Thinking Long Serving LLM Reasoning Efficiently and Accurately](#Thinking-Short-and-Right-Over-Thinking-Long-Serving-LLM-Reasoning-Efficiently-and-Accurately)
* [HeteroSpec Leveraging Contextual Heterogeneity for Efficient Speculative Decoding](#HeteroSpec-Leveraging-Contextual-Heterogeneity-for-Efficient-Speculative-Decoding)
* [Learning Wavelet-Sparse FDK for 3D Cone-Beam CT Reconstruction](#Learning-Wavelet-Sparse-FDK-for-3D-Cone-Beam-CT-Reconstruction)
* [Ocean wave spectrum reconstruction from HF radar data and its application to wave height estimation](#Ocean-wave-spectrum-reconstruction-from-HF-radar-data-and-its-application-to-wave-height-estimation)
* [FreeKV Boosting KV Cache Retrieval for Efficient LLM Inference](#FreeKV-Boosting-KV-Cache-Retrieval-for-Efficient-LLM-Inference)
* [Automatic mixed precision for optimizing gained time with constrained loss mean-squared-error based on model partition to sequential sub-graphs](#Automatic-mixed-precision-for-optimizing-gained-time-with-constrained-loss-mean-squared-error-based-on-model-partition-to-sequential-sub-graphs)
* [Simplicity is Key An Unsupervised Pretraining Approach for Sparse Radio Channels](#Simplicity-is-Key-An-Unsupervised-Pretraining-Approach-for-Sparse-Radio-Channels)
* [A Skull-Adaptive Framework for AI-Based 3D Transcranial Focused Ultrasound Simulation](#A-Skull-Adaptive-Framework-for-AI-Based-3D-Transcranial-Focused-Ultrasound-Simulation)
* [DGRO Enhancing LLM Reasoning via Exploration-Exploitation Control and Reward Variance Management](#DGRO-Enhancing-LLM-Reasoning-via-Exploration-Exploitation-Control-and-Reward-Variance-Management)
* [A3  an Analytical Low-Rank Approximation Framework for Attention](#A3--an-Analytical-Low-Rank-Approximation-Framework-for-Attention)
* [Efficient training for large-scale optical neural network using an evolutionary strategy and attention pruning](#Efficient-training-for-large-scale-optical-neural-network-using-an-evolutionary-strategy-and-attention-pruning)
* [Lecture Notes WISPs in gamma-ray astrophysics](#Lecture-Notes-WISPs-in-gamma-ray-astrophysics)
* [Accelerate TarFlow Sampling with GS-Jacobi Iteration](#Accelerate-TarFlow-Sampling-with-GS-Jacobi-Iteration)
* [AdaToken-3D Dynamic Spatial Gating for Efficient 3D Large Multimodal-Models Reasoning](#AdaToken-3D-Dynamic-Spatial-Gating-for-Efficient-3D-Large-Multimodal-Models-Reasoning)
* [Pyramid Sparse Transformer Enhancing Multi-Scale Feature Fusion with Dynamic Token Selection](#Pyramid-Sparse-Transformer-Enhancing-Multi-Scale-Feature-Fusion-with-Dynamic-Token-Selection)
* [LiDAR MOT-DETR A LiDAR-based Two-Stage Transformer for 3D Multiple Object Tracking](#LiDAR-MOT-DETR-A-LiDAR-based-Two-Stage-Transformer-for-3D-Multiple-Object-Tracking)
* [MVAR Visual Autoregressive Modeling with Scale and Spatial Markovian Conditioning](#MVAR-Visual-Autoregressive-Modeling-with-Scale-and-Spatial-Markovian-Conditioning)
* [Accelerating Adaptive Retrieval Augmented Generation via Instruction-Driven Representation Reduction of Retrieval Overlaps](#Accelerating-Adaptive-Retrieval-Augmented-Generation-via-Instruction-Driven-Representation-Reduction-of-Retrieval-Overlaps)
* [FLASH Latent-Aware Semi-Autoregressive Speculative Decoding for Multimodal Tasks](#FLASH-Latent-Aware-Semi-Autoregressive-Speculative-Decoding-for-Multimodal-Tasks)
* [Josephson Junctions in the Age of Quantum Discovery](#Josephson-Junctions-in-the-Age-of-Quantum-Discovery)
* [ToTRL Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving](#ToTRL-Unlock-LLM-Tree-of-Thoughts-Reasoning-Potential-through-Puzzles-Solving)
* [LLM-based Query Expansion Fails for Unfamiliar and Ambiguous Queries](#LLM-based-Query-Expansion-Fails-for-Unfamiliar-and-Ambiguous-Queries)
* [Exploring Federated Pruning for Large Language Models](#Exploring-Federated-Pruning-for-Large-Language-Models)
* [Multi-head Temporal Latent Attention](#Multi-head-Temporal-Latent-Attention)
* [GANCompress GAN-Enhanced Neural Image Compression with Binary Spherical Quantization](#GANCompress-GAN-Enhanced-Neural-Image-Compression-with-Binary-Spherical-Quantization)
* [Exploring Sparsity for Parameter Efficient Fine Tuning Using Wavelets](#Exploring-Sparsity-for-Parameter-Efficient-Fine-Tuning-Using-Wavelets)
* [Introspective Growth Automatically Advancing LLM Expertise in Technology Judgment](#Introspective-Growth-Automatically-Advancing-LLM-Expertise-in-Technology-Judgment)
* [LLM-CoT Enhanced Graph Neural Recommendation with Harmonized Group Policy Optimization](#LLM-CoT-Enhanced-Graph-Neural-Recommendation-with-Harmonized-Group-Policy-Optimization)
* [STAR Stage-Wise Attention-Guided Token Reduction for Efficient Large Vision-Language Models Inference](#STAR-Stage-Wise-Attention-Guided-Token-Reduction-for-Efficient-Large-Vision-Language-Models-Inference)
* [SenseFlow A Physics-Informed and Self-Ensembling Iterative Framework for Power Flow Estimation](#SenseFlow-A-Physics-Informed-and-Self-Ensembling-Iterative-Framework-for-Power-Flow-Estimation)
* [CALM Co-evolution of Algorithms and Language Model for Automatic Heuristic Design](#CALM-Co-evolution-of-Algorithms-and-Language-Model-for-Automatic-Heuristic-Design)
* [PMQ-VE Progressive Multi-Frame Quantization for Video Enhancement](#PMQ-VE-Progressive-Multi-Frame-Quantization-for-Video-Enhancement)
* [LightRetriever A LLM-based Hybrid Retrieval Architecture with 1000x Faster Query Inference](#LightRetriever-A-LLM-based-Hybrid-Retrieval-Architecture-with-1000x-Faster-Query-Inference)
* [One-for-All Pruning A Universal Model for Customized Compression of Large Language Models](#One-for-All-Pruning-A-Universal-Model-for-Customized-Compression-of-Large-Language-Models)
* [LLM-DSE Searching Accelerator Parameters with LLM Agents](#LLM-DSE-Searching-Accelerator-Parameters-with-LLM-Agents)
* [Cyclic-Shift Sparse Kronecker Tensor Classifier for Signal-Region Detection in Neuroimaging](#Cyclic-Shift-Sparse-Kronecker-Tensor-Classifier-for-Signal-Region-Detection-in-Neuroimaging)
* [Discovering Symbolic Differential Equations with Symmetry Invariants](#Discovering-Symbolic-Differential-Equations-with-Symmetry-Invariants)
* [Learning High-Order Relationships with Hypergraph Attention-based Spatio-Temporal Aggregation for Brain Disease Analysis](#Learning-High-Order-Relationships-with-Hypergraph-Attention-based-Spatio-Temporal-Aggregation-for-Brain-Disease-Analysis)
* [Tiny QA Benchmark++ Ultra-Lightweight, Synthetic Multilingual Dataset Generation & Smoke-Tests for Continuous LLM Evaluation](#Tiny-QA-Benchmark++-Ultra-Lightweight,-Synthetic-Multilingual-Dataset-Generation-&-Smoke-Tests-for-Continuous-LLM-Evaluation)
* [Accelerating Diffusion-based Super-Resolution with Dynamic Time-Spatial Sampling](#Accelerating-Diffusion-based-Super-Resolution-with-Dynamic-Time-Spatial-Sampling)
* [AI-Driven Automation Can Become the Foundation of Next-Era Science of Science Research](#AI-Driven-Automation-Can-Become-the-Foundation-of-Next-Era-Science-of-Science-Research)
* [SpatialCrafter Unleashing the Imagination of Video Diffusion Models for Scene Reconstruction from Limited Observations](#SpatialCrafter-Unleashing-the-Imagination-of-Video-Diffusion-Models-for-Scene-Reconstruction-from-Limited-Observations)
* [Integrating Model-based Control and RL for Sim2Real Transfer of Tight Insertion Policies](#Integrating-Model-based-Control-and-RL-for-Sim2Real-Transfer-of-Tight-Insertion-Policies)
* [S-Crescendo A Nested Transformer Weaving Framework for Scalable Nonlinear System in S-Domain Representation](#S-Crescendo-A-Nested-Transformer-Weaving-Framework-for-Scalable-Nonlinear-System-in-S-Domain-Representation)
* [FastCar Cache Attentive Replay for Fast Auto-Regressive Video Generation on the Edge](#FastCar-Cache-Attentive-Replay-for-Fast-Auto-Regressive-Video-Generation-on-the-Edge)
* [SplInterp Improving our Understanding and Training of Sparse Autoencoders](#SplInterp-Improving-our-Understanding-and-Training-of-Sparse-Autoencoders)
* [DraftAttention Fast Video Diffusion via Low-Resolution Attention Guidance](#DraftAttention-Fast-Video-Diffusion-via-Low-Resolution-Attention-Guidance)
* [Chain-of-Model Learning for Language Model](#Chain-of-Model-Learning-for-Language-Model)
* [Feature Hedging Correlated Features Break Narrow Sparse Autoencoders](#Feature-Hedging-Correlated-Features-Break-Narrow-Sparse-Autoencoders)
* [ZeroTuning Unlocking the Initial Token's Power to Enhance Large Language Models Without Training](#ZeroTuning-Unlocking-the-Initial-Token's-Power-to-Enhance-Large-Language-Models-Without-Training)
* [Reinforcement Learning Finetunes Small Subnetworks in Large Language Models](#Reinforcement-Learning-Finetunes-Small-Subnetworks-in-Large-Language-Models)
* [Attend to Not Attended Structure-then-Detail Token Merging for Post-training DiT Acceleration](#Attend-to-Not-Attended-Structure-then-Detail-Token-Merging-for-Post-training-DiT-Acceleration)
* [Qronos Correcting the Past by Shaping the Future... in Post-Training Quantization](#Qronos-Correcting-the-Past-by-Shaping-the-Future...-in-Post-Training-Quantization)
* [Ambiguity Resolution in Text-to-Structured Data Mapping](#Ambiguity-Resolution-in-Text-to-Structured-Data-Mapping)
* [SageAttention3 Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training](#SageAttention3-Microscaling-FP4-Attention-for-Inference-and-An-Exploration-of-8-Bit-Training)
* [MegaScale-MoE Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production](#MegaScale-MoE-Large-Scale-Communication-Efficient-Training-of-Mixture-of-Experts-Models-in-Production)
* [MID-L Matrix-Interpolated Dropout Layer with Layer-wise Neuron Selection](#MID-L-Matrix-Interpolated-Dropout-Layer-with-Layer-wise-Neuron-Selection)
* [MoE-CAP Benchmarking Cost, Accuracy and Performance of Sparse Mixture-of-Experts Systems](#MoE-CAP-Benchmarking-Cost,-Accuracy-and-Performance-of-Sparse-Mixture-of-Experts-Systems)
* [Dynamic Base model Shift for Delta Compression](#Dynamic-Base-model-Shift-for-Delta-Compression)
* [Wave turbulence, thermalization and multimode locking in optical fibers](#Wave-turbulence,-thermalization-and-multimode-locking-in-optical-fibers)
* [Fourier Low-rank and Sparse Tensor for Efficient Tensor Completion](#Fourier-Low-rank-and-Sparse-Tensor-for-Efficient-Tensor-Completion)
* [Delta Attention Fast and Accurate Sparse Attention Inference by Delta Correction](#Delta-Attention-Fast-and-Accurate-Sparse-Attention-Inference-by-Delta-Correction)
* [Memory-Efficient Orthogonal Fine-Tuning with Principal Subspace Adaptation](#Memory-Efficient-Orthogonal-Fine-Tuning-with-Principal-Subspace-Adaptation)
* [From Intent Discovery to Recognition with Topic Modeling and Synthetic Data](#From-Intent-Discovery-to-Recognition-with-Topic-Modeling-and-Synthetic-Data)
* [Gaussian Weight Sampling for Scalable, Efficient and Stable Pseudo-Quantization Training](#Gaussian-Weight-Sampling-for-Scalable,-Efficient-and-Stable-Pseudo-Quantization-Training)
* [InfiJanice Joint Analysis and In-situ Correction Engine for Quantization-Induced Math Degradation in Large Language Models](#InfiJanice-Joint-Analysis-and-In-situ-Correction-Engine-for-Quantization-Induced-Math-Degradation-in-Large-Language-Models)
* [Maximizing Asynchronicity in Event-based Neural Networks](#Maximizing-Asynchronicity-in-Event-based-Neural-Networks)
* [Hybrid-Emba3D Geometry-Aware and Cross-Path Feature Hybrid Enhanced State Space Model for Point Cloud Classification](#Hybrid-Emba3D-Geometry-Aware-and-Cross-Path-Feature-Hybrid-Enhanced-State-Space-Model-for-Point-Cloud-Classification)
* [Addition is almost all you need Compressing neural networks with double binary factorization](#Addition-is-almost-all-you-need-Compressing-neural-networks-with-double-binary-factorization)
* [Space Group Equivariant Crystal Diffusion](#Space-Group-Equivariant-Crystal-Diffusion)
* [Group-in-Group Policy Optimization for LLM Agent Training](#Group-in-Group-Policy-Optimization-for-LLM-Agent-Training)
* [MPS-Prover Advancing Stepwise Theorem Proving by Multi-Perspective Search and Data Curation](#MPS-Prover-Advancing-Stepwise-Theorem-Proving-by-Multi-Perspective-Search-and-Data-Curation)
* [SubGCache Accelerating Graph-based RAG with Subgraph-level KV Cache](#SubGCache-Accelerating-Graph-based-RAG-with-Subgraph-level-KV-Cache)
* [Semantic Aware Linear Transfer by Recycling Pre-trained Language Models for Cross-lingual Transfer](#Semantic-Aware-Linear-Transfer-by-Recycling-Pre-trained-Language-Models-for-Cross-lingual-Transfer)
* [Accurate KV Cache Quantization with Outlier Tokens Tracing](#Accurate-KV-Cache-Quantization-with-Outlier-Tokens-Tracing)
* [Explain What You Mean Intent Augmented Knowledge Graph Recommender Built With An LLM](#Explain-What-You-Mean-Intent-Augmented-Knowledge-Graph-Recommender-Built-With-An-LLM)
* [Improve Rule Retrieval and Reasoning with Self-Induction and Relevance ReEstimate](#Improve-Rule-Retrieval-and-Reasoning-with-Self-Induction-and-Relevance-ReEstimate)
* [PoE-World Compositional World Modeling with Products of Programmatic Experts](#PoE-World-Compositional-World-Modeling-with-Products-of-Programmatic-Experts)
* [Attention-Based Reward Shaping for Sparse and Delayed Rewards](#Attention-Based-Reward-Shaping-for-Sparse-and-Delayed-Rewards)
* [Bridging BCI and Communications A MIMO Framework for EEG-to-ECoG Wireless Channel Modeling](#Bridging-BCI-and-Communications-A-MIMO-Framework-for-EEG-to-ECoG-Wireless-Channel-Modeling)
* [EdgeMM Multi-Core CPU with Heterogeneous AI-Extension and Activation-aware Weight Pruning for Multimodal LLMs at Edge](#EdgeMM-Multi-Core-CPU-with-Heterogeneous-AI-Extension-and-Activation-aware-Weight-Pruning-for-Multimodal-LLMs-at-Edge)


## An Effective Training Framework for Light-Weight Automatic Speech Recognition Models

>Authors: Abdul Hannan, Alessio Brutti, Shah Nawaz, Mubashir Noman

>2025-05-22

> http://arxiv.org/abs/2505.16991v1

Recent advancement in deep learning encouraged developing large automatic
speech recognition (ASR) models that achieve promising results while ignoring
computational and memory constraints. However, deploying such models on low
resource devices is impractical despite of their favorable performance.
Existing approaches (**pruning**, distillation, layer skip etc.) transform the
large models into smaller ones at the cost of significant performance
degradation or require prolonged training of smaller models for better
performance. To address these issues, we introduce an efficacious two-step
representation learning based approach capable of producing several small sized
models from a single large model ensuring considerably better performance in
limited number of epochs. Comprehensive experimentation on ASR benchmarks
reveals the efficacy of our approach, achieving three-fold training speed-up
and up to 12.54% word error rate improvement.


## UFT Unifying Supervised and Reinforcement Fine-Tuning

>Authors: Mingyang Liu, Gabriele Farina, Asuman Ozdaglar

>2025-05-22

> http://arxiv.org/abs/2505.16984v1

Post-training has demonstrated its importance in enhancing the reasoning
capabilities of large language models (LLMs). The primary post-training methods
can be categorized into supervised fine-tuning (SFT) and reinforcement
fine-tuning (RFT). SFT is efficient and well-suited for small language models,
but it may lead to overfitting and limit the reasoning abilities of larger
models. In contrast, RFT generally yields better generalization but depends
heavily on the strength of the base model. To address the limitations of SFT
and RFT, we propose Unified Fine-Tuning (UFT), a novel post-training paradigm
that unifies SFT and RFT into a single, integrated process. UFT enables the
model to effectively explore solutions while incorporating informative
supervision signals, bridging the gap between memorizing and thinking
underlying existing methods. Notably, UFT outperforms both SFT and RFT in
general, regardless of model sizes. Furthermore, we theoretically prove that
UFT breaks RFT's inherent exponential sample complexity bottleneck, showing for
the first time that unified training can exponentially accelerate convergence
on long-horizon reasoning tasks.


## Beyond Correlation Towards Causal Large Language Model Agents in Biomedicine

>Authors: Adib Bazgir, Amir Habibdoust Lafmajani, Yuwen Zhang

>2025-05-22

> http://arxiv.org/abs/2505.16982v1

Large Language Models (LLMs) show promise in biomedicine but lack true causal
understanding, relying instead on correlations. This paper envisions causal LLM
agents that integrate multimodal data (text, images, genomics, etc.) and
perform intervention-based reasoning to infer cause-and-effect. Addressing this
requires overcoming key challenges: designing safe, controllable agentic
frameworks; developing rigorous benchmarks for causal evaluation; integrating
heterogeneous data sources; and synergistically combining LLMs with structured
knowledge (KGs) and formal causal inference tools. Such agents could unlock
transformative opportunities, including accelerating drug discovery through
automated hypothesis generation and simulation, enabling personalized medicine
through patient-specific causal models. This research agenda aims to foster
interdisciplinary efforts, bridging causal concepts and foundation models to
develop reliable AI partners for biomedical progress.


## Fixing Data That Hurts Performance Cascading LLMs to Relabel Hard Negatives for Robust Information Retrieval

>Authors: Nandan Thakur, Crystina Zhang, Xueguang Ma, Jimmy Lin

>2025-05-22

> http://arxiv.org/abs/2505.16967v1

Training robust retrieval and reranker models typically relies on large-scale
retrieval datasets; for example, the BGE collection contains 1.6 million
query-passage pairs sourced from various data sources. However, we find that
certain datasets can negatively impact model effectiveness -- **pruning** 8 out of
15 datasets from the BGE collection reduces the training set size by
2.35$\times$ and increases nDCG@10 on BEIR by 1.0 point. This motivates a
deeper examination of training data quality, with a particular focus on "false
negatives", where relevant passages are incorrectly labeled as irrelevant. We
propose a simple, cost-effective approach using cascading LLM prompts to
identify and relabel hard negatives. Experimental results show that relabeling
false negatives with true positives improves both E5 (base) and Qwen2.5-7B
retrieval models by 0.7-1.4 nDCG@10 on BEIR and by 1.7-1.8 nDCG@10 on zero-shot
AIR-Bench evaluation. Similar gains are observed for rerankers fine-tuned on
the relabeled data, such as Qwen2.5-3B on BEIR. The reliability of the
cascading design is further supported by human annotation results, where we
find judgment by GPT-4o shows much higher agreement with humans than
GPT-4o-mini.


## MedFrameQA A Multi-Image Medical VQA Benchmark for Clinical Reasoning

>Authors: Suhao Yu, Haojin Wang, Juncheng Wu, Cihang Xie, Yuyin Zhou

>2025-05-22

> http://arxiv.org/abs/2505.16964v1

Existing medical VQA benchmarks mostly focus on single-image analysis, yet
clinicians almost always compare a series of images before reaching a
diagnosis. To better approximate this workflow, we introduce MedFrameQA -- the
first benchmark that explicitly evaluates multi-image reasoning in medical VQA.
To build MedFrameQA both at scale and in high-quality, we develop 1) an
automated pipeline that extracts temporally coherent frames from medical videos
and constructs VQA items whose content evolves logically across images, and 2)
a multiple-stage filtering strategy, including model-based and manual review,
to preserve data clarity, difficulty, and medical relevance. The resulting
dataset comprises 2,851 VQA pairs (gathered from 9,237 high-quality frames in
3,420 videos), covering nine human body systems and 43 organs; every question
is accompanied by two to five images. We comprehensively benchmark ten advanced
Multimodal LLMs -- both proprietary and open source, with and without explicit
reasoning modules -- on MedFrameQA. The evaluation challengingly reveals that
all models perform poorly, with most accuracies below 50%, and accuracy
fluctuates as the number of images per question increases. Error analysis
further shows that models frequently ignore salient findings, mis-aggregate
evidence across images, and propagate early mistakes through their reasoning
chains; results also vary substantially across body systems, organs, and
modalities. We hope this work can catalyze research on clinically grounded,
multi-image reasoning and accelerate progress toward more capable diagnostic AI
systems.


## From Reality to Virtual Worlds The Role of Photogrammetry in Game Development

>Authors: Santiago Berrezueta-Guzman, Andrei Koshelev, Stefan Wagner

>2025-05-22

> http://arxiv.org/abs/2505.16951v1

Photogrammetry is transforming digital content creation by enabling the rapid
conversion of real-world objects into highly detailed 3D models. This paper
evaluates the role of RealityCapture, a GPU-accelerated photogrammetry tool, in
game development of Virtual Reality (VR). We assess its efficiency,
reconstruction accuracy, and integration with Unreal Engine, comparing its
advantages and limitations against traditional modeling workflows.
Additionally, we examined user preferences between designed 3D assets and
photogrammetry-generated models. The results revealed that while photogrammetry
enhances realism and interactivity, users slightly preferred manually designed
models for small, manipulable elements because of the level of detail. However,
from a developer perspective, RealityCapture significantly reduces development
time while maintaining geometric precision and photorealistic textures. Despite
its reliance on high-performance hardware, its automation, scalability, and
seamless integration with real-time rendering engines make it a valuable tool
for game developers and VR creators. Future improvements in AI-driven
optimization and cloud-based processing could enhance accessibility, broadening
its applications in gaming, cultural heritage preservation, and simulation.


## Bottlenecked Transformers Periodic KV Cache Abstraction for Generalised Reasoning

>Authors: Adnan Oomerjee, Zafeirios Fountas, Zhongwei Yu, Haitham Bou-Ammar, Jun Wang

>2025-05-22

> http://arxiv.org/abs/2505.16950v1

Despite their impressive capabilities, Large Language Models struggle with
generalisation beyond their training distribution, often exhibiting
sophisticated pattern interpolation rather than true abstract reasoning
(extrapolation). In this work, we approach this limitation through the lens of
Information Bottleneck (IB) theory, which posits that model generalisation
emerges from an optimal balance between input compression and retention of
predictive information in latent representations. We prove using IB theory that
decoder-only Transformers are inherently constrained in their ability to form
task-optimal sequence representations. We then use this result to demonstrate
that periodic global transformation of the internal sequence-level
representations (**KV** cache) is a necessary computational step for improving
Transformer generalisation in reasoning tasks. Based on these theoretical
insights, we propose a modification to the Transformer architecture, in the
form of an additional module that globally rewrites the **KV** cache at periodic
intervals, shifting its capacity away from memorising input prefixes and toward
encoding features most useful for predicting future tokens. Our model delivers
substantial gains on mathematical reasoning benchmarks, outperforming both
vanilla Transformers with up to 3.5x more parameters, as well as
heuristic-driven **pruning** mechanisms for cache compression. Our approach can be
seen as a principled generalisation of existing **KV**-cache compression methods;
whereas such methods focus solely on compressing input representations, they
often do so at the expense of retaining predictive information, and thus their
capabilities are inherently bounded by those of an unconstrained model. This
establishes a principled framework to manipulate Transformer memory using
information theory, addressing fundamental reasoning limitations that scaling
alone cannot overcome.


## MixAT Combining Continuous and Discrete Adversarial Training for LLMs

>Authors: Csaba Dékány, Stefan Balauca, Robin Staab, Dimitar I. Dimitrov, Martin Vechev

>2025-05-22

> http://arxiv.org/abs/2505.16947v1

Despite recent efforts in Large Language Models (LLMs) safety and alignment,
current adversarial attacks on frontier LLMs are still able to force harmful
generations consistently. Although adversarial training has been widely studied
and shown to significantly improve the robustness of traditional machine
learning models, its strengths and weaknesses in the context of LLMs are less
understood. Specifically, while existing discrete adversarial attacks are
effective at producing harmful content, training LLMs with concrete adversarial
prompts is often computationally expensive, leading to reliance on continuous
relaxations. As these relaxations do not correspond to discrete input tokens,
such latent training methods often leave models vulnerable to a diverse set of
discrete attacks. In this work, we aim to bridge this gap by introducing MixAT,
a novel method that combines stronger discrete and faster continuous attacks
during training. We rigorously evaluate MixAT across a wide spectrum of
state-of-the-art attacks, proposing the At Least One Attack Success Rate
(ALO-ASR) metric to capture the worst-case vulnerability of models. We show
MixAT achieves substantially better robustness (ALO-ASR < 20%) compared to
prior defenses (ALO-ASR > 50%), while maintaining a runtime comparable to
methods based on continuous relaxations. We further analyze MixAT in realistic
deployment settings, exploring how chat templates, **quantization**, low-rank
adapters, and temperature affect both adversarial training and evaluation,
revealing additional blind spots in current methodologies. Our results
demonstrate that MixAT's discrete-continuous defense offers a principled and
superior robustness-accuracy tradeoff with minimal computational overhead,
highlighting its promise for building safer LLMs. We provide our code and
models at https://github.com/insait-institute/MixAT.


## NovelSeek When Agent Becomes the Scientist -- Building Closed-Loop System from Hypothesis to Verification

>Authors: NovelSeek Team, Bo Zhang, Shiyang Feng, Xiangchao Yan, Jiakang Yuan, Zhiyin Yu, Xiaohan He, Songtao Huang, Shaowei Hou, Zheng Nie, Zhilong Wang, Jinyao Liu, Runmin Ma, Tianshuo Peng, Peng Ye, Dongzhan Zhou, Shufei Zhang, Xiaosong Wang, Yilan Zhang, Meng Li, Zhongying Tu, Xiangyu Yue, Wangli Ouyang, Bowen Zhou, Lei Bai

>2025-05-22

> http://arxiv.org/abs/2505.16938v1

Artificial Intelligence (AI) is accelerating the transformation of scientific
research paradigms, not only enhancing research efficiency but also driving
innovation. We introduce NovelSeek, a unified closed-loop multi-agent framework
to conduct Autonomous Scientific Research (ASR) across various scientific
research fields, enabling researchers to tackle complicated problems in these
fields with unprecedented speed and precision. NovelSeek highlights three key
advantages: 1) Scalability: NovelSeek has demonstrated its versatility across
12 scientific research tasks, capable of generating innovative ideas to enhance
the performance of baseline code. 2) Interactivity: NovelSeek provides an
interface for human expert feedback and multi-agent interaction in automated
end-to-end processes, allowing for the seamless integration of domain expert
knowledge. 3) Efficiency: NovelSeek has achieved promising performance gains in
several scientific fields with significantly less time cost compared to human
efforts. For instance, in reaction yield prediction, it increased from 27.6% to
35.4% in just 12 hours; in enhancer activity prediction, accuracy rose from
0.52 to 0.79 with only 4 hours of processing; and in 2D semantic segmentation,
precision advanced from 78.8% to 81.0% in a mere 30 hours.


## REPA Works Until It Doesn't Early-Stopped, Holistic Alignment Supercharges Diffusion Training

>Authors: Ziqiao Wang, Wangbo Zhao, Yuhao Zhou, Zekai Li, Zhiyuan Liang, Mingjia Shi, Xuanlei Zhao, Pengfei Zhou, Kaipeng Zhang, Zhangyang Wang, Kai Wang, Yang You

>2025-05-22

> http://arxiv.org/abs/2505.16792v1

Diffusion Transformers (DiTs) deliver state-of-the-art image quality, yet
their training remains notoriously slow. A recent remedy -- representation
alignment (REPA) that matches DiT hidden features to those of a non-generative
teacher (e.g. DINO) -- dramatically accelerates the early epochs but plateaus
or even degrades performance later. We trace this failure to a capacity
mismatch: once the generative student begins modelling the joint data
distribution, the teacher's lower-dimensional embeddings and attention patterns
become a straitjacket rather than a guide. We then introduce HASTE (Holistic
Alignment with Stage-wise Termination for Efficient training), a two-phase
schedule that keeps the help and drops the hindrance. Phase I applies a
holistic alignment loss that simultaneously distills attention maps (relational
priors) and feature projections (semantic anchors) from the teacher into
mid-level layers of the DiT, yielding rapid convergence. Phase II then performs
one-shot termination that deactivates the alignment loss, once a simple trigger
such as a fixed iteration is hit, freeing the DiT to focus on denoising and
exploit its generative capacity. HASTE speeds up training of diverse DiTs
without architecture changes. On ImageNet 256X256, it reaches the vanilla
SiT-XL/2 baseline FID in 50 epochs and matches REPA's best FID in 500 epochs,
amounting to a 28X reduction in optimization steps. HASTE also improves
text-to-image DiTs on MS-COCO, demonstrating to be a simple yet principled
recipe for efficient diffusion training across various tasks. Our code is
available at https://github.com/NUS-HPC-AI-Lab/HASTE .


## Mitigating Overfitting in Medical Imaging Self-Supervised Pretraining vs. ImageNet Transfer Learning for Dermatological Diagnosis

>Authors: Iván Matas, Carmen Serrano, Miguel Nogales, David Moreno, Lara Ferrándiz, Teresa Ojeda, Begoña Acha

>2025-05-22

> http://arxiv.org/abs/2505.16773v1

Deep learning has transformed computer vision but relies heavily on large
labeled datasets and computational resources. Transfer learning, particularly
fine-tuning pretrained models, offers a practical alternative; however, models
pretrained on natural image datasets such as ImageNet may fail to capture
domain-specific characteristics in medical imaging. This study introduces an
unsupervised learning framework that extracts high-value dermatological
features instead of relying solely on ImageNet-based pretraining. We employ a
Variational Autoencoder (VAE) trained from scratch on a proprietary
dermatological dataset, allowing the model to learn a structured and clinically
relevant latent space. This self-supervised feature extractor is then compared
to an ImageNet-pretrained backbone under identical classification conditions,
highlighting the trade-offs between general-purpose and domain-specific
pretraining. Our results reveal distinct learning patterns. The self-supervised
model achieves a final validation loss of 0.110 (-33.33%), while the
ImageNet-pretrained model stagnates at 0.100 (-16.67%), indicating overfitting.
Accuracy trends confirm this: the self-supervised model improves from 45% to
65% (+44.44%) with a near-zero overfitting gap, whereas the ImageNet-pretrained
model reaches 87% (+50.00%) but plateaus at 75% (+19.05%), with its overfitting
gap increasing to +0.060. These findings suggest that while ImageNet
pretraining accelerates convergence, it also amplifies overfitting on
non-clinically relevant features. In contrast, self-supervised learning
achieves steady improvements, stronger generalization, and superior
adaptability, underscoring the importance of domain-specific feature extraction
in medical imaging.


## TRIM Achieving Extreme Sparsity with Targeted Row-wise Iterative Metric-driven Pruning

>Authors: Florentin Beck, William Rudman, Carsten Eickhoff

>2025-05-22

> http://arxiv.org/abs/2505.16743v1

Large Language Models (LLMs) present significant computational and memory
challenges due to their extensive size, making **pruning** essential for their
efficient deployment. Existing one-shot **pruning** methods often apply uniform
**sparsity** constraints across layers or within each layer, resulting in
suboptimal performance, especially at high **sparsity** ratios. This work
introduces TRIM (Targeted Row-wise Iterative Metric-driven **pruning**), a novel
approach that applies varying **sparsity** ratios to individual output dimensions
(rows) within each layer. TRIM employs an iterative adjustment process guided
by quality metrics to optimize dimension-wise **sparsity** allocation, focusing on
reducing variance in quality retention across outputs to preserve critical
information. TRIM can be seamlessly integrated with existing layer-wise **pruning**
strategies. Our evaluations on perplexity and zero-shot tasks across diverse
LLM families (Qwen2.5, LLaMA-2, and OPT) and **sparsity** levels demonstrate that
TRIM achieves new state-of-the-art results and enhances stability. For
instance, at 80% **sparsity**, TRIM reduces perplexity by 48% for Qwen2.5-14B and
over 90% for OPT-13B compared to baseline methods. We conclude that
fine-grained, dimension-wise **sparsity** adaptation is crucial for pushing the
limits of extreme LLM compression. Code available at:
https://github.com/flobk/TRIM


## Training Long-Context LLMs Efficiently via Chunk-wise Optimization

>Authors: Wenhao Li, Yuxin Zhang, Gen Luo, Daohai Yu, Rongrong Ji

>2025-05-22

> http://arxiv.org/abs/2505.16710v1

While long-context large language models (LLMs) exhibit remarkable document
processing capabilities, their prohibitively high training costs often hinder
customized applications. To mitigate this issue, we propose \textit{Sequential
Chunk-wise Optimization} (SeCO), a memory-efficient training paradigm that
partitions lengthy inputs into manageable chunks. Each chunk independently
constructs its computational graph and performs localized backpropagation,
ensuring that only one chunk's forward activations are stored in memory.
Building on SeCO, we further introduce \textit{Sparse Chunk-wise Optimization}
(SpaCO), which reduces computational overhead by selectively propagating
gradients to specific chunks and incorporates a carefully designed compensation
factor to ensure unbiased gradient estimation. SpaCO decouples the
computational cost of backpropagation from the context length, enabling
training time to gradually converge to inference time as sequences become
longer. Implemented as lightweight training wrappers, both SeCO and SpaCO offer
substantial practical benefits. For example, when fine-tuning an 8B model with
LoRA on a single RTX 3090 GPU, SeCO expands maximum sequence length from 1K to
16K tokens, while SpaCO demonstrates accelerated training speed -- achieving up
to 3x faster than SeCO under the same experimental setup. These innovations
provide new insights into optimizing long-context models, making them more
accessible for practical applications. We have open-sourced the code at
\href{https://github.com/wenhaoli-xmu/seco}{here}.


## R1-ShareVL Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO

>Authors: Huanjin Yao, Qixiang Yin, Jingyi Zhang, Min Yang, Yibo Wang, Wenhao Wu, Fei Su, Li Shen, Minghui Qiu, Dacheng Tao, Jiaxing Huang

>2025-05-22

> http://arxiv.org/abs/2505.16673v1

In this work, we aim to incentivize the reasoning ability of Multimodal Large
Language Models (MLLMs) via reinforcement learning (RL) and develop an
effective approach that mitigates the **sparse** reward and advantage vanishing
issues during RL. To this end, we propose Share-GRPO, a novel RL approach that
tackle these issues by exploring and sharing diverse reasoning trajectories
over expanded question space. Specifically, Share-GRPO first expands the
question space for a given question via data transformation techniques, and
then encourages MLLM to effectively explore diverse reasoning trajectories over
the expanded question space and shares the discovered reasoning trajectories
across the expanded questions during RL. In addition, Share-GRPO also shares
reward information during advantage computation, which estimates solution
advantages hierarchically across and within question variants, allowing more
accurate estimation of relative advantages and improving the stability of
policy training. Extensive evaluations over six widely-used reasoning
benchmarks showcase the superior performance of our method. Code will be
available at https://github.com/HJYao00/R1-ShareVL.


## Open and Sustainable AI challenges, opportunities and the road ahead in the life sciences

>Authors: Gavin Farrell, Eleni Adamidi, Rafael Andrade Buono, Mihail Anton, Omar Abdelghani Attafi, Salvador Capella Gutierrez, Emidio Capriotti, Leyla Jael Castro, Davide Cirillo, Lisa Crossman, Christophe Dessimoz, Alexandros Dimopoulos, Raul Fernandez-Diaz, Styliani-Christina Fragkouli, Carole Goble, Wei Gu, John M. Hancock, Alireza Khanteymoori, Tom Lenaerts, Fabio G. Liberante, Peter Maccallum, Alexander Miguel Monzon, Magnus Palmblad, Lucy Poveda, Ovidiu Radulescu, Denis C. Shields, Shoaib Sufi, Thanasis Vergoulis, Fotis Psomopoulos, Silvio C. E. Tosatto

>2025-05-22

> http://arxiv.org/abs/2505.16619v1

Artificial intelligence (AI) has recently seen transformative breakthroughs
in the life sciences, expanding possibilities for researchers to interpret
biological information at an unprecedented capacity, with novel applications
and advances being made almost daily. In order to maximise return on the
growing investments in AI-based life science research and accelerate this
progress, it has become urgent to address the exacerbation of long-standing
research challenges arising from the rapid adoption of AI methods. We review
the increased erosion of trust in AI research outputs, driven by the issues of
poor reusability and reproducibility, and highlight their consequent impact on
environmental sustainability. Furthermore, we discuss the fragmented components
of the AI ecosystem and lack of guiding pathways to best support Open and
Sustainable AI (OSAI) model development. In response, this perspective
introduces a practical set of OSAI recommendations directly mapped to over 300
components of the AI ecosystem. Our work connects researchers with relevant AI
resources, facilitating the implementation of sustainable, reusable and
transparent AI. Built upon life science community consensus and aligned to
existing efforts, the outputs of this perspective are designed to aid the
future development of policy and structured pathways for guiding AI
implementation.


## Steering Large Language Models for Machine Translation Personalization

>Authors: Daniel Scalena, Gabriele Sarti, Arianna Bisazza, Elisabetta Fersini, Malvina Nissim

>2025-05-22

> http://arxiv.org/abs/2505.16612v1

High-quality machine translation systems based on large language models
(LLMs) have simplified the production of personalized translations reflecting
specific stylistic constraints. However, these systems still struggle in
settings where stylistic requirements are less explicit and might be harder to
convey via prompting. We explore various strategies for personalizing
LLM-generated translations in low-resource settings, focusing on the
challenging literary translation domain. We explore prompting strategies and
inference-time interventions for steering model generations towards a
personalized style, and propose a contrastive framework exploiting latent
concepts extracted from **sparse** autoencoders to identify salient personalization
properties. Our results show that steering achieves strong personalization
while preserving translation quality. We further examine the impact of steering
on LLM representations, finding model layers with a relevant impact for
personalization are impacted similarly by multi-shot prompting and our steering
method, suggesting similar mechanism at play.


## URLs Help, Topics Guide Understanding Metadata Utility in LLM Training

>Authors: Dongyang Fan, Vinko Sabolčec, Martin Jaggi

>2025-05-22

> http://arxiv.org/abs/2505.16570v1

Large Language Models (LLMs) are commonly pretrained on vast corpora of text
without utilizing contextual metadata such as source, quality, or topic,
leading to a context-free learning paradigm. While recent studies suggest that
adding metadata like URL information as context (i.e., auxiliary inputs not
used in the loss calculation) can improve training efficiency and downstream
performance, they offer limited understanding of which types of metadata are
truly effective and under what conditions. In this work, we conduct a
systematic evaluation and find that not all metadata types contribute equally.
Only URL context speeds up training, whereas quality scores and topic/format
domain information offer no clear benefit. Furthermore, the improved downstream
performances of URL conditioning emerge only when longer prompts are used at
inference time. In addition, we demonstrate that context-aware pretraining
enables more controllable generation than context-free pretraining, in a
classifier-free guidance fashion. Although topic and format metadata do not
accelerate training, they are effective for steering outputs, offering
human-interpretable control over generation.


## SHaDe Compact and Consistent Dynamic 3D Reconstruction via Tri-Plane Deformation and Latent Diffusion

>Authors: Asrar Alruwayqi

>2025-05-22

> http://arxiv.org/abs/2505.16535v1

We present a novel framework for dynamic 3D scene reconstruction that
integrates three key components: an explicit tri-plane deformation field, a
view-conditioned canonical radiance field with spherical harmonics (SH)
attention, and a temporally-aware latent diffusion prior. Our method encodes 4D
scenes using three orthogonal 2D feature planes that evolve over time, enabling
efficient and compact spatiotemporal representation. These features are
explicitly warped into a canonical space via a deformation offset field,
eliminating the need for MLP-based motion modeling.
  In canonical space, we replace traditional MLP decoders with a structured
SH-based rendering head that synthesizes view-dependent color via attention
over learned frequency bands improving both interpretability and rendering
efficiency. To further enhance fidelity and temporal consistency, we introduce
a transformer-guided latent diffusion module that refines the tri-plane and
deformation features in a compressed latent space. This generative module
denoises scene representations under ambiguous or out-of-distribution (OOD)
motion, improving generalization.
  Our model is trained in two stages: the diffusion module is first pre-trained
independently, and then fine-tuned jointly with the full pipeline using a
combination of image reconstruction, diffusion denoising, and temporal
consistency losses. We demonstrate state-of-the-art results on synthetic
benchmarks, surpassing recent methods such as HexPlane and 4D Gaussian
Splatting in visual quality, temporal coherence, and robustness to **sparse**-view
dynamic inputs.


## Causal-Invariant Cross-Domain Out-of-Distribution Recommendation

>Authors: Jiajie Zhu, Yan Wang, Feng Zhu, Pengfei Ding, Hongyang Liu, Zhu Sun

>2025-05-22

> http://arxiv.org/abs/2505.16532v1

Cross-Domain Recommendation (CDR) aims to leverage knowledge from a
relatively data-richer source domain to address the data **sparsity** problem in a
relatively data-**sparse**r target domain. While CDR methods need to address the
distribution shifts between different domains, i.e., cross-domain distribution
shifts (CDDS), they typically assume independent and identical distribution
(IID) between training and testing data within the target domain. However, this
IID assumption rarely holds in real-world scenarios due to single-domain
distribution shift (SDDS). The above two co-existing distribution shifts lead
to out-of-distribution (OOD) environments that hinder effective knowledge
transfer and generalization, ultimately degrading recommendation performance in
CDR. To address these co-existing distribution shifts, we propose a novel
Causal-Invariant Cross-Domain Out-of-distribution Recommendation framework,
called CICDOR. In CICDOR, we first learn dual-level causal structures to infer
domain-specific and domain-shared causal-invariant user preferences for
tackling both CDDS and SDDS under OOD environments in CDR. Then, we propose an
LLM-guided confounder discovery module that seamlessly integrates LLMs with a
conventional causal discovery method to extract observed confounders for
effective deconfounding, thereby enabling accurate causal-invariant preference
inference. Extensive experiments on two real-world datasets demonstrate the
superior recommendation accuracy of CICDOR over state-of-the-art methods across
various OOD scenarios.


## DuFFin A Dual-Level Fingerprinting Framework for LLMs IP Protection

>Authors: Yuliang Yan, Haochun Tang, Shuo Yan, Enyan Dai

>2025-05-22

> http://arxiv.org/abs/2505.16530v1

Large language models (LLMs) are considered valuable Intellectual Properties
(IP) for legitimate owners due to the enormous computational cost of training.
It is crucial to protect the IP of LLMs from malicious stealing or unauthorized
deployment. Despite existing efforts in watermarking and fingerprinting LLMs,
these methods either impact the text generation process or are limited in
white-box access to the suspect model, making them impractical. Hence, we
propose DuFFin, a novel $\textbf{Du}$al-Level $\textbf{Fin}$gerprinting
$\textbf{F}$ramework for black-box setting ownership verification. DuFFin
extracts the trigger pattern and the knowledge-level fingerprints to identify
the source of a suspect model. We conduct experiments on a variety of models
collected from the open-source website, including four popular base models as
protected LLMs and their fine-tuning, **quantization**, and safety alignment
versions, which are released by large companies, start-ups, and individual
users. Results show that our method can accurately verify the copyright of the
base protected LLM on their model variants, achieving the IP-ROC metric greater
than 0.95. Our code is available at
https://github.com/yuliangyan0807/llm-fingerprint.


## AnchorFormer Differentiable Anchor Attention for Efficient Vision Transformer

>Authors: Jiquan Shan, Junxiao Wang, Lifeng Zhao, Liang Cai, Hongyuan Zhang, Ioannis Liritzis

>2025-05-22

> http://arxiv.org/abs/2505.16463v1

Recently, vision transformers (ViTs) have achieved excellent performance on
vision tasks by measuring the global self-attention among the image patches.
Given $n$ patches, they will have quadratic complexity such as
$\mathcal{O}(n^2)$ and the time cost is high when splitting the input image
with a small granularity. Meanwhile, the pivotal information is often randomly
gathered in a few regions of an input image, some tokens may not be helpful for
the downstream tasks. To handle this problem, we introduce an anchor-based
efficient vision transformer (AnchorFormer), which employs the anchor tokens to
learn the pivotal information and accelerate the inference. Firstly, by
estimating the bipartite attention between the anchors and tokens, the
complexity will be reduced from $\mathcal{O}(n^2)$ to $\mathcal{O}(mn)$, where
$m$ is an anchor number and $m < n$. Notably, by representing the anchors with
the neurons in a neural layer, we can differentiable learn these distributions
and approximate global self-attention through the Markov process. Moreover, we
extend the proposed model to three downstream tasks including classification,
detection, and segmentation. Extensive experiments show the effectiveness of
our AnchorFormer, e.g., achieving up to a 9.0% higher accuracy or 46.7% FLOPs
reduction on ImageNet classification, 81.3% higher mAP on COCO detection under
comparable FLOPs, as compared to the current baselines.


## TAT-VPR Ternary Adaptive Transformer for Dynamic and Efficient Visual Place Recognition

>Authors: Oliver Grainge, Michael Milford, Indu Bodala, Sarvapali D. Ramchurn, Shoaib Ehsan

>2025-05-22

> http://arxiv.org/abs/2505.16447v1

TAT-VPR is a ternary-**quantize**d transformer that brings dynamic
accuracy-efficiency trade-offs to visual SLAM loop-closure. By fusing ternary
weights with a learned activation-**sparsity** gate, the model can control
computation by up to 40% at run-time without degrading performance (Recall@1).
The proposed two-stage distillation pipeline preserves descriptor quality,
letting it run on micro-UAV and embedded SLAM stacks while matching
state-of-the-art localization accuracy.


## FPQVAR Floating Point Quantization for Visual Autoregressive Model with FPGA Hardware Co-design

>Authors: Renjie Wei, Songqiang Xu, Qingyu Guo, Meng Li

>2025-05-22

> http://arxiv.org/abs/2505.16335v1

Visual autoregressive (VAR) modeling has marked a paradigm shift in image
generation from next-token prediction to next-scale prediction. VAR predicts a
set of tokens at each step from coarse to fine scale, leading to better image
quality and faster inference speed compared to existing diffusion models.
However, the large parameter size and computation cost hinder its deployment on
edge devices. To reduce the memory and computation cost, we propose FPQVAR, an
efficient post-training floating-point (FP) **quantization** framework for VAR
featuring algorithm and hardware co-design. At the algorithm level, we first
identify the challenges of quantizing VAR. To address them, we propose Dual
Format Quantization for the highly imbalanced input activation. We further
propose Group-wise Hadamard Transformation and GHT-Aware Learnable
Transformation to address the time-varying outlier channels. At the hardware
level, we design the first **low-bit** FP **quantize**r and multiplier with lookup
tables on FPGA and propose the first FPGA-based VAR accelerator featuring
**low-bit** FP computation and an elaborate two-level pipeline. Extensive
experiments show that compared to the state-of-the-art **quantization** method, our
proposed FPQVAR significantly improves Fr\'echet Inception Distance (FID) from
10.83 to 3.58, Inception Score (IS) from 175.9 to 241.5 under 4-bit
**quantization**. FPQVAR also significantly improves the performance of 6-bit
**quantize**d VAR, bringing it on par with the FP16 model. Our accelerator on
AMD-Xilinx VCK190 FPGA achieves a throughput of 1.1 image/s, which is 3.1x
higher than the integer-based accelerator. It also demonstrates 3.6x and 2.8x
higher energy efficiency compared to the integer-based accelerator and GPU
baseline, respectively.


## EquivPruner Boosting Efficiency and Quality in LLM-Based Search via Action Pruning

>Authors: Jiawei Liu, Qisi Chen, Jianshu Zhang, Quan Liu, Defu Lian

>2025-05-22

> http://arxiv.org/abs/2505.16312v1

Large Language Models (LLMs) excel at complex reasoning through search
algorithms, yet current strategies often suffer from massive token consumption
due to redundant exploration of semantically equivalent steps. Existing
semantic similarity methods struggle to accurately identify such equivalence in
domain-specific contexts like mathematical reasoning. To address this, we
propose EquivPruner, a simple yet effective approach that identifies and prunes
semantically equivalent actions during LLM reasoning search. We also introduce
MathEquiv, the first dataset we created for mathematical statement equivalence,
which enables the training of a lightweight equivalence detector. Extensive
experiments across various models and tasks demonstrate that EquivPruner
significantly reduces token consumption, improving searching efficiency and
often bolstering reasoning accuracy. For instance, when applied to
Qwen2.5-Math-7B-Instruct on GSM8K, EquivPruner reduced token consumption by
48.1\% while also improving accuracy. Our code is available at
https://github.com/Lolo1222/EquivPruner.


## Artificial Intelligence for Direct Prediction of Molecular Dynamics Across Chemical Space

>Authors: Fuchun Ge, Pavlo O. Dral

>2025-05-22

> http://arxiv.org/abs/2505.16301v1

Molecular dynamics (MD) is a powerful tool for exploring the behavior of
atomistic systems, but its reliance on sequential numerical integration limits
simulation efficiency. We present MDtrajNet-1, a foundational AI model that
directly generates MD trajectories across chemical space, bypassing force
calculations and integration. This approach accelerates simulations by up to
two orders of magnitude compared to traditional MD, even those enhanced by
machine-learning interatomic potentials. MDtrajNet-1 combines equivariant
neural networks with a Transformer-based architecture to achieve strong
accuracy and transferability in predicting long-time trajectories for both
known and unseen systems. Remarkably, the errors of the trajectories generated
by MDtrajNet-1 for various molecular systems are close to those of the
conventional ab initio MD. The model's flexible design supports diverse
application scenarios, including different statistical ensembles, boundary
conditions, and interaction types. By overcoming the intrinsic speed barrier of
conventional MD, MDtrajNet-1 opens new frontiers in efficient and scalable
atomistic simulations.


## ARPOEnd-to-End Policy Optimization for GUI Agents with Experience Replay

>Authors: Fanbin Lu, Zhisheng Zhong, Shu Liu, Chi-Wing Fu, Jiaya Jia

>2025-05-22

> http://arxiv.org/abs/2505.16282v1

Training large language models (LLMs) as interactive agents for controlling
graphical user interfaces (GUIs) presents a unique challenge to optimize
long-horizon action sequences with multimodal feedback from complex
environments. While recent works have advanced multi-turn reinforcement
learning (RL) for reasoning and tool-using capabilities in LLMs, their
application to GUI-based agents remains relatively underexplored due to the
difficulty of **sparse** rewards, delayed feedback, and high rollout costs. In this
paper, we investigate end-to-end policy optimization for vision-language-based
GUI agents with the aim of improving performance on complex, long-horizon
computer tasks. We propose Agentic Replay Policy Optimization (ARPO), an
end-to-end RL approach that augments Group Relative Policy Optimization (GRPO)
with a replay buffer to reuse the successful experience across training
iterations. To further stabilize the training process, we propose a task
selection strategy that filters tasks based on baseline agent performance,
allowing the agent to focus on learning from informative interactions.
Additionally, we compare ARPO with offline preference optimization approaches,
highlighting the advantages of policy-based methods in GUI environments.
Experiments on the OSWorld benchmark demonstrate that ARPO achieves competitive
results, establishing a new performance baseline for LLM-based GUI agents
trained via reinforcement learning. Our findings underscore the effectiveness
of reinforcement learning for training multi-turn, vision-language GUI agents
capable of managing complex real-world UI interactions. Codes and
models:https://github.com/dvlab-research/ARPO.git.


## Align-GRAG Reasoning-Guided Dual Alignment for Graph Retrieval-Augmented Generation

>Authors: Derong Xu, Pengyue Jia, Xiaopeng Li, Yingyi Zhang, Maolin Wang, Qidong Liu, Xiangyu Zhao, Yichao Wang, Huifeng Guo, Ruiming Tang, Enhong Chen, Tong Xu

>2025-05-22

> http://arxiv.org/abs/2505.16237v1

Large language models (LLMs) have demonstrated remarkable capabilities, but
still struggle with issues like hallucinations and outdated information.
Retrieval-augmented generation (RAG) addresses these issues by grounding LLM
outputs in external knowledge with an Information Retrieval (IR) system.
Building on this foundation, graph-based RAG systems go a step further by
retrieving subgraphs, which preserve the relationships between knowledge
entities and provide more comprehensive context. However, graph RAG faces two
challenges: (1) Retrieving relevant information introduces irrelevant nodes
(especially in dense graph databases, where retrieval usually extends to
adjacent nodes), and leads to overly lengthy inputs that hinder efficiency; (2)
The representation gap between graph and language during generation with LLMs
limits the ability to fully leverage graph structures for enhanced
understanding. To address these limitations, we propose Align-GRAG, a novel
reasoning-guided dual alignment framework in post-retrieval phrase. It first
formulates a subgraph by retrieving nodes and edges. Then an Aligner is
proposed to jointly optimizes a graph encoder with LLM-summarized reasoning. It
achieves dual alignment of graph node and representation by leveraging KL
divergence loss and contrastive loss, facilitating efficient **pruning** of
irrelevant knowledge and establishing a unified semantic space. The Generator
integrates the aligned graph data with LLM to produce coherent and accurate
answers. Experiments on GraphQA benchmark across three tasks (including common
sense reasoning, scene graph understanding, and knowledge graph reasoning)
validate the effectiveness of our method. The code will be available upon
accepted.


## NQKV A KV Cache Quantization Scheme Based on Normal Distribution Characteristics

>Authors: Zhihang Cai, Xingjun Zhang, Zhendong Tan, Zheng Wei

>2025-05-22

> http://arxiv.org/abs/2505.16210v1

Large Language Models (LLMs) have demonstrated remarkable proficiency across
a wide range of tasks. However, LLMs often require larger batch sizes to
enhance throughput or longer context lengths to meet task demands, which
significantly increases the memory resource consumption of the Key-Value (**KV**)
cache during inference, becoming a major bottleneck in LLM deployment. To
address this issue, **quantization** is a common and straightforward approach.
Currently, **quantization** methods for activations are limited to 8-bit, and
**quantization** to even lower bits can lead to substantial accuracy drops. To
further save space by quantizing the **KV** cache to even lower bits, we analyzed
the element distribution of the **KV** cache and designed the NQ**KV** algorithm. Since
the elements within each block of the **KV** cache follow a normal distribution,
NQ**KV** employs per-block quantile **quantization** to achieve
information-theoretically optimal **quantization** error. Without significantly
compromising model output quality, NQ**KV** enables the OPT model to perform
inference with an 2x larger batch size or a 4x longer context length, and it
improves throughput by 9.3x compared to when the **KV** cache is not used.


## SAE-SSV Supervised Steering in Sparse Representation Spaces for Reliable Control of Language Models

>Authors: Zirui He, Mingyu Jin, Bo Shen, Ali Payani, Yongfeng Zhang, Mengnan Du

>2025-05-22

> http://arxiv.org/abs/2505.16188v1

Large language models (LLMs) have demonstrated impressive capabilities in
natural language understanding and generation, but controlling their behavior
reliably remains challenging, especially in open-ended generation settings.
This paper introduces a novel supervised steering approach that operates in
**sparse**, interpretable representation spaces. We employ **sparse** autoencoders
(SAEs)to obtain **sparse** latent representations that aim to disentangle semantic
attributes from model activations. Then we train linear classifiers to identify
a small subspace of task-relevant dimensions in latent representations.
Finally, we learn supervised steering vectors constrained to this subspace,
optimized to align with target behaviors. Experiments across sentiment,
truthfulness, and politics polarity steering tasks with multiple LLMs
demonstrate that our supervised steering vectors achieve higher success rates
with minimal degradation in generation quality compared to existing methods.
Further analysis reveals that a notably small subspace is sufficient for
effective steering, enabling more targeted and interpretable interventions.


## Generative Latent Coding for Ultra-Low Bitrate Image and Video Compression

>Authors: Linfeng Qi, Zhaoyang Jia, Jiahao Li, Bin Li, Houqiang Li, Yan Lu

>2025-05-22

> http://arxiv.org/abs/2505.16177v1

Most existing approaches for image and video compression perform transform
coding in the pixel space to reduce redundancy. However, due to the
misalignment between the pixel-space distortion and human perception, such
schemes often face the difficulties in achieving both high-realism and
high-fidelity at ultra-low bitrate. To solve this problem, we propose
\textbf{G}enerative \textbf{L}atent \textbf{C}oding (\textbf{GLC}) models for
image and video compression, termed GLC-image and GLC-Video. The transform
coding of GLC is conducted in the latent space of a generative vector-**quantize**d
variational auto-encoder (VQ-VAE). Compared to the pixel-space, such a latent
space offers greater **sparsity**, richer semantics and better alignment with human
perception, and show its advantages in achieving high-realism and high-fidelity
compression. To further enhance performance, we improve the hyper prior by
introducing a spatial categorical hyper module in GLC-image and a
spatio-temporal categorical hyper module in GLC-video. Additionally, the
code-prediction-based loss function is proposed to enhance the semantic
consistency. Experiments demonstrate that our scheme shows high visual quality
at ultra-low bitrate for both image and video compression. For image
compression, GLC-image achieves an impressive bitrate of less than $0.04$ bpp,
achieving the same FID as previous SOTA model MS-ILLM while using $45\%$ fewer
bitrate on the CLIC 2020 test set. For video compression, GLC-video achieves
65.3\% bitrate saving over PLVC in terms of DISTS.


## QuickVideo Real-Time Long Video Understanding with System Algorithm Co-Design

>Authors: Benjamin Schneider, Dongfu Jiang, Chao Du, Tianyu Pang, Wenhu Chen

>2025-05-22

> http://arxiv.org/abs/2505.16175v1

Long-video understanding has emerged as a crucial capability in real-world
applications such as video surveillance, meeting summarization, educational
lecture analysis, and sports broadcasting. However, it remains computationally
prohibitive for VideoLLMs, primarily due to two bottlenecks: 1) sequential
video decoding, the process of converting the raw bit stream to RGB frames can
take up to a minute for hour-long video inputs, and 2) costly prefilling of up
to several million tokens for LLM inference, resulting in high latency and
memory use. To address these challenges, we propose QuickVideo, a
system-algorithm co-design that substantially accelerates long-video
understanding to support real-time downstream applications. It comprises three
key innovations: QuickDecoder, a parallelized CPU-based video decoder that
achieves 2-3 times speedup by splitting videos into keyframe-aligned intervals
processed concurrently; QuickPrefill, a memory-efficient prefilling method
using **KV**-cache **pruning** to support more frames with less GPU memory; and an
overlapping scheme that overlaps CPU video decoding with GPU inference.
Together, these components infernece time reduce by a minute on long video
inputs, enabling scalable, high-quality video understanding even on limited
hardware. Experiments show that QuickVideo generalizes across durations and
sampling rates, making long video processing feasible in practice.


## KNN-SSD Enabling Dynamic Self-Speculative Decoding via Nearest Neighbor Layer Set Optimization

>Authors: Mingbo Song, Heming Xia, Jun Zhang, Chak Tou Leong, Qiancheng Xu, Wenjie Li, Sujian Li

>2025-05-22

> http://arxiv.org/abs/2505.16162v1

Speculative Decoding (SD) has emerged as a widely used paradigm to accelerate
the inference of large language models (LLMs) without compromising generation
quality. It works by efficiently drafting multiple tokens using a compact model
and then verifying them in parallel using the target LLM. Notably,
Self-Speculative Decoding proposes skipping certain layers to construct the
draft model, which eliminates the need for additional parameters or training.
Despite its strengths, we observe in this work that drafting with layer
skipping exhibits significant sensitivity to domain shifts, leading to a
substantial drop in **acceleration** performance. To enhance the domain
generalizability of this paradigm, we introduce KNN-SSD, an algorithm that
leverages K-Nearest Neighbor (KNN) search to match different skipped layers
with various domain inputs. We evaluated our algorithm in various models and
multiple tasks, observing that its application leads to 1.3x-1.6x speedup in
LLM inference.


## Breaking Complexity Barriers High-Resolution Image Restoration with Rank Enhanced Linear Attention

>Authors: Yuang Ai, Huaibo Huang, Tao Wu, Qihang Fan, Ran He

>2025-05-22

> http://arxiv.org/abs/2505.16157v1

Transformer-based models have made remarkable progress in image restoration
(IR) tasks. However, the quadratic complexity of self-attention in Transformer
hinders its applicability to high-resolution images. Existing methods mitigate
this issue with **sparse** or window-based attention, yet inherently limit global
context modeling. Linear attention, a variant of softmax attention,
demonstrates promise in global context modeling while maintaining linear
complexity, offering a potential solution to the above challenge. Despite its
efficiency benefits, vanilla linear attention suffers from a significant
performance drop in IR, largely due to the low-rank nature of its attention
map. To counter this, we propose Rank Enhanced Linear Attention (RELA), a
simple yet effective method that enriches feature representations by
integrating a lightweight depthwise convolution. Building upon RELA, we propose
an efficient and effective image restoration Transformer, named LAformer.
LAformer achieves effective global perception by integrating linear attention
and channel attention, while also enhancing local fitting capabilities through
a convolutional gated feed-forward network. Notably, LAformer eliminates
hardware-inefficient operations such as softmax and window shifting, enabling
efficient processing of high-resolution images. Extensive experiments across 7
IR tasks and 21 benchmarks demonstrate that LAformer outperforms SOTA methods
and offers significant computational advantages.


## Hierarchical Safety Realignment Lightweight Restoration of Safety in Pruned Large Vision-Language Models

>Authors: Yue Li, Xin Yi, Dongsheng Shi, Gerard de Melo, Xiaoling Wang, Linlin Wang

>2025-05-22

> http://arxiv.org/abs/2505.16104v1

With the increasing size of Large Vision-Language Models (LVLMs), network
**pruning** techniques aimed at compressing models for deployment in
resource-constrained environments have garnered significant attention. However,
we observe that **pruning** often leads to a degradation in safety performance. To
address this issue, we present a novel and lightweight approach, termed
Hierarchical Safety Realignment (HSR). HSR operates by first quantifying the
contribution of each attention head to safety, identifying the most critical
ones, and then selectively restoring neurons directly within these attention
heads that play a pivotal role in maintaining safety. This process
hierarchically realigns the safety of pruned LVLMs, progressing from the
attention head level to the neuron level. We validate HSR across various models
and **pruning** strategies, consistently achieving notable improvements in safety
performance. To our knowledge, this is the first work explicitly focused on
restoring safety in LVLMs post-**pruning**.


## Not All Models Suit Expert Offloading On Local Routing Consistency of Mixture-of-Expert Models

>Authors: Jingcong Liang, Siyuan Wang, Miren Tian, Yitong Li, Duyu Tang, Zhongyu Wei

>2025-05-21

> http://arxiv.org/abs/2505.16056v1

Mixture-of-Experts (MoE) enables efficient scaling of large language models
(LLMs) with **sparse**ly activated experts during inference. To effectively deploy
large MoE models on memory-constrained devices, many systems introduce *expert
offloading* that caches a subset of experts in fast memory, leaving others on
slow memory to run on CPU or load on demand. While some research has exploited
the locality of expert activations, where consecutive tokens activate similar
experts, the degree of this **local routing consistency** varies across models
and remains understudied. In this paper, we propose two metrics to measure
local routing consistency of MoE models: (1) **Segment Routing Best Performance
(SRP)**, which evaluates how well a fixed group of experts can cover the needs
of a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which
measures the optimal segment-level cache hit rate under a given cache size
limit. We analyzed 20 MoE LLMs with diverse sizes and architectures and found
that models that apply MoE on every layer and do not use shared experts exhibit
the highest local routing consistency. We further showed that
domain-specialized experts contribute more to routing consistency than
vocabulary-specialized ones, and that most models can balance between cache
effectiveness and efficiency with cache sizes approximately 2x the active
experts. These findings pave the way for memory-efficient MoE design and
deployment without compromising inference speed. We publish the code for
replicating experiments at https://github.com/ljcleo/moe-lrc .


## Interpretability Illusions with Sparse Autoencoders Evaluating Robustness of Concept Representations

>Authors: Aaron J. Li, Suraj Srinivas, Usha Bhalla, Himabindu Lakkaraju

>2025-05-21

> http://arxiv.org/abs/2505.16004v1

Sparse autoencoders (SAEs) are commonly used to interpret the internal
activations of large language models (LLMs) by mapping them to
human-interpretable concept representations. While existing evaluations of SAEs
focus on metrics such as the reconstruction-**sparsity** tradeoff, human
(auto-)interpretability, and feature disentanglement, they overlook a critical
aspect: the robustness of concept representations to input perturbations. We
argue that robustness must be a fundamental consideration for concept
representations, reflecting the fidelity of concept labeling. To this end, we
formulate robustness quantification as input-space optimization problems and
develop a comprehensive evaluation framework featuring realistic scenarios in
which adversarial perturbations are crafted to manipulate SAE representations.
Empirically, we find that tiny adversarial input perturbations can effectively
manipulate concept-based interpretations in most scenarios without notably
affecting the outputs of the base LLMs themselves. Overall, our results suggest
that SAE concept representations are fragile and may be ill-suited for
applications in model monitoring and oversight.


## Analyzing Hierarchical Structure in Vision Models with Sparse Autoencoders

>Authors: Matthew Lyle Olson, Musashi Hinck, Neale Ratzlaff, Changbai Li, Phillip Howard, Vasudev Lal, Shao-Yen Tseng

>2025-05-21

> http://arxiv.org/abs/2505.15970v1

The ImageNet hierarchy provides a structured taxonomy of object categories,
offering a valuable lens through which to analyze the representations learned
by deep vision models. In this work, we conduct a comprehensive analysis of how
vision models encode the ImageNet hierarchy, leveraging Sparse Autoencoders
(SAEs) to probe their internal representations. SAEs have been widely used as
an explanation tool for large language models (LLMs), where they enable the
discovery of semantically meaningful features. Here, we extend their use to
vision models to investigate whether learned representations align with the
ontological structure defined by the ImageNet taxonomy. Our results show that
SAEs uncover hierarchical relationships in model activations, revealing an
implicit encoding of taxonomic structure. We analyze the consistency of these
representations across different layers of the popular vision foundation model
DINOv2 and provide insights into how deep vision models internalize
hierarchical category information by increasing information in the class token
through each layer. Our study establishes a framework for systematic
hierarchical analysis of vision model representations and highlights the
potential of SAEs as a tool for probing semantic structure in deep networks.


## Directional Sparsity Based Statistical Channel Estimation for 6D Movable Antenna Communications

>Authors: Xiaodan Shao, Rui Zhang, Jihong Park, Tony Q. S. Quek, Robert Schober, Xuemin Shen

>2025-05-21

> http://arxiv.org/abs/2505.15947v1

Six-dimensional movable antenna (6DMA) is an innovative and transformative
technology to improve wireless network capacity by adjusting the 3D positions
and 3D rotations of antennas/surfaces (sub-arrays) based on the channel spatial
distribution. For optimization of the antenna positions and rotations, the
acquisition of statistical channel state information (CSI) is essential for
6DMA systems. In this paper, we unveil for the first time a new
\textbf{\textit{directional **sparsity**}} property of the 6DMA channels between
the base station (BS) and the distributed users, where each user has
significant channel gains only with a (small) subset of 6DMA position-rotation
pairs, which can receive direct/reflected signals from the user. By exploiting
this property, a covariance-based algorithm is proposed for estimating the
statistical CSI in terms of the average channel power at a small number of 6DMA
positions and rotations. Based on such limited channel power estimation, the
average channel powers for all possible 6DMA positions and rotations in the BS
movement region are reconstructed by further estimating the multi-path average
power and direction-of-arrival (DOA) vectors of all users. Simulation results
show that the proposed directional **sparsity**-based algorithm can achieve higher
channel power estimation accuracy than existing benchmark schemes, while
requiring a lower pilot overhead.


## Bloch oscillation with a diatomic tight-binding model on quantum computers

>Authors: Peng Guo, Jaime Park, Frank X. Lee

>2025-05-21

> http://arxiv.org/abs/2505.15945v1

We aim to explore a more efficient way to simulate few-body dynamics on
quantum computers. Instead of mapping the second **quantization** of the system
Hamiltonian to qubit Pauli gates representation via the Jordan-Wigner
transform, we propose to use the few-body Hamiltonian matrix under the
statevector basis representation which is more economical on the required
number of quantum registers. For a single-particle excitation state on a
one-dimensional chain, $\Gamma$ qubits can simulate $N=2^\Gamma$ number of
sites, in comparison to $N$ qubits for $N$ sites via the Jordan-Wigner
approach. A two-band diatomic tight-binding model is used to demonstrate the
effectiveness of the statevector basis representation. Both one-particle and
two-particle quantum circuits are constructed and some numerical tests on IBM
hardware are presented.


## Is (Selective) Round-To-Nearest Quantization All You Need?

>Authors: Alex Kogan

>2025-05-21

> http://arxiv.org/abs/2505.15909v1

Quantization became a necessary tool for serving ever-increasing Large
Language Models (LLMs). RTN (Round-to-Nearest) is perhaps the simplest
**quantization** technique that has been around well before LLMs surged to the
forefront of machine learning (ML) research. Yet, it has been largely dismissed
by recent and more advanced **quantization** methods that claim superiority over
RTN in nearly every aspect of performance. This work aims to dispel this
established point of view, showing that RTN is not only much cheaper to apply,
but also its token generation throughput can be better than and accuracy can be
similar to more advanced alternatives. In particular, we discuss our
implementation of RTN based on the recent Marlin kernels and demonstrate how
the accuracy of RTN can be gradually improved by selectively increasing the
data precision format of certain model layers and modules. Based on our
results, we argue that RTN presents a viable and practical choice for
quantizing LLMs.


## STAR-R1 Spacial TrAnsformation Reasoning by Reinforcing Multimodal LLMs

>Authors: Zongzhao Li, Zongyang Ma, Mingze Li, Songyou Li, Yu Rong, Tingyang Xu, Ziqi Zhang, Deli Zhao, Wenbing Huang

>2025-05-21

> http://arxiv.org/abs/2505.15804v1

Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities across diverse tasks, yet they lag significantly behind humans in
spatial reasoning. We investigate this gap through Transformation-Driven Visual
Reasoning (TVR), a challenging task requiring identification of object
transformations across images under varying viewpoints. While traditional
Supervised Fine-Tuning (SFT) fails to generate coherent reasoning paths in
cross-view settings, **sparse**-reward Reinforcement Learning (RL) suffers from
inefficient exploration and slow convergence. To address these limitations, we
propose STAR-R1, a novel framework that integrates a single-stage RL paradigm
with a fine-grained reward mechanism tailored for TVR. Specifically, STAR-R1
rewards partial correctness while penalizing excessive enumeration and passive
inaction, enabling efficient exploration and precise reasoning. Comprehensive
evaluations demonstrate that STAR-R1 achieves state-of-the-art performance
across all 11 metrics, outperforming SFT by 23% in cross-view scenarios.
Further analysis reveals STAR-R1's anthropomorphic behavior and highlights its
unique ability to compare all objects for improving spatial reasoning. Our work
provides critical insights in advancing the research of MLLMs and reasoning
models. The codes, model weights, and data will be publicly available at
https://github.com/zongzhao23/STAR-R1.


## Adaptive Estimation and Learning under Temporal Distribution Shift

>Authors: Dheeraj Baby, Yifei Tang, Hieu Duy Nguyen, Yu-Xiang Wang, Rohit Pyati

>2025-05-21

> http://arxiv.org/abs/2505.15803v1

In this paper, we study the problem of estimation and learning under temporal
distribution shift. Consider an observation sequence of length $n$, which is a
noisy realization of a time-varying groundtruth sequence. Our focus is to
develop methods to estimate the groundtruth at the final time-step while
providing sharp point-wise estimation error rates. We show that, without prior
knowledge on the level of temporal shift, a wavelet soft-thresholding estimator
provides an optimal estimation error bound for the groundtruth. Our proposed
estimation method generalizes existing researches Mazzetto and Upfal (2023) by
establishing a connection between the sequence's non-stationarity level and the
**sparsity** in the wavelet-transformed domain. Our theoretical findings are
validated by numerical experiments. Additionally, we applied the estimator to
derive **sparsity**-aware excess risk bounds for binary classification under
distribution shift and to develop computationally efficient training
objectives. As a final contribution, we draw parallels between our results and
the classical signal processing problem of total-variation denoising (Mammen
and van de Geer,1997; Tibshirani, 2014), uncovering novel optimal algorithms
for such task.


## dKV-Cache The Cache for Diffusion Language Models

>Authors: Xinyin Ma, Runpeng Yu, Gongfan Fang, Xinchao Wang

>2025-05-21

> http://arxiv.org/abs/2505.15781v1

Diffusion Language Models (DLMs) have been seen as a promising competitor for
autoregressive language models. However, diffusion language models have long
been constrained by slow inference. A core challenge is that their
non-autoregressive architecture and bidirectional attention preclude the
key-value cache that accelerates decoding. We address this bottleneck by
proposing a **KV**-cache-like mechanism, delayed **KV**-Cache, for the denoising
process of DLMs. Our approach is motivated by the observation that different
tokens have distinct representation dynamics throughout the diffusion process.
Accordingly, we propose a delayed and conditioned caching strategy for key and
value states. We design two complementary variants to cache key and value
step-by-step: (1) d**KV**-Cache-Decode, which provides almost lossless
**acceleration**, and even improves performance on long sequences, suggesting that
existing DLMs may under-utilise contextual information during inference. (2)
d**KV**-Cache-Greedy, which has aggressive caching with reduced lifespan, achieving
higher speed-ups with quadratic time complexity at the cost of some performance
degradation. d**KV**-Cache, in final, achieves from 2-10x speedup in inference,
largely narrowing the gap between ARs and DLMs. We evaluate our d**KV**-Cache on
several benchmarks, delivering **acceleration** across general language
understanding, mathematical, and code-generation benchmarks. Experiments
demonstrate that cache can also be used in DLMs, even in a training-free manner
from current DLMs.


## ConvSearch-R1 Enhancing Query Reformulation for Conversational Search with Reasoning via Reinforcement Learning

>Authors: Changtai Zhu, Siyin Wang, Ruijun Feng, Kai Song, Xipeng Qiu

>2025-05-21

> http://arxiv.org/abs/2505.15776v1

Conversational search systems require effective handling of context-dependent
queries that often contain ambiguity, omission, and coreference. Conversational
Query Reformulation (CQR) addresses this challenge by transforming these
queries into self-contained forms suitable for off-the-shelf retrievers.
However, existing CQR approaches suffer from two critical constraints: high
dependency on costly external supervision from human annotations or large
language models, and insufficient alignment between the rewriting model and
downstream retrievers. We present ConvSearch-R1, the first self-driven
framework that completely eliminates dependency on external rewrite supervision
by leveraging reinforcement learning to optimize reformulation directly through
retrieval signals. Our novel two-stage approach combines Self-Driven Policy
Warm-Up to address the cold-start problem through retrieval-guided
self-distillation, followed by Retrieval-Guided Reinforcement Learning with a
specially designed rank-incentive reward shaping mechanism that addresses the
**sparsity** issue in conventional retrieval metrics. Extensive experiments on
TopiOCQA and QReCC datasets demonstrate that ConvSearch-R1 significantly
outperforms previous state-of-the-art methods, achieving over 10% improvement
on the challenging TopiOCQA dataset while using smaller 3B parameter models
without any external supervision.


## VocalBench Benchmarking the Vocal Conversational Abilities for Speech Interaction Models

>Authors: Heyang Liu, Yuhao Wang, Ziyang Cheng, Ronghua Wu, Qunshan Gu, Yanfeng Wang, Yu Wang

>2025-05-21

> http://arxiv.org/abs/2505.15727v1

The rapid advancement of large language models (LLMs) has accelerated the
development of multi-modal models capable of vocal communication. Unlike
text-based interactions, speech conveys rich and diverse information, including
semantic content, acoustic variations, paralanguage cues, and environmental
context. However, existing evaluations of speech interaction models
predominantly focus on the quality of their textual responses, often
overlooking critical aspects of vocal performance and lacking benchmarks with
vocal-specific test instances. To address this gap, we propose VocalBench, a
comprehensive benchmark designed to evaluate speech interaction models'
capabilities in vocal communication. VocalBench comprises 9,400 carefully
curated instances across four key dimensions: semantic quality, acoustic
performance, conversational abilities, and robustness. It covers 16 fundamental
skills essential for effective vocal interaction. Experimental results reveal
significant variability in current model capabilities, each exhibiting distinct
strengths and weaknesses, and provide valuable insights to guide future
research in speech-based interaction systems. Code and evaluation instances are
available at https://github.com/SJTU-OmniAgent/VocalBench.


## ThinkLess A Training-Free Inference-Efficient Method for Reducing Reasoning Redundancy

>Authors: Gengyang Li, Yifeng Gao, Yuming Li, Yunfang Wu

>2025-05-21

> http://arxiv.org/abs/2505.15684v1

While Chain-of-Thought (CoT) prompting improves reasoning in large language
models (LLMs), the excessive length of reasoning tokens increases latency and
**KV** cache memory usage, and may even truncate final answers under context
limits. We propose ThinkLess, an inference-efficient framework that terminates
reasoning generation early and maintains output quality without modifying the
model. Atttention analysis reveals that answer tokens focus minimally on
earlier reasoning steps and primarily attend to the reasoning terminator token,
due to information migration under causal masking. Building on this insight,
ThinkLess inserts the terminator token at earlier positions to skip redundant
reasoning while preserving the underlying knowledge transfer. To prevent format
discruption casued by early termination, ThinkLess employs a lightweight
post-regulation mechanism, relying on the model's natural instruction-following
ability to produce well-structured answers. Without fine-tuning or auxiliary
data, ThinkLess achieves comparable accuracy to full-length CoT decoding while
greatly reducing decoding time and memory consumption.


## A Federated Splitting Framework for LLMs Security, Efficiency, and Adaptability

>Authors: Zishuai Zhang, Hainan Zhang, Jiaying Zheng, Ziwei Wang, Yongxin Tong, Jin Dong, Zhiming Zheng

>2025-05-21

> http://arxiv.org/abs/2505.15683v1

Private data is typically larger and of higher quality than public data,
offering great potential to improve LLM. However, its scattered distribution
across data silos and the high computational demands of LLMs limit their
deployment in federated environments. To address this, the transformer-based
split learning model has emerged, offloading most model parameters to the
server while retaining only the embedding and output layers on clients to
ensure privacy. However, it still faces significant challenges in security,
efficiency, and adaptability: 1) embedding gradients are vulnerable to attacks,
leading to reverse engineering of private data; 2) the autoregressive nature of
LLMs means that federated split learning can only train and infer sequentially,
causing high communication overhead; 3) fixed partition points lack
adaptability to downstream tasks. In this paper, we introduce FL-LLaMA, a
secure, efficient, and adaptive federated split framework based on LLaMA2.
First, we place some input and output blocks on the local client and inject
Gaussian noise into forward-pass hidden states, enabling secure end-to-end
propagation. Second, we employ client-batch and server-hierarchical strategies
to achieve parallel training, along with attention-mask compression and **KV**
cache mechanisms to accelerate inference, reducing communication costs
effectively. Third, we allow users to dynamically adjust the partition points
for input/output blocks based on specific task requirements and hardware
limitations. Experiments on NLU, summarization and conversational QA tasks show
that FL-LLaMA maintains performance comparable to centralized LLaMA2, and
achieves up to 2x train speedups and 8x inference speedups. Further analysis of
privacy attacks and different partition points also demonstrates the
effectiveness of FL-LLaMA in security and adaptability.


## Feature Extraction and Steering for Enhanced Chain-of-Thought Reasoning in Language Models

>Authors: Zihao Li, Xu Wang, Yuzhe Yang, Ziyu Yao, Haoyi Xiong, Mengnan Du

>2025-05-21

> http://arxiv.org/abs/2505.15634v1

Large Language Models (LLMs) demonstrate the ability to solve reasoning and
mathematical problems using the Chain-of-Thought (CoT) technique. Expanding CoT
length, as seen in models such as DeepSeek-R1, significantly enhances this
reasoning for complex problems, but requires costly and high-quality long CoT
data and fine-tuning. This work, inspired by the deep thinking paradigm of
DeepSeek-R1, utilizes a steering technique to enhance the reasoning ability of
an LLM without external datasets. Our method first employs Sparse Autoencoders
(SAEs) to extract interpretable features from vanilla CoT. These features are
then used to steer the LLM's internal states during generation. Recognizing
that many LLMs do not have corresponding pre-trained SAEs, we further introduce
a novel SAE-free steering algorithm, which directly computes steering
directions from the residual activations of an LLM, obviating the need for an
explicit SAE. Experimental results demonstrate that both our SAE-based and
subsequent SAE-free steering algorithms significantly enhance the reasoning
capabilities of LLMs.


## Guidelines for the Quality Assessment of Energy-Aware NAS Benchmarks

>Authors: Nick Kocher, Christian Wassermann, Leona Hennig, Jonas Seng, Holger Hoos, Kristian Kersting, Marius Lindauer, Matthias Müller

>2025-05-21

> http://arxiv.org/abs/2505.15631v1

Neural Architecture Search (NAS) accelerates progress in deep learning
through systematic refinement of model architectures. The downside is
increasingly large energy consumption during the search process.
Surrogate-based benchmarking mitigates the cost of full training by querying a
pre-trained surrogate to obtain an estimate for the quality of the model.
Specifically, energy-aware benchmarking aims to make it possible for NAS to
favourably trade off model energy consumption against accuracy. Towards this
end, we propose three design principles for such energy-aware benchmarks: (i)
reliable power measurements, (ii) a wide range of GPU usage, and (iii) holistic
cost reporting. We analyse EA-HAS-Bench based on these principles and find that
the choice of GPU measurement API has a large impact on the quality of results.
Using the Nvidia System Management Interface (SMI) on top of its underlying
library influences the sampling rate during the initial data collection,
returning faulty low-power estimations. This results in poor correlation with
accurate measurements obtained from an external power meter. With this study,
we bring to attention several key considerations when performing energy-aware
surrogate-based benchmarking and derive first guidelines that can help design
novel benchmarks. We show a narrow usage range of the four GPUs attached to our
device, ranging from 146 W to 305 W in a single-GPU setting, and narrowing down
even further when using all four GPUs. To improve holistic energy reporting, we
propose calibration experiments over assumptions made in popular tools, such as
Code Carbon, thus achieving reductions in the maximum inaccuracy from 10.3 % to
8.9 % without and to 6.6 % with prior estimation of the expected load on the
device.


## Mechanistic Insights into Grokking from the Embedding Layer

>Authors: H. V. AlquBoj, Hilal AlQuabeh, Velibor Bojkovic, Munachiso Nwadike, Kentaro Inui

>2025-05-21

> http://arxiv.org/abs/2505.15624v1

Grokking, a delayed generalization in neural networks after perfect training
performance, has been observed in Transformers and MLPs, but the components
driving it remain underexplored. We show that embeddings are central to
grokking: introducing them into MLPs induces delayed generalization in modular
arithmetic tasks, whereas MLPs without embeddings can generalize immediately.
Our analysis identifies two key mechanisms: (1) Embedding update dynamics,
where rare tokens stagnate due to **sparse** gradient updates and weight decay, and
(2) Bilinear coupling, where the interaction between embeddings and downstream
weights introduces saddle points and increases sensitivity to initialization.
To confirm these mechanisms, we investigate frequency-aware sampling, which
balances token updates by minimizing gradient variance, and embedding-specific
learning rates, derived from the asymmetric curvature of the bilinear loss
landscape. We prove that an adaptive learning rate ratio,
\(\frac{\eta_E}{\eta_W} \propto \frac{\sigma_{\max}(E)}{\sigma_{\max}(W)} \cdot
\frac{f_W}{f_E}\), mitigates bilinear coupling effects, accelerating
convergence. Our methods not only improve grokking dynamics but also extend to
broader challenges in Transformer optimization, where bilinear interactions
hinder efficient training.


## Temporal Spectrum Cartography in Low-Altitude Economy Networks A Generative AI Framework with Multi-Agent Learning

>Authors: Changyuan Zhao, Ruichen Zhang, Jiacheng Wang, Dusit Niyato, Geng Sun, Hongyang Du, Zan Li, Abbas Jamalipour, Dong In Kim

>2025-05-21

> http://arxiv.org/abs/2505.15571v1

This paper introduces a two-stage generative AI (GenAI) framework tailored
for temporal spectrum cartography in low-altitude economy networks (LAENets).
LAENets, characterized by diverse aerial devices such as UAVs, rely heavily on
wireless communication technologies while facing challenges, including spectrum
congestion and dynamic environmental interference. Traditional spectrum
cartography methods have limitations in handling the temporal and spatial
complexities inherent to these networks. Addressing these challenges, the
proposed framework first employs a Reconstructive Masked Autoencoder (RecMAE)
capable of accurately reconstructing spectrum maps from **sparse** and temporally
varying sensor data using a novel dual-mask mechanism. This approach
significantly enhances the precision of reconstructed radio frequency (RF)
power maps. In the second stage, the Multi-agent Diffusion Policy (MADP) method
integrates diffusion-based reinforcement learning to optimize the trajectories
of dynamic UAV sensors. By leveraging temporal-attention encoding, this method
effectively manages spatial exploration and exploitation to minimize cumulative
reconstruction errors. Extensive numerical experiments validate that this
integrated GenAI framework outperforms traditional interpolation methods and
deep learning baselines by achieving 57.35% and 88.68% reconstruction error
reduction, respectively. The proposed trajectory planner substantially improves
spectrum map accuracy, reconstruction stability, and sensor deployment
efficiency in dynamically evolving low-altitude environments.


## Plasma-state metasurfaces for ultra-intensive field manipulation

>Authors: Zi-Yu Chen, Hao Xu, Jiao Jia, Yanjie Chen, Siyu Chen, Yan Zhang, Mingxuan Wei, Minghao Ma, Runze Li, Fan Yang, Mo Li, Guangwei Lu, Weijun Zhou, Hanmi Mou, Zhuofan Zhang, Zhida Yang, Jian Gao, Feng liu, Boyuan Li, Min Chen, Liming Chen, Yongtian Wang, Lingling Huang, Wenchao Yan, Shuang Zhang, Jie Zhang

>2025-05-21

> http://arxiv.org/abs/2505.15567v1

High-power lasers offer ultrahigh intensities for plasma interactions, but
they lack advanced techniques to control the properties of the fields, because
no optical elements could withstand their high intensities. The vibrant field
of metasurfaces has transformed modern optics by enabling unprecedented control
over light at subwavelength through deliberate design. However, metasurfaces
have traditionally been limited to solid-state materials and low light
intensities. Extending the sophisticated capabilities of metasurfaces from
solids into the plasma realm would open new horizons for high-field science.
Here, we experimentally demonstrate plasma-state metasurfaces (PSMs) through
the photonic spin Hall effect and stable-propagating vortex beam generation
irradiated by intense light. Time-resolved pump-probe measurements reveal that
the functionality of PSMs can persist for several picoseconds, making them
suitable for controlling ultra-intense femtosecond lasers, even in
state-of-the-art multi-petawatt systems. Harnessing the powerful toolkit of
metasurfaces, this approach holds the promise to revolutionize our ability to
manipulate the amplitude, phase, polarization, and wavefront of high-power
lasers during their pulse duration. It also opens new possibilities for
innovative applications in laser-plasma interactions such as compact particle
**acceleration** and novel radiation sources.


## Robo-DM Data Management For Large Robot Datasets

>Authors: Kaiyuan Chen, Letian Fu, David Huang, Yanxiang Zhang, Lawrence Yunliang Chen, Huang Huang, Kush Hari, Ashwin Balakrishna, Ted Xiao, Pannag R Sanketi, John Kubiatowicz, Ken Goldberg

>2025-05-21

> http://arxiv.org/abs/2505.15558v1

Recent results suggest that very large datasets of teleoperated robot
demonstrations can be used to train transformer-based models that have the
potential to generalize to new scenes, robots, and tasks. However, curating,
distributing, and loading large datasets of robot trajectories, which typically
consist of video, textual, and numerical modalities - including streams from
multiple cameras - remains challenging. We propose Robo-DM, an efficient
open-source cloud-based data management toolkit for collecting, sharing, and
learning with robot data. With Robo-DM, robot datasets are stored in a
self-contained format with Extensible Binary Meta Language (EBML). Robo-DM can
significantly reduce the size of robot trajectory data, transfer costs, and
data load time during training. Compared to the RLDS format used in OXE
datasets, Robo-DM's compression saves space by up to 70x (lossy) and 3.5x
(lossless). Robo-DM also accelerates data retrieval by load-balancing video
decoding with memory-mapped decoding caches. Compared to LeRobot, a framework
that also uses lossy video compression, Robo-DM is up to 50x faster when
decoding sequentially. We physically evaluate a model trained by Robo-DM with
lossy compression, a pick-and-place task, and In-Context Robot Transformer.
Robo-DM uses 75x compression of the original dataset and does not suffer
reduction in downstream task accuracy.


## Evaluate Bias without Manual Test Sets A Concept Representation Perspective for LLMs

>Authors: Lang Gao, Kaiyang Wan, Wei Liu, Chenxi Wang, Zirui Song, Zixiang Xu, Yanbo Wang, Veselin Stoyanov, Xiuying Chen

>2025-05-21

> http://arxiv.org/abs/2505.15524v1

Bias in Large Language Models (LLMs) significantly undermines their
reliability and fairness. We focus on a common form of bias: when two reference
concepts in the model's concept space, such as sentiment polarities (e.g.,
"positive" and "negative"), are asymmetrically correlated with a third, target
concept, such as a reviewing aspect, the model exhibits unintended bias. For
instance, the understanding of "food" should not skew toward any particular
sentiment. Existing bias evaluation methods assess behavioral differences of
LLMs by constructing labeled data for different social groups and measuring
model responses across them, a process that requires substantial human effort
and captures only a limited set of social concepts. To overcome these
limitations, we propose BiasLens, a test-set-free bias analysis framework based
on the structure of the model's vector space. BiasLens combines Concept
Activation Vectors (CAVs) with Sparse Autoencoders (SAEs) to extract
interpretable concept representations, and quantifies bias by measuring the
variation in representational similarity between the target concept and each of
the reference concepts. Even without labeled data, BiasLens shows strong
agreement with traditional bias evaluation metrics (Spearman correlation r >
0.85). Moreover, BiasLens reveals forms of bias that are difficult to detect
using existing methods. For example, in simulated clinical scenarios, a
patient's insurance status can cause the LLM to produce biased diagnostic
assessments. Overall, BiasLens offers a scalable, interpretable, and efficient
paradigm for bias discovery, paving the way for improving fairness and
transparency in LLMs.


## Hunyuan-TurboS Advancing Large Language Models through Mamba-Transformer Synergy and Adaptive Chain-of-Thought

>Authors: Tencent Hunyuan Team, Ao Liu, Botong Zhou, Can Xu, Chayse Zhou, ChenChen Zhang, Chengcheng Xu, Chenhao Wang, Decheng Wu, Dengpeng Wu, Dian Jiao, Dong Du, Dong Wang, Feng Zhang, Fengzong Lian, Guanghui Xu, Guanwei Zhang, Hai Wang, Haipeng Luo, Han Hu, Huilin Xu, Jiajia Wu, Jianchen Zhu, Jianfeng Yan, Jiaqi Zhu, Jihong Zhang, Jinbao Xue, Jun Xia, Junqiang Zheng, Kai Liu, Kai Zhang, Kai Zheng, Kejiao Li, Keyao Wang, Lan Jiang, Lixin Liu, Lulu Wu, Mengyuan Huang, Peijie Yu, Peiqi Wang, Qian Wang, Qianbiao Xiang, Qibin Liu, Qingfeng Sun, Richard Guo, Ruobing Xie, Saiyong Yang, Shaohua Chen, Shihui Hu, Shuai Li, Shuaipeng Li, Shuang Chen, Suncong Zheng, Tao Yang, Tian Zhang, Tinghao Yu, Weidong Han, Weijie Liu, Weijin Zhou, Weikang Wang, Wesleye Chen, Xiao Feng, Xiaoqin Ren, Xingwu Sun, Xiong Kuang, Xuemeng Huang, Xun Cao, Yanfeng Chen, Yang Du, Yang Zhen, Yangyu Tao, Yaping Deng, Yi Shen, Yigeng Hong, Yiqi Chen, Yiqing Huang, Yuchi Deng, Yue Mao, Yulong Wang, Yuyuan Zeng, Zenan Xu, Zhanhui Kang, Zhe Zhao, ZhenXiang Yan, Zheng Fang, Zhichao Hu, Zhongzhi Chen, Zhuoyu Li, Zongwei Li, Alex Yan, Ande Liang, Baitong Liu, Beiping Pan, Bin Xing, Binghong Wu, Bingxin Qu, Bolin Ni, Boyu Wu, Chen Li, Cheng Jiang, Cheng Zhang, Chengjun Liu, Chengxu Yang, Chengzhong Xu, Chiyu Wang, Chong Zha, Daisy Yi, Di Wang, Fanyang Lu, Fei Chen, Feifei Liu, Feng Zheng, Guanghua Yu, Guiyang Li, Guohua Wang, Haisheng Lin, Han Liu, Han Wang, Hao Fei, Hao Lu, Haoqing Jiang, Haoran Sun, Haotian Zhu, Huangjin Dai, Huankui Chen, Huawen Feng, Huihui Cai, Huxin Peng, Jackson Lv, Jiacheng Shi, Jiahao Bu, Jianbo Li, Jianglu Hu, Jiangtao Guan, Jianing Xu, Jianwei Cai, Jiarong Zhang, Jiawei Song, Jie Jiang, Jie Liu, Jieneng Yang, Jihong Zhang, Jin lv, Jing Zhao, Jinjian Li, Jinxing Liu, Jun Zhao, Juntao Guo, Kai Wang, Kan Wu, Lei Fu, Lei He, Lei Wang, Li Liu, Liang Dong, Liya Zhan, Long Cheng, Long Xu, Mao Zheng, Meng Liu, Mengkang Hu, Nanli Chen, Peirui Chen, Peng He, Pengju Pan, Pengzhi Wei, Qi Yang, Qi Yi, Roberts Wang, Rongpeng Chen, Rui Sun, Rui Yang, Ruibin Chen, Ruixu Zhou, Shaofeng Zhang, Sheng Zhang, Shihao Xu, Shuaishuai Chang, Shulin Liu, SiQi Wang, Songjia Feng, Songling Yuan, Tao Zhang, Tianjiao Lang, Tongkai Li, Wei Deng, Wei Li, Weichao Wang, Weigang Zhang, Weixuan Sun, Wen Ouyang, Wenxiang Jiao, Wenzhi Sun, Wenzhuo Jia, Xiang Zhang, Xiangyu He, Xianshun Ren, XiaoYing Zhu, Xiaolong Guo, Xiaoxue Li, Xiaoyu Ma, Xican Lu, Xinhua Feng, Xinting Huang, Xinyu Guan, Xirui Li, Xu Zhang, Xudong Gao, Xun Luo, Xuxiang Qi, Yangkun Chen, Yangyu Tao, Yanling Xiao, Yantao Mai, Yanze Chen, Yao Ding, Yeting Yang, YiFan Song, Yifan Yang, Yijiao Zhu, Yinhe Wu, Yixian Liu, Yong Yang, Yuanjun Cai, Yuanlin Tu, Yue Zhang, Yufei Huang, Yuhang Zhou, Yuhao Jiang, Yuhong Liu, Yuhui Hu, Yujin Lin, Yun Yang, Yunhao Wang, Yusong Zhang, Zekun Wu, Zelong Zhang, Zhan Yu, Zhaoliang Yang, Zhe Zhao, Zheng Li, Zhenyu Huang, Zhiguang Liu, Zhijiang Xu, Zhiqing Kui, Zhiyin Zeng, Zhiyuan Xiong, Zhuo Han, Zifan Wu, Zigang Geng, Zilong Zhao, Ziyan Tang, Ziyuan Zhu, Zonglei Zhu, Zhijiang Xu

>2025-05-21

> http://arxiv.org/abs/2505.15431v2

As Large Language Models (LLMs) rapidly advance, we introduce Hunyuan-TurboS,
a novel large hybrid Transformer-Mamba Mixture of Experts (MoE) model. It
synergistically combines Mamba's long-sequence processing efficiency with
Transformer's superior contextual understanding. Hunyuan-TurboS features an
adaptive long-short chain-of-thought (CoT) mechanism, dynamically switching
between rapid responses for simple queries and deep "thinking" modes for
complex problems, optimizing computational resources. Architecturally, this 56B
activated (560B total) parameter model employs 128 layers (Mamba2, Attention,
FFN) with an innovative AMF/MF block pattern. Faster Mamba2 ensures linear
complexity, Grouped-Query Attention minimizes **KV** cache, and FFNs use an MoE
structure. Pre-trained on 16T high-quality tokens, it supports a 256K context
length and is the first industry-deployed large-scale Mamba model. Our
comprehensive post-training strategy enhances capabilities via Supervised
Fine-Tuning (3M instructions), a novel Adaptive Long-short CoT Fusion method,
Multi-round Deliberation Learning for iterative improvement, and a two-stage
Large-scale Reinforcement Learning process targeting STEM and general
instruction-following. Evaluations show strong performance: overall top 7 rank
on LMSYS Chatbot Arena with a score of 1356, outperforming leading models like
Gemini-2.0-Flash-001 (1352) and o4-mini-2025-04-16 (1345). TurboS also achieves
an average of 77.9% across 23 automated benchmarks. Hunyuan-TurboS balances
high performance and efficiency, offering substantial capabilities at lower
inference costs than many reasoning models, establishing a new paradigm for
efficient large-scale pre-trained models.


## Efficient Data Driven Mixture-of-Expert Extraction from Trained Networks

>Authors: Uranik Berisha, Jens Mehnert, Alexandru Paul Condurache

>2025-05-21

> http://arxiv.org/abs/2505.15414v1

Vision Transformers have emerged as the state-of-the-art models in various
Computer Vision tasks, but their high computational and resource demands pose
significant challenges. While Mixture-of-Experts (MoE) can make these models
more efficient, they often require costly retraining or even training from
scratch. Recent developments aim to reduce these computational costs by
leveraging pretrained networks. These have been shown to produce **sparse**
activation patterns in the Multi-Layer Perceptrons (MLPs) of the encoder
blocks, allowing for conditional activation of only relevant subnetworks for
each sample. Building on this idea, we propose a new method to construct MoE
variants from pretrained models. Our approach extracts expert subnetworks from
the model's MLP layers post-training in two phases. First, we cluster output
activations to identify distinct activation patterns. In the second phase, we
use these clusters to extract the corresponding subnetworks responsible for
producing them. On ImageNet-1k recognition tasks, we demonstrate that these
extracted experts can perform surprisingly well out of the box and require only
minimal fine-tuning to regain 98% of the original performance, all while
reducing MACs and model size, by up to 36% and 32% respectively.


## FlowKV Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management

>Authors: Xiang Liu, Hong Chen, Xuming Hu, Xiaowen Chu

>2025-05-21

> http://arxiv.org/abs/2505.15347v1

Large Language Models (LLMs) are increasingly deployed in multi-turn
conversational applications, where the management of the Key-Value (**KV**) Cache
presents a significant bottleneck. The linear growth of the **KV** Cache with
dialogue history imposes substantial computational costs, and existing eviction
strategies often degrade performance by repeatedly compressing early
conversational context, leading to information loss and context forgetting.
This paper introduces Flow**KV**, a novel \textbf{multi-turn isolation mechanism}
for **KV** Cache management, which can be applied to any **KV** Cache compression
method without training. Flow**KV**'s core innovation is a multi-turn isolation
mechanism that preserves the accumulated compressed **KV** cache from past turns.
Compression is then strategically applied only to the newly generated **KV** pairs
of the latest completed turn, effectively preventing the re-compression of
older context and thereby mitigating catastrophic forgetting. Our results
demonstrate that Flow**KV** consistently and significantly outperforms baseline
strategies in maintaining instruction-following accuracy and user preference
retention from 10.90\% to 75.40\%, particularly in later conversational turns.


## SSR Speculative Parallel Scaling Reasoning in Test-time

>Authors: Yuanlin Chu, Bo Wang, Xiang Liu, Hong Chen, Aiwei Liu, Xuming Hu

>2025-05-21

> http://arxiv.org/abs/2505.15340v1

Large language models (LLMs) have achieved impressive results on multi-step
mathematical reasoning, yet at the cost of high computational overhead. This
challenge is particularly acute for test-time scaling methods such as parallel
decoding, which increase answer diversity but scale poorly in efficiency. To
address this efficiency-accuracy trade-off, we propose SSR (Speculative
Parallel Scaling Reasoning), a training-free framework that leverages a key
insight: by introducing speculative decoding at the step level, we can
accelerate reasoning without sacrificing correctness. SSR integrates two
components: a Selective Parallel Module (SPM) that identifies a small set of
promising reasoning strategies via model-internal scoring, and Step-level
Speculative Decoding (SSD), which enables efficient draft-target collaboration
for fine-grained reasoning **acceleration**. Experiments on three mathematical
benchmarks-AIME 2024, MATH-500, and LiveMathBench - demonstrate that SSR
achieves strong gains over baselines. For instance, on LiveMathBench, SSR
improves pass@1 accuracy by 13.84% while reducing computation to 80.5% of the
baseline FLOPs. On MATH-500, SSR reduces compute to only 30% with no loss in
accuracy.


## SoftHGNN Soft Hypergraph Neural Networks for General Visual Recognition

>Authors: Mengqi Lei, Yihong Wu, Siqi Li, Xinhu Zheng, Juan Wang, Yue Gao, Shaoyi Du

>2025-05-21

> http://arxiv.org/abs/2505.15325v1

Visual recognition relies on understanding both the semantics of image tokens
and the complex interactions among them. Mainstream self-attention methods,
while effective at modeling global pair-wise relations, fail to capture
high-order associations inherent in real-world scenes and often suffer from
redundant computation. Hypergraphs extend conventional graphs by modeling
high-order interactions and offer a promising framework for addressing these
limitations. However, existing hypergraph neural networks typically rely on
static and hard hyperedge assignments, leading to excessive and redundant
hyperedges with hard binary vertex memberships that overlook the continuity of
visual semantics. To overcome these issues, we present Soft Hypergraph Neural
Networks (SoftHGNNs), which extend the methodology of hypergraph computation,
to make it truly efficient and versatile in visual recognition tasks. Our
framework introduces the concept of soft hyperedges, where each vertex is
associated with hyperedges via continuous participation weights rather than
hard binary assignments. This dynamic and differentiable association is
achieved by using the learnable hyperedge prototype. Through similarity
measurements between token features and the prototype, the model generates
semantically rich soft hyperedges. SoftHGNN then aggregates messages over soft
hyperedges to capture high-order semantics. To further enhance efficiency when
scaling up the number of soft hyperedges, we incorporate a **sparse** hyperedge
selection mechanism that activates only the top-k important hyperedges, along
with a load-balancing regularizer to ensure balanced hyperedge utilization.
Experimental results across three tasks on five datasets demonstrate that
SoftHGNN efficiently captures high-order associations in visual scenes,
achieving significant performance improvements.


## LiveVLM Efficient Online Video Understanding via Streaming-Oriented KV Cache and Retrieval

>Authors: Zhenyu Ning, Guangda Liu, Qihao Jin, Wenchao Ding, Minyi Guo, Jieru Zhao

>2025-05-21

> http://arxiv.org/abs/2505.15269v1

Recent developments in Video Large Language Models (Video LLMs) have enabled
models to process long video sequences and demonstrate remarkable performance.
Nonetheless, studies predominantly focus on offline video question answering,
neglecting memory usage and response speed that are essential in various
real-world applications, such as Deepseek services, autonomous driving, and
robotics. To mitigate these challenges, we propose $\textbf{LiveVLM}$, a
training-free framework specifically designed for streaming, online video
understanding and real-time interaction. Unlike existing works that process
videos only after one question is posed, LiveVLM constructs an innovative
streaming-oriented **KV** cache to process video streams in real-time, retain
long-term video details and eliminate redundant **KV**s, ensuring prompt responses
to user queries. For continuous video streams, LiveVLM generates and compresses
video key-value tensors (video **KV**s) to reserve visual information while
improving memory efficiency. Furthermore, when a new question is proposed,
LiveVLM incorporates an online question-answering process that efficiently
fetches both short-term and long-term visual information, while minimizing
interference from redundant context. Extensive experiments demonstrate that
LiveVLM enables the foundation LLaVA-OneVision model to process 44$\times$
number of frames on the same device, and achieves up to 5$\times$ speedup in
response speed compared with SoTA online methods at an input of 256 frames,
while maintaining the same or better model performance.


## An Efficient Private GPT Never Autoregressively Decodes

>Authors: Zhengyi Li, Yue Guan, Kang Yang, Yu Feng, Ning Liu, Yu Yu, Jingwen Leng, Minyi Guo

>2025-05-21

> http://arxiv.org/abs/2505.15252v1

The wide deployment of the generative pre-trained transformer (GPT) has
raised privacy concerns for both clients and servers. While cryptographic
primitives can be employed for secure GPT inference to protect the privacy of
both parties, they introduce considerable performance overhead.To accelerate
secure inference, this study proposes a public decoding and secure verification
approach that utilizes public GPT models, motivated by the observation that
securely decoding one and multiple tokens takes a similar latency. The client
uses the public model to generate a set of tokens, which are then securely
verified by the private model for acceptance. The efficiency of our approach
depends on the acceptance ratio of tokens proposed by the public model, which
we improve from two aspects: (1) a private sampling protocol optimized for
cryptographic primitives and (2) model alignment using knowledge distillation.
Our approach improves the efficiency of secure decoding while maintaining the
same level of privacy and generation quality as standard secure decoding.
Experiments demonstrate a $2.1\times \sim 6.0\times$ speedup compared to
standard decoding across three pairs of public-private models and different
network conditions.


## X-GRM Large Gaussian Reconstruction Model for Sparse-view X-rays to Computed Tomography

>Authors: Yifan Liu, Wuyang Li, Weihao Yu, Chenxin Li, Alexandre Alahi, Max Meng, Yixuan Yuan

>2025-05-21

> http://arxiv.org/abs/2505.15235v1

Computed Tomography serves as an indispensable tool in clinical workflows,
providing non-invasive visualization of internal anatomical structures.
Existing CT reconstruction works are limited to small-capacity model
architecture, inflexible volume representation, and small-scale training data.
In this paper, we present X-GRM (X-ray Gaussian Reconstruction Model), a large
feedforward model for reconstructing 3D CT from **sparse**-view 2D X-ray
projections. X-GRM employs a scalable transformer-based architecture to encode
an arbitrary number of **sparse** X-ray inputs, where tokens from different views
are integrated efficiently. Then, tokens are decoded into a new volume
representation, named Voxel-based Gaussian Splatting (VoxGS), which enables
efficient CT volume extraction and differentiable X-ray rendering. To support
the training of X-GRM, we collect ReconX-15K, a large-scale CT reconstruction
dataset containing around 15,000 CT/X-ray pairs across diverse organs,
including the chest, abdomen, pelvis, and tooth etc. This combination of a
high-capacity model, flexible volume representation, and large-scale training
data empowers our model to produce high-quality reconstructions from various
testing inputs, including in-domain and out-domain X-ray projections. Project
Page: https://github.com/CUHK-AIM-Group/X-GRM.


## A novel framework for detecting multiple change points in functional data sequences

>Authors: Zhiqing Fang, Xin Liu

>2025-05-21

> http://arxiv.org/abs/2505.15188v1

Detecting multiple change points in functional data sequences has been
increasingly popular and critical in various scientific fields. In this
article, we propose a novel two-stage framework for detecting multiple change
points in functional data sequences, named as detection by Group Selection and
Partial F-test (GS-PF). The detection problem is firstly transformed into a
high-dimensional **sparse** estimation problem via functional basis expansion, and
the penalized group selection is applied to estimate the number and locations
of candidate change points in the first stage. To further circumvent the issue
of overestimating the true number of change points in practice, a partial
F-test is applied in the second stage to filter redundant change points so that
the false discovery rate of the F-test for multiple change points is
controlled. Additionally, in order to reduce complexity of the proposed GS-PF
method, a link parameter is adopted to generate candidate sets of potential
change points, which greatly reduces the number of detected change points and
improves the efficiency. Asymptotic results are established and validated to
guarantee detection consistency of the proposed GS-PF method, and its
performance is evaluated through intensive simulations and real data analysis,
compared with the state-of-the-art detecting methods. Our findings indicate
that the proposed GS-PF method exhibits detection consistency in different
scenarios, which endows our method with the capability for efficient and robust
detection of multiple change points in functional data sequences.


## Time Tracker Mixture-of-Experts-Enhanced Foundation Time Series Forecasting Model with Decoupled Training Pipelines

>Authors: Xiaohou Shi, Ke Li, Aobo Liang, Yan Sun

>2025-05-21

> http://arxiv.org/abs/2505.15151v1

In the past few years, time series foundation models have achieved superior
predicting accuracy. However, real-world time series often exhibit significant
diversity in their temporal patterns across different time spans and domains,
making it challenging for a single model architecture to fit all complex
scenarios. In addition, time series data may have multiple variables exhibiting
complex correlations between each other. Recent mainstream works have focused
on modeling times series in a channel-independent manner in both pretraining
and finetuning stages, overlooking the valuable inter-series dependencies. To
this end, we propose \textbf{Time Tracker} for better predictions on
multivariate time series data. Firstly, we leverage **sparse** mixture of experts
(MoE) within Transformers to handle the modeling of diverse time series
patterns, thereby alleviating the learning difficulties of a single model while
improving its generalization. Besides, we propose Any-variate Attention,
enabling a unified model structure to seamlessly handle both univariate and
multivariate time series, thereby supporting channel-independent modeling
during pretraining and channel-mixed modeling for finetuning. Furthermore, we
design a graph learning module that constructs relations among sequences from
frequency-domain features, providing more precise guidance to capture
inter-series dependencies in channel-mixed modeling. Based on these
advancements, Time Tracker achieves state-of-the-art performance in predicting
accuracy, model generalization and adaptability.


## BanditSpec Adaptive Speculative Decoding via Bandit Algorithms

>Authors: Yunlong Hou, Fengzhuo Zhang, Cunxiao Du, Xuan Zhang, Jiachun Pan, Tianyu Pang, Chao Du, Vincent Y. F. Tan, Zhuoran Yang

>2025-05-21

> http://arxiv.org/abs/2505.15141v1

Speculative decoding has emerged as a popular method to accelerate the
inference of Large Language Models (LLMs) while retaining their superior text
generation performance. Previous methods either adopt a fixed speculative
decoding configuration regardless of the prefix tokens, or train draft models
in an offline or online manner to align them with the context. This paper
proposes a training-free online learning framework to adaptively choose the
configuration of the hyperparameters for speculative decoding as text is being
generated. We first formulate this hyperparameter selection problem as a
Multi-Armed Bandit problem and provide a general speculative decoding framework
BanditSpec. Furthermore, two bandit-based hyperparameter selection algorithms,
UCBSpec and EXP3Spec, are designed and analyzed in terms of a novel quantity,
the stopping time regret. We upper bound this regret under both stochastic and
adversarial reward settings. By deriving an information-theoretic impossibility
result, it is shown that the regret performance of UCBSpec is optimal up to
universal constants. Finally, extensive empirical experiments with LLaMA3 and
Qwen2 demonstrate that our algorithms are effective compared to existing
methods, and the throughput is close to the oracle best hyperparameter in
simulated real-life LLM serving scenarios with diverse input prompts.


## Multicrossmodal Automated Agent for Integrating Diverse Materials Science Data

>Authors: Adib Bazgir, Rama chandra Praneeth Madugula, Yuwen Zhang

>2025-05-21

> http://arxiv.org/abs/2505.15132v1

We introduce a multicrossmodal LLM-agent framework motivated by the growing
volume and diversity of materials-science data ranging from high-resolution
microscopy and dynamic simulation videos to tabular experiment logs and
sprawling literature archives. While recent AI efforts have accelerated
individual tasks such as property prediction or image classification, they
typically treat each modality in isolation, leaving rich cross-modal
correlations unexplored and forcing researchers to perform laborious manual
integration. Moreover, existing multimodal foundation models often require
expensive retraining or fine-tuning on domain data, and current multi-agent
systems in materials informatics address only narrow subtasks. To overcome
these obstacles, we design a coordinated team of specialized LLM agents, each
equipped with domain-adapted prompts and plugins that project their outputs
into a shared embedding space. A dynamic gating mechanism then weights and
merges these insights, enabling unified reasoning over heterogeneous inputs
without ever modifying the underlying LLM weights. We validate our approach on
challenging case studies and demonstrate substantial gains in retrieval
accuracy (85%), captioning fidelity, and integrated coverage (35%) compared to
single-modality and zero-shot baselines. Our work paves the way for AI digital
researchers capable of bridging data silos and accelerating the
materials-discovery cycle. The code is available at
https://github.com/adibgpt/Multicrossmodal-Autonomous-Materials-Science-Agent.


## Adaptive Inertial Method

>Authors: Han Long, Bingsheng He, Yinyu Ye, Jiheng Zhang

>2025-05-21

> http://arxiv.org/abs/2505.15114v1

In this paper, we introduce the Adaptive Inertial Method (AIM), a novel
framework for accelerated first-order methods through a customizable inertial
term. We provide a rigorous convergence analysis establishing a global
convergence rate of O(1/k) under mild conditions, requiring only convexity and
local Lipschitz differentiability of the objective function. Our method enables
adaptive parameter selection for the inertial term without manual tuning.
Furthermore, we derive the particular form of the inertial term that transforms
AIM into a new Quasi-Newton method. Notably, under specific circumstances, AIM
coincides with the regularized Newton method, achieving an accelerated rate of
O(1/k^2) without Hessian inversions. Through extensive numerical experiments,
we demonstrate that AIM exhibits superior performance across diverse
optimization problems, highlighting its practical effectiveness.


## StepSearch Igniting LLMs Search Ability via Step-Wise Proximal Policy Optimization

>Authors: Ziliang Wang, Xuhui Zheng, Kang An, Cijun Ouyang, Jialu Cai, Yuhang Wang, Yichao Wu

>2025-05-21

> http://arxiv.org/abs/2505.15107v1

Efficient multi-hop reasoning requires Large Language Models (LLMs) based
agents to acquire high-value external knowledge iteratively. Previous work has
explored reinforcement learning (RL) to train LLMs to perform search-based
document retrieval, achieving notable improvements in QA performance, but
underperform on complex, multi-hop QA resulting from the **sparse** rewards from
global signal only. To address this gap in existing research, we introduce
StepSearch, a framework for search LLMs that trained with step-wise proximal
policy optimization method. It consists of richer and more detailed
intermediate search rewards and token-level process supervision based on
information gain and redundancy penalties to better guide each search step. We
constructed a fine-grained question-answering dataset containing
sub-question-level search trajectories based on open source datasets through a
set of data pipeline method. On standard multi-hop QA benchmarks, it
significantly outperforms global-reward baselines, achieving 11.2% and 4.2%
absolute improvements for 3B and 7B models over various search with RL
baselines using only 19k training data, demonstrating the effectiveness of
fine-grained, stepwise supervision in optimizing deep search LLMs. Our
implementation is publicly available at
https://github.com/zxh20001117/StepSearch.


## SUS backprop linear backpropagation algorithm for long inputs in transformers

>Authors: Sergey Pankov, Georges Harik

>2025-05-21

> http://arxiv.org/abs/2505.15080v1

It is straightforward to design an unbiased gradient estimator that
stochastically cuts the backpropagation flow through any part of a
computational graph. By cutting the parts that have little effect on the
computation, one can potentially save a significant amount of back-propagation
computation in exchange for a minimal increase in the stochastic gradient
variance, in some situations. Such a situation occurs in the attention
mechanism of the transformer architecture. For long sequences, attention
becomes the limiting factor, as its compute requirements increase quadratically
with sequence length $n$. At the same time, most attention weights become very
small, as most attention heads tend to connect a given token with only a small
fraction of other tokens in the sequence. These weights become promising
targets for cutting backpropagation. We propose a simple probabilistic rule
controlled by a single parameter $c$ that cuts backpropagation through most
attention weights, leaving at most $c$ interactions per token per attention
head. This brings a factor of $c/n$ reduction in the compute required for the
attention backpropagation, turning it from quadratic $O(n^2)$ to linear
complexity $O(nc)$. We have empirically verified that, for a typical
transformer model, cutting $99\%$ of the attention gradient flow (i.e. choosing
$c \sim 20-30$) results in relative gradient variance increase of only about
$1\%$ for $n \sim 2000$, and it decreases with $n$. This approach is amenable
to efficient **sparse** matrix implementation, thus being promising for making the
cost of a backward pass negligible relative to the cost of a forward pass when
training a transformer model on long sequences.


## Self-GIVE Associative Thinking from Limited Structured Knowledge for Enhanced Large Language Model Reasoning

>Authors: Jiashu He, Jinxuan Fan, Bowen Jiang, Ignacio Houine, Dan Roth, Alejandro Ribeiro

>2025-05-21

> http://arxiv.org/abs/2505.15062v1

When addressing complex questions that require new information, people often
associate the question with existing knowledge to derive a sensible answer. For
instance, when evaluating whether melatonin aids insomnia, one might associate
"hormones helping mental disorders" with "melatonin being a hormone and
insomnia a mental disorder" to complete the reasoning. Large Language Models
(LLMs) also require such associative thinking, particularly in resolving
scientific inquiries when retrieved knowledge is insufficient and does not
directly answer the question. Graph Inspired Veracity Extrapolation (GIVE)
addresses this by using a knowledge graph (KG) to extrapolate structured
knowledge. However, it involves the construction and **pruning** of many
hypothetical triplets, which limits efficiency and generalizability. We propose
Self-GIVE, a retrieve-RL framework that enhances LLMs with automatic
associative thinking through reinforcement learning. Self-GIVE extracts
structured information and entity sets to assist the model in linking to the
queried concepts. We address GIVE's key limitations: (1) extensive LLM calls
and token overhead for knowledge extrapolation, (2) difficulty in deploying on
smaller LLMs (3B or 7B) due to complex instructions, and (3) inaccurate
knowledge from LLM **pruning**. Specifically, after fine-tuning using self-GIVE
with a 135 node UMLS KG, it improves the performance of the Qwen2.5 3B and 7B
models by up to $\textbf{28.5%$\rightarrow$71.4%}$ and
$\textbf{78.6$\rightarrow$90.5%}$ in samples $\textbf{unseen}$ in challenging
biomedical QA tasks. In particular, Self-GIVE allows the 7B model to match or
outperform GPT3.5 turbo with GIVE, while cutting token usage by over 90\%.
Self-GIVE enhances the scalable integration of structured retrieval and
reasoning with associative thinking.


## PiFlow Principle-aware Scientific Discovery with Multi-Agent Collaboration

>Authors: Yingming Pu, Tao Lin, Hongyu Chen

>2025-05-21

> http://arxiv.org/abs/2505.15047v1

Large Language Model (LLM)-based multi-agent systems (MAS) demonstrate
remarkable potential for scientific discovery. Existing approaches, however,
often automate scientific discovery using predefined workflows that lack
rationality constraints. This often leads to aimless hypothesizing and a
failure to consistently link hypotheses with evidence, thereby hindering
systematic uncertainty reduction. Overcoming these limitations fundamentally
requires systematic uncertainty reduction. We introduce \texttt{PiFlow}, an
information-theoretical framework, treating automated scientific discovery as a
structured uncertainty reduction problem guided by principles (e.g., scientific
laws). In evaluations across three distinct scientific domains -- discovering
nanomaterial structures, bio-molecules, and superconductor candidates with
targeted properties -- our method significantly improves discovery efficiency,
reflected by a 73.55\% increase in the Area Under the Curve (AUC) of property
values versus exploration steps, and enhances solution quality by 94.06\%
compared to a vanilla agent system. Overall, \texttt{PiFlow} serves as a
Plug-and-Play method, establishing a novel paradigm shift in highly efficient
automated scientific discovery, paving the way for more robust and accelerated
AI-driven research. Code is publicly available at our
\href{https://github.com/amair-lab/PiFlow}{GitHub}.


## Denoising Concept Vectors with Sparse Autoencoders for Improved Language Model Steering

>Authors: Haiyan Zhao, Xuansheng Wu, Fan Yang, Bo Shen, Ninghao Liu, Mengnan Du

>2025-05-21

> http://arxiv.org/abs/2505.15038v1

Linear Concept Vectors have proven effective for steering large language
models (LLMs). While existing approaches like linear probing and
difference-in-means derive these vectors from LLM hidden representations,
diverse data introduces noises (i.e., irrelevant features) that challenge
steering robustness. To address this, we propose Sparse Autoencoder-Denoised
Concept Vectors (SDCV), which uses Sparse Autoencoders to filter out noisy
features from hidden representations. When applied to linear probing and
difference-in-means, our method improves their steering success rates. We
validate our noise hypothesis through counterfactual experiments and feature
visualizations.


## Harnessing On-Device Large Language Model Empirical Results and Implications for AI PC

>Authors: Qingyu Song, Peiyu Liao, Wenqian Zhao, Yiwen Wang, Shoubo Hu, Hui-Ling Zhen, Ning Jiang, Mingxuan Yuan

>2025-05-21

> http://arxiv.org/abs/2505.15030v2

The increasing deployment of Large Language Models (LLMs) on edge devices,
driven by model advancements and hardware improvements, offers significant
privacy benefits. However, these on-device LLMs inherently face performance
limitations due to reduced model capacity and necessary compression techniques.
To address this, we introduce a systematic methodology -- encompassing model
capability, development efficiency, and system resources -- for evaluating
on-device LLMs. Our comprehensive evaluation, encompassing models from 0.5B to
14B parameters and seven post-training **quantization** (PTQ) methods on commodity
laptops, yields several critical insights: 1) System-level metrics exhibit
near-linear scaling with effective bits-per-weight (BPW). 2) A practical
threshold exists around $\sim$3.5 effective BPW, larger models subjected to
**low-bit** **quantization** consistently outperform smaller models utilizing higher
bit-precision. 3) Quantization with low BPW incurs marginal accuracy loss but
significant memory savings. 4) Determined by low-level implementation specifics
power consumption on CPU, where computation-intensive operations spend more
power than memory-intensive ones. These findings offer crucial insights and
practical guidelines for the efficient deployment and optimized configuration
of LLMs on resource-constrained edge devices. Our codebase is available at
https://github.com/simmonssong/LLMOnDevice.


## Rate-Distortion Optimization with Non-Reference Metrics for UGC Compression

>Authors: Samuel Fernández-Menduiña, Xin Xiong, Eduardo Pavez, Antonio Ortega, Neil Birkbeck, Balu Adsumilli

>2025-05-21

> http://arxiv.org/abs/2505.15003v1

Service providers must encode a large volume of noisy videos to meet the
demand for user-generated content (UGC) in online video-sharing platforms.
However, low-quality UGC challenges conventional codecs based on
rate-distortion optimization (RDO) with full-reference metrics (FRMs). While
effective for pristine videos, FRMs drive codecs to preserve artifacts when the
input is degraded, resulting in suboptimal compression. A more suitable
approach used to assess UGC quality is based on non-reference metrics (NRMs).
However, RDO with NRMs as a measure of distortion requires an iterative
workflow of encoding, decoding, and metric evaluation, which is computationally
impractical. This paper overcomes this limitation by linearizing the NRM around
the uncompressed video. The resulting cost function enables block-wise bit
allocation in the transform domain by estimating the alignment of the
**quantization** error with the gradient of the NRM. To avoid large deviations from
the input, we add sum of squared errors (SSE) regularization. We derive
expressions for both the SSE regularization parameter and the Lagrangian, akin
to the relationship used for SSE-RDO. Experiments with images and videos show
bitrate savings of more than 30\% over SSE-RDO using the target NRM, with no
decoder complexity overhead and minimal encoder complexity increase.


## Effective and Efficient Schema-aware Information Extraction Using On-Device Large Language Models

>Authors: Zhihao Wen, Sheng Liang, Yaxiong Wu, Yongyue Zhang, Yong Liu

>2025-05-21

> http://arxiv.org/abs/2505.14992v1

Information extraction (IE) plays a crucial role in natural language
processing (NLP) by converting unstructured text into structured knowledge.
Deploying computationally intensive large language models (LLMs) on
resource-constrained devices for information extraction is challenging,
particularly due to issues like hallucinations, limited context length, and
high latency-especially when handling diverse extraction schemas. To address
these challenges, we propose a two-stage information extraction approach
adapted for on-device LLMs, called Dual-LoRA with Incremental Schema Caching
(DLISC), which enhances both schema identification and schema-aware extraction
in terms of effectiveness and efficiency. In particular, DLISC adopts an
Identification LoRA module for retrieving the most relevant schemas to a given
query, and an Extraction LoRA module for performing information extraction
based on the previously selected schemas. To accelerate extraction inference,
Incremental Schema Caching is incorporated to reduce redundant computation,
substantially improving efficiency. Extensive experiments across multiple
information extraction datasets demonstrate notable improvements in both
effectiveness and efficiency.


## CRAFT Training-Free Cascaded Retrieval for Tabular QA

>Authors: Adarsh Singh, Kushal Raj Bhandari, Jianxi Gao, Soham Dan, Vivek Gupta

>2025-05-21

> http://arxiv.org/abs/2505.14984v1

Table Question Answering (TQA) involves retrieving relevant tables from a
large corpus to answer natural language queries. Traditional dense retrieval
models, such as DTR and ColBERT, not only incur high computational costs for
large-scale retrieval tasks but also require retraining or fine-tuning on new
datasets, limiting their adaptability to evolving domains and knowledge. In
this work, we propose $\textbf{CRAFT}$, a cascaded retrieval approach that
first uses a **sparse** retrieval model to filter a subset of candidate tables
before applying more computationally expensive dense models and neural
re-rankers. Our approach achieves better retrieval performance than
state-of-the-art (SOTA) **sparse**, dense, and hybrid retrievers. We further
enhance table representations by generating table descriptions and titles using
Gemini Flash 1.5. End-to-end TQA results using various Large Language Models
(LLMs) on NQ-Tables, a subset of the Natural Questions Dataset, demonstrate
$\textbf{CRAFT}$ effectiveness.


## Reinforcement Learning from User Feedback

>Authors: Eric Han, Jun Chen, Karthik Abinav Sankararaman, Xiaoliang Peng, Tengyu Xu, Eryk Helenowski, Kaiyan Peng, Mrinal Kumar, Sinong Wang, Han Fang, Arya Talebzadeh

>2025-05-20

> http://arxiv.org/abs/2505.14946v1

As large language models (LLMs) are increasingly deployed in diverse user
facing applications, aligning them with real user preferences becomes
essential. Existing methods like Reinforcement Learning from Human Feedback
(RLHF) rely on expert annotators trained on manually defined guidelines, whose
judgments may not reflect the priorities of everyday users. We introduce
Reinforcement Learning from User Feedback (RLUF), a framework for aligning LLMs
directly to implicit signals from users in production. RLUF addresses key
challenges of user feedback: user feedback is often binary (e.g., emoji
reactions), **sparse**, and occasionally adversarial. We train a reward model,
P[Love], to predict the likelihood that an LLM response will receive a Love
Reaction, a lightweight form of positive user feedback, and integrate P[Love]
into a multi-objective policy optimization framework alongside helpfulness and
safety objectives. In large-scale experiments, we show that P[Love] is
predictive of increased positive feedback and serves as a reliable offline
evaluator of future user behavior. Policy optimization using P[Love]
significantly raises observed positive-feedback rates, including a 28% increase
in Love Reactions during live A/B tests. However, optimizing for positive
reactions introduces reward hacking challenges, requiring careful balancing of
objectives. By directly leveraging implicit signals from users, RLUF offers a
path to aligning LLMs with real-world user preferences at scale.


## Scaling Laws for State Dynamics in Large Language Models

>Authors: Jacob X Li, Shreyas S Raman, Jessica Wan, Fahad Samman, Jazlyn Lin

>2025-05-20

> http://arxiv.org/abs/2505.14892v1

Large Language Models (LLMs) are increasingly used in tasks requiring
internal state tracking, yet their ability to model state transition dynamics
remains poorly understood. We evaluate how well LLMs capture deterministic
state dynamics across 3 domains: Box Tracking, Abstract DFA Sequences, and
Complex Text Games, each formalizable as a finite-state system. Across tasks,
we find that next-state prediction accuracy degrades with increasing
state-space size and **sparse** transitions. GPT-2 XL reaches about 70% accuracy in
low-complexity settings but drops below 30% when the number of boxes or states
exceeds 5 or 10, respectively. In DFA tasks, Pythia-1B fails to exceed 50%
accuracy when the number of states is > 10 and transitions are < 30. Through
activation patching, we identify attention heads responsible for propagating
state information: GPT-2 XL Layer 22 Head 20, and Pythia-1B Heads at Layers 10,
11, 12, and 14. While these heads successfully move relevant state features,
action information is not reliably routed to the final token, indicating weak
joint state-action reasoning. Our results suggest that state tracking in LLMs
emerges from distributed interactions of next-token heads rather than explicit
symbolic computation.


## Polar Sparsity High Throughput Batched LLM Inferencing with Scalable Contextual Sparsity

>Authors: Susav Shrestha, Brad Settlemyer, Nikoli Dryden, Narasimha Reddy

>2025-05-20

> http://arxiv.org/abs/2505.14884v1

Accelerating large language model (LLM) inference is critical for real-world
deployments requiring high throughput and low latency. Contextual **sparsity**,
where each token dynamically activates only a small subset of the model
parameters, shows promise but does not scale to large batch sizes due to union
of active neurons quickly approaching dense computation. We introduce Polar
Sparsity, highlighting a key shift in **sparsity** importance from MLP to Attention
layers as we scale batch size and sequence length. While MLP layers become more
compute-efficient under batching, their **sparsity** vanishes. In contrast,
attention becomes increasingly more expensive at scale, while their head
**sparsity** remains stable and batch-invariant. We develop hardware-efficient,
**sparsity**-aware GPU kernels for selective MLP and Attention computations,
delivering up to \(2.2\times\) end-to-end speedups for models like OPT, LLaMA-2
\& 3, across various batch sizes and sequence lengths without compromising
accuracy. To our knowledge, this is the first work to demonstrate that
contextual **sparsity** can scale effectively to large batch sizes, delivering
substantial inference **acceleration** with minimal changes, making Polar Sparsity
practical for large-scale, high-throughput LLM deployment systems. Our code is
available at: https://github.com/susavlsh10/Polar-Sparsity.


## Saten Sparse Augmented Tensor Networks for Post-Training Compression of Large Language Models

>Authors: Ryan Solgi, Kai Zhen, Rupak Vignesh Swaminathan, Nathan Susanj, Athanasios Mouchtaris, Siegfried Kunzmann, Zheng Zhang

>2025-05-20

> http://arxiv.org/abs/2505.14871v1

The efficient implementation of large language models (LLMs) is crucial for
deployment on resource-constrained devices. Low-rank tensor compression
techniques, such as tensor-train (TT) networks, have been widely studied for
over-parameterized neural networks. However, their applications to compress
pre-trained large language models (LLMs) for downstream tasks (post-training)
remains challenging due to the high-rank nature of pre-trained LLMs and the
lack of access to pretraining data. In this study, we investigate low-rank
tensorized LLMs during fine-tuning and propose **sparse** augmented tensor networks
(Saten) to enhance their performance. The proposed Saten framework enables full
model compression. Experimental results demonstrate that Saten enhances both
accuracy and compression efficiency in tensorized language models, achieving
state-of-the-art performance.


## Balanced and Elastic End-to-end Training of Dynamic LLMs

>Authors: Mohamed Wahib, Muhammed Abdullah Soyturk, Didem Unat

>2025-05-20

> http://arxiv.org/abs/2505.14864v1

To reduce computational and memory costs in Large Language Models (LLMs),
dynamic workload reduction schemes like Mixture of Experts (MoEs), parameter
**pruning**, layer freezing, **sparse** attention, early token exit, and Mixture of
Depths (MoDs) have emerged. However, these methods introduce severe workload
imbalances, limiting their practicality for large-scale distributed training.
We propose DynMo, an autonomous dynamic load balancing solution that ensures
optimal compute distribution when using pipeline parallelism in training
dynamic models. DynMo adaptively balances workloads, dynamically packs tasks
into fewer workers to free idle resources, and supports both multi-GPU
single-node and multi-node systems. Compared to static training methods
(Megatron-LM, DeepSpeed), DynMo accelerates training by up to 1.23x (MoEs),
3.18x (**pruning**), 2.23x (layer freezing), 4.02x (**sparse** attention), 4.52x (early
exit), and 1.17x (MoDs). DynMo is available at
https://anonymous.4open.science/r/DynMo-4D04/.


## Grouping First, Attending Smartly Training-Free Acceleration for Diffusion Transformers

>Authors: Sucheng Ren, Qihang Yu, Ju He, Alan Yuille, Liang-Chieh Chen

>2025-05-20

> http://arxiv.org/abs/2505.14687v1

Diffusion-based Transformers have demonstrated impressive generative
capabilities, but their high computational costs hinder practical deployment,
for example, generating an $8192\times 8192$ image can take over an hour on an
A100 GPU. In this work, we propose GRAT (\textbf{GR}ouping first,
\textbf{AT}tending smartly), a training-free attention **acceleration** strategy
for fast image and video generation without compromising output quality. The
key insight is to exploit the inherent **sparsity** in learned attention maps
(which tend to be locally focused) in pretrained Diffusion Transformers and
leverage better GPU parallelism. Specifically, GRAT first partitions contiguous
tokens into non-overlapping groups, aligning both with GPU execution patterns
and the local attention structures learned in pretrained generative
Transformers. It then accelerates attention by having all query tokens within
the same group share a common set of attendable key and value tokens. These key
and value tokens are further restricted to structured regions, such as
surrounding blocks or criss-cross regions, significantly reducing computational
overhead (e.g., attaining a \textbf{35.8$\times$} speedup over full attention
when generating $8192\times 8192$ images) while preserving essential attention
patterns and long-range context. We validate GRAT on pretrained Flux and
HunyuanVideo for image and video generation, respectively. In both cases, GRAT
achieves substantially faster inference without any fine-tuning, while
maintaining the performance of full attention. We hope GRAT will inspire future
research on accelerating Diffusion Transformers for scalable visual generation.


## A General Framework for Group Sparsity in Hyperspectral Unmixing Using Endmember Bundles

>Authors: Gokul Bhusal, Yifei Lou, Cristina Garcia-Cardona, Ekaterina Merkurjev

>2025-05-20

> http://arxiv.org/abs/2505.14634v1

Due to low spatial resolution, hyperspectral data often consists of mixtures
of contributions from multiple materials. This limitation motivates the task of
hyperspectral unmixing (HU), a fundamental problem in hyperspectral imaging. HU
aims to identify the spectral signatures (\textit{endmembers}) of the materials
present in an observed scene, along with their relative proportions
(\textit{fractional abundance}) in each pixel. A major challenge lies in the
class variability in materials, which hinders accurate representation by a
single spectral signature, as assumed in the conventional linear mixing model.
Moreover, To address this issue, we propose using group **sparsity** after
representing each material with a set of spectral signatures, known as
endmember bundles, where each group corresponds to a specific material. In
particular, we develop a bundle-based framework that can enforce either
inter-group **sparsity** or **sparsity** within and across groups (SWAG) on the
abundance coefficients. Furthermore, our framework offers the flexibility to
incorporate a variety of **sparsity**-promoting penalties, among which the
transformed $\ell_1$ (TL1) penalty is a novel regularization in the HU
literature. Extensive experiments conducted on both synthetic and real
hyperspectral data demonstrate the effectiveness and superiority of the
proposed approaches.


## TinyV Reducing False Negatives in Verification Improves RL for LLM Reasoning

>Authors: Zhangchen Xu, Yuetai Li, Fengqing Jiang, Bhaskar Ramasubramanian, Luyao Niu, Bill Yuchen Lin, Radha Poovendran

>2025-05-20

> http://arxiv.org/abs/2505.14625v2

Reinforcement Learning (RL) has become a powerful tool for enhancing the
reasoning abilities of large language models (LLMs) by optimizing their
policies with reward signals. Yet, RL's success relies on the reliability of
rewards, which are provided by verifiers. In this paper, we expose and analyze
a widespread problem--false negatives--where verifiers wrongly reject correct
model outputs. Our in-depth study of the Big-Math-RL-Verified dataset reveals
that over 38% of model-generated responses suffer from false negatives, where
the verifier fails to recognize correct answers. We show, both empirically and
theoretically, that these false negatives severely impair RL training by
depriving the model of informative gradient signals and slowing convergence. To
mitigate this, we propose tinyV, a lightweight LLM-based verifier that augments
existing rule-based methods, which dynamically identifies potential false
negatives and recovers valid responses to produce more accurate reward
estimates. Across multiple math-reasoning benchmarks, integrating TinyV boosts
pass rates by up to 10% and accelerates convergence relative to the baseline.
Our findings highlight the critical importance of addressing verifier false
negatives and offer a practical approach to improve RL-based fine-tuning of
LLMs. Our code is available at https://github.com/uw-nsl/TinyV.


## Toward Reliable Biomedical Hypothesis Generation Evaluating Truthfulness and Hallucination in Large Language Models

>Authors: Guangzhi Xiong, Eric Xie, Corey Williams, Myles Kim, Amir Hassan Shariatmadari, Sikun Guo, Stefan Bekiranov, Aidong Zhang

>2025-05-20

> http://arxiv.org/abs/2505.14599v1

Large language models (LLMs) have shown significant potential in scientific
disciplines such as biomedicine, particularly in hypothesis generation, where
they can analyze vast literature, identify patterns, and suggest research
directions. However, a key challenge lies in evaluating the truthfulness of
generated hypotheses, as verifying their accuracy often requires substantial
time and resources. Additionally, the hallucination problem in LLMs can lead to
the generation of hypotheses that appear plausible but are ultimately
incorrect, undermining their reliability. To facilitate the systematic study of
these challenges, we introduce TruthHypo, a benchmark for assessing the
capabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD,
a knowledge-based hallucination detector to evaluate how well hypotheses are
grounded in existing knowledge. Our results show that LLMs struggle to generate
truthful hypotheses. By analyzing hallucinations in reasoning steps, we
demonstrate that the groundedness scores provided by KnowHD serve as an
effective metric for filtering truthful hypotheses from the diverse outputs of
LLMs. Human evaluations further validate the utility of KnowHD in identifying
truthful hypotheses and accelerating scientific discovery. Our data and source
code are available at https://github.com/Teddy-XiongGZ/TruthHypo.


## Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with Capability in Mind for Better Reasoning

>Authors: Shangziqi Zhao, Jiahao Yuan, Guisong Yang, Usman Naseem

>2025-05-20

> http://arxiv.org/abs/2505.14582v1

Long chain-of-thought (Long-CoT) reasoning improves accuracy in LLMs, yet its
verbose, self-reflective style often hinders effective distillation into small
language models (SLMs). We revisit Long-CoT compression through the lens of
capability alignment and ask: Can **pruning** improve reasoning? We propose
Prune-on-Logic, a structure-aware framework that transforms Long-CoT into logic
graphs and selectively prunes low-utility reasoning steps under
self-verification constraints. Through systematic analysis across three **pruning**
strategies -- targeting entire chains, core reasoning, and verification -- we
find that **pruning** verification steps yields consistent accuracy gains while
reducing inference cost, outperforming token-level baselines and uncompressed
fine-tuning. In contrast, **pruning** reasoning or all-chain steps degrades
performance, revealing that small models benefit not from shorter CoTs, but
from semantically leaner ones. Our findings highlight **pruning** as a structural
optimization strategy for aligning CoT reasoning with SLM capacity.


## Breaking Bad Tokens Detoxification of LLMs Using Sparse Autoencoders

>Authors: Agam Goyal, Vedant Rathi, William Yeh, Yian Wang, Yuen Chen, Hari Sundaram

>2025-05-20

> http://arxiv.org/abs/2505.14536v1

Large language models (LLMs) are now ubiquitous in user-facing applications,
yet they still generate undesirable toxic outputs, including profanity,
vulgarity, and derogatory remarks. Although numerous detoxification methods
exist, most apply broad, surface-level fixes and can therefore easily be
circumvented by jailbreak attacks. In this paper we leverage **sparse**
autoencoders (SAEs) to identify toxicity-related directions in the residual
stream of models and perform targeted activation steering using the
corresponding decoder vectors. We introduce three tiers of steering
aggressiveness and evaluate them on GPT-2 Small and Gemma-2-2B, revealing
trade-offs between toxicity reduction and language fluency. At stronger
steering strengths, these causal interventions surpass competitive baselines in
reducing toxicity by up to 20%, though fluency can degrade noticeably on GPT-2
Small depending on the aggressiveness. Crucially, standard NLP benchmark scores
upon steering remain stable, indicating that the model's knowledge and general
abilities are preserved. We further show that feature-splitting in wider SAEs
hampers safety interventions, underscoring the importance of disentangled
feature learning. Our findings highlight both the promise and the current
limitations of SAE-based causal interventions for LLM detoxification, further
suggesting practical guidelines for safer language-model deployment.


## Interpretable Dual-Stream Learning for Local Wind Hazard Prediction in Vulnerable Communities

>Authors: Mahmuda Akhter Nishu, Chenyu Huang, Milad Roohi, Xin Zhong

>2025-05-20

> http://arxiv.org/abs/2505.14522v1

Wind hazards such as tornadoes and straight-line winds frequently affect
vulnerable communities in the Great Plains of the United States, where limited
infrastructure and **sparse** data coverage hinder effective emergency response.
Existing forecasting systems focus primarily on meteorological elements and
often fail to capture community-specific vulnerabilities, limiting their
utility for localized risk assessment and resilience planning. To address this
gap, we propose an interpretable dual-stream learning framework that integrates
structured numerical weather data with unstructured textual event narratives.
Our architecture combines a Random Forest and RoBERTa-based transformer through
a late fusion mechanism, enabling robust and context-aware wind hazard
prediction. The system is tailored for underserved tribal communities and
supports block-level risk assessment. Experimental results show significant
performance gains over traditional baselines. Furthermore, gradient-based
sensitivity and ablation studies provide insight into the model's
decision-making process, enhancing transparency and operational trust. The
findings demonstrate both predictive effectiveness and practical value in
supporting emergency preparedness and advancing community resilience.


## ModRWKV Transformer Multimodality in Linear Time

>Authors: Jiale Kang, Ziyin Yue, Qingyu Yin, Jiang Rui, Weile Li, Zening Lu, Zhouran Ji

>2025-05-20

> http://arxiv.org/abs/2505.14505v1

Currently, most multimodal studies are based on large language models (LLMs)
with quadratic-complexity Transformer architectures. While linear models like
RNNs enjoy low inference costs, their application has been largely limited to
the text-only modality. This work explores the capabilities of modern RNN
architectures in multimodal contexts. We propose ModRW**KV**-a decoupled multimodal
framework built upon the RW**KV**7 architecture as its LLM backbone-which achieves
multi-source information fusion through dynamically adaptable heterogeneous
modality encoders. We designed the multimodal modules in ModRW**KV** with an
extremely lightweight architecture and, through extensive experiments,
identified a configuration that achieves an optimal balance between performance
and computational efficiency. ModRW**KV** leverages the pretrained weights of the
RW**KV**7 LLM for initialization, which significantly accelerates multimodal
training. Comparative experiments with different pretrained checkpoints further
demonstrate that such initialization plays a crucial role in enhancing the
model's ability to understand multimodal signals. Supported by extensive
experiments, we conclude that modern RNN architectures present a viable
alternative to Transformers in the domain of multimodal large language models
(MLLMs). Furthermore, we identify the optimal configuration of the ModRW**KV**
architecture through systematic exploration.


## Video Compression Commander Plug-and-Play Inference Acceleration for Video Large Language Models

>Authors: Xuyang Liu, Yiyu Wang, Junpeng Ma, Linfeng Zhang

>2025-05-20

> http://arxiv.org/abs/2505.14454v1

Video large language models (VideoLLM) excel at video understanding, but face
efficiency challenges due to the quadratic complexity of abundant visual
tokens. Our systematic analysis of token compression methods for VideoLLMs
reveals two critical issues: (i) overlooking distinctive visual signals across
frames, leading to information loss; (ii) suffering from implementation
constraints, causing incompatibility with modern architectures or efficient
operators. To address these challenges, we distill three design principles for
VideoLLM token compression and propose a plug-and-play inference **acceleration**
framework "Video Compression Commander" (VidCom2). By quantifying each frame's
uniqueness, VidCom2 adaptively adjusts compression intensity across frames,
effectively preserving essential information while reducing redundancy in video
sequences. Extensive experiments across various VideoLLMs and benchmarks
demonstrate the superior performance and efficiency of our VidCom2. With only
25% visual tokens, VidCom2 achieves 99.6% of the original performance on
LLaVA-OV while reducing 70.8% of the LLM generation latency. Notably, our Frame
Compression Adjustment strategy is compatible with other token compression
methods to further improve their performance. Our code is available at
https://github.com/xuyang-liu16/VidCom2.


## SAE-FiRE Enhancing Earnings Surprise Predictions Through Sparse Autoencoder Feature Selection

>Authors: Huopu Zhang, Yanguang Liu, Mengnan Du

>2025-05-20

> http://arxiv.org/abs/2505.14420v1

Predicting earnings surprises through the analysis of earnings conference
call transcripts has attracted increasing attention from the financial research
community. Conference calls serve as critical communication channels between
company executives, analysts, and shareholders, offering valuable
forward-looking information. However, these transcripts present significant
analytical challenges, typically containing over 5,000 words with substantial
redundancy and industry-specific terminology that creates obstacles for
language models. In this work, we propose the Sparse Autoencoder for Financial
Representation Enhancement (SAE-FiRE) framework to address these limitations by
extracting key information while eliminating redundancy. SAE-FiRE employs
Sparse Autoencoders (SAEs) to efficiently identify patterns and filter out
noises, and focusing specifically on capturing nuanced financial signals that
have predictive power for earnings surprises. Experimental results indicate
that the proposed method can significantly outperform comparing baselines.


## OmniGenBench A Modular Platform for Reproducible Genomic Foundation Models Benchmarking

>Authors: Heng Yang, Jack Cole, Yuan Li, Renzhi Chen, Geyong Min, Ke Li

>2025-05-20

> http://arxiv.org/abs/2505.14402v1

The code of nature, embedded in DNA and RNA genomes since the origin of life,
holds immense potential to impact both humans and ecosystems through genome
modeling. Genomic Foundation Models (GFMs) have emerged as a transformative
approach to decoding the genome. As GFMs scale up and reshape the landscape of
AI-driven genomics, the field faces an urgent need for rigorous and
reproducible evaluation. We present OmniGenBench, a modular benchmarking
platform designed to unify the data, model, benchmarking, and interpretability
layers across GFMs. OmniGenBench enables standardized, one-command evaluation
of any GFM across five benchmark suites, with seamless integration of over 31
open-source models. Through automated pipelines and community-extensible
features, the platform addresses critical reproducibility challenges, including
data transparency, model interoperability, benchmark fragmentation, and
black-box interpretability. OmniGenBench aims to serve as foundational
infrastructure for reproducible genomic AI research, accelerating trustworthy
discovery and collaborative innovation in the era of genome-scale modeling.


## Log-Augmented Generation Scaling Test-Time Reasoning with Reusable Computation

>Authors: Peter Baile Chen, Yi Zhang, Dan Roth, Samuel Madden, Jacob Andreas, Michael Cafarella

>2025-05-20

> http://arxiv.org/abs/2505.14398v1

While humans naturally learn and adapt from past experiences, large language
models (LLMs) and their agentic counterparts struggle to retain reasoning from
previous tasks and apply them in future contexts. To address this limitation,
we propose a novel framework, log-augmented generation (LAG) that directly
reuses prior computation and reasoning from past logs at test time to enhance
model's ability to learn from previous tasks and perform better on new, unseen
challenges, all while keeping the system efficient and scalable. Specifically,
our system represents task logs using key-value (**KV**) caches, encoding the full
reasoning context of prior tasks while storing **KV** caches for only a selected
subset of tokens. When a new task arises, LAG retrieves the **KV** values from
relevant logs to augment generation. Our approach differs from reflection-based
memory mechanisms by directly reusing prior reasoning and computations without
requiring additional steps for knowledge extraction or distillation. Our method
also goes beyond existing **KV** caching techniques, which primarily target
efficiency gains rather than improving accuracy. Experiments on knowledge- and
reasoning-intensive datasets demonstrate that our method significantly
outperforms standard agentic systems that do not utilize logs, as well as
existing solutions based on reflection and **KV** cache techniques.


## Layer-wise Quantization for Quantized Optimistic Dual Averaging

>Authors: Anh Duc Nguyen, Ilia Markov, Frank Zhengqing Wu, Ali Ramezani-Kebrya, Kimon Antonakopoulos, Dan Alistarh, Volkan Cevher

>2025-05-20

> http://arxiv.org/abs/2505.14371v1

Modern deep neural networks exhibit heterogeneity across numerous layers of
various types such as residuals, multi-head attention, etc., due to varying
structures (dimensions, activation functions, etc.), distinct representation
characteristics, which impact predictions. We develop a general layer-wise
**quantization** framework with tight variance and code-length bounds, adapting to
the heterogeneities over the course of training. We then apply a new layer-wise
**quantization** technique within distributed variational inequalities (VIs),
proposing a novel Quantized Optimistic Dual Averaging (QODA) algorithm with
adaptive learning rates, which achieves competitive convergence rates for
monotone VIs. We empirically show that QODA achieves up to a $150\%$ speedup
over the baselines in end-to-end training time for training Wasserstein GAN on
$12+$ GPUs.


## Scaling and Enhancing LLM-based AVSR A Sparse Mixture of Projectors Approach

>Authors: Umberto Cappellazzo, Minsu Kim, Stavros Petridis, Daniele Falavigna, Alessio Brutti

>2025-05-20

> http://arxiv.org/abs/2505.14336v2

Audio-Visual Speech Recognition (AVSR) enhances robustness in noisy
environments by integrating visual cues. While recent advances integrate Large
Language Models (LLMs) into AVSR, their high computational cost hinders
deployment in resource-constrained settings. To address this, we propose
Llama-SMoP, an efficient Multimodal LLM that employs a Sparse Mixture of
Projectors (SMoP) module to scale model capacity without increasing inference
costs. By incorporating **sparse**ly-gated mixture-of-experts (MoE) projectors,
Llama-SMoP enables the use of smaller LLMs while maintaining strong
performance. We explore three SMoP configurations and show that Llama-SMoP DEDR
(Disjoint-Experts, Disjoint-Routers), which uses modality-specific routers and
experts, achieves superior performance on ASR, VSR, and AVSR tasks. Ablation
studies confirm its effectiveness in expert activation, scalability, and noise
robustness.


## Low-Cost FlashAttention with Fused Exponential and Multiplication Hardware Operators

>Authors: Kosmas Alexandridis, Vasileios Titopoulos, Giorgos Dimitrakopoulos

>2025-05-20

> http://arxiv.org/abs/2505.14314v1

Attention mechanisms, particularly within Transformer architectures and large
language models (LLMs), have revolutionized sequence modeling in machine
learning and artificial intelligence applications. To compute attention for
increasingly long sequences, specialized accelerators have been proposed to
execute key attention steps directly in hardware. Among the various recently
proposed architectures, those based on variants of the FlashAttention
algorithm, originally designed for GPUs, stand out due to their optimized
computation, tiling capabilities, and reduced memory traffic. In this work, we
focus on optimizing the kernel of floating-point-based FlashAttention using new
hardware operators that fuse the computation of exponentials and vector
multiplications, e.g., e^x, V. The proposed ExpMul hardware operators
significantly reduce the area and power costs of FlashAttention-based hardware
accelerators. When implemented in a 28nm ASIC technology, they achieve
improvements of 28.8% in area and 17.6% in power, on average, compared to
state-of-the-art hardware architectures with separate exponentials and vector
multiplications hardware operators.


## HausaNLP Current Status, Challenges and Future Directions for Hausa Natural Language Processing

>Authors: Shamsuddeen Hassan Muhammad, Ibrahim Said Ahmad, Idris Abdulmumin, Falalu Ibrahim Lawan, Babangida Sani, Sukairaj Hafiz Imam, Yusuf Aliyu, Sani Abdullahi Sani, Ali Usman Umar, Kenneth Church, Vukosi Marivate

>2025-05-20

> http://arxiv.org/abs/2505.14311v1

Hausa Natural Language Processing (NLP) has gained increasing attention in
recent years, yet remains understudied as a low-resource language despite
having over 120 million first-language (L1) and 80 million second-language (L2)
speakers worldwide. While significant advances have been made in high-resource
languages, Hausa NLP faces persistent challenges, including limited open-source
datasets and inadequate model representation. This paper presents an overview
of the current state of Hausa NLP, systematically examining existing resources,
research contributions, and gaps across fundamental NLP tasks: text
classification, machine translation, named entity recognition, speech
recognition, and question answering. We introduce HausaNLP
(https://catalog.hausanlp.org), a curated catalog that aggregates datasets,
tools, and research works to enhance accessibility and drive further
development. Furthermore, we discuss challenges in integrating Hausa into large
language models (LLMs), addressing issues of suboptimal tokenization and
dialectal variation. Finally, we propose strategic research directions
emphasizing dataset expansion, improved language modeling approaches, and
strengthened community collaboration to advance Hausa NLP. Our work provides
both a foundation for accelerating Hausa NLP progress and valuable insights for
broader multilingual NLP research.


## Scaling Law for Quantization-Aware Training

>Authors: Mengzhao Chen, Chaoyi Zhang, Jing Liu, Yutao Zeng, Zeyue Xue, Zhiheng Liu, Yunshui Li, Jin Ma, Jie Huang, Xun Zhou, Ping Luo

>2025-05-20

> http://arxiv.org/abs/2505.14302v1

Large language models (LLMs) demand substantial computational and memory
resources, creating deployment challenges. Quantization-aware training (QAT)
addresses these challenges by reducing model precision while maintaining
performance. However, the scaling behavior of QAT, especially at 4-bit
precision (W4A4), is not well understood. Existing QAT scaling laws often
ignore key factors such as the number of training tokens and **quantization**
granularity, which limits their applicability. This paper proposes a unified
scaling law for QAT that models **quantization** error as a function of model size,
training data volume, and **quantization** group size. Through 268 QAT experiments,
we show that **quantization** error decreases as model size increases, but rises
with more training tokens and coarser **quantization** granularity. To identify the
sources of W4A4 **quantization** error, we decompose it into weight and activation
components. Both components follow the overall trend of W4A4 **quantization**
error, but with different sensitivities. Specifically, weight **quantization**
error increases more rapidly with more training tokens. Further analysis shows
that the activation **quantization** error in the FC2 layer, caused by outliers, is
the primary bottleneck of W4A4 QAT **quantization** error. By applying
mixed-precision **quantization** to address this bottleneck, we demonstrate that
weight and activation **quantization** errors can converge to similar levels.
Additionally, with more training data, weight **quantization** error eventually
exceeds activation **quantization** error, suggesting that reducing weight
**quantization** error is also important in such scenarios. These findings offer
key insights for improving QAT research and development.


## Speculative Decoding Reimagined for Multimodal Large Language Models

>Authors: Luxi Lin, Zhihang Lin, Zhanpeng Zeng, Rongrong Ji

>2025-05-20

> http://arxiv.org/abs/2505.14260v1

This paper introduces Multimodal Speculative Decoding (MSD) to accelerate
Multimodal Large Language Models (MLLMs) inference. Speculative decoding has
been shown to accelerate Large Language Models (LLMs) without sacrificing
accuracy. However, current speculative decoding methods for MLLMs fail to
achieve the same speedup as they do for LLMs. To address this, we reimagine
speculative decoding specifically for MLLMs. Our analysis of MLLM
characteristics reveals two key design principles for MSD: (1) Text and visual
tokens have fundamentally different characteristics and need to be processed
separately during drafting. (2) Both language modeling ability and visual
perception capability are crucial for the draft model. For the first principle,
MSD decouples text and visual tokens in the draft model, allowing each to be
handled based on its own characteristics. For the second principle, MSD uses a
two-stage training strategy: In stage one, the draft model is trained on
text-only instruction-tuning datasets to improve its language modeling ability.
In stage two, MSD gradually introduces multimodal data to enhance the visual
perception capability of the draft model. Experiments show that MSD boosts
inference speed by up to $2.29\times$ for LLaVA-1.5-7B and up to $2.46\times$
for LLaVA-1.5-13B on multimodal benchmarks, demonstrating its effectiveness.
Our code is available at https://github.com/Lyn-Lucy/MSD.


## MatchDance Collaborative Mamba-Transformer Architecture Matching for High-Quality 3D Dance Synthesis

>Authors: Kaixing Yang, Xulong Tang, Yuxuan Hu, Jiahao Yang, Hongyan Liu, Qinnan Zhang, Jun He, Zhaoxin Fan

>2025-05-20

> http://arxiv.org/abs/2505.14222v2

Music-to-dance generation represents a challenging yet pivotal task at the
intersection of choreography, virtual reality, and creative content generation.
Despite its significance, existing methods face substantial limitation in
achieving choreographic consistency. To address the challenge, we propose
MatchDance, a novel framework for music-to-dance generation that constructs a
latent representation to enhance choreographic consistency. MatchDance employs
a two-stage design: (1) a Kinematic-Dynamic-based Quantization Stage (KDQS),
which encodes dance motions into a latent representation by Finite Scalar
Quantization (FSQ) with kinematic-dynamic constraints and reconstructs them
with high fidelity, and (2) a Hybrid Music-to-Dance Generation Stage(HMDGS),
which uses a Mamba-Transformer hybrid architecture to map music into the latent
representation, followed by the KDQS decoder to generate 3D dance motions.
Additionally, a music-dance retrieval framework and comprehensive metrics are
introduced for evaluation. Extensive experiments on the FineDance dataset
demonstrate state-of-the-art performance. Code will be released upon
acceptance.


## Embedded Mean Field Reinforcement Learning for Perimeter-defense Game

>Authors: Li Wang, Xin Yu, Xuxin Lv, Gangzheng Ai, Wenjun Wu

>2025-05-20

> http://arxiv.org/abs/2505.14209v1

With the rapid advancement of unmanned aerial vehicles (UAVs) and missile
technologies, perimeter-defense game between attackers and defenders for the
protection of critical regions have become increasingly complex and
strategically significant across a wide range of domains. However, existing
studies predominantly focus on small-scale, simplified two-dimensional
scenarios, often overlooking realistic environmental perturbations, motion
dynamics, and inherent heterogeneity--factors that pose substantial challenges
to real-world applicability. To bridge this gap, we investigate large-scale
heterogeneous perimeter-defense game in a three-dimensional setting,
incorporating realistic elements such as motion dynamics and wind fields. We
derive the Nash equilibrium strategies for both attackers and defenders,
characterize the victory regions, and validate our theoretical findings through
extensive simulations. To tackle large-scale heterogeneous control challenges
in defense strategies, we propose an Embedded Mean-Field Actor-Critic (EMFAC)
framework. EMFAC leverages representation learning to enable high-level action
aggregation in a mean-field manner, supporting scalable coordination among
defenders. Furthermore, we introduce a lightweight agent-level attention
mechanism based on reward representation, which selectively filters
observations and mean-field information to enhance decision-making efficiency
and accelerate convergence in large-scale tasks. Extensive simulations across
varying scales demonstrate the effectiveness and adaptability of EMFAC, which
outperforms established baselines in both convergence speed and overall
performance. To further validate practicality, we test EMFAC in small-scale
real-world experiments and conduct detailed analyses, offering deeper insights
into the framework's effectiveness in complex scenarios.


## MSDformer Multi-scale Discrete Transformer For Time Series Generation

>Authors: Zhicheng Chen, Shibo Feng, Xi Xiao, Zhong Zhang, Qing Li, Xingyu Gao, Peilin Zhao

>2025-05-20

> http://arxiv.org/abs/2505.14202v1

Discrete Token Modeling (DTM), which employs vector **quantization** techniques,
has demonstrated remarkable success in modeling non-natural language
modalities, particularly in time series generation. While our prior work
SDformer established the first DTM-based framework to achieve state-of-the-art
performance in this domain, two critical limitations persist in existing DTM
approaches: 1) their inability to capture multi-scale temporal patterns
inherent to complex time series data, and 2) the absence of theoretical
foundations to guide model optimization. To address these challenges, we
proposes a novel multi-scale DTM-based time series generation method, called
Multi-Scale Discrete Transformer (MSDformer). MSDformer employs a multi-scale
time series tokenizer to learn discrete token representations at multiple
scales, which jointly characterize the complex nature of time series data.
Subsequently, MSDformer applies a multi-scale autoregressive token modeling
technique to capture the multi-scale patterns of time series within the
discrete latent space. Theoretically, we validate the effectiveness of the DTM
method and the rationality of MSDformer through the rate-distortion theorem.
Comprehensive experiments demonstrate that MSDformer significantly outperforms
state-of-the-art methods. Both theoretical analysis and experimental results
demonstrate that incorporating multi-scale information and modeling multi-scale
patterns can substantially enhance the quality of generated time series in
DTM-based approaches. The code will be released upon acceptance.


## FLASH-D FlashAttention with Hidden Softmax Division

>Authors: Kosmas Alexandridis, Vasileios Titopoulos, Giorgos Dimitrakopoulos

>2025-05-20

> http://arxiv.org/abs/2505.14201v1

The transformer's attention mechanism has revolutionized AI and machine
learning, with its efficient computation being crucial to its performance.
However, calculating attention involves matrix operations interspersed with
softmax rescaling, which inherently slows down computation and requires
processing the entire input sequence. Building on online softmax computation,
FlashAttention integrates softmax calculation with matrix arithmetic, enabling
tiled computation independent of sequence length. While optimized for GPUs,
FlashAttention's simplicity makes it amenable to direct hardware **acceleration**.
This work re-evaluates the core FlashAttention kernel, presenting FLASH-D a
mathematically equivalent, yet simplified, formulation that achieves: (a)
hiding softmax division within other non-linear function evaluations; (b)
inherently numerically stable computation of exponentials, eliminating the need
for maximum value subtraction; and (c) a reduction in computational cost
without introducing numerical approximations to the FlashAttention kernel.
Importantly, the essential FlashAttention properties that facilitate efficient
tiled implementation are fully preserved. Hardware implementation results at
28nm demonstrate that this proposed formulation achieves a 22.8% reduction in
area and a 20.3% reduction in power, on average, compared to state-of-the-art
parallel hardware architectures without any performance penalty.


## Capturing the Effects of Quantization on Trojans in Code LLMs

>Authors: Aftab Hussain, Sadegh AlMahdi Kazemi Zarkouei, Md Rafiqul Islam Rabin, Mohammad Amin Alipour, Sen Lin, Bowen Xu

>2025-05-20

> http://arxiv.org/abs/2505.14200v1

Large language models of code exhibit high capability in performing diverse
software engineering tasks, such as code translation, defect detection,
text-to-code generation, and code summarization. While their ability to enhance
developer productivity has spurred widespread use, these models have also seen
substantial growth in size, often reaching billions of parameters. This scale
demands efficient memory resource usage, prompting practitioners to use
optimization techniques such as model **quantization**. Quantization uses smaller
bit representations for the model parameters, reducing the precision of the
weights. In this work, we investigate the impact of **quantization** on the risk of
data poisoning attacks on these models, specifically examining whether it
mitigates or exacerbates such vulnerabilities. We focus on two large language
models, Meta's Llama-2-7b and CodeLlama-7b, applied to an SQL code generation
task. Additionally, we introduce a new metric for measuring trojan signals in
compromised models. We find that **quantization** has differing effects on
code-generating LLMs: while reducing precision does not significantly alter
Llama-2's behavior, it boosts performance and reduces attack success rates in
CodeLlama, particularly at 4-bit precision.


## M3Depth Wavelet-Enhanced Depth Estimation on Mars via Mutual Boosting of Dual-Modal Data

>Authors: Junjie Li, Jiawei Wang, Miyu Li, Yu Liu, Yumei Wang, Haitao Xu

>2025-05-20

> http://arxiv.org/abs/2505.14159v1

Depth estimation plays a great potential role in obstacle avoidance and
navigation for further Mars exploration missions. Compared to traditional
stereo matching, learning-based stereo depth estimation provides a data-driven
approach to infer dense and precise depth maps from stereo image pairs.
However, these methods always suffer performance degradation in environments
with **sparse** textures and lacking geometric constraints, such as the
unstructured terrain of Mars. To address these challenges, we propose M3Depth,
a depth estimation model tailored for Mars rovers. Considering the **sparse** and
smooth texture of Martian terrain, which is primarily composed of low-frequency
features, our model incorporates a convolutional kernel based on wavelet
transform that effectively captures low-frequency response and expands the
receptive field. Additionally, we introduce a consistency loss that explicitly
models the complementary relationship between depth map and surface normal map,
utilizing the surface normal as a geometric constraint to enhance the accuracy
of depth estimation. Besides, a pixel-wise refinement module with mutual
boosting mechanism is designed to iteratively refine both depth and surface
normal predictions. Experimental results on synthetic Mars datasets with depth
annotations show that M3Depth achieves a significant 16% improvement in depth
estimation accuracy compared to other state-of-the-art methods in depth
estimation. Furthermore, the model demonstrates strong applicability in
real-world Martian scenarios, offering a promising solution for future Mars
exploration missions.


## MultiHal Multilingual Dataset for Knowledge-Graph Grounded Evaluation of LLM Hallucinations

>Authors: Ernests Lavrinovics, Russa Biswas, Katja Hose, Johannes Bjerva

>2025-05-20

> http://arxiv.org/abs/2505.14101v1

Large Language Models (LLMs) have inherent limitations of faithfulness and
factuality, commonly referred to as hallucinations. Several benchmarks have
been developed that provide a test bed for factuality evaluation within the
context of English-centric datasets, while relying on supplementary informative
context like web links or text passages but ignoring the available structured
factual resources. To this end, Knowledge Graphs (KGs) have been identified as
a useful aid for hallucination mitigation, as they provide a structured way to
represent the facts about entities and their relations with minimal linguistic
overhead. We bridge the lack of KG paths and multilinguality for factual
language modeling within the existing hallucination evaluation benchmarks and
propose a KG-based multilingual, multihop benchmark called \textbf{MultiHal}
framed for generative text evaluation. As part of our data collection pipeline,
we mined 140k KG-paths from open-domain KGs, from which we pruned noisy
KG-paths, curating a high-quality subset of 25.9k. Our baseline evaluation
shows an absolute scale increase by approximately 0.12 to 0.36 points for the
semantic similarity score in KG-RAG over vanilla QA across multiple languages
and multiple models, demonstrating the potential of KG integration. We
anticipate MultiHal will foster future research towards several graph-based
hallucination mitigation and fact-checking tasks.


## CE-LSLM Efficient Large-Small Language Model Inference and Communication via Cloud-Edge Collaboration

>Authors: Pengyan Zhu, Tingting Yang

>2025-05-20

> http://arxiv.org/abs/2505.14085v1

Emerging intelligent service scenarios in 6G communication impose stringent
requirements for low latency, high reliability, and privacy preservation.
Generative large language models (LLMs) are gradually becoming key enablers for
the integration of semantic communication and computation. However, due to the
limited computational resources of edge devices and the increasing complexity
of heterogeneous terminal access, existing centralized inference approaches
fail to meet the dual demands of response efficiency and data privacy in
edge-side inference tasks. To address these challenges, this paper proposes a
novel collaborative inference architecture that integrates cloud-based LLMs
with edge-deployed small language models (SLMs), enabling dynamic scheduling
and sharing of semantic-level intermediate states, and establishing a unified
computation-communication paradigm tailored for 6G networks. Specifically, a
key-value (**KV**) cache reuse mechanism is introduced to enhance the semantic
understanding of edge models through contextual guidance from the cloud, while
significantly reducing edge-side computational and storage overhead.
Furthermore, a cross-node parallel scheduling mechanism is proposed to achieve
asynchronous coordination between model state loading and decoding computation,
thereby improving edge responsiveness. In addition, we investigate layer
alignment and representation compression strategies between heterogeneous
models to alleviate the communication burden on the edge. Experimental results
demonstrate that the proposed architecture exhibits superior adaptability and
scalability in terms of inference latency, system stability, and concurrent
processing capacity.


## Quantum Internet, Governance, Trust, and the Promise of Secure Communication On building a Quantum Internet that will be used

>Authors: Pieter E. Vermaas, Luca Possati, Zeki C. Seskir

>2025-05-20

> http://arxiv.org/abs/2505.15852v1

The development of quantum technologies has been accelerating in the last
decade, turning them into emerging technologies that need explicit attention by
decision-makers at national funding agencies, companies and governments. In
this paper we consider the governance of quantum internet, a new type of
communication network developed for the promise that it can deliver inherently
secure communication solely through its technical design. This paper gives a
general analysis of the functions quantum internet offer, and then challenges
this proposition by arguing that trust, an essential precondition for users to
adopt this technology, cannot be guaranteed by technical features alone.
Instead, trust is fundamentally a social phenomenon, shaped by how quantum
internet is governed, operated, and regulated. Therefore, the ultimate success
of quantum internet in fulfilling its promise of secure communication will
depend not just on its technical hardware and software but also on the policies
and frameworks for running these capacities, and on the public trust in those
policies and frameworks. With this argument we arrive at recommendations to
decision-makers for developing quantum internet and its governance in a way
that quantum internet that can be trusted for its promised capacities.


## Textual Steering Vectors Can Improve Visual Understanding in Multimodal Large Language Models

>Authors: Woody Haosheng Gan, Deqing Fu, Julian Asilis, Ollie Liu, Dani Yogatama, Vatsal Sharan, Robin Jia, Willie Neiswanger

>2025-05-20

> http://arxiv.org/abs/2505.14071v1

Steering methods have emerged as effective and targeted tools for guiding
large language models' (LLMs) behavior without modifying their parameters.
Multimodal large language models (MLLMs), however, do not currently enjoy the
same suite of techniques, due in part to their recency and architectural
diversity. Inspired by this gap, we investigate whether MLLMs can be steered
using vectors derived from their text-only LLM backbone, via **sparse**
autoencoders (SAEs), mean shift, and linear probing. We find that text-derived
steering consistently enhances multimodal accuracy across diverse MLLM
architectures and visual tasks. In particular, mean shift boosts spatial
relationship accuracy on CV-Bench by up to +7.3% and counting accuracy by up to
+3.3%, outperforming prompting and exhibiting strong generalization to
out-of-distribution datasets. These results highlight textual steering vectors
as a powerful, efficient mechanism for enhancing grounding in MLLMs with
minimal additional data collection and computational overhead.


## Process vs. Outcome Reward Which is Better for Agentic RAG Reinforcement Learning

>Authors: Wenlin Zhang, Xiangyang Li, Kuicai Dong, Yichao Wang, Pengyue Jia, Xiaopeng Li, Yingyi Zhang, Derong Xu, Zhaocheng Du, Huifeng Guo, Ruiming Tang, Xiangyu Zhao

>2025-05-20

> http://arxiv.org/abs/2505.14069v2

Retrieval-augmented generation (RAG) enhances the text generation
capabilities of large language models (LLMs) by integrating external knowledge
and up-to-date information. However, traditional RAG systems are limited by
static workflows and lack the adaptability required for multistep reasoning and
complex task management. To address these limitations, agentic RAG systems
(e.g., DeepResearch) have been proposed, enabling dynamic retrieval strategies,
iterative context refinement, and adaptive workflows for handling complex
search queries beyond the capabilities of conventional RAG. Recent advances,
such as Search-R1, have demonstrated promising gains using outcome-based
reinforcement learning, where the correctness of the final answer serves as the
reward signal. Nevertheless, such outcome-supervised agentic RAG methods face
challenges including low exploration efficiency, gradient conflict, and **sparse**
reward signals. To overcome these challenges, we propose to utilize
fine-grained, process-level rewards to improve training stability, reduce
computational costs, and enhance efficiency. Specifically, we introduce a novel
method ReasonRAG that automatically constructs RAG-ProGuide, a high-quality
dataset providing process-level rewards for (i) query generation, (ii) evidence
extraction, and (iii) answer generation, thereby enhancing model inherent
capabilities via process-supervised reinforcement learning. With the
process-level policy optimization, the proposed framework empowers LLMs to
autonomously invoke search, generate queries, extract relevant evidence, and
produce final answers. Compared to existing approaches such as Search-R1 and
traditional RAG systems, ReasonRAG, leveraging RAG-ProGuide, achieves superior
performance on five benchmark datasets using only 5k training instances,
significantly fewer than the 90k training instances required by Search-R1.


## Unsupervised Graph Clustering with Deep Structural Entropy

>Authors: Jingyun Zhang, Hao Peng, Li Sun, Guanlin Wu, Chunyang Liu, Zhengtao Yu

>2025-05-20

> http://arxiv.org/abs/2505.14040v1

Research on Graph Structure Learning (GSL) provides key insights for
graph-based clustering, yet current methods like Graph Neural Networks (GNNs),
Graph Attention Networks (GATs), and contrastive learning often rely heavily on
the original graph structure. Their performance deteriorates when the original
graph's adjacency matrix is too **sparse** or contains noisy edges unrelated to
clustering. Moreover, these methods depend on learning node embeddings and
using traditional techniques like k-means to form clusters, which may not fully
capture the underlying graph structure between nodes. To address these
limitations, this paper introduces DeSE, a novel unsupervised graph clustering
framework incorporating Deep Structural Entropy. It enhances the original graph
with quantified structural information and deep neural networks to form
clusters. Specifically, we first propose a method for calculating structural
entropy with soft assignment, which quantifies structure in a differentiable
form. Next, we design a Structural Learning layer (SLL) to generate an
attributed graph from the original feature data, serving as a target to enhance
and optimize the original structural graph, thereby mitigating the issue of
**sparse** connections between graph nodes. Finally, our clustering assignment
method (ASS), based on GNNs, learns node embeddings and a soft assignment
matrix to cluster on the enhanced graph. The ASS layer can be stacked to meet
downstream task requirements, minimizing structural entropy for stable
clustering and maximizing node consistency with edge-based cross-entropy loss.
Extensive comparative experiments are conducted on four benchmark datasets
against eight representative unsupervised graph clustering baselines,
demonstrating the superiority of the DeSE in both effectiveness and
interpretability.


## Towards Efficient Multi-Scale Deformable Attention on NPU

>Authors: Chenghuan Huang, Zhigeng Xu, Chong Sun, Chen Li, Ziyang Ma

>2025-05-20

> http://arxiv.org/abs/2505.14022v1

Multi-scale deformable attention (MSDA) is a flexible and powerful feature
extraction mechanism for visual tasks, but its random-access grid sampling
strategy poses significant optimization challenges, especially on
domain-specific accelerators such as NPUs. In this work, we present a co-design
approach that systematically rethinks memory access and computation strategies
for MSDA on the Ascend NPU architecture. With this co-design approach, our
implementation supports both efficient forward and backward computation, is
fully adapted for training workloads, and incorporates a suite of
hardware-aware optimizations. Extensive experiments show that our solution
achieves up to $5.9\times$ (forward), $8.9\times$ (backward), and $7.3\times$
(end-to-end training) speedup over the grid sample-based baseline, and
$1.9\times$, $2.4\times$, and $2.0\times$ **acceleration** over the latest vendor
library, respectively.


## Quaff Quantized Parameter-Efficient Fine-Tuning under Outlier Spatial Stability Hypothesis

>Authors: Hong Huang, Dapeng Wu

>2025-05-20

> http://arxiv.org/abs/2505.14742v1

Large language models (LLMs) have made exciting achievements across various
domains, yet their deployment on resource-constrained personal devices remains
hindered by the prohibitive computational and memory demands of task-specific
fine-tuning. While **quantization** offers a pathway to efficiency, existing
methods struggle to balance performance and overhead, either incurring high
computational/memory costs or failing to address activation outliers, a
critical bottleneck in **quantize**d fine-tuning. To address these challenges, we
propose the Outlier Spatial Stability Hypothesis (OSSH): During fine-tuning,
certain activation outlier channels retain stable spatial positions across
training iterations. Building on OSSH, we propose Quaff, a Quantized
parameter-efficient fine-tuning framework for LLMs, optimizing low-precision
activation representations through targeted momentum scaling. Quaff dynamically
suppresses outliers exclusively in invariant channels using lightweight
operations, eliminating full-precision weight storage and global rescaling
while reducing **quantization** errors. Extensive experiments across ten benchmarks
validate OSSH and demonstrate Quaff's efficacy. Specifically, on the GPQA
reasoning benchmark, Quaff achieves a 1.73x latency reduction and 30% memory
savings over full-precision fine-tuning while improving accuracy by 0.6% on the
Phi-3 model, reconciling the triple trade-off between efficiency, performance,
and deployability. By enabling consumer-grade GPU fine-tuning (e.g., RTX 2080
Super) without sacrificing model utility, Quaff democratizes personalized LLM
deployment. The code is available at https://github.com/Little0o0/Quaff.git.


## UHD Image Dehazing via anDehazeFormer with Atmospheric-aware KV Cache

>Authors: Pu Wang, Pengwen Dai, Chen Wu, Yeying Jin, Dianjie Lu, Guijuan Zhang, Youshan Zhang, Zhuoran Zheng

>2025-05-20

> http://arxiv.org/abs/2505.14010v1

In this paper, we propose an efficient visual transformer framework for
ultra-high-definition (UHD) image dehazing that addresses the key challenges of
slow training speed and high memory consumption for existing methods. Our
approach introduces two key innovations: 1) an \textbf{a}daptive
\textbf{n}ormalization mechanism inspired by the nGPT architecture that enables
ultra-fast and stable training with a network with a restricted range of
parameter expressions; and 2) we devise an atmospheric scattering-aware **KV**
caching mechanism that dynamically optimizes feature preservation based on the
physical haze formation model. The proposed architecture improves the training
convergence speed by \textbf{5 $\times$} while reducing memory overhead,
enabling real-time processing of 50 high-resolution images per second on an
RTX4090 GPU. Experimental results show that our approach maintains
state-of-the-art dehazing quality while significantly improving computational
efficiency for 4K/8K image restoration tasks. Furthermore, we provide a new
dehazing image interpretable method with the help of an integrated gradient
attribution map. Our code can be found here:
https://anonymous.4open.science/r/anDehazeFormer-632E/README.md.


## R&D-Agent Automating Data-Driven AI Solution Building Through LLM-Powered Automated Research, Development, and Evolution

>Authors: Xu Yang, Xiao Yang, Shikai Fang, Bowen Xian, Yuante Li, Jian Wang, Minrui Xu, Haoran Pan, Xinpeng Hong, Weiqing Liu, Yelong Shen, Weizhu Chen, Jiang Bian

>2025-05-20

> http://arxiv.org/abs/2505.14738v1

Recent advances in AI and ML have transformed data science, yet increasing
complexity and expertise requirements continue to hinder progress. While
crowdsourcing platforms alleviate some challenges, high-level data science
tasks remain labor-intensive and iterative. To overcome these limitations, we
introduce R&D-Agent, a dual-agent framework for iterative exploration. The
Researcher agent uses performance feedback to generate ideas, while the
Developer agent refines code based on error feedback. By enabling multiple
parallel exploration traces that merge and enhance one another, R&D-Agent
narrows the gap between automated solutions and expert-level performance.
Evaluated on MLE-Bench, R&D-Agent emerges as the top-performing machine
learning engineering agent, demonstrating its potential to accelerate
innovation and improve precision across diverse data science applications. We
have open-sourced R&D-Agent on GitHub: https://github.com/microsoft/RD-Agent.


## Through a Compressed Lens Investigating the Impact of Quantization on LLM Explainability and Interpretability

>Authors: Qianli Wang, Mingyang Wang, Nils Feldhus, Simon Ostermann, Yuan Cao, Hinrich Schütze, Sebastian Möller, Vera Schmitt

>2025-05-20

> http://arxiv.org/abs/2505.13963v1

Quantization methods are widely used to accelerate inference and streamline
the deployment of large language models (LLMs). While prior research has
extensively investigated the degradation of various LLM capabilities due to
**quantization**, its effects on model explainability and interpretability, which
are crucial for understanding decision-making processes, remain unexplored. To
address this gap, we conduct comprehensive experiments using three common
**quantization** techniques at distinct bit widths, in conjunction with two
explainability methods, counterfactual examples and natural language
explanations, as well as two interpretability approaches, knowledge
memorization analysis and latent multi-hop reasoning analysis. We complement
our analysis with a thorough user study, evaluating selected explainability
methods. Our findings reveal that, depending on the configuration, **quantization**
can significantly impact model explainability and interpretability. Notably,
the direction of this effect is not consistent, as it strongly depends on (1)
the **quantization** method, (2) the explainability or interpretability approach,
and (3) the evaluation protocol. In some settings, human evaluation shows that
**quantization** degrades explainability, while in others, it even leads to
improvements. Our work serves as a cautionary tale, demonstrating that
**quantization** can unpredictably affect model transparency. This insight has
important implications for deploying LLMs in applications where transparency is
a critical requirement.


## Leveraging Multivariate Long-Term History Representation for Time Series Forecasting

>Authors: Huiliang Zhang, Di Wu, Arnaud Zinflou, Stephane Dellacherie, Mouhamadou Makhtar Dione, Benoit Boulet

>2025-05-20

> http://arxiv.org/abs/2505.14737v1

Multivariate Time Series (MTS) forecasting has a wide range of applications
in both industry and academia. Recent advances in Spatial-Temporal Graph Neural
Network (STGNN) have achieved great progress in modelling spatial-temporal
correlations. Limited by computational complexity, most STGNNs for MTS
forecasting focus primarily on short-term and local spatial-temporal
dependencies. Although some recent methods attempt to incorporate univariate
history into modeling, they still overlook crucial long-term spatial-temporal
similarities and correlations across MTS, which are essential for accurate
forecasting. To fill this gap, we propose a framework called the Long-term
Multivariate History Representation (LMHR) Enhanced STGNN for MTS forecasting.
Specifically, a Long-term History Encoder (LHEncoder) is adopted to effectively
encode the long-term history into segment-level contextual representations and
reduce point-level noise. A non-parametric Hierarchical Representation
Retriever (HRetriever) is designed to include the spatial information in the
long-term spatial-temporal dependency modelling and pick out the most valuable
representations with no additional training. A Transformer-based Aggregator
(TAggregator) selectively fuses the **sparse**ly retrieved contextual
representations based on the ranking positional embedding efficiently.
Experimental results demonstrate that LMHR outperforms typical STGNNs by 10.72%
on the average prediction horizons and state-of-the-art methods by 4.12% on
several real-world datasets. Additionally, it consistently improves prediction
accuracy by 9.8% on the top 10% of rapidly changing patterns across the
datasets.


## Reasoning Path Compression Compressing Generation Trajectories for Efficient LLM Reasoning

>Authors: Jiwon Song, Dongwon Jo, Yulhwa Kim, Jae-Joon Kim

>2025-05-20

> http://arxiv.org/abs/2505.13866v1

Recent reasoning-focused language models achieve high accuracy by generating
lengthy intermediate reasoning paths before producing final answers. While this
approach is effective in solving problems that require logical thinking, long
reasoning paths significantly increase memory usage and throughput of token
generation, limiting the practical deployment of such models. We propose
Reasoning Path Compression (RPC), a training-free method that accelerates
inference by leveraging the semantic **sparsity** of reasoning paths. RPC
periodically compresses the **KV** cache by retaining **KV** cache that receive high
importance score, which are computed using a selector window composed of
recently generated queries. Experiments show that RPC improves generation
throughput of QwQ-32B by up to 1.60$\times$ compared to the inference with full
**KV** cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our
findings demonstrate that semantic **sparsity** in reasoning traces can be
effectively exploited for compression, offering a practical path toward
efficient deployment of reasoning LLMs. Our code is available at
https://github.com/jiwonsong-dev/ReasoningPathCompression.


## Learning Spatio-Temporal Dynamics for Trajectory Recovery via Time-Aware Transformer

>Authors: Tian Sun, Yuqi Chen, Baihua Zheng, Weiwei Sun

>2025-05-20

> http://arxiv.org/abs/2505.13857v1

In real-world applications, GPS trajectories often suffer from low sampling
rates, with large and irregular intervals between consecutive GPS points. This
**sparse** characteristic presents challenges for their direct use in GPS-based
systems. This paper addresses the task of map-constrained trajectory recovery,
aiming to enhance trajectory sampling rates of GPS trajectories. Previous
studies commonly adopt a sequence-to-sequence framework, where an encoder
captures the trajectory patterns and a decoder reconstructs the target
trajectory. Within this framework, effectively representing the road network
and extracting relevant trajectory features are crucial for overall
performance. Despite advancements in these models, they fail to fully leverage
the complex spatio-temporal dynamics present in both the trajectory and the
road network.
  To overcome these limitations, we categorize the spatio-temporal dynamics of
trajectory data into two distinct aspects: spatial-temporal traffic dynamics
and trajectory dynamics. Furthermore, We propose TedTrajRec, a novel method for
trajectory recovery. To capture spatio-temporal traffic dynamics, we introduce
PD-GNN, which models periodic patterns and learns topologically aware dynamics
concurrently for each road segment. For spatio-temporal trajectory dynamics, we
present TedFormer, a time-aware Transformer that incorporates temporal dynamics
for each GPS location by integrating closed-form neural ordinary differential
equations into the attention mechanism. This allows TedFormer to effectively
handle irregularly sampled data. Extensive experiments on three real-world
datasets demonstrate the superior performance of TedTrajRec. The code is
publicly available at https://github.com/ysygMhdxw/TEDTrajRec/.


## EfficientLLM Efficiency in Large Language Models

>Authors: Zhengqing Yuan, Weixiang Sun, Yixin Liu, Huichi Zhou, Rong Zhou, Yiyang Li, Zheyuan Zhang, Wei Song, Yue Huang, Haolong Jia, Keerthiram Murugesan, Yu Wang, Lifang He, Jianfeng Gao, Lichao Sun, Yanfang Ye

>2025-05-20

> http://arxiv.org/abs/2505.13840v1

Large Language Models (LLMs) have driven significant progress, yet their
growing parameter counts and context windows incur prohibitive compute, energy,
and monetary costs. We introduce EfficientLLM, a novel benchmark and the first
comprehensive empirical study evaluating efficiency techniques for LLMs at
scale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our
study systematically explores three key axes: (1) architecture pretraining
(efficient attention variants: MQA, GQA, MLA, NSA; **sparse** Mixture-of-Experts
(MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and
(3) inference (**quantization** methods: int4, float16). We define six fine-grained
metrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy
Consumption, Compression Rate) to capture hardware saturation,
latency-throughput balance, and carbon cost. Evaluating over 100
model-technique pairs (0.5B-72B parameters), we derive three core insights: (i)
Efficiency involves quantifiable trade-offs: no single method is universally
optimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by
40%, while int4 **quantization** cuts memory/energy by up to 3.9x at a 3-5%
accuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal
memory-latency trade-offs for constrained devices, MLA achieves lowest
perplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency
only beyond 14B parameters. (iii) Techniques generalize across modalities: we
extend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and
Vision-Language Models (Qwen2.5-VL), confirming effective transferability. By
open-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM
provides essential guidance for researchers and engineers navigating the
efficiency-performance landscape of next-generation foundation models.


## Predicting Neo-Adjuvant Chemotherapy Response in Triple-Negative Breast Cancer Using Pre-Treatment Histopathologic Images

>Authors: Hikmat Khan, Ziyu Su, Huina Zhang, Yihong Wang, Bohan Ning, Shi Wei, Hua Guo, Zaibo Li, Muhammad Khalid Khan Niazi

>2025-05-20

> http://arxiv.org/abs/2505.14730v1

Triple-negative breast cancer (TNBC) is an aggressive subtype defined by the
lack of estrogen receptor (ER), progesterone receptor (PR), and human epidermal
growth factor receptor 2 (HER2) expression, resulting in limited targeted
treatment options. Neoadjuvant chemotherapy (NACT) is the standard treatment
for early-stage TNBC, with pathologic complete response (pCR) serving as a key
prognostic marker; however, only 40-50% of patients with TNBC achieve pCR.
Accurate prediction of NACT response is crucial to optimize therapy, avoid
ineffective treatments, and improve patient outcomes. In this study, we
developed a deep learning model to predict NACT response using pre-treatment
hematoxylin and eosin (H&E)-stained biopsy images. Our model achieved promising
results in five-fold cross-validation (accuracy: 82%, AUC: 0.86, F1-score:
0.84, sensitivity: 0.85, specificity: 0.81, precision: 0.80). Analysis of model
attention maps in conjunction with multiplexed immunohistochemistry (mIHC) data
revealed that regions of high predictive importance consistently colocalized
with tumor areas showing elevated PD-L1 expression, CD8+ T-cell infiltration,
and CD163+ macrophage density - all established biomarkers of treatment
response. Our findings indicate that incorporating IHC-derived immune profiling
data could substantially improve model interpretability and predictive
performance. Furthermore, this approach may accelerate the discovery of novel
histopathological biomarkers for NACT and advance the development of
personalized treatment strategies for TNBC patients.


## Lorentz force on superconducting vortices near line defects

>Authors: Ruby A. Shi

>2025-05-20

> http://arxiv.org/abs/2505.13798v1

In type-II superconductors, magnetic flux penetrates in the form of **quantize**d
vortices whose dissipative motion, driven by the Lorentz force, can degrade
superconductivity. Understanding vortex dynamics in both homogeneous regions
and near unavoidable structural defects is crucial for superconducting
applications. This study examines a scenario in which a superconducting quantum
interference device (SQUID) scans across a thin-film superconductor containing
line defects. We first estimate the radial Lorentz force on a vortex in a
homogeneous region using both analytical methods and numerical simulations
based on the fast Fourier transform algorithm. For a film with a Pearl length
of 400 micrometers and a SQUID height of 4 micrometers, we find that the SQUID
tip can exert a force of approximately 3 femtonewtons on a vortex. We then
evaluate the Lorentz force on vortices near two parallel line defects. Our
results show that the Lorentz force is enhanced for vortices pinned on or
between line defects. Vortices pinned on the line defects experience force
enhancement predominantly perpendicular to the defects, while vortices in
between experience enhancement along the defect direction. These findings
enable more accurate estimation of Lorentz forces on vortices near line defects
in thin-film superconductors and contribute to the broader understanding of
vortex pinning and dynamics in defect-engineered superconductors. The methods
can be extended to bulk superconductors and generalized to other defect
geometries.


## Causal Head Gating A Framework for Interpreting Roles of Attention Heads in Transformers

>Authors: Andrew Nam, Henry Conklin, Yukang Yang, Thomas Griffiths, Jonathan Cohen, Sarah-Jane Leslie

>2025-05-19

> http://arxiv.org/abs/2505.13737v1

We present causal head gating (CHG), a scalable method for interpreting the
functional roles of attention heads in transformer models. CHG learns soft
gates over heads and assigns them a causal taxonomy - facilitating,
interfering, or irrelevant - based on their impact on task performance. Unlike
prior approaches in mechanistic interpretability, which are hypothesis-driven
and require prompt templates or target labels, CHG applies directly to any
dataset using standard next-token prediction. We evaluate CHG across multiple
large language models (LLMs) in the Llama 3 model family and diverse tasks,
including syntax, commonsense, and mathematical reasoning, and show that CHG
scores yield causal - not merely correlational - insight, validated via
ablation and causal mediation analyses. We also introduce contrastive CHG, a
variant that isolates sub-circuits for specific task components. Our findings
reveal that LLMs contain multiple **sparse**, sufficient sub-circuits, that
individual head roles depend on interactions with others (low modularity), and
that instruction following and in-context learning rely on separable
mechanisms.


## Nonlinear Nonlocal Comparing A. O. Barut's Theory to Mine with special emphasis on That Dot on the Screen

>Authors: W. David Wick

>2025-05-19

> http://arxiv.org/abs/2505.13704v1

In the 1980's and 90's, A. O Barut and colleagues developed a nonperturbative
approach to electrodynamics eschewing so-called ``second-**quantization**". Based
on incorporation of self-energy terms, the resulting nonlinear and nonlocal
theory explained many well-known phenomena of atomic and radiation physics. In
2017, this author introduced a nonlinear, nonlocal theory with the intent of
resolving the Measurement Problem. Barut also suggested that his theory
resolved such paradoxes. Here I compare the two theories with special attention
to That Dot on the Screen.


## Fine-tuning Quantized Neural Networks with Zeroth-order Optimization

>Authors: Sifeng Shang, Jiayi Zhou, Chenyu Lin, Minxian Li, Kaiyang Zhou

>2025-05-19

> http://arxiv.org/abs/2505.13430v1

As the size of large language models grows exponentially, GPU memory has
become a bottleneck for adapting these models to downstream tasks. In this
paper, we aim to push the limits of memory-efficient training by minimizing
memory usage on model weights, gradients, and optimizer states, within a
unified framework. Our idea is to eliminate both gradients and optimizer states
using zeroth-order optimization, which approximates gradients by perturbing
weights during forward passes to identify gradient directions. To minimize
memory usage on weights, we employ model **quantization**, e.g., converting from
bfloat16 to int4. However, directly applying zeroth-order optimization to
**quantize**d weights is infeasible due to the precision gap between discrete
weights and continuous gradients, which would otherwise require de-**quantization**
and re-**quantization**. To overcome this challenge, we propose Quantized
Zeroth-order Optimization (QZO), a novel approach that perturbs the continuous
**quantization** scale for gradient estimation and uses a directional derivative
clipping method to stabilize training. QZO is orthogonal to both scalar-based
and codebook-based post-training **quantization** methods. Compared to
full-parameter fine-tuning in bfloat16, QZO can reduce the total memory cost by
more than 18$\times$ for 4-bit LLMs, and enables fine-tuning Llama-2-13B and
Stable Diffusion 3.5 Large within a single 24GB GPU.


## Faster Video Diffusion with Trainable Sparse Attention

>Authors: Peiyuan Zhang, Haofeng Huang, Yongqi Chen, Will Lin, Zhengzhong Liu, Ion Stoica, Eric Xing, Hao Zhang

>2025-05-19

> http://arxiv.org/abs/2505.13389v2

Scaling video diffusion transformers (DiTs) is limited by their quadratic 3D
attention, even though most of the attention mass concentrates on a small
subset of positions. We turn this observation into VSA, a trainable,
hardware-efficient **sparse** attention that replaces full attention at \emph{both}
training and inference. In VSA, a lightweight coarse stage pools tokens into
tiles and identifies high-weight \emph{critical tokens}; a fine stage computes
token-level attention only inside those tiles subjecting to block computing
layout to ensure hard efficiency. This leads to a single differentiable kernel
that trains end-to-end, requires no post-hoc profiling, and sustains 85\% of
FlashAttention3 MFU. We perform a large sweep of ablation studies and
scaling-law experiments by pretraining DiTs from 60M to 1.4B parameters. VSA
reaches a Pareto point that cuts training FLOPS by 2.53$\times$ with no drop in
diffusion loss. Retrofitting the open-source Wan-2.1 model speeds up attention
time by 6$\times$ and lowers end-to-end generation time from 31s to 18s with
comparable quality. These results establish trainable **sparse** attention as a
practical alternative to full attention and a key enabler for further scaling
of video diffusion models.


## Occult Optimizing Collaborative Communication across Experts for Accelerated Parallel MoE Training and Inference

>Authors: Shuqing Luo, Pingzhi Li, Jie Peng, Hanrui Wang, Yang, Zhao, Yu, Cao, Yu Cheng, Tianlong Chen

>2025-05-19

> http://arxiv.org/abs/2505.13345v1

Mixture-of-experts (MoE) architectures could achieve impressive computational
efficiency with expert parallelism, which relies heavily on all-to-all
communication across devices. Unfortunately, such communication overhead
typically constitutes a significant portion of the total runtime, hampering the
scalability of distributed training and inference for modern MoE models
(consuming over $40\%$ runtime in large-scale training). In this paper, we
first define collaborative communication to illustrate this intrinsic
limitation, and then propose system- and algorithm-level innovations to reduce
communication costs. Specifically, given a pair of experts co-activated by one
token, we call them "collaborated", which comprises $2$ cases as intra- and
inter-collaboration, depending on whether they are kept on the same device. Our
pilot investigations reveal that augmenting the proportion of
intra-collaboration can accelerate expert parallelism at scale. It motivates us
to strategically optimize collaborative communication for accelerated MoE
training and inference, dubbed Occult. Our designs are capable of either
delivering exact results with reduced communication cost or controllably
minimizing the cost with collaboration **pruning**, materialized by modified
fine-tuning. Comprehensive experiments on various MoE-LLMs demonstrate that
Occult can be faster than popular state-of-the-art inference or training
frameworks (more than $1.5\times$ speed up across multiple tasks and models)
with comparable or superior quality compared to the standard fine-tuning. Code
is available at
$\href{https://github.com/UNITES-Lab/Occult}{https://github.com/UNITES-Lab/Occult}$.


## Thinking Short and Right Over Thinking Long Serving LLM Reasoning Efficiently and Accurately

>Authors: Yuhang Wang, Youhe Jiang, Bin Cui, Fangcheng Fu

>2025-05-19

> http://arxiv.org/abs/2505.13326v1

Recent advances in test-time scaling suggest that Large Language Models
(LLMs) can gain better capabilities by generating Chain-of-Thought reasoning
(analogous to human thinking) to respond a given request, and meanwhile
exploring more reasoning branches (i.e., generating multiple responses and
ensembling them) can improve the final output quality. However, when
incorporating the two scaling dimensions, we find that the system efficiency is
dampened significantly for two reasons. Firstly, the time cost to generate the
final output increases substantially as many reasoning branches would be
trapped in the over-thinking dilemma, producing excessively long responses.
Secondly, generating multiple reasoning branches for each request increases
memory consumption, which is unsuitable for LLM serving since we can only batch
a limited number of requests to process simultaneously. To address this, we
present SART, a serving framework for efficient and accurate LLM reasoning. The
essential idea is to manage the thinking to be short and right, rather than
long. For one thing, we devise a redundant sampling with early stopping
approach based on empirical observations and theoretic analysis, which
increases the likelihood of obtaining short-thinking responses when sampling
reasoning branches. For another, we propose to dynamically prune low-quality
branches so that only right-thinking branches are maintained, reducing the
memory consumption and allowing us to batch more requests. Experimental results
demonstrate that SART not only improves the accuracy of LLM reasoning but also
enhances the serving efficiency, outperforming existing methods by up to 28.2
times and on average 15.7 times in terms of efficiency when achieving the same
level of accuracy.


## HeteroSpec Leveraging Contextual Heterogeneity for Efficient Speculative Decoding

>Authors: Siran Liu, Yang Ye, Qianchao Zhu, Zheng Cao, Yongchao He

>2025-05-19

> http://arxiv.org/abs/2505.13254v1

Autoregressive decoding, the standard approach for Large Language Model (LLM)
inference, remains a significant bottleneck due to its sequential nature. While
speculative decoding algorithms mitigate this inefficiency through parallel
verification, they fail to exploit the inherent heterogeneity in linguistic
complexity, a key factor leading to suboptimal resource allocation. We address
this by proposing HeteroSpec, a heterogeneity-adaptive speculative decoding
framework that dynamically optimizes computational resource allocation based on
linguistic context complexity. HeteroSpec introduces two key mechanisms: (1) A
novel cumulative meta-path Top-$K$ entropy metric for efficiently identifying
predictable contexts. (2) A dynamic resource allocation strategy based on
data-driven entropy partitioning, enabling adaptive speculative expansion and
**pruning** tailored to local context difficulty. Evaluated on five public
benchmarks and four models, HeteroSpec achieves an average speedup of
4.26$\times$. It consistently outperforms state-of-the-art EAGLE-3 across
speedup rates, average acceptance length, and verification cost. Notably,
HeteroSpec requires no draft model retraining, incurs minimal overhead, and is
orthogonal to other **acceleration** techniques. It demonstrates enhanced
**acceleration** with stronger draft models, establishing a new paradigm for
context-aware LLM inference **acceleration**.


## Learning Wavelet-Sparse FDK for 3D Cone-Beam CT Reconstruction

>Authors: Yipeng Sun, Linda-Sophie Schneider, Chengze Ye, Mingxuan Gu, Siyuan Mei, Siming Bayer, Andreas Maier

>2025-05-19

> http://arxiv.org/abs/2505.13579v1

Cone-Beam Computed Tomography (CBCT) is essential in medical imaging, and the
Feldkamp-Davis-Kress (FDK) algorithm is a popular choice for reconstruction due
to its efficiency. However, FDK is susceptible to noise and artifacts. While
recent deep learning methods offer improved image quality, they often increase
computational complexity and lack the interpretability of traditional methods.
In this paper, we introduce an enhanced FDK-based neural network that maintains
the classical algorithm's interpretability by selectively integrating trainable
elements into the cosine weighting and filtering stages. Recognizing the
challenge of a large parameter space inherent in 3D CBCT data, we leverage
wavelet transformations to create **sparse** representations of the cosine weights
and filters. This strategic sparsification reduces the parameter count by
$93.75\%$ without compromising performance, accelerates convergence, and
importantly, maintains the inference computational cost equivalent to the
classical FDK algorithm. Our method not only ensures volumetric consistency and
boosts robustness to noise, but is also designed for straightforward
integration into existing CT reconstruction pipelines. This presents a
pragmatic enhancement that can benefit clinical applications, particularly in
environments with computational limitations.


## Ocean wave spectrum reconstruction from HF radar data and its application to wave height estimation

>Authors: Kaede Watanabe, Toshiaki Yachimura, Tsubasa Terada, Hiroshi Kameda, Ryuhei Takahashi, Hiroshi Suito

>2025-05-19

> http://arxiv.org/abs/2505.13132v1

Real-time estimation of ocean wave heights using high-frequency (HF) radar
has attracted great attention. This method offers the benefit of easy
maintenance by virtue of its ground-based installation. However, it is
adversely affected by issues such as low estimation accuracy. As described
herein, we propose an algorithm based on the nonnegative **sparse** regularization
method to estimate the energy distribution of the component waves, known as the
ocean wave spectrum, from HF radar data. After proving a stability estimate of
this algorithm, we perform numerical simulations to verify the proposed
method's effectiveness.


## FreeKV Boosting KV Cache Retrieval for Efficient LLM Inference

>Authors: Guangda Liu, Chengwei Li, Zhenyu Ning, Jing Lin, Yiwu Yao, Danning Ke, Minyi Guo, Jieru Zhao

>2025-05-19

> http://arxiv.org/abs/2505.13109v1

Large language models (LLMs) have been widely deployed with rapidly expanding
context windows to support increasingly demanding applications. However, long
contexts pose significant deployment challenges, primarily due to the **KV** cache
whose size grows proportionally with context length. While **KV** cache compression
methods are proposed to address this issue, **KV** dropping methods incur
considerable accuracy loss, and **KV** retrieval methods suffer from significant
efficiency bottlenecks. We propose Free**KV**, an algorithm-system co-optimization
framework to enhance **KV** retrieval efficiency while preserving accuracy. On the
algorithm side, Free**KV** introduces speculative retrieval to shift the **KV**
selection and recall processes out of the critical path, combined with
fine-grained correction to ensure accuracy. On the system side, Free**KV** employs
hybrid **KV** layouts across CPU and GPU memory to eliminate fragmented data
transfers, and leverages double-buffered streamed recall to further improve
efficiency. Experiments demonstrate that Free**KV** achieves near-lossless accuracy
across various scenarios and models, delivering up to 13$\times$ speedup
compared to SOTA **KV** retrieval methods.


## Automatic mixed precision for optimizing gained time with constrained loss mean-squared-error based on model partition to sequential sub-graphs

>Authors: Shmulik Markovich-Golan, Daniel Ohayon, Itay Niv, Yair Hanani

>2025-05-19

> http://arxiv.org/abs/2505.13060v1

Quantization is essential for Neural Network (NN) compression, reducing model
size and computational demands by using lower bit-width data types, though
aggressive reduction often hampers accuracy. Mixed Precision (MP) mitigates
this tradeoff by varying the numerical precision across network layers. This
study focuses on automatically selecting an optimal MP configuration within
Post-Training Quantization (PTQ) for inference. The first key contribution is a
novel sensitivity metric derived from a first-order Taylor series expansion of
the loss function as a function of **quantization** errors in weights and
activations. This metric, based on the Mean Square Error (MSE) of the loss, is
efficiently calculated per layer using high-precision forward and backward
passes over a small calibration dataset. The metric is additive across layers,
with low calibration memory overhead as weight optimization is unnecessary. The
second contribution is an accurate hardware-aware method for predicting MP time
gain by modeling it as additive for sequential sub-graphs. An algorithm
partitions the model graph into sequential subgraphs, measuring time gain for
each configuration using a few samples. After calibrating per-layer sensitivity
and time gain, an Integer Programming (IP) problem is formulated to maximize
time gain while keeping loss MSE below a set threshold. Memory gain and
theoretical time gain based on Multiply and Accumulate (MAC) operations are
also considered. Rigorous experiments on the Intel Gaudi 2 accelerator validate
the approach on several Large Language Models (LLMs).


## Simplicity is Key An Unsupervised Pretraining Approach for Sparse Radio Channels

>Authors: Jonathan Ott, Maximilian Stahlke, Tobias Feigl, Bjoern M. Eskofier, Christopher Mutschler

>2025-05-19

> http://arxiv.org/abs/2505.13055v1

We introduce the Sparse pretrained Radio Transformer (SpaRTran), an
unsupervised representation learning approach based on the concept of
compressed sensing for radio channels. Our approach learns embeddings that
focus on the physical properties of radio propagation, to create the optimal
basis for fine-tuning on radio-based downstream tasks. SpaRTran uses a **sparse**
gated autoencoder that induces a simplicity bias to the learned
representations, resembling the **sparse** nature of radio propagation. For signal
reconstruction, it learns a dictionary that holds atomic features, which
increases flexibility across signal waveforms and spatiotemporal signal
patterns. Our experiments show that SpaRTran reduces errors by up to 85 %
compared to state-of-the-art methods when fine-tuned on radio fingerprinting, a
challenging downstream task. In addition, our method requires less pretraining
effort and offers greater flexibility, as we train it solely on individual
radio signals. SpaRTran serves as an excellent base model that can be
fine-tuned for various radio-based downstream tasks, effectively reducing the
cost for labeling. In addition, it is significantly more versatile than
existing methods and demonstrates superior generalization.


## A Skull-Adaptive Framework for AI-Based 3D Transcranial Focused Ultrasound Simulation

>Authors: Vinkle Srivastav, Juliette Puel, Jonathan Vappou, Elijah Van Houten, Paolo Cabras, Nicolas Padoy

>2025-05-19

> http://arxiv.org/abs/2505.12998v1

Transcranial focused ultrasound (tFUS) is an emerging modality for
non-invasive brain stimulation and therapeutic intervention, offering
millimeter-scale spatial precision and the ability to target deep brain
structures. However, the heterogeneous and anisotropic nature of the human
skull introduces significant distortions to the propagating ultrasound
wavefront, which require time-consuming patient-specific planning and
corrections using numerical solvers for accurate targeting. To enable
data-driven approaches in this domain, we introduce TFUScapes, the first
large-scale, high-resolution dataset of tFUS simulations through anatomically
realistic human skulls derived from T1-weighted MRI images. We have developed a
scalable simulation engine pipeline using the k-Wave pseudo-spectral solver,
where each simulation returns a steady-state pressure field generated by a
focused ultrasound transducer placed at realistic scalp locations. In addition
to the dataset, we present DeepTFUS, a deep learning model that estimates
normalized pressure fields directly from input 3D CT volumes and transducer
position. The model extends a U-Net backbone with transducer-aware
conditioning, incorporating Fourier-encoded position embeddings and MLP layers
to create global transducer embeddings. These embeddings are fused with U-Net
encoder features via feature-wise modulation, dynamic convolutions, and
cross-attention mechanisms. The model is trained using a combination of
spatially weighted and gradient-sensitive loss functions, enabling it to
approximate high-fidelity wavefields. The TFUScapes dataset is publicly
released to accelerate research at the intersection of computational acoustics,
neurotechnology, and deep learning. The project page is available at
https://github.com/CAMMA-public/TFUScapes.


## DGRO Enhancing LLM Reasoning via Exploration-Exploitation Control and Reward Variance Management

>Authors: Xuerui Su, Liya Guo, Yue Wang, Yi Zhu, Zhiming Ma, Zun Wang, Yuting Liu

>2025-05-19

> http://arxiv.org/abs/2505.12951v1

Inference scaling further accelerates Large Language Models (LLMs) toward
Artificial General Intelligence (AGI), with large-scale Reinforcement Learning
(RL) to unleash long Chain-of-Thought reasoning. Most contemporary reasoning
approaches usually rely on handcrafted rule-based reward functions. However,
the tarde-offs of exploration and exploitation in RL algorithms involves
multiple complex considerations, and the theoretical and empirical impacts of
manually designed reward functions remain insufficiently explored. In this
paper, we propose Decoupled Group Reward Optimization (DGRO), a general RL
algorithm for LLM reasoning. On the one hand, DGRO decouples the traditional
regularization coefficient into two independent hyperparameters: one scales the
policy gradient term, and the other regulates the distance from the sampling
policy. This decoupling not only enables precise control over balancing
exploration and exploitation, but also can be seamlessly extended to Online
Policy Mirror Descent (OPMD) algorithms in Kimi k1.5 and Direct Reward
Optimization. On the other hand, we observe that reward variance significantly
affects both convergence speed and final model performance. We conduct both
theoretical analysis and extensive empirical validation to assess DGRO,
including a detailed ablation study that investigates its performance and
optimization dynamics. Experimental results show that DGRO achieves
state-of-the-art performance on the Logic dataset with an average accuracy of
96.9\%, and demonstrates strong generalization across mathematical benchmarks.


## A3  an Analytical Low-Rank Approximation Framework for Attention

>Authors: Jeffrey T. H. Wong, Cheng Zhang, Xinye Cao, Pedro Gimenes, George A. Constantinides, Wayne Luk, Yiren Zhao

>2025-05-19

> http://arxiv.org/abs/2505.12942v1

Large language models have demonstrated remarkable performance; however,
their massive parameter counts make deployment highly expensive. Low-rank
approximation offers a promising compression solution, yet existing approaches
have two main limitations: (1) They focus on minimizing the output error of
individual linear layers, without considering the architectural characteristics
of Transformers, and (2) they decompose a large weight matrix into two small
low-rank matrices. Consequently, these methods often fall short compared to
other compression techniques like **pruning** and **quantization**, and introduce
runtime overhead such as the extra GEMM kernel launches for decomposed small
matrices. To address these limitations, we propose $\tt A^\tt 3$, a
post-training low-rank approximation framework. $\tt A^\tt 3$ splits a
Transformer layer into three functional components, namely $\tt QK$, $\tt OV$,
and $\tt MLP$. For each component, $\tt A^\tt 3$ provides an analytical
solution that reduces the hidden dimension size inside each component while
minimizing the component's functional loss ($\it i.e.$, error in attention
scores, attention outputs, and MLP outputs). This approach directly reduces
model sizes, **KV** cache sizes, and FLOPs without introducing any runtime
overheads. In addition, it provides a new narrative in advancing the
optimization problem from singular linear layer loss optimization toward
improved end-to-end performance. Through extensive experiments, we show that
$\tt A^\tt 3$ maintains superior performance compared to SoTAs. For example,
under the same reduction budget in computation and memory, our low-rank
approximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2,
outperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the
versatility of $\tt A^\tt 3$, including **KV** cache compression, **quantization**, and
mixed-rank assignments for enhanced performance.


## Efficient training for large-scale optical neural network using an evolutionary strategy and attention pruning

>Authors: Zhiwei Yang, Zeyang Fan, Yihang Lai, Qi Chen, Tian Zhang, Jian Dai, Kun Xu

>2025-05-19

> http://arxiv.org/abs/2505.12906v1

MZI-based block optical neural networks (BONNs), which can achieve
large-scale network models, have increasingly drawn attentions. However, the
robustness of the current training algorithm is not high enough. Moreover,
large-scale BONNs usually contain numerous trainable parameters, resulting in
expensive computation and power consumption. In this article, by **pruning** matrix
blocks and directly optimizing the individuals in population, we propose an
on-chip covariance matrix adaptation evolution strategy and attention-based
**pruning** (CAP) algorithm for large-scale BONNs. The calculated results
demonstrate that the CAP algorithm can prune 60% and 80% of the parameters for
MNIST and Fashion-MNIST datasets, respectively, while only degrades the
performance by 3.289% and 4.693%. Considering the influence of dynamic noise in
phase shifters, our proposed CAP algorithm (performance degradation of 22.327%
for MNIST dataset and 24.019% for Fashion-MNIST dataset utilizing a poor
fabricated chip and electrical control with a standard deviation of 0.5)
exhibits strongest robustness compared with both our previously reported block
adjoint training algorithm (43.963% and 41.074%) and the covariance matrix
adaptation evolution strategy (25.757% and 32.871%), respectively. Moreover,
when 60% of the parameters are pruned, the CAP algorithm realizes 88.5%
accuracy in experiment for the simplified MNIST dataset, which is similar to
the simulation result without noise (92.1%). Additionally, we simulationally
and experimentally demonstrate that using MZIs with only internal phase
shifters to construct BONNs is an efficient way to reduce both the system area
and the required trainable parameters. Notably, our proposed CAP algorithm show
excellent potential for larger-scale network models and more complex tasks.


## Lecture Notes WISPs in gamma-ray astrophysics

>Authors: Francesca Calore, Christopher Eckner

>2025-05-19

> http://arxiv.org/abs/2505.12905v1

These lecture notes provide an overview of high-energy astrophysical
processes involving axions, axion-like particles (ALPs), and other weakly
interacting slim particles (WISPs) focusing on their potential observational
signatures in astrophysical environments. After introducing key concepts in
high-energy astrophysics, we present the fundamental properties of WISPs,
emphasizing their phenomenological implications. Particular attention is given
to ALP-photon conversion in strong magnetic fields and the possible decay
signatures of ALPs in sources such as active galactic nuclei, galaxy clusters,
and cosmic-ray accelerators. These effects can lead to distinctive
modifications in astrophysical spectra, spatial distributions, and polarization
patterns, providing unique probes of physics beyond the Standard Model. We
discuss their role in dark matter scenarios and their potential impact on
high-energy observations. The lecture series is supplemented by hands-on
tutorials, including exercises on axion electrodynamics and an analysis of
gamma-ray data from NGC 1275 to search for ALP-photon conversion signatures.


## Accelerate TarFlow Sampling with GS-Jacobi Iteration

>Authors: Ben Liu, Zhen Qin

>2025-05-19

> http://arxiv.org/abs/2505.12849v1

Image generation models have achieved widespread applications. As an
instance, the TarFlow model combines the transformer architecture with
Normalizing Flow models, achieving state-of-the-art results on multiple
benchmarks. However, due to the causal form of attention requiring sequential
computation, TarFlow's sampling process is extremely slow. In this paper, we
demonstrate that through a series of optimization strategies, TarFlow sampling
can be greatly accelerated by using the Gauss-Seidel-Jacobi (abbreviated as
GS-Jacobi) iteration method. Specifically, we find that blocks in the TarFlow
model have varying importance: a small number of blocks play a major role in
image generation tasks, while other blocks contribute relatively little; some
blocks are sensitive to initial values and prone to numerical overflow, while
others are relatively robust. Based on these two characteristics, we propose
the Convergence Ranking Metric (CRM) and the Initial Guessing Metric (IGM): CRM
is used to identify whether a TarFlow block is "simple" (converges in few
iterations) or "tough" (requires more iterations); IGM is used to evaluate
whether the initial value of the iteration is good. Experiments on four TarFlow
models demonstrate that GS-Jacobi sampling can significantly enhance sampling
efficiency while maintaining the quality of generated images (measured by FID),
achieving speed-ups of 4.53x in Img128cond, 5.32x in AFHQ, 2.96x in
Img64uncond, and 2.51x in Img64cond without degrading FID scores or sample
quality. Code and checkpoints are accessible on
https://github.com/encoreus/GS-Jacobi_for_TarFlow


## AdaToken-3D Dynamic Spatial Gating for Efficient 3D Large Multimodal-Models Reasoning

>Authors: Kai Zhang, Xingyu Chen, Xiaofeng Zhang

>2025-05-19

> http://arxiv.org/abs/2505.12782v1

Large Multimodal Models (LMMs) have become a pivotal research focus in deep
learning, demonstrating remarkable capabilities in 3D scene understanding.
However, current 3D LMMs employing thousands of spatial tokens for multimodal
reasoning suffer from critical inefficiencies: excessive computational overhead
and redundant information flows. Unlike 2D VLMs processing single images, 3D
LMMs exhibit inherent architectural redundancy due to the heterogeneous
mechanisms between spatial tokens and visual tokens. To address this challenge,
we propose AdaToken-3D, an adaptive spatial token optimization framework that
dynamically prunes redundant tokens through spatial contribution analysis. Our
method automatically tailors **pruning** strategies to different 3D LMM
architectures by quantifying token-level information flows via attention
pattern mining. Extensive experiments on LLaVA-3D (a 7B parameter 3D-LMM)
demonstrate that AdaToken-3D achieves 21\% faster inference speed and 63\%
FLOPs reduction while maintaining original task accuracy. Beyond efficiency
gains, this work systematically investigates redundancy patterns in multimodal
spatial information flows through quantitative token interaction analysis. Our
findings reveal that over 60\% of spatial tokens contribute minimally ($<$5\%)
to the final predictions, establishing theoretical foundations for efficient 3D
multimodal learning.


## Pyramid Sparse Transformer Enhancing Multi-Scale Feature Fusion with Dynamic Token Selection

>Authors: Junyi Hu, Tian Bai, Fengyi Wu, Zhenming Peng, Yi Zhang

>2025-05-19

> http://arxiv.org/abs/2505.12772v2

Feature fusion is critical for high-performance vision models but often
incurs prohibitive complexity. However, prevailing attention-based fusion
methods often involve significant computational complexity and implementation
challenges, limiting their efficiency in resource-constrained environments. To
address these issues, we introduce the Pyramid Sparse Transformer (PST), a
lightweight, plug-and-play module that integrates coarse-to-fine token
selection and shared attention parameters to reduce computation while
preserving spatial detail. PST can be trained using only coarse attention and
seamlessly activated at inference for further accuracy gains without
retraining. When added to state-of-the-art real-time detection models, such as
YOLOv11-N/S/M, PST yields mAP improvements of 0.9%, 0.5%, and 0.4% on MS COCO
with minimal latency impact. Likewise, embedding PST into ResNet-18/50/101 as
backbones, boosts ImageNet top-1 accuracy by 6.5%, 1.7%, and 1.0%,
respectively. These results demonstrate PST's effectiveness as a simple,
hardware-friendly enhancement for both detection and classification tasks.


## LiDAR MOT-DETR A LiDAR-based Two-Stage Transformer for 3D Multiple Object Tracking

>Authors: Martha Teiko Teye, Ori Maoz, Matthias Rottmann

>2025-05-19

> http://arxiv.org/abs/2505.12753v2

Multi-object tracking from LiDAR point clouds presents unique challenges due
to the **sparse** and irregular nature of the data, compounded by the need for
temporal coherence across frames. Traditional tracking systems often rely on
hand-crafted features and motion models, which can struggle to maintain
consistent object identities in crowded or fast-moving scenes. We present a
lidar-based two-staged DETR inspired transformer; a smoother and tracker. The
smoother stage refines lidar object detections, from any off-the-shelf
detector, across a moving temporal window. The tracker stage uses a DETR-based
attention block to maintain tracks across time by associating tracked objects
with the refined detections using the point cloud as context. The model is
trained on the datasets nuScenes and KITTI in both online and offline (forward
peeking) modes demonstrating strong performance across metrics such as
ID-switch and multiple object tracking accuracy (MOTA). The numerical results
indicate that the online mode outperforms the lidar-only baseline and SOTA
models on the nuScenes dataset, with an aMOTA of 0.722 and an aMOTP of 0.475,
while the offline mode provides an additional 3 pp aMOTP.


## MVAR Visual Autoregressive Modeling with Scale and Spatial Markovian Conditioning

>Authors: Jinhua Zhang, Wei Long, Minghao Han, Weiyi You, Shuhang Gu

>2025-05-19

> http://arxiv.org/abs/2505.12742v1

Essential to visual generation is efficient modeling of visual data priors.
Conventional next-token prediction methods define the process as learning the
conditional probability distribution of successive tokens. Recently, next-scale
prediction methods redefine the process to learn the distribution over
multi-scale representations, significantly reducing generation latency.
However, these methods condition each scale on all previous scales and require
each token to consider all preceding tokens, exhibiting scale and spatial
redundancy. To better model the distribution by mitigating redundancy, we
propose Markovian Visual AutoRegressive modeling (MVAR), a novel autoregressive
framework that introduces scale and spatial Markov assumptions to reduce the
complexity of conditional probability modeling. Specifically, we introduce a
scale-Markov trajectory that only takes as input the features of adjacent
preceding scale for next-scale prediction, enabling the adoption of a parallel
training strategy that significantly reduces GPU memory consumption.
Furthermore, we propose spatial-Markov attention, which restricts the attention
of each token to a localized neighborhood of size k at corresponding positions
on adjacent scales, rather than attending to every token across these scales,
for the pursuit of reduced modeling complexity. Building on these improvements,
we reduce the computational complexity of attention calculation from O(N^2) to
O(Nk), enabling training with just eight NVIDIA RTX 4090 GPUs and eliminating
the need for **KV** cache during inference. Extensive experiments on ImageNet
demonstrate that MVAR achieves comparable or superior performance with both
small model trained from scratch and large fine-tuned models, while reducing
the average GPU memory footprint by 3.0x.


## Accelerating Adaptive Retrieval Augmented Generation via Instruction-Driven Representation Reduction of Retrieval Overlaps

>Authors: Jie Ou, Jinyu Guo, Shuaihong Jiang, Zhaokun Wang, Libo Qin, Shunyu Yao, Wenhong Tian

>2025-05-19

> http://arxiv.org/abs/2505.12731v1

Retrieval-augmented generation (RAG) has emerged as a pivotal method for
expanding the knowledge of large language models. To handle complex queries
more effectively, researchers developed Adaptive-RAG (A-RAG) to enhance the
generated quality through multiple interactions with external knowledge bases.
Despite its effectiveness, A-RAG exacerbates the pre-existing efficiency
challenges inherent in RAG, which are attributable to its reliance on multiple
iterations of generation. Existing A-RAG approaches process all retrieved
contents from scratch. However, they ignore the situation where there is a
significant overlap in the content of the retrieval results across rounds. The
overlapping content is redundantly represented, which leads to a large
proportion of repeated computations, thus affecting the overall efficiency. To
address this issue, this paper introduces a model-agnostic approach that can be
generally applied to A-RAG methods, which is dedicated to reducing the
redundant representation process caused by the overlapping of retrieval
results. Specifically, we use cache access and parallel generation to speed up
the prefilling and decoding stages respectively. Additionally, we also propose
an instruction-driven module to further guide the model to more effectively
attend to each part of the content in a more suitable way for LLMs. Experiments
show that our approach achieves 2.79 and 2.33 times significant **acceleration** on
average for prefilling and decoding respectively while maintaining equal
generation quality.


## FLASH Latent-Aware Semi-Autoregressive Speculative Decoding for Multimodal Tasks

>Authors: Zihua Wang, Ruibo Li, Haozhe Du, Joey Tianyi Zhou, Yu Zhang, Xu Yang

>2025-05-19

> http://arxiv.org/abs/2505.12728v1

Large language and multimodal models (LLMs and LMMs) exhibit strong inference
capabilities but are often limited by slow decoding speeds. This challenge is
especially acute in LMMs, where visual inputs typically comprise more tokens
with lower information density than text -- an issue exacerbated by recent
trends toward finer-grained visual tokenizations to boost performance.
Speculative decoding has been effective in accelerating LLM inference by using
a smaller draft model to generate candidate tokens, which are then selectively
verified by the target model, improving speed without sacrificing output
quality. While this strategy has been extended to LMMs, existing methods
largely overlook the unique properties of visual inputs and depend solely on
text-based draft models. In this work, we propose \textbf{FLASH} (Fast
Latent-Aware Semi-Autoregressive Heuristics), a speculative decoding framework
designed specifically for LMMs, which leverages two key properties of
multimodal data to design the draft model. First, to address redundancy in
visual tokens, we propose a lightweight latent-aware token compression
mechanism. Second, recognizing that visual objects often co-occur within a
scene, we employ a semi-autoregressive decoding strategy to generate multiple
tokens per forward pass. These innovations accelerate draft decoding while
maintaining high acceptance rates, resulting in faster overall inference.
Experiments show that FLASH significantly outperforms prior speculative
decoding approaches in both unimodal and multimodal settings, achieving up to
\textbf{2.68$\times$} speed-up on video captioning and \textbf{2.55$\times$} on
visual instruction tuning tasks compared to the original LMM.


## Josephson Junctions in the Age of Quantum Discovery

>Authors: Hyunseong Kim, Gyunghyun Jang, Seungwon Jin, Dongbin Shin, Hyeon-Jin Shin, Jie Luo, Irfan Siddiqi, Yosep Kim, Hoon Hahn Yoon, Long B. Nguyen

>2025-05-19

> http://arxiv.org/abs/2505.12724v1

The unique combination of energy conservation and nonlinear behavior
exhibited by Josephson junctions has driven transformative advances in modern
quantum technologies based on superconducting circuits. These superconducting
devices underpin essential developments across quantum computing, quantum
sensing, and quantum communication and open pathways to innovative applications
in nonreciprocal electronics. These developments are enabled by recent
breakthroughs in nanofabrication and characterization methodologies,
substantially enhancing device performance and scalability. The resulting
innovations reshape our understanding of quantum systems and enable practical
applications. This perspective explores the foundational role of Josephson
junctions research in propelling quantum technologies forward. We underscore
the critical importance of synergistic progress in material science, device
characterization, and nanofabrication to catalyze the next wave of
breakthroughs and accelerate the transition from fundamental discoveries to
industrial-scale quantum utilities. Drawing parallels with the transformative
impact of transistor-based integrated circuits during the Information Age, we
envision Josephson junction-based circuits as central to driving a similar
revolution in the emerging Quantum Age.


## ToTRL Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving

>Authors: Haoyuan Wu, Xueyi Chen, Rui Ming, Jilong Gao, Shoubo Hu, Zhuolun He, Bei Yu

>2025-05-19

> http://arxiv.org/abs/2505.12717v1

Large language models (LLMs) demonstrate significant reasoning capabilities,
particularly through long chain-of-thought (CoT) processes, which can be
elicited by reinforcement learning (RL). However, prolonged CoT reasoning
presents limitations, primarily verbose outputs due to excessive introspection.
The reasoning process in these LLMs often appears to follow a trial-and-error
methodology rather than a systematic, logical deduction. In contrast,
tree-of-thoughts (ToT) offers a conceptually more advanced approach by modeling
reasoning as an exploration within a tree structure. This reasoning structure
facilitates the parallel generation and evaluation of multiple reasoning
branches, allowing for the active identification, assessment, and **pruning** of
unproductive paths. This process can potentially lead to improved performance
and reduced token costs. Building upon the long CoT capability of LLMs, we
introduce tree-of-thoughts RL (ToTRL), a novel on-policy RL framework with a
rule-based reward. ToTRL is designed to guide LLMs in developing the parallel
ToT strategy based on the sequential CoT strategy. Furthermore, we employ LLMs
as players in a puzzle game during the ToTRL training process. Solving puzzle
games inherently necessitates exploring interdependent choices and managing
multiple constraints, which requires the construction and exploration of a
thought tree, providing challenging tasks for cultivating the ToT reasoning
capability. Our empirical evaluations demonstrate that our ToTQwen3-8B model,
trained with our ToTRL, achieves significant improvement in performance and
reasoning efficiency on complex reasoning tasks.


## LLM-based Query Expansion Fails for Unfamiliar and Ambiguous Queries

>Authors: Kenya Abe, Kunihiro Takeoka, Makoto P. Kato, Masafumi Oyamada

>2025-05-19

> http://arxiv.org/abs/2505.12694v1

Query expansion (QE) enhances retrieval by incorporating relevant terms, with
large language models (LLMs) offering an effective alternative to traditional
rule-based and statistical methods. However, LLM-based QE suffers from a
fundamental limitation: it often fails to generate relevant knowledge,
degrading search performance. Prior studies have focused on hallucination, yet
its underlying cause--LLM knowledge deficiencies--remains underexplored. This
paper systematically examines two failure cases in LLM-based QE: (1) when the
LLM lacks query knowledge, leading to incorrect expansions, and (2) when the
query is ambiguous, causing biased refinements that narrow search coverage. We
conduct controlled experiments across multiple datasets, evaluating the effects
of knowledge and query ambiguity on retrieval performance using **sparse** and
dense retrieval models. Our results reveal that LLM-based QE can significantly
degrade the retrieval effectiveness when knowledge in the LLM is insufficient
or query ambiguity is high. We introduce a framework for evaluating QE under
these conditions, providing insights into the limitations of LLM-based
retrieval augmentation.


## Exploring Federated Pruning for Large Language Models

>Authors: Pengxin Guo, Yinong Wang, Wei Li, Mengting Liu, Ming Li, Jinkai Zheng, Liangqiong Qu

>2025-05-19

> http://arxiv.org/abs/2505.13547v1

LLM **pruning** has emerged as a promising technology for compressing LLMs,
enabling their deployment on resource-limited devices. However, current
methodologies typically require access to public calibration samples, which can
be challenging to obtain in privacy-sensitive domains. To address this issue,
we introduce FedPrLLM, a comprehensive federated **pruning** framework designed for
the privacy-preserving compression of LLMs. In FedPrLLM, each client only needs
to calculate a **pruning** mask matrix based on its local calibration data and
share it with the server to prune the global model. This approach allows for
collaborative **pruning** of the global model with the knowledge of each client
while maintaining local data privacy. Additionally, we conduct extensive
experiments to explore various possibilities within the FedPrLLM framework,
including different comparison groups, **pruning** strategies, and the decision to
scale weights. Our extensive evaluation reveals that one-shot **pruning** with
layer comparison and no weight scaling is the optimal choice within the
FedPrLLM framework. We hope our work will help guide future efforts in **pruning**
LLMs in privacy-sensitive fields. Our code is available at
https://github.com/Pengxin-Guo/FedPrLLM.


## Multi-head Temporal Latent Attention

>Authors: Keqi Deng, Philip C. Woodland

>2025-05-19

> http://arxiv.org/abs/2505.13544v2

While Transformer self-attention offers strong parallelism, the Key-Value
(**KV**) cache grows linearly with sequence length and becomes a bottleneck for
inference efficiency. Multi-head latent attention was recently developed to
compress the **KV** cache into a low-rank latent space. This paper proposes
Multi-head Temporal Latent Attention (MTLA), which further reduces the **KV** cache
size along the temporal dimension, greatly lowering the memory footprint of
self-attention inference. MTLA employs a hyper-network to dynamically merge
temporally adjacent **KV** cache vectors. To address the mismatch between the
compressed **KV** cache and processed sequence lengths, a stride-aware causal mask
is proposed to ensure efficient parallel training and consistency with
inference behaviour. Experiments across tasks, including speech translation,
speech recognition, speech understanding and text summarisation, demonstrate
that MTLA achieves competitive performance compared to standard Multi-Head
Attention (MHA), while greatly improving inference speed and GPU memory usage.
For example, on a English-German speech translation task, MTLA achieves a 5.3x
speedup and a reduction in GPU memory usage by a factor of 8.3 compared to MHA,
while maintaining translation quality.


## GANCompress GAN-Enhanced Neural Image Compression with Binary Spherical Quantization

>Authors: Karthik Sivakoti

>2025-05-19

> http://arxiv.org/abs/2505.13542v1

The exponential growth of visual data in digital communications has
intensified the need for efficient compression techniques that balance
rate-distortion performance with computational feasibility. While recent neural
compression approaches have shown promise, they still struggle with fundamental
challenges: preserving perceptual quality at high compression ratios,
computational efficiency, and adaptability to diverse visual content. This
paper introduces GANCompress, a novel neural compression framework that
synergistically combines Binary Spherical Quantization (BSQ) with Generative
Adversarial Networks (GANs) to address these challenges. Our approach employs a
transformer-based autoencoder with an enhanced BSQ bottleneck that projects
latent representations onto a hypersphere, enabling efficient discretization
with bounded **quantization** error. This is followed by a specialized GAN
architecture incorporating frequency-domain attention and color consistency
optimization. Experimental results demonstrate that GANCompress achieves
substantial improvement in compression efficiency -- reducing file sizes by up
to 100x with minimal visual distortion. Our method outperforms traditional
codecs like H.264 by 12-15% in perceptual metrics while maintaining comparable
PSNR/SSIM values, with 2.4x faster encoding and decoding speeds. On standard
benchmarks including ImageNet-1k and COCO2017, GANCompress sets a new
state-of-the-art, reducing FID from 0.72 to 0.41 (43% improvement) compared to
previous methods while maintaining higher throughput. This work presents a
significant advancement in neural compression technology with promising
applications for real-time visual communication systems.


## Exploring Sparsity for Parameter Efficient Fine Tuning Using Wavelets

>Authors: Ahmet Bilican, M. Akın Yılmaz, A. Murat Tekalp, R. Gökberk Cinbiş

>2025-05-18

> http://arxiv.org/abs/2505.12532v1

Efficiently adapting large foundation models is critical, especially with
tight compute and memory budgets. Parameter-Efficient Fine-Tuning (PEFT)
methods such as LoRA offer limited granularity and effectiveness in
few-parameter regimes. We propose Wavelet Fine-Tuning (WaveFT), a novel PEFT
method that learns highly **sparse** updates in the wavelet domain of residual
matrices. WaveFT allows precise control of trainable parameters, offering
fine-grained capacity adjustment and excelling with remarkably low parameter
count, potentially far fewer than LoRA's minimum -- ideal for extreme
parameter-efficient scenarios. In order to demonstrate the effect of the
wavelet transform, we compare WaveFT with a special case, called SHiRA, that
entails applying **sparse** updates directly in the weight domain. Evaluated on
personalized text-to-image generation using Stable Diffusion XL as baseline,
WaveFT significantly outperforms LoRA and other PEFT methods, especially at low
parameter counts; achieving superior subject fidelity, prompt alignment, and
image diversity.


## Introspective Growth Automatically Advancing LLM Expertise in Technology Judgment

>Authors: Siyang Wu, Honglin Bao, Nadav Kunievsky, James A. Evans

>2025-05-18

> http://arxiv.org/abs/2505.12452v1

Large language models (LLMs) increasingly demonstrate signs of conceptual
understanding, yet much of their internal knowledge remains latent, loosely
structured, and difficult to access or evaluate. We propose self-questioning as
a lightweight and scalable strategy to improve LLMs' understanding,
particularly in domains where success depends on fine-grained semantic
distinctions. To evaluate this approach, we introduce a challenging new
benchmark of 1.3 million post-2015 computer science patent pairs, characterized
by dense technical jargon and strategically complex writing. The benchmark
centers on a pairwise differentiation task: can a model distinguish between
closely related but substantively different inventions? We show that prompting
LLMs to generate and answer their own questions - targeting the background
knowledge required for the task - significantly improves performance. These
self-generated questions and answers activate otherwise underutilized internal
knowledge. Allowing LLMs to retrieve answers from external scientific texts
further enhances performance, suggesting that model knowledge is compressed and
lacks the full richness of the training data. We also find that
chain-of-thought prompting and self-questioning converge, though
self-questioning remains more effective for improving understanding of
technical concepts. Notably, we uncover an asymmetry in prompting: smaller
models often generate more fundamental, more open-ended, better-aligned
questions for mid-sized models than large models with better understanding do,
revealing a new strategy for cross-model collaboration. Altogether, our
findings establish self-questioning as both a practical mechanism for
automatically improving LLM comprehension, especially in domains with **sparse**
and underrepresented knowledge, and a diagnostic probe of how internal and
external knowledge are organized.


## LLM-CoT Enhanced Graph Neural Recommendation with Harmonized Group Policy Optimization

>Authors: Hailong Luo, Bin Wu, Hongyong Jia, Qingqing Zhu, Lianlei Shan

>2025-05-18

> http://arxiv.org/abs/2505.12396v1

Graph neural networks (GNNs) have advanced recommender systems by modeling
interaction relationships. However, existing graph-based recommenders rely on
**sparse** ID features and do not fully exploit textual information, resulting in
low information density within representations. Furthermore, graph contrastive
learning faces challenges. Random negative sampling can introduce false
negative samples, while fixed temperature coefficients cannot adapt to the
heterogeneity of different nodes. In addition, current efforts to enhance
recommendations with large language models (LLMs) have not fully utilized their
Chain-of-Thought (CoT) reasoning capabilities to guide representation learning.
To address these limitations, we introduces LGHRec (LLM-CoT Enhanced Graph
Neural Recommendation with Harmonized Group Policy Optimization). This
framework leverages the CoT reasoning ability of LLMs to generate semantic IDs,
enriching reasoning processes and improving information density and semantic
quality of representations. Moreover, we design a reinforcement learning
algorithm, Harmonized Group Policy Optimization (HGPO), to optimize negative
sampling strategies and temperature coefficients in contrastive learning. This
approach enhances long-tail recommendation performance and ensures optimization
consistency across different groups. Experimental results on three datasets
demonstrate that LGHRec improves representation quality through semantic IDs
generated by LLM's CoT reasoning and effectively boosts contrastive learning
with HGPO. Our method outperforms several baseline models. The code is
available at: https://anonymous.4open.science/r/LLM-Rec.


## STAR Stage-Wise Attention-Guided Token Reduction for Efficient Large Vision-Language Models Inference

>Authors: Yichen Guo, Hanze Li, Zonghao Zhang, Jinhao You, Kai Tang, Xiande Huang

>2025-05-18

> http://arxiv.org/abs/2505.12359v1

Although large vision-language models (LVLMs) leverage rich visual token
representations to achieve strong performance on multimodal tasks, these tokens
also introduce significant computational overhead during inference. Existing
training-free token **pruning** methods typically adopt a single-stage strategy,
focusing either on visual self-attention or visual-textual cross-attention.
However, such localized perspectives often overlook the broader information
flow across the model, leading to substantial performance degradation,
especially under high **pruning** ratios. In this work, we propose STAR (Stage-wise
Attention-guided token Reduction), a training-free, plug-and-play framework
that approaches token **pruning** from a global perspective. Instead of **pruning** at
a single point, STAR performs attention-guided reduction in two complementary
stages: an early-stage **pruning** based on visual self-attention to remove
redundant low-level features, and a later-stage **pruning** guided by cross-modal
attention to discard task-irrelevant tokens. This holistic approach allows STAR
to significantly reduce computational cost while better preserving
task-critical information. Extensive experiments across multiple LVLM
architectures and benchmarks show that STAR achieves strong **acceleration** while
maintaining comparable, and in some cases even improved performance.


## SenseFlow A Physics-Informed and Self-Ensembling Iterative Framework for Power Flow Estimation

>Authors: Zhen Zhao, Wenqi Huang, Zicheng Wang, Jiaxuan Hou, Peng Li, Lei Bai

>2025-05-18

> http://arxiv.org/abs/2505.12302v1

Power flow estimation plays a vital role in ensuring the stability and
reliability of electrical power systems, particularly in the context of growing
network complexities and renewable energy integration. However, existing
studies often fail to adequately address the unique characteristics of power
systems, such as the **sparsity** of network connections and the critical
importance of the unique Slack node, which poses significant challenges in
achieving high-accuracy estimations. In this paper, we present SenseFlow, a
novel physics-informed and self-ensembling iterative framework that integrates
two main designs, the Physics-Informed Power Flow Network (FlowNet) and
Self-Ensembling Iterative Estimation (SeIter), to carefully address the unique
properties of the power system and thereby enhance the power flow estimation.
Specifically, SenseFlow enforces the FlowNet to gradually predict
high-precision voltage magnitudes and phase angles through the iterative SeIter
process. On the one hand, FlowNet employs the Virtual Node Attention and
Slack-Gated Feed-Forward modules to facilitate efficient global-local
communication in the face of network **sparsity** and amplify the influence of the
Slack node on angle predictions, respectively. On the other hand, SeIter
maintains an exponential moving average of FlowNet's parameters to create a
robust ensemble model that refines power state predictions throughout the
iterative fitting process. Experimental results demonstrate that SenseFlow
outperforms existing methods, providing a promising solution for high-accuracy
power flow estimation across diverse grid configurations.


## CALM Co-evolution of Algorithms and Language Model for Automatic Heuristic Design

>Authors: Ziyao Huang, Weiwei Wu, Kui Wu, Jianping Wang, Wei-Bin Lee

>2025-05-18

> http://arxiv.org/abs/2505.12285v1

Tackling complex optimization problems often relies on expert-designed
heuristics, typically crafted through extensive trial and error. Recent
advances demonstrate that large language models (LLMs), when integrated into
well-designed evolutionary search frameworks, can autonomously discover
high-performing heuristics at a fraction of the traditional cost. However,
existing approaches predominantly rely on verbal guidance, i.e., manipulating
the prompt generation process, to steer the evolution of heuristics, without
adapting the underlying LLM. We propose a hybrid framework that combines verbal
and numerical guidance, the latter achieved by fine-tuning the LLM via
reinforcement learning based on the quality of generated heuristics. This joint
optimization allows the LLM to co-evolve with the search process. Our method
outperforms state-of-the-art (SOTA) baselines across various optimization
tasks, running locally on a single 24GB GPU using a 7B model with INT4
**quantization**. It surpasses methods that rely solely on verbal guidance, even
when those use significantly more powerful API-based models.


## PMQ-VE Progressive Multi-Frame Quantization for Video Enhancement

>Authors: ZhanFeng Feng, Long Peng, Xin Di, Yong Guo, Wenbo Li, Yulun Zhang, Renjing Pei, Yang Wang, Yang Cao, Zheng-Jun Zha

>2025-05-18

> http://arxiv.org/abs/2505.12266v1

Multi-frame video enhancement tasks aim to improve the spatial and temporal
resolution and quality of video sequences by leveraging temporal information
from multiple frames, which are widely used in streaming video processing,
surveillance, and generation. Although numerous Transformer-based enhancement
methods have achieved impressive performance, their computational and memory
demands hinder deployment on edge devices. Quantization offers a practical
solution by reducing the bit-width of weights and activations to improve
efficiency. However, directly applying existing **quantization** methods to video
enhancement tasks often leads to significant performance degradation and loss
of fine details. This stems from two limitations: (a) inability to allocate
varying representational capacity across frames, which results in suboptimal
dynamic range adaptation; (b) over-reliance on full-precision teachers, which
limits the learning of **low-bit** student models. To tackle these challenges, we
propose a novel **quantization** method for video enhancement: Progressive
Multi-Frame Quantization for Video Enhancement (PMQ-VE). This framework
features a coarse-to-fine two-stage process: Backtracking-based Multi-Frame
Quantization (BMFQ) and Progressive Multi-Teacher Distillation (PMTD). BMFQ
utilizes a percentile-based initialization and iterative search with **pruning**
and backtracking for robust clipping bounds. PMTD employs a progressive
distillation strategy with both full-precision and multiple high-bit (INT)
teachers to enhance **low-bit** models' capacity and quality. Extensive experiments
demonstrate that our method outperforms existing approaches, achieving
state-of-the-art performance across multiple tasks and benchmarks.The code will
be made publicly available at: https://github.com/xiaoBIGfeng/PMQ-VE.


## LightRetriever A LLM-based Hybrid Retrieval Architecture with 1000x Faster Query Inference

>Authors: Guangyuan Ma, Yongliang Ma, Xuanrui Gou, Zhenpeng Su, Ming Zhou, Songlin Hu

>2025-05-18

> http://arxiv.org/abs/2505.12260v1

Large Language Models (LLMs)-based hybrid retrieval uses LLMs to encode
queries and documents into low-dimensional dense or high-dimensional **sparse**
vectors. It retrieves documents relevant to search queries based on vector
similarities. Documents are pre-encoded offline, while queries arrive in
real-time, necessitating an efficient online query encoder. Although LLMs
significantly enhance retrieval capabilities, serving deeply parameterized LLMs
slows down query inference throughput and increases demands for online
deployment resources. In this paper, we propose LightRetriever, a novel
LLM-based hybrid retriever with extremely lightweight query encoders. Our
method retains a full-sized LLM for document encoding, but reduces the workload
of query encoding to no more than an embedding lookup. Compared to serving a
full-sized LLM on an H800 GPU, our approach achieves over a 1000x speedup for
query inference with GPU **acceleration**, and even a 20x speedup without GPU.
Experiments on large-scale retrieval benchmarks demonstrate that our method
generalizes well across diverse retrieval tasks, retaining an average of 95%
full-sized performance.


## One-for-All Pruning A Universal Model for Customized Compression of Large Language Models

>Authors: Rongguang Ye, Ming Tang

>2025-05-18

> http://arxiv.org/abs/2505.12216v1

Existing **pruning** methods for large language models (LLMs) focus on achieving
high compression rates while maintaining model performance. Although these
methods have demonstrated satisfactory performance in handling a single user's
compression request, their processing time increases linearly with the number
of requests, making them inefficient for real-world scenarios with multiple
simultaneous requests. To address this limitation, we propose a Univeral Model
for Customized Compression (UniCuCo) for LLMs, which introduces a StratNet that
learns to map arbitrary requests to their optimal **pruning** strategy. The
challenge in training StratNet lies in the high computational cost of
evaluating **pruning** strategies and the non-differentiable nature of the **pruning**
process, which hinders gradient backpropagation for StratNet updates. To
overcome these challenges, we leverage a Gaussian process to approximate the
evaluation process. Since the gradient of the Gaussian process is computable,
we can use it to approximate the gradient of the non-differentiable **pruning**
process, thereby enabling StratNet updates. Experimental results show that
UniCuCo is 28 times faster than baselines in processing 64 requests, while
maintaining comparable accuracy to baselines.


## LLM-DSE Searching Accelerator Parameters with LLM Agents

>Authors: Hanyu Wang, Xinrui Wu, Zijian Ding, Su Zheng, Chengyue Wang, Tony Nowatzki, Yizhou Sun, Jason Cong

>2025-05-18

> http://arxiv.org/abs/2505.12188v2

Even though high-level synthesis (HLS) tools mitigate the challenges of
programming domain-specific accelerators (DSAs) by raising the abstraction
level, optimizing hardware directive parameters remains a significant hurdle.
Existing heuristic and learning-based methods struggle with adaptability and
sample efficiency. We present LLM-DSE, a multi-agent framework designed
specifically for optimizing HLS directives. Combining LLM with design space
exploration (DSE), our explorer coordinates four agents: Router, Specialists,
Arbitrator, and Critic. These multi-agent components interact with various
tools to accelerate the optimization process. LLM-DSE leverages essential
domain knowledge to identify efficient parameter combinations while maintaining
adaptability through verbal learning from online interactions. Evaluations on
the HLSyn dataset demonstrate that LLM-DSE achieves substantial $2.55\times$
performance gains over state-of-the-art methods, uncovering novel designs while
reducing runtime. Ablation studies validate the effectiveness and necessity of
the proposed agent interactions. Our code is open-sourced here:
https://github.com/Nozidoali/LLM-DSE.


## Cyclic-Shift Sparse Kronecker Tensor Classifier for Signal-Region Detection in Neuroimaging

>Authors: Hsin-Hsiung Huang, Yuh-Haur Chen, Teng Zhang

>2025-05-17

> http://arxiv.org/abs/2505.12113v1

This study proposes a cyclic-shift logistic **sparse** Kronecker product
decomposition (SKPD) model for high-dimensional tensor data, enhancing the SKPD
framework with a cyclic-shift mechanism for binary classification. The method
enables interpretable and scalable analysis of brain MRI data, detecting
disease-relevant regions through a structured low-rank factorization. By
incorporating a second spatially shifted view of the data, the cyclic-shift
logistic SKPD improves robustness to misalignment across subjects, a common
challenge in neuroimaging. We provide asymptotic consistency guarantees under a
restricted isometry condition adapted to logistic loss. Simulations confirm the
model's ability to recover spatial signals under noise and identify optimal
patch sizes for factor decomposition. Application to OASIS-1 and ADNI-1
datasets demonstrates that the model achieves strong classification accuracy
and localizes estimated coefficients in clinically relevant brain regions, such
as the hippocampus. A data-driven slice selection strategy further improves
interpretability in 2D projections. The proposed framework offers a principled,
interpretable, and computationally efficient tool for neuroimaging-based
disease diagnosis, with potential extensions to multi-class settings and more
complex transformations.


## Discovering Symbolic Differential Equations with Symmetry Invariants

>Authors: Jianke Yang, Manu Bhat, Bryan Hu, Yadi Cao, Nima Dehmamy, Robin Walters, Rose Yu

>2025-05-17

> http://arxiv.org/abs/2505.12083v1

Discovering symbolic differential equations from data uncovers fundamental
dynamical laws underlying complex systems. However, existing methods often
struggle with the vast search space of equations and may produce equations that
violate known physical laws. In this work, we address these problems by
introducing the concept of \textit{symmetry invariants} in equation discovery.
We leverage the fact that differential equations admitting a symmetry group can
be expressed in terms of differential invariants of symmetry transformations.
Thus, we propose to use these invariants as atomic entities in equation
discovery, ensuring the discovered equations satisfy the specified symmetry.
Our approach integrates seamlessly with existing equation discovery methods
such as **sparse** regression and genetic programming, improving their accuracy and
efficiency. We validate the proposed method through applications to various
physical systems, such as fluid and reaction-diffusion, demonstrating its
ability to recover parsimonious and interpretable equations that respect the
laws of physics.


## Learning High-Order Relationships with Hypergraph Attention-based Spatio-Temporal Aggregation for Brain Disease Analysis

>Authors: Wenqi Hu, Xuerui Su, Guanliang Li, Yidi Pan, Aijing Lin

>2025-05-17

> http://arxiv.org/abs/2505.12068v1

Traditional functional connectivity based on functional magnetic resonance
imaging (fMRI) can only capture pairwise interactions between brain regions.
Hypergraphs, which reveal high-order relationships among multiple brain
regions, have been widely used for disease analysis. However, existing methods
often rely on predefined hypergraph structures, limiting their ability to model
complex patterns. Moreover, temporal information, an essential component of
brain high-order relationships, is frequently overlooked. To address these
limitations, we propose a novel framework that jointly learns informative and
**sparse** high-order brain structures along with their temporal dynamics. Inspired
by the information bottleneck principle, we introduce an objective that
maximizes information and minimizes redundancy, aiming to retain
disease-relevant high-order features while suppressing irrelevant signals. Our
model comprises a multi-hyperedge binary mask module for hypergraph structure
learning, a hypergraph self-attention aggregation module that captures spatial
features through adaptive attention across nodes and hyperedges, and a
spatio-temporal low-dimensional network for extracting discriminative
spatio-temporal representations for disease classification. Experiments on
benchmark fMRI datasets demonstrate that our method outperforms the
state-of-the-art approaches and successfully identifies meaningful high-order
brain interactions. These findings provide new insights into brain network
modeling and the study of neuropsychiatric disorders.


## Tiny QA Benchmark++ Ultra-Lightweight, Synthetic Multilingual Dataset Generation & Smoke-Tests for Continuous LLM Evaluation

>Authors: Vincent Koc

>2025-05-17

> http://arxiv.org/abs/2505.12058v1

Tiny QA Benchmark++ (TQB++) presents an ultra-lightweight, multilingual
smoke-test suite designed to give large-language-model (LLM) pipelines a
unit-test style safety net dataset that runs in seconds with minimal cost. Born
out of the tight feedback-loop demands building the Comet Opik
prompt-optimization SDK, where waiting on heavyweight benchmarks breaks
developer flow. TQB++ couples a 52-item English gold set (less than 20 kB) with
a tiny synthetic-data generator pypi package built on provider-agnostic
LiteLLM. The generator lets practitioners mint their own tiny packs in any
language, domain, or difficulty, while ten ready-made packs already cover
Arabic, Chinese, French, German, Japanese, Korean, Portuguese, Russian,
Spanish, and Turkish. Every dataset ships with Croissant metadata and
plug-and-play files for OpenAI-Evals, LangChain, and standard CI tools, so
teams can drop deterministic micro-benchmarks directly into pull-request gates,
prompt-engineering loops, and production dashboards without touching GPU
budgets. A complete TQB++ run adds only a few seconds to pipeline latency yet
reliably flags prompt-template errors, tokenizer drift, and fine-tuning
side-effects long before full-scale suites like MMLU or BIG-Bench would finish
configuring. The entire framework is released to accelerate continuous,
resource-efficient quality assurance across the generative-AI ecosystem.


## Accelerating Diffusion-based Super-Resolution with Dynamic Time-Spatial Sampling

>Authors: Rui Qin, Qijie Wang, Ming Sun, Haowei Zhu, Chao Zhou, Bin Wang

>2025-05-17

> http://arxiv.org/abs/2505.12048v2

Diffusion models have gained attention for their success in modeling complex
distributions, achieving impressive perceptual quality in SR tasks. However,
existing diffusion-based SR methods often suffer from high computational costs,
requiring numerous iterative steps for training and inference. Existing
**acceleration** techniques, such as distillation and solver optimization, are
generally task-agnostic and do not fully leverage the specific characteristics
of low-level tasks like super-resolution (SR). In this study, we analyze the
frequency- and spatial-domain properties of diffusion-based SR methods,
revealing key insights into the temporal and spatial dependencies of
high-frequency signal recovery. Specifically, high-frequency details benefit
from concentrated optimization during early and late diffusion iterations,
while spatially textured regions demand adaptive denoising strategies. Building
on these observations, we propose the Time-Spatial-aware Sampling strategy
(TSS) for the **acceleration** of Diffusion SR without any extra training cost. TSS
combines Time Dynamic Sampling (TDS), which allocates more iterations to
refining textures, and Spatial Dynamic Sampling (SDS), which dynamically
adjusts strategies based on image content. Extensive evaluations across
multiple benchmarks demonstrate that TSS achieves state-of-the-art (SOTA)
performance with significantly fewer iterations, improving MUSIQ scores by 0.2
- 3.0 and outperforming the current **acceleration** methods with only half the
number of steps.


## AI-Driven Automation Can Become the Foundation of Next-Era Science of Science Research

>Authors: Renqi Chen, Haoyang Su, Shixiang Tang, Zhenfei Yin, Qi Wu, Hui Li, Ye Sun, Nanqing Dong, Wanli Ouyang, Philip Torr

>2025-05-17

> http://arxiv.org/abs/2505.12039v1

The Science of Science (SoS) explores the mechanisms underlying scientific
discovery, and offers valuable insights for enhancing scientific efficiency and
fostering innovation. Traditional approaches often rely on simplistic
assumptions and basic statistical tools, such as linear regression and
rule-based simulations, which struggle to capture the complexity and scale of
modern research ecosystems. The advent of artificial intelligence (AI) presents
a transformative opportunity for the next generation of SoS, enabling the
automation of large-scale pattern discovery and uncovering insights previously
unattainable. This paper offers a forward-looking perspective on the
integration of Science of Science with AI for automated research pattern
discovery and highlights key open challenges that could greatly benefit from
AI. We outline the advantages of AI over traditional methods, discuss potential
limitations, and propose pathways to overcome them. Additionally, we present a
preliminary multi-agent system as an illustrative example to simulate research
societies, showcasing AI's ability to replicate real-world research patterns
and accelerate progress in Science of Science research.


## SpatialCrafter Unleashing the Imagination of Video Diffusion Models for Scene Reconstruction from Limited Observations

>Authors: Songchun Zhang, Huiyao Xu, Sitong Guo, Zhongwei Xie, Pengwei Liu, Hujun Bao, Weiwei Xu, Changqing Zou

>2025-05-17

> http://arxiv.org/abs/2505.11992v1

Novel view synthesis (NVS) boosts immersive experiences in computer vision
and graphics. Existing techniques, though progressed, rely on dense multi-view
observations, restricting their application. This work takes on the challenge
of reconstructing photorealistic 3D scenes from **sparse** or single-view inputs.
We introduce SpatialCrafter, a framework that leverages the rich knowledge in
video diffusion models to generate plausible additional observations, thereby
alleviating reconstruction ambiguity. Through a trainable camera encoder and an
epipolar attention mechanism for explicit geometric constraints, we achieve
precise camera control and 3D consistency, further reinforced by a unified
scale estimation strategy to handle scale discrepancies across datasets.
Furthermore, by integrating monocular depth priors with semantic features in
the video latent space, our framework directly regresses 3D Gaussian primitives
and efficiently processes long-sequence features using a hybrid network
structure. Extensive experiments show our method enhances **sparse** view
reconstruction and restores the realistic appearance of 3D scenes.


## Integrating Model-based Control and RL for Sim2Real Transfer of Tight Insertion Policies

>Authors: Isidoros Marougkas, Dhruv Metha Ramesh, Joe H. Doerr, Edgar Granados, Aravind Sivaramakrishnan, Abdeslam Boularias, Kostas E. Bekris

>2025-05-17

> http://arxiv.org/abs/2505.11858v1

Object insertion under tight tolerances ($< \hspace{-.02in} 1mm$) is an
important but challenging assembly task as even small errors can result in
undesirable contacts. Recent efforts focused on Reinforcement Learning (RL),
which often depends on careful definition of dense reward functions. This work
proposes an effective strategy for such tasks that integrates traditional
model-based control with RL to achieve improved insertion accuracy. The policy
is trained exclusively in simulation and is zero-shot transferred to the real
system. It employs a potential field-based controller to acquire a model-based
policy for inserting a plug into a socket given full observability in
simulation. This policy is then integrated with residual RL, which is trained
in simulation given only a **sparse**, goal-reaching reward. A curriculum scheme
over observation noise and action magnitude is used for training the residual
RL policy. Both policy components use as input the SE(3) poses of both the plug
and the socket and return the plug's SE(3) pose transform, which is executed by
a robotic arm using a controller. The integrated policy is deployed on the real
system without further training or fine-tuning, given a visual SE(3) object
tracker. The proposed solution and alternatives are evaluated across a variety
of objects and conditions in simulation and reality. The proposed approach
outperforms recent RL-based methods in this domain and prior efforts with
hybrid policies. Ablations highlight the impact of each component of the
approach.


## S-Crescendo A Nested Transformer Weaving Framework for Scalable Nonlinear System in S-Domain Representation

>Authors: Junlang Huang, Hao Chen, Li Luo, Yong Cai, Lexin Zhang, Tianhao Ma, Yitian Zhang, Zhong Guan

>2025-05-17

> http://arxiv.org/abs/2505.11843v1

Simulation of high-order nonlinear system requires extensive computational
resources, especially in modern VLSI backend design where bifurcation-induced
instability and chaos-like transient behaviors pose challenges. We present
S-Crescendo - a nested transformer weaving framework that synergizes S-domain
with neural operators for scalable time-domain prediction in high-order
nonlinear networks, alleviating the computational bottlenecks of conventional
solvers via Newton-Raphson method. By leveraging the partial-fraction
decomposition of an n-th order transfer function into first-order modal terms
with repeated poles and residues, our method bypasses the conventional Jacobian
matrix-based iterations and efficiently reduces computational complexity from
cubic $O(n^3)$ to linear $O(n)$.The proposed architecture seamlessly integrates
an S-domain encoder with an attention-based correction operator to
simultaneously isolate dominant response and adaptively capture higher-order
non-linearities. Validated on order-1 to order-10 networks, our method achieves
up to 0.99 test-set ($R^2$) accuracy against HSPICE golden waveforms and
accelerates simulation by up to 18(X), providing a scalable, physics-aware
framework for high-dimensional nonlinear modeling.


## FastCar Cache Attentive Replay for Fast Auto-Regressive Video Generation on the Edge

>Authors: Xuan Shen, Weize Ma, Yufa Zhou, Enhao Tang, Yanyue Xie, Zhengang Li, Yifan Gong, Quanyi Wang, Henghui Ding, Yiwei Wang, Yanzhi Wang, Pu Zhao, Jun Lin, Jiuxiang Gu

>2025-05-17

> http://arxiv.org/abs/2505.14709v1

Auto-regressive (AR) models, initially successful in language generation,
have recently shown promise in visual generation tasks due to their superior
sampling efficiency. Unlike image generation, video generation requires a
substantially larger number of tokens to produce coherent temporal frames,
resulting in significant overhead during the decoding phase. Our key
observations are: (i) MLP modules in the decode phase dominate the inference
latency, and (ii) there exists high temporal redundancy in MLP outputs of
adjacent frames. In this paper, we propose the \textbf{FastCar} framework to
accelerate the decode phase for the AR video generation by exploring the
temporal redundancy. The Temporal Attention Score (TAS) is proposed to
determine whether to apply the replay strategy (\textit{i.e.}, reusing cached
MLP outputs from the previous frame to reduce redundant computations) with
detailed theoretical analysis and justification. Also, we develop a hardware
accelerator on FPGA with Dynamic Resource Scheduling (DRS) based on TAS to
enable better resource utilization and faster inference. Experimental results
demonstrate the effectiveness of our method, which outperforms traditional
**sparse** attention approaches with more than 2.1x decoding speedup and higher
energy efficiency on the edge. Furthermore, by combining FastCar and **sparse**
attention, FastCar can boost the performance of **sparse** attention with
alleviated drifting, demonstrating our unique advantages for high-resolution
and long-duration video generation. Code:
https://github.com/shawnricecake/fast-car


## SplInterp Improving our Understanding and Training of Sparse Autoencoders

>Authors: Jeremy Budd, Javier Ideami, Benjamin Macdowall Rynne, Keith Duggar, Randall Balestriero

>2025-05-17

> http://arxiv.org/abs/2505.11836v1

Sparse autoencoders (SAEs) have received considerable recent attention as
tools for mechanistic interpretability, showing success at extracting
interpretable features even from very large LLMs. However, this research has
been largely empirical, and there have been recent doubts about the true
utility of SAEs. In this work, we seek to enhance the theoretical understanding
of SAEs, using the spline theory of deep learning. By situating SAEs in this
framework: we discover that SAEs generalise ``$k$-means autoencoders'' to be
piecewise affine, but sacrifice accuracy for interpretability vs. the optimal
``$k$-means-esque plus local principal component analysis (PCA)'' piecewise
affine autoencoder. We characterise the underlying geometry of (TopK) SAEs
using power diagrams. And we develop a novel proximal alternating method SGD
(PAM-SGD) algorithm for training SAEs, with both solid theoretical foundations
and promising empirical results in MNIST and LLM experiments, particularly in
sample efficiency and (in the LLM setting) improved **sparsity** of codes. All code
is available at: https://github.com/splInterp2025/splInterp


## DraftAttention Fast Video Diffusion via Low-Resolution Attention Guidance

>Authors: Xuan Shen, Chenxia Han, Yufa Zhou, Yanyue Xie, Yifan Gong, Quanyi Wang, Yiwei Wang, Yanzhi Wang, Pu Zhao, Jiuxiang Gu

>2025-05-17

> http://arxiv.org/abs/2505.14708v1

Diffusion transformer-based video generation models (DiTs) have recently
attracted widespread attention for their excellent generation quality. However,
their computational cost remains a major bottleneck-attention alone accounts
for over 80% of total latency, and generating just 8 seconds of 720p video
takes tens of minutes-posing serious challenges to practical application and
scalability. To address this, we propose the DraftAttention, a training-free
framework for the **acceleration** of video diffusion transformers with dynamic
**sparse** attention on GPUs. We apply down-sampling to each feature map across
frames in the compressed latent space, enabling a higher-level receptive field
over the latent composed of hundreds of thousands of tokens. The low-resolution
draft attention map, derived from draft query and key, exposes redundancy both
spatially within each feature map and temporally across frames. We reorder the
query, key, and value based on the draft attention map to guide the **sparse**
attention computation in full resolution, and subsequently restore their
original order after the attention computation. This reordering enables
structured **sparsity** that aligns with hardware-optimized execution. Our
theoretical analysis demonstrates that the low-resolution draft attention
closely approximates the full attention, providing reliable guidance for
constructing accurate **sparse** attention. Experimental results show that our
method outperforms existing **sparse** attention approaches in video generation
quality and achieves up to 1.75x end-to-end speedup on GPUs. Code:
https://github.com/shawnricecake/draft-attention


## Chain-of-Model Learning for Language Model

>Authors: Kaitao Song, Xiaohua Wang, Xu Tan, Huiqiang Jiang, Chengruidong Zhang, Yongliang Shen, Cen LU, Zihao Li, Zifan Song, Caihua Shan, Yansen Wang, Kan Ren, Xiaoqing Zheng, Tao Qin, Yuqing Yang, Dongsheng Li, Lili Qiu

>2025-05-17

> http://arxiv.org/abs/2505.11820v1

In this paper, we propose a novel learning paradigm, termed Chain-of-Model
(CoM), which incorporates the causal relationship into the hidden states of
each layer as a chain style, thereby introducing great scaling efficiency in
model training and inference flexibility in deployment. We introduce the
concept of Chain-of-Representation (CoR), which formulates the hidden states at
each layer as a combination of multiple sub-representations (i.e., chains) at
the hidden dimension level. In each layer, each chain from the output
representations can only view all of its preceding chains in the input
representations. Consequently, the model built upon CoM framework can
progressively scale up the model size by increasing the chains based on the
previous models (i.e., chains), and offer multiple sub-models at varying sizes
for elastic inference by using different chain numbers. Based on this
principle, we devise Chain-of-Language-Model (CoLM), which incorporates the
idea of CoM into each layer of Transformer architecture. Based on CoLM, we
further introduce CoLM-Air by introducing a **KV** sharing mechanism, that computes
all keys and values within the first chain and then shares across all chains.
This design demonstrates additional extensibility, such as enabling seamless LM
switching, prefilling **acceleration** and so on. Experimental results demonstrate
our CoLM family can achieve comparable performance to the standard Transformer,
while simultaneously enabling greater flexiblity, such as progressive scaling
to improve training efficiency and offer multiple varying model sizes for
elastic inference, paving a a new way toward building language models. Our code
will be released in the future at: https://github.com/microsoft/CoLM.


## Feature Hedging Correlated Features Break Narrow Sparse Autoencoders

>Authors: David Chanin, Tomáš Dulka, Adrià Garriga-Alonso

>2025-05-16

> http://arxiv.org/abs/2505.11756v1

It is assumed that **sparse** autoencoders (SAEs) decompose polysemantic
activations into interpretable linear directions, as long as the activations
are composed of **sparse** linear combinations of underlying features. However, we
find that if an SAE is more narrow than the number of underlying "true
features" on which it is trained, and there is correlation between features,
the SAE will merge components of correlated features together, thus destroying
monosemanticity. In LLM SAEs, these two conditions are almost certainly true.
This phenomenon, which we call feature hedging, is caused by SAE reconstruction
loss, and is more severe the narrower the SAE. In this work, we introduce the
problem of feature hedging and study it both theoretically in toy models and
empirically in SAEs trained on LLMs. We suspect that feature hedging may be one
of the core reasons that SAEs consistently underperform supervised baselines.
Finally, we use our understanding of feature hedging to propose an improved
variant of matryoshka SAEs. Our work shows there remain fundamental issues with
SAEs, but we are hopeful that that highlighting feature hedging will catalyze
future advances that allow SAEs to achieve their full potential of interpreting
LLMs at scale.


## ZeroTuning Unlocking the Initial Token's Power to Enhance Large Language Models Without Training

>Authors: Feijiang Han, Xiaodong Yu, Jianheng Tang, Lyle Ungar

>2025-05-16

> http://arxiv.org/abs/2505.11739v1

Recently, training-free methods for improving large language models (LLMs)
have attracted growing interest, with token-level attention tuning emerging as
a promising and interpretable direction. However, existing methods typically
rely on auxiliary mechanisms to identify important or irrelevant task-specific
tokens, introducing potential bias and limiting applicability. In this paper,
we uncover a surprising and elegant alternative: the semantically empty initial
token is a powerful and underexplored control point for optimizing model
behavior. Through theoretical analysis, we show that tuning the initial token's
attention sharpens or flattens the attention distribution over subsequent
tokens, and its role as an attention sink amplifies this effect. Empirically,
we find that: (1) tuning its attention improves LLM performance more
effectively than tuning other task-specific tokens; (2) the effect follows a
consistent trend across layers, with earlier layers having greater impact, but
varies across attention heads, with different heads showing distinct
preferences in how they attend to this token. Based on these findings, we
propose ZeroTuning, a training-free approach that improves LLM performance by
applying head-specific attention adjustments to this special token. Despite
tuning only one token, ZeroTuning achieves higher performance on text
classification, multiple-choice, and multi-turn conversation tasks across
models such as Llama, Qwen, and DeepSeek. For example, ZeroTuning improves
Llama-3.1-8B by 11.71% on classification, 2.64% on QA tasks, and raises its
multi-turn score from 7.804 to 7.966. The method is also robust to limited
resources, few-shot settings, long contexts, **quantization**, decoding strategies,
and prompt variations. Our work sheds light on a previously overlooked control
point in LLMs, offering new insights into both inference-time tuning and model
interpretability.


## Reinforcement Learning Finetunes Small Subnetworks in Large Language Models

>Authors: Sagnik Mukherjee, Lifan Yuan, Dilek Hakkani-Tur, Hao Peng

>2025-05-16

> http://arxiv.org/abs/2505.11711v1

Reinforcement learning (RL) yields substantial improvements in large language
models (LLMs) downstream task performance and alignment with human values.
Surprisingly, such large gains result from updating only a small subnetwork
comprising just 5 percent to 30 percent of the parameters, with the rest
effectively unchanged. We refer to this phenomenon as parameter update **sparsity**
induced by RL. It is observed across all 7 widely used RL algorithms (e.g.,
PPO, GRPO, DPO) and all 10 LLMs from different families in our experiments.
This **sparsity** is intrinsic and occurs without any explicit **sparsity** promoting
regularizations or architectural constraints. Finetuning the subnetwork alone
recovers the test accuracy, and, remarkably, produces a model nearly identical
to the one obtained via full finetuning. The subnetworks from different random
seeds, training data, and even RL algorithms show substantially greater overlap
than expected by chance. Our analysis suggests that this **sparsity** is not due to
updating only a subset of layers, instead, nearly all parameter matrices
receive similarly **sparse** updates. Moreover, the updates to almost all parameter
matrices are nearly full-rank, suggesting RL updates a small subset of
parameters that nevertheless span almost the full subspaces that the parameter
matrices can represent. We conjecture that the this update **sparsity** can be
primarily attributed to training on data that is near the policy distribution,
techniques that encourage the policy to remain close to the pretrained model,
such as the KL regularization and gradient clipping, have limited impact.


## Attend to Not Attended Structure-then-Detail Token Merging for Post-training DiT Acceleration

>Authors: Haipeng Fang, Sheng Tang, Juan Cao, Enshuo Zhang, Fan Tang, Tong-Yee Lee

>2025-05-16

> http://arxiv.org/abs/2505.11707v1

Diffusion transformers have shown exceptional performance in visual
generation but incur high computational costs. Token reduction techniques that
compress models by sharing the denoising process among similar tokens have been
introduced. However, existing approaches neglect the denoising priors of the
diffusion models, leading to suboptimal **acceleration** and diminished image
quality. This study proposes a novel concept: attend to prune feature
redundancies in areas not attended by the diffusion process. We analyze the
location and degree of feature redundancies based on the structure-then-detail
denoising priors. Subsequently, we introduce SDTM, a structure-then-detail
token merging approach that dynamically compresses feature redundancies.
Specifically, we design dynamic visual token merging, compression ratio
adjusting, and prompt reweighting for different stages. Served in a
post-training way, the proposed method can be integrated seamlessly into any
DiT architecture. Extensive experiments across various backbones, schedulers,
and datasets showcase the superiority of our method, for example, it achieves
1.55 times **acceleration** with negligible impact on image quality. Project page:
https://github.com/ICTMCG/SDTM.


## Qronos Correcting the Past by Shaping the Future... in Post-Training Quantization

>Authors: Shihao Zhang, Haoyu Zhang, Ian Colbert, Rayan Saab

>2025-05-16

> http://arxiv.org/abs/2505.11695v1

We introduce Qronos -- a new state-of-the-art post-training **quantization**
algorithm that sequentially rounds and updates neural network weights. Qronos
not only explicitly corrects errors due to both weight and activation
**quantization**, but also errors resulting from quantizing previous layers. Our
iterative algorithm is based on an interpretable and disciplined optimization
framework that subsumes and surpasses existing data-driven approaches. At each
step, Qronos alternates between error correction and diffusion via optimal
update rules. Importantly, we prove that Qronos admits an efficient
implementation that uses the Cholesky decomposition for solving least-squares
problems. We also demonstrate that Qronos is compatible with existing
transformation techniques such as Hadamard-based incoherence processing and
weight-activation scaling equalization, among others. We evaluate Qronos using
recent autoregressive language generation models in the Llama3 family; Qronos
consistently outperforms previous state-of-the-art adaptive rounding methods
when quantizing the weights, activations, and/or **KV** caches.


## Ambiguity Resolution in Text-to-Structured Data Mapping

>Authors: Zhibo Hu, Chen Wang, Yanfeng Shu, Hye-Young Paik, Liming Zhu

>2025-05-16

> http://arxiv.org/abs/2505.11679v1

Ambiguity in natural language is a significant obstacle for achieving
accurate text to structured data mapping through large language models (LLMs),
which affects the performance of tasks such as mapping text to agentic tool
calling and text-to-SQL queries. Existing methods of ambiguity handling either
exploit ReACT framework to produce the correct mapping through trial and error,
or supervised fine tuning to guide models to produce a biased mapping to
improve certain tasks. In this paper, we adopt a different approach that
characterizes the representation difference of ambiguous text in the latent
space and leverage the difference to identify ambiguity before mapping them to
structured data. To detect ambiguity of a sentence, we focused on the
relationship between ambiguous questions and their interpretations and what
cause the LLM ignore multiple interpretations. Different to the distance
calculated by dense embedding vectors, we utilize the observation that
ambiguity is caused by concept missing in latent space of LLM to design a new
distance measurement, computed through the path kernel by the integral of
gradient values for each concepts from **sparse**-autoencoder (SAE) under each
state. We identify patterns to distinguish ambiguous questions with this
measurement. Based on our observation, We propose a new framework to improve
the performance of LLMs on ambiguous agentic tool calling through missing
concepts prediction.


## SageAttention3 Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training

>Authors: Jintao Zhang, Jia Wei, Pengle Zhang, Xiaoming Xu, Haofeng Huang, Haoxu Wang, Kai Jiang, Jun Zhu, Jianfei Chen

>2025-05-16

> http://arxiv.org/abs/2505.11594v1

The efficiency of attention is important due to its quadratic time
complexity. We enhance the efficiency of attention through two key
contributions: First, we leverage the new FP4 Tensor Cores in Blackwell GPUs to
accelerate attention computation. Our implementation achieves 1038 TOPS on
RTX5090, which is a 5x speedup over the fastest FlashAttention on RTX5090.
Experiments show that our FP4 attention can accelerate inference of various
models in a plug-and-play way. Second, we pioneer **low-bit** attention to training
tasks. Existing **low-bit** attention works like FlashAttention3 and SageAttention
focus only on inference. However, the efficiency of training large models is
also important. To explore whether **low-bit** attention can be effectively applied
to training tasks, we design an accurate and efficient 8-bit attention for both
forward and backward propagation. Experiments indicate that 8-bit attention
achieves lossless performance in fine-tuning tasks but exhibits slower
convergence in pretraining tasks. The code will be available at
https://github.com/thu-ml/SageAttention.


## MegaScale-MoE Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production

>Authors: Chao Jin, Ziheng Jiang, Zhihao Bai, Zheng Zhong, Juncai Liu, Xiang Li, Ningxin Zheng, Xi Wang, Cong Xie, Qi Huang, Wen Heng, Yiyuan Ma, Wenlei Bao, Size Zheng, Yanghua Peng, Haibin Lin, Xuanzhe Liu, Xin Jin, Xin Liu

>2025-05-16

> http://arxiv.org/abs/2505.11432v2

We present MegaScale-MoE, a production system tailored for the efficient
training of large-scale mixture-of-experts (MoE) models. MoE emerges as a
promising architecture to scale large language models (LLMs) to unprecedented
sizes, thereby enhancing model performance. However, existing MoE training
systems experience a degradation in training efficiency, exacerbated by the
escalating scale of MoE models and the continuous evolution of hardware.
  Recognizing the pivotal role of efficient communication in enhancing MoE
training, MegaScale-MoE customizes communication-efficient parallelism
strategies for attention and FFNs in each MoE layer and adopts a holistic
approach to overlap communication with computation at both inter- and
intra-operator levels. Additionally, MegaScale-MoE applies communication
compression with adjusted communication patterns to lower precision, further
improving training efficiency. When training a 352B MoE model on 1,440 NVIDIA
Hopper GPUs, MegaScale-MoE achieves a training throughput of 1.41M tokens/s,
improving the efficiency by 1.88$\times$ compared to Megatron-LM. We share our
operational experience in accelerating MoE training and hope that by offering
our insights in system design, this work will motivate future research in MoE
systems.


## MID-L Matrix-Interpolated Dropout Layer with Layer-wise Neuron Selection

>Authors: Pouya Shaeri, Ariane Middel

>2025-05-16

> http://arxiv.org/abs/2505.11416v1

Modern neural networks often activate all neurons for every input, leading to
unnecessary computation and inefficiency. We introduce Matrix-Interpolated
Dropout Layer (MID-L), a novel module that dynamically selects and activates
only the most informative neurons by interpolating between two transformation
paths via a learned, input-dependent gating vector. Unlike conventional dropout
or static **sparsity** methods, MID-L employs a differentiable Top-k masking
strategy, enabling per-input adaptive computation while maintaining end-to-end
differentiability. MID-L is model-agnostic and integrates seamlessly into
existing architectures. Extensive experiments on six benchmarks, including
MNIST, CIFAR-10, CIFAR-100, SVHN, UCI Adult, and IMDB, show that MID-L achieves
up to average 55\% reduction in active neurons, 1.7$\times$ FLOPs savings, and
maintains or exceeds baseline accuracy. We further validate the informativeness
and selectivity of the learned neurons via Sliced Mutual Information (SMI) and
observe improved robustness under overfitting and noisy data conditions.
Additionally, MID-L demonstrates favorable inference latency and memory usage
profiles, making it suitable for both research exploration and deployment on
compute-constrained systems. These results position MID-L as a general-purpose,
plug-and-play dynamic computation layer, bridging the gap between dropout
regularization and efficient inference.


## MoE-CAP Benchmarking Cost, Accuracy and Performance of Sparse Mixture-of-Experts Systems

>Authors: Yinsicheng Jiang, Yao Fu, Yeqi Huang, Ping Nie, Zhan Lu, Leyang Xue, Congjie He, Man-Kit Sit, Jilong Xue, Li Dong, Ziming Miao, Dayou Du, Tairan Xu, Kai Zou, Edoardo Ponti, Luo Mai

>2025-05-16

> http://arxiv.org/abs/2505.11415v2

The **sparse** Mixture-of-Experts (MoE) architecture is increasingly favored for
scaling Large Language Models (LLMs) efficiently, but it depends on
heterogeneous compute and memory resources. These factors jointly affect system
Cost, Accuracy, and Performance (CAP), making trade-offs inevitable. Existing
benchmarks often fail to capture these trade-offs accurately, complicating
practical deployment decisions. To address this, we introduce MoE-CAP, a
benchmark specifically designed for MoE systems. Our analysis reveals that
achieving an optimal balance across CAP is difficult with current hardware; MoE
systems typically optimize two of the three dimensions at the expense of the
third-a dynamic we term the MoE-CAP trade-off. To visualize this, we propose
the CAP Radar Diagram. We further introduce **sparsity**-aware performance
metrics-Sparse Memory Bandwidth Utilization (S-MBU) and Sparse Model FLOPS
Utilization (S-MFU)-to enable accurate performance benchmarking of MoE systems
across diverse hardware platforms and deployment scenarios.


## Dynamic Base model Shift for Delta Compression

>Authors: Chenyu Huang, Peng Ye, Shenghe Zheng, Xiaohui Wang, Lei Bai, Tao Chen, Wanli Ouyang

>2025-05-16

> http://arxiv.org/abs/2505.11344v1

Transformer-based models with the pretrain-finetune paradigm bring about
significant progress, along with the heavy storage and deployment costs of
finetuned models on multiple tasks. Delta compression attempts to lower the
costs by reducing the redundancy of delta parameters (i.e., the difference
between the finetuned and pre-trained model weights) through **pruning** or
**quantization**. However, existing methods by default employ the pretrained model
as the base model and compress the delta parameters for every task, which may
causes significant performance degradation, especially when the compression
rate is extremely high. To tackle this issue, we investigate the impact of
different base models on the performance of delta compression and find that the
pre-trained base model can hardly be optimal. To this end, we propose Dynamic
Base Model Shift (DBMS), which dynamically adapts the base model to the target
task before performing delta compression. Specifically, we adjust two
parameters, which respectively determine the magnitude of the base model shift
and the overall scale of delta compression, to boost the compression
performance on each task. Through low-cost learning of these two parameters,
our DBMS can maintain most of the finetuned model's performance even under an
extremely high compression ratio setting, significantly surpassing existing
methods. Moreover, our DBMS is orthogonal and can be integrated with a variety
of other methods, and it has been evaluated across different types of models
including language, vision transformer, and multi-modal models.


## Wave turbulence, thermalization and multimode locking in optical fibers

>Authors: M. Ferraro, K. Baudin, M. Gervaziev, A. Fusaro, A. Picozzi, J. Garnier, G. Millot, D. Kharenko, E. Podivilov, S. Babin, F. Mangini, S. Wabnitz

>2025-05-16

> http://arxiv.org/abs/2505.11299v1

We present a comprehensive overview of recent advances in theory and
experiments on complex light propagation phenomena in nonlinear multimode
fibers. On the basis of the wave turbulence theory, we derive kinetic equations
describing the out-of-equilibrium process of optical thermalization toward the
Rayleigh-Jeans (RJ) equilibrium distribution. Our theory explains the effect of
beam self-cleaning (BSC) in graded-index (GRIN) fibers, whereby a speckled beam
transforms into a bell-shaped beam at the fiber output. We theoretically
explore the role of random refractive index fluctuations along the fiber, and
show how these imperfections can assist the observation of BSC in a practical
experimental setting. This conclusion is supported by the derivation of wave
turbulence kinetic equations that account for the presence of a time-dependent
disorder (random mode coupling). The kinetic theory reveals that a weak
disorder accelerates the rate of RJ thermalization and condensation. On the
other hand, although strong disorder is expected to suppress wave condensation,
the kinetic equation reveals that an out-of-equilibrium process of condensation
and RJ thermalization can still occur. The kinetic equations are validated by
numerical simulations of the nonlinear Schrodinger equation. We outline a
series of recent experiments, which permit to confirm the statistical mechanics
approach for describing beam propagation and thermalization. For example, we
highlight the demonstration of entropy growth, and point out that there are
inherent limits to peak-power scaling in multimode fiber lasers. We conclude by
pointing out the experimental observation that BSC is accompanied by an effect
of modal phase-locking. From the one hand this explains the observed
preservation of the spatial coherence of the beam, but also it points to the
need of extending current descriptions in future research.


## Fourier Low-rank and Sparse Tensor for Efficient Tensor Completion

>Authors: Jingyang Li, Jiuqian Shang, Yang Chen

>2025-05-16

> http://arxiv.org/abs/2505.11261v1

Tensor completion is crucial in many scientific domains with missing data
problems. Traditional low-rank tensor models, including CP, Tucker, and
Tensor-Train, exploit low-dimensional structures to recover missing data.
However, these methods often treat all tensor modes symmetrically, failing to
capture the unique spatiotemporal patterns inherent in scientific data, where
the temporal component exhibits both low-frequency stability and high-frequency
variations. To address this, we propose a novel model, \underline{F}ourier
\underline{Lo}w-rank and \underline{S}parse \underline{T}ensor (FLoST), which
decomposes the tensor along the temporal dimension using a Fourier transform.
This approach captures low-frequency components with low-rank matrices and
high-frequency fluctuations with **sparsity**, resulting in a hybrid structure that
efficiently models both smooth and localized variations. Compared to the
well-known tubal-rank model, which assumes low-rankness across all frequency
components, FLoST requires significantly fewer parameters, making it
computationally more efficient, particularly when the time dimension is large.
Through theoretical analysis and empirical experiments, we demonstrate that
FLoST outperforms existing tensor completion models in terms of both accuracy
and computational efficiency, offering a more interpretable solution for
spatiotemporal data reconstruction.


## Delta Attention Fast and Accurate Sparse Attention Inference by Delta Correction

>Authors: Jeffrey Willette, Heejun Lee, Sung Ju Hwang

>2025-05-16

> http://arxiv.org/abs/2505.11254v1

The attention mechanism of a transformer has a quadratic complexity, leading
to high inference costs and latency for long sequences. However, attention
matrices are mostly **sparse**, which implies that many entries may be omitted from
computation for efficient inference. Sparse attention inference methods aim to
reduce this computational burden; however, they also come with a troublesome
performance degradation. We discover that one reason for this degradation is
that the **sparse** calculation induces a distributional shift in the attention
outputs. The distributional shift causes decoding-time queries to fail to align
well with the appropriate keys from the prefill stage, leading to a drop in
performance. We propose a simple, novel, and effective procedure for correcting
this distributional shift, bringing the distribution of **sparse** attention
outputs closer to that of quadratic attention. Our method can be applied on top
of any **sparse** attention method, and results in an average 36%pt performance
increase, recovering 88% of quadratic attention accuracy on the 131K RULER
benchmark when applied on top of sliding window attention with sink tokens
while only adding a small overhead. Our method can maintain approximately 98.5%
**sparsity** over full quadratic attention, making our model 32 times faster than
Flash Attention 2 when processing 1M token prefills.


## Memory-Efficient Orthogonal Fine-Tuning with Principal Subspace Adaptation

>Authors: Fei Wu, Jia Hu, Geyong Min, Shiqiang Wang

>2025-05-16

> http://arxiv.org/abs/2505.11235v1

Driven by the relentless growth in model parameters, which renders full
fine-tuning prohibitively expensive for large-scale deployment,
parameter-efficient fine-tuning (PEFT) has emerged as a crucial approach for
rapidly adapting large models to a wide range of downstream tasks. Among the
PEFT family, orthogonal fine-tuning and its variants have demonstrated
remarkable performance by preserving hyperspherical energy, which encodes
pairwise angular similarity between neurons. However, these methods are
inherently memory-inefficient due to the need to store intermediate activations
from multiple full-dimensional **sparse** matrices. To address this limitation, we
propose Memory-efficient Orthogonal Fine-Tuning (MOFT) with principal subspace
adaptation. Specifically, we first establish a theoretical condition under
which orthogonal transformations within a low-rank subspace preserve
hyperspherical energy. Based on this insight, we constrain orthogonal
fine-tuning to the principal subspace defined by the top-r components obtained
through singular value decomposition and impose an additional constraint on the
projection matrix to satisfy the preservation condition. To enhance MOFT's
flexibility across tasks, we relax strict orthogonality by introducing two
learnable scaling vectors. Extensive experiments on 37 diverse tasks and four
models across NLP and CV demonstrate that MOFT consistently outperforms key
baselines while significantly reducing the memory footprint of orthogonal
fine-tuning.


## From Intent Discovery to Recognition with Topic Modeling and Synthetic Data

>Authors: Aaron Rodrigues, Mahmood Hegazy, Azzam Naeem

>2025-05-16

> http://arxiv.org/abs/2505.11176v1

Understanding and recognizing customer intents in AI systems is crucial,
particularly in domains characterized by short utterances and the cold start
problem, where recommender systems must include new products or services
without sufficient real user data. Customer utterances are characterized by
infrequent word co-occurences and high term variability, which poses
significant challenges for traditional methods in specifying distinct user
needs and preparing synthetic queries. To address this, we propose an agentic
LLM framework for topic modeling and synthetic query generation, which
accelerates the discovery and recognition of customer intents. We first apply
hierarchical topic modeling and intent discovery to expand a human-curated
taxonomy from 36 generic user intents to 278 granular intents, demonstrating
the potential of LLMs to significantly enhance topic specificity and diversity.
Next, to support newly discovered intents and address the cold start problem,
we generate synthetic user query data, which augments real utterances and
reduces dependency on human annotation, especially in low-resource settings.
Topic model experiments show substantial improvements in coherence and
relevance after topic expansion, while synthetic data experiments indicate that
in-class few-shot prompting significantly improves the quality and utility of
synthetic queries without compromising diversity. We also show that
LLM-generated intent descriptions and keywords can effectively substitute for
human-curated versions when used as context for synthetic query generation. Our
research underscores the scalability and utility of LLM agents in topic
modeling and highlights the strategic use of synthetic utterances to enhance
dataset variability and coverage for intent recognition. We present a
comprehensive and robust framework for online discovery and recognition of new
customer intents in dynamic domains.


## Gaussian Weight Sampling for Scalable, Efficient and Stable Pseudo-Quantization Training

>Authors: Myeonghwan Ahn, Sungjoo Yoo

>2025-05-16

> http://arxiv.org/abs/2505.11170v1

Ever-growing scale of large language models (LLMs) is pushing for improved
efficiency, favoring fully **quantize**d training (FQT) over BF16. While FQT
accelerates training, it faces consistency challenges and requires searching
over an exponential number of cases, each needing over 200B tokens to ensure
stability.
  Pseudo-**quantization** training (PQT) addresses the issues of FQT, although it
is not well-studied. We explore the practical implications of PQT in detail and
propose a noise distribution $R$ that is floating-point (FP)-friendly, with
ideal properties including stochastic precision annealing. As a result, the
proposed method serves as an effective theoretical foundation for low-precision
FP parameters through PQT, utilizing efficient fake **quantization** via an
addition and subsequent FP casting.
  We demonstrate that Gaussian weight sampling is (1) scalable: supports
low-precision FP parameters down to FP6 and high-precision noise up to 9-bit
with BF16 operator. The proposed method is (2) efficient: incurring
computational overhead as low as 1.40\% on the A100 GPU in terms of Llama2
training tokens per second, and requiring 2 bytes per parameter in GPU memory.
We demonstrate that PQT with Gaussian weight sampling is (3) stable: closely
following or even surpassing performance of the BF16 baseline while
pre-training GPT2 and Llama2 models with up to 1B parameters and 300B tokens.


## InfiJanice Joint Analysis and In-situ Correction Engine for Quantization-Induced Math Degradation in Large Language Models

>Authors: Zhen Li, Yupeng Su, Songmiao Wang, Runming Yang, Congkai Xie, Aofan Liu, Ming Li, Jiannong Cao, Yuan Xie, Ngai Wong, Hongxia Yang

>2025-05-16

> http://arxiv.org/abs/2505.11574v1

Large Language Models (LLMs) have demonstrated impressive performance on
complex reasoning benchmarks such as GSM8K, MATH, and AIME. However, the
substantial computational demands of these tasks pose significant challenges
for real-world deployment. Model **quantization** has emerged as a promising
approach to reduce memory footprint and inference latency by representing
weights and activations with lower bit-widths. In this work, we conduct a
comprehensive study of mainstream **quantization** methods(e.g., AWQ, GPTQ,
SmoothQuant) on the most popular open-sourced models (e.g., Qwen2.5, LLaMA3
series), and reveal that **quantization** can degrade mathematical reasoning
accuracy by up to 69.81%. To better understand this degradation, we develop an
automated assignment and judgment pipeline that qualitatively categorizes
failures into four error types and quantitatively identifies the most impacted
reasoning capabilities. Building on these findings, we employ an automated
data-curation pipeline to construct a compact "Silver Bullet" datasets.
Training a **quantize**d model on as few as 332 carefully selected examples for
just 3-5 minutes on a single GPU is enough to restore its reasoning accuracy to
match that of the full-precision baseline.


## Maximizing Asynchronicity in Event-based Neural Networks

>Authors: Haiqing Hao, Nikola Zubić, Weihua He, Zhipeng Sui, Davide Scaramuzza, Wenhui Wang

>2025-05-16

> http://arxiv.org/abs/2505.11165v1

Event cameras deliver visual data with high temporal resolution, low latency,
and minimal redundancy, yet their asynchronous, **sparse** sequential nature
challenges standard tensor-based machine learning (ML). While the recent
asynchronous-to-synchronous (A2S) paradigm aims to bridge this gap by
asynchronously encoding events into learned representations for ML pipelines,
existing A2S approaches often sacrifice representation expressivity and
generalizability compared to dense, synchronous methods. This paper introduces
EVA (EVent Asynchronous representation learning), a novel A2S framework to
generate highly expressive and generalizable event-by-event representations.
Inspired by the analogy between events and language, EVA uniquely adapts
advances from language modeling in linear attention and self-supervised
learning for its construction. In demonstration, EVA outperforms prior A2S
methods on recognition tasks (DVS128-Gesture and N-Cars), and represents the
first A2S framework to successfully master demanding detection tasks, achieving
a remarkable 47.7 mAP on the Gen1 dataset. These results underscore EVA's
transformative potential for advancing real-time event-based vision
applications.


## Hybrid-Emba3D Geometry-Aware and Cross-Path Feature Hybrid Enhanced State Space Model for Point Cloud Classification

>Authors: Bin Liu, Chunyang Wang, Xuelian Liu, Guan Xi, Ge Zhang, Ziteng Yao, Mengxue Dong

>2025-05-16

> http://arxiv.org/abs/2505.11099v1

The point cloud classification tasks face the dual challenge of efficiently
extracting local geometric features while maintaining model complexity. The
Mamba architecture utilizes the linear complexity advantage of state space
models (SSMs) to overcome the computational bottleneck of Transformers while
balancing global modeling capabilities. However, the inherent contradiction
between its unidirectional dependency and the unordered nature of point clouds
impedes modeling spatial correlation in local neighborhoods, thus constraining
geometric feature extraction. This paper proposes Hybrid-Emba3D, a
bidirectional Mamba model enhanced by geometry-feature coupling and cross-path
feature hybridization. The Local geometric pooling with geometry-feature
coupling mechanism significantly enhances local feature discriminative power
via coordinated propagation and dynamic aggregation of geometric information
between local center points and their neighborhoods, without introducing
additional parameters. The designed Collaborative feature enhancer adopts
dual-path hybridization, effectively handling local mutations and **sparse** key
signals, breaking through the limitations of traditional SSM long-range
modeling. Experimental results demonstrate that the proposed model achieves a
new SOTA classification accuracy of 95.99% on ModelNet40 with only 0.03M
additional.


## Addition is almost all you need Compressing neural networks with double binary factorization

>Authors: Vladimír Boža, Vladimír Macko

>2025-05-16

> http://arxiv.org/abs/2505.11076v1

Binary **quantization** approaches, which replace weight matrices with binary
matrices and substitute costly multiplications with cheaper additions, offer a
computationally efficient approach to address the increasing computational and
storage requirements of Large Language Models (LLMs). However, the severe
**quantization** constraint ($\pm1$) can lead to significant accuracy degradation.
In this paper, we propose Double Binary Factorization (DBF), a novel method
that factorizes dense weight matrices into products of two binary (sign)
matrices, each accompanied by scaling vectors. DBF preserves the efficiency
advantages of binary representations while achieving compression rates that are
competitive with or superior to state-of-the-art methods. Specifically, in a
1-bit per weight range, DBF is better than existing binarization approaches. In
a 2-bit per weight range, DBF is competitive with the best **quantization** methods
like QuIP\# and QTIP. Unlike most existing compression techniques, which offer
limited compression level choices, DBF allows fine-grained control over
compression ratios by adjusting the factorization's intermediate dimension.
Based on this advantage, we further introduce an algorithm for estimating
non-uniform layer-wise compression ratios for DBF, based on previously
developed channel **pruning** criteria.
  Code available at: https://github.com/usamec/double_binary


## Space Group Equivariant Crystal Diffusion

>Authors: Rees Chang, Angela Pak, Alex Guerra, Ni Zhan, Nick Richardson, Elif Ertekin, Ryan P. Adams

>2025-05-16

> http://arxiv.org/abs/2505.10994v1

Accelerating inverse design of crystalline materials with generative models
has significant implications for a range of technologies. Unlike other atomic
systems, 3D crystals are invariant to discrete groups of isometries called the
space groups. Crucially, these space group symmetries are known to heavily
influence materials properties. We propose SGEquiDiff, a crystal generative
model which naturally handles space group constraints with space group
invariant likelihoods. SGEquiDiff consists of an SE(3)-invariant, telescoping
discrete sampler of crystal lattices; permutation-invariant, transformer-based
autoregressive sampling of Wyckoff positions, elements, and numbers of
symmetrically unique atoms; and space group equivariant diffusion of atomic
coordinates. We show that space group equivariant vector fields automatically
live in the tangent spaces of the Wyckoff positions. SGEquiDiff achieves
state-of-the-art performance on standard benchmark datasets as assessed by
quantitative proxy metrics and quantum mechanical calculations.


## Group-in-Group Policy Optimization for LLM Agent Training

>Authors: Lang Feng, Zhenghai Xue, Tingcong Liu, Bo An

>2025-05-16

> http://arxiv.org/abs/2505.10978v1

Recent advances in group-based reinforcement learning (RL) have driven
frontier large language models (LLMs) in single-turn tasks like mathematical
reasoning. However, their scalability to long-horizon LLM agent training
remains limited. Unlike static tasks, agent-environment interactions unfold
over many steps and often yield **sparse** or delayed rewards, making credit
assignment across individual steps significantly more challenging. In this
work, we propose Group-in-Group Policy Optimization (GiGPO), a novel RL
algorithm that achieves fine-grained credit assignment for LLM agents while
preserving the appealing properties of group-based RL: critic-free, low memory,
and stable convergence. GiGPO introduces a two-level structure for estimating
relative advantage: (i) At the episode-level, GiGPO computes macro relative
advantages based on groups of complete trajectories; (ii) At the step-level,
GiGPO introduces an anchor state grouping mechanism that retroactively
constructs step-level groups by identifying repeated environment states across
trajectories. Actions stemming from the same state are grouped together,
enabling micro relative advantage estimation. This hierarchical structure
effectively captures both global trajectory quality and local step
effectiveness without relying on auxiliary models or additional rollouts. We
evaluate GiGPO on two challenging agent benchmarks, ALFWorld and WebShop, using
Qwen2.5-1.5B-Instruct and Qwen2.5-7B-Instruct. Crucially, GiGPO delivers
fine-grained per-step credit signals and achieves performance gains of > 12\%
on ALFWorld and > 9\% on WebShop over the GRPO baseline: all while maintaining
the same GPU memory overhead, identical LLM rollout, and incurring little to no
additional time cost.


## MPS-Prover Advancing Stepwise Theorem Proving by Multi-Perspective Search and Data Curation

>Authors: Zhenwen Liang, Linfeng Song, Yang Li, Tao Yang, Feng Zhang, Haitao Mi, Dong Yu

>2025-05-16

> http://arxiv.org/abs/2505.10962v1

Automated Theorem Proving (ATP) in formal languages remains a formidable
challenge in AI, demanding rigorous logical deduction and navigating vast
search spaces. While large language models (LLMs) have shown promising
performance, existing stepwise provers often suffer from biased search
guidance, leading to inefficiencies and suboptimal proof strategies. This paper
introduces the Multi-Perspective Search Prover (MPS-Prover), a novel stepwise
ATP system designed to overcome these limitations. MPS-Prover incorporates two
key innovations: a highly effective post-training data curation strategy that
prunes approximately 40% of redundant training data without sacrificing
performance, and a multi-perspective tree search mechanism. This search
integrates a learned critic model with strategically designed heuristic rules
to diversify tactic selection, prevent getting trapped in unproductive states,
and enhance search robustness. Extensive evaluations demonstrate that
MPS-Prover achieves state-of-the-art performance on multiple challenging
benchmarks, including miniF2F and ProofNet, outperforming prior 7B parameter
models. Furthermore, our analyses reveal that MPS-Prover generates
significantly shorter and more diverse proofs compared to existing stepwise and
whole-proof methods, highlighting its efficiency and efficacy. Our work
advances the capabilities of LLM-based formal reasoning and offers a robust
framework and a comprehensive analysis for developing more powerful theorem
provers.


## SubGCache Accelerating Graph-based RAG with Subgraph-level KV Cache

>Authors: Qiuyu Zhu, Liang Zhang, Qianxiong Xu, Cheng Long, Jie Zhang

>2025-05-16

> http://arxiv.org/abs/2505.10951v2

Graph-based retrieval-augmented generation (RAG) enables large language
models (LLMs) to incorporate structured knowledge via graph retrieval as
contextual input, enhancing more accurate and context-aware reasoning. We
observe that for different queries, it could retrieve similar subgraphs as
prompts, and thus we propose SubGCache, which aims to reduce inference latency
by reusing computation across queries with similar structural prompts (i.e.,
subgraphs). Specifically, SubGCache clusters queries based on subgraph
embeddings, constructs a representative subgraph for each cluster, and
pre-computes the key-value (**KV**) cache of the representative subgraph. For each
query with its retrieved subgraph within a cluster, it reuses the pre-computed
**KV** cache of the representative subgraph of the cluster without computing the **KV**
tensors again for saving computation. Experiments on two new datasets across
multiple LLM backbones and graph-based RAG frameworks demonstrate that
SubGCache consistently reduces inference latency with comparable and even
improved generation quality, achieving up to 6.68$\times$ reduction in
time-to-first-token (TTFT).


## Semantic Aware Linear Transfer by Recycling Pre-trained Language Models for Cross-lingual Transfer

>Authors: Seungyoon Lee, Seongtae Hong, Hyeonseok Moon, Heuiseok Lim

>2025-05-16

> http://arxiv.org/abs/2505.10945v2

Large Language Models (LLMs) increasingly incorporate multilingual
capabilities, fueling the demand to transfer them into target language-specific
models. However, most approaches, which blend the source model's embedding by
replacing the source vocabulary with the target language-specific vocabulary,
may constrain expressive capacity in the target language since the source model
is predominantly trained on English data. In this paper, we propose Semantic
Aware Linear Transfer (SALT), a novel cross-lingual transfer technique that
recycles embeddings from target language Pre-trained Language Models (PLMs) to
transmit the deep representational strengths of PLM-derived embedding to LLMs.
SALT derives unique regression lines based on the similarity in the overlap of
the source and target vocabularies, to handle each non-overlapping token's
embedding space. Our extensive experiments show that SALT significantly
outperforms other transfer methods and achieves lower loss with accelerating
faster convergence during language adaptation. Notably, SALT obtains remarkable
performance in cross-lingual understanding setups compared to other methods.
Furthermore, we highlight the scalable use of PLMs to enhance the functionality
of contemporary LLMs by conducting experiments with varying architectures.


## Accurate KV Cache Quantization with Outlier Tokens Tracing

>Authors: Yi Su, Yuechi Zhou, Quantong Qiu, Juntao Li, Qingrong Xia, Ping Li, Xinyu Duan, Zhefeng Wang, Min Zhang

>2025-05-16

> http://arxiv.org/abs/2505.10938v1

The impressive capabilities of Large Language Models (LLMs) come at the cost
of substantial computational resources during deployment. While **KV** Cache can
significantly reduce recomputation during inference, it also introduces
additional memory overhead. **KV** Cache **quantization** presents a promising
solution, striking a good balance between memory usage and accuracy. Previous
research has shown that the Keys are distributed by channel, while the Values
are distributed by token. Consequently, the common practice is to apply
channel-wise **quantization** to the Keys and token-wise **quantization** to the
Values. However, our further investigation reveals that a small subset of
unusual tokens exhibit unique characteristics that deviate from this pattern,
which can substantially impact **quantization** accuracy. To address this, we
develop a simple yet effective method to identify these tokens accurately
during the decoding process and exclude them from **quantization** as outlier
tokens, significantly improving overall accuracy. Extensive experiments show
that our method achieves significant accuracy improvements under 2-bit
**quantization** and can deliver a 6.4 times reduction in memory usage and a 2.3
times increase in throughput.


## Explain What You Mean Intent Augmented Knowledge Graph Recommender Built With An LLM

>Authors: Wenqing Zheng, Noah Fatsi, Daniel Barcklow, Dmitri Kalaev, Steven Yao, Owen Reinert, C. Bayan Bruss, Daniele Rosa

>2025-05-16

> http://arxiv.org/abs/2505.10900v2

Interaction **sparsity** is a long-standing challenge in recommendation systems.
Sparsity manifests in environments with disproportional cardinality of
groupings of entities, such as users and products in an online marketplace. It
is also found for newly introduced entities, described as the cold-start
problem. Recent efforts to mitigate this issue either enrich the connectivity
data by incorporating social networks or external knowledge graphs, or
fine-tune LLMs into interaction augmenters or next-item recommenders. However,
these techniques tend to be resource demanding, requiring high computational
power. They also have several limitations, including data availability, low
quality, or synthetic noise issues. In this work, we propose LLM-based Intent
Knowledge Graph Recommender (IKGR), a novel framework that leverages
retrieval-augmented generation and an encoding approach to construct and
densify a knowledge graph. IKGR leverages latent user-item affinities from an
interaction knowledge graph and further densifies it through mutual intent
connectivity. This addresses **sparsity** issues and allows the model to make
intent-grounded recommendations with an interpretable embedding translation
layer. Through extensive experiments on real-world datasets, we demonstrate
that IKGR overcomes knowledge gaps and achieves substantial gains over
state-of-the-art baselines on both publicly available and our internal
recommendation datasets.


## Improve Rule Retrieval and Reasoning with Self-Induction and Relevance ReEstimate

>Authors: Ziyang Huang, Wangtao Sun, Jun Zhao, Kang Liu

>2025-05-16

> http://arxiv.org/abs/2505.10870v1

This paper systematically addresses the challenges of rule retrieval, a
crucial yet underexplored area. Vanilla retrieval methods using **sparse** or dense
retrievers to directly search for relevant rules to support downstream
reasoning, often suffer from low accuracy. This is primarily due to a
significant semantic gap between the instantiated facts in the queries and the
abstract representations of the rules. Such misalignment results in suboptimal
retrieval quality, which in turn negatively impacts reasoning performance. To
overcome these challenges, we propose Self-Induction Augmented Retrieval
(SIAR), a novel approach that utilizes Large Language Models (LLMs) to induce
potential inferential rules that might offer benefits for reasoning by
abstracting the underlying knowledge and logical structure in queries. These
induced rules are then used for query augmentation to improve retrieval
effectiveness. Additionally, we introduce Rule Relevance ReEstimate (R$^3$), a
method that re-estimates the relevance of retrieved rules by assessing whether
the abstract knowledge they contain can be instantiated to align with the facts
in the queries and the helpfulness for reasoning. Extensive experiments across
various settings demonstrate the effectiveness and versatility of our proposed
methods.


## PoE-World Compositional World Modeling with Products of Programmatic Experts

>Authors: Wasu Top Piriyakulkij, Yichao Liang, Hao Tang, Adrian Weller, Marta Kryven, Kevin Ellis

>2025-05-16

> http://arxiv.org/abs/2505.10819v2

Learning how the world works is central to building AI agents that can adapt
to complex environments. Traditional world models based on deep learning demand
vast amounts of training data, and do not flexibly update their knowledge from
**sparse** observations. Recent advances in program synthesis using Large Language
Models (LLMs) give an alternate approach which learns world models represented
as source code, supporting strong generalization from little data. To date,
application of program-structured world models remains limited to natural
language and grid-world domains. We introduce a novel program synthesis method
for effectively modeling complex, non-gridworld domains by representing a world
model as an exponentially-weighted product of programmatic experts (PoE-World)
synthesized by LLMs. We show that this approach can learn complex, stochastic
world models from just a few observations. We evaluate the learned world models
by embedding them in a model-based planning agent, demonstrating efficient
performance and generalization to unseen levels on Atari's Pong and Montezuma's
Revenge. We release our code and display the learned world models and videos of
the agent's gameplay at https://topwasu.github.io/poe-world.


## Attention-Based Reward Shaping for Sparse and Delayed Rewards

>Authors: Ian Holmes, Min Chi

>2025-05-16

> http://arxiv.org/abs/2505.10802v1

Sparse and delayed reward functions pose a significant obstacle for
real-world Reinforcement Learning (RL) applications. In this work, we propose
Attention-based REward Shaping (ARES), a general and robust algorithm which
uses a transformer's attention mechanism to generate shaped rewards and create
a dense reward function for any environment. ARES requires a set of episodes
and their final returns as input. It can be trained entirely offline and is
able to generate meaningful shaped rewards even when using small datasets or
episodes produced by agents taking random actions. ARES is compatible with any
RL algorithm and can handle any level of reward **sparsity**. In our experiments,
we focus on the most challenging case where rewards are fully delayed until the
end of each episode. We evaluate ARES across a diverse range of environments,
widely used RL algorithms, and baseline methods to assess the effectiveness of
the shaped rewards it produces. Our results show that ARES can significantly
improve learning in delayed reward settings, enabling RL agents to train in
scenarios that would otherwise require impractical amounts of data or even be
unlearnable. To our knowledge, ARES is the first approach that works fully
offline, remains robust to extreme reward delays and low-quality data, and is
not limited to goal-based tasks.


## Bridging BCI and Communications A MIMO Framework for EEG-to-ECoG Wireless Channel Modeling

>Authors: Jiaheng Wang, Zhenyu Wang, Tianheng Xu, Yuan Si, Ang Li, Ting Zhou, Xi Zhao, Honglin Hu

>2025-05-16

> http://arxiv.org/abs/2505.10786v1

As a method to connect human brain and external devices, Brain-computer
interfaces (BCIs) are receiving extensive research attention. Recently, the
integration of communication theory with BCI has emerged as a popular trend,
offering potential to enhance system performance and shape next-generation
communications.
  A key challenge in this field is modeling the brain wireless communication
channel between intracranial electrocorticography (ECoG) emitting neurons and
extracranial electroencephalography (EEG) receiving electrodes. However, the
complex physiology of brain challenges the application of traditional channel
modeling methods, leaving relevant research in its infancy. To address this
gap, we propose a frequency-division multiple-input multiple-output (MIMO)
estimation framework leveraging simultaneous macaque EEG and ECoG recordings,
while employing neurophysiology-informed regularization to suppress noise
interference. This approach reveals profound similarities between neural signal
propagation and multi-antenna communication systems. Experimental results show
improved estimation accuracy over conventional methods while highlighting a
trade-off between frequency resolution and temporal stability determined by
signal duration. This work establish a conceptual bridge between neural
interfacing and communication theory, accelerating synergistic developments in
both fields.


## EdgeMM Multi-Core CPU with Heterogeneous AI-Extension and Activation-aware Weight Pruning for Multimodal LLMs at Edge

>Authors: Kangbo Bai, Le Ye, Ru Huang, Tianyu Jia

>2025-05-16

> http://arxiv.org/abs/2505.10782v1

Emerging multimodal LLMs (MLLMs) exhibit strong cross-modality perception and
reasoning capabilities and hold great potential for various applications at
edge. However, MLLMs typically consist of a compute-intensive modality encoder
and a memory-bound LLM decoder, leading to distinct bottlenecks for hardware
designs. In this work, we present a multi-core CPU solution with heterogeneous
AI extensions, which are based on either the compute-centric systolic array or
memory-centric digital compute-in-memory (CIM) co-processors. In addition,
dynamic activation-aware weight **pruning** and bandwidth management are developed
to enhance bandwidth efficiency and core utilization, improving overall
performance. We implemented our solution using commercial 22nm technology. For
representative MLLMs, our evaluations show EdgeMM can achieve 2.84x performance
speedup compared to laptop 3060 GPU.

