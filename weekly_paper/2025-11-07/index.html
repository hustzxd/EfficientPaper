<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>2025-11-07 - Efficient Paper</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.css" rel="stylesheet" />
        <link href="../../css/extra.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "2025-11-07";
        var mkdocs_page_input_path = "weekly_paper/2025-11-07.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
 <link href="../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }</style> <script src="../../assets/javascripts/glightbox.min.js"></script></head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> Efficient Paper
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Paper List</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../cls_year/">By Year</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cls_keyword/">By Keyword</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cls_publication/">By Publication</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cls_institution/">By Institution</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cls_author/">By Author</a>
                  </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../baseline_methods_graph/">Graph</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Weekly Paper</span></p>
              <ul class="current">
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">2025-11-07</a>
    <ul class="current">
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../2025-11-14/">2025-11-14</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../2025-11-21/">2025-11-21</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../2025-11-28/">2025-11-28</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../2025-12-05/">2025-12-05</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../2025-12-12/">2025-12-12</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../2025-12-19/">2025-12-19</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../2025-12-26/">2025-12-26</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Legacy</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../legacy/2025-07-25/">2025-07-25</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../legacy/2025-08-01/">2025-08-01</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../legacy/2025-08-08/">2025-08-08</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../legacy/2025-08-15/">2025-08-15</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../legacy/2025-08-22/">2025-08-22</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../legacy/2025-08-29/">2025-08-29</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../legacy/2025-09-05/">2025-09-05</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../legacy/2025-09-15/">2025-09-15</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../legacy/2025-09-19/">2025-09-19</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../legacy/2025-09-26/">2025-09-26</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../legacy/2025-09-28/">2025-09-28</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../legacy/2025-10-09/">2025-10-09</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../legacy/2025-10-17/">2025-10-17</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../legacy/2025-10-24/">2025-10-24</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../legacy/2025-10-31/">2025-10-31</a>
                </li>
    </ul>
                  </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../contributors/">Contributors</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">Efficient Paper</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Weekly Paper</li>
      <li class="breadcrumb-item active">2025-11-07</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
  
                <h1 id="2025-11-07">2025-11-07</h1>
<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#whisper-leak-a-side-channel-attack-on-large-language-models">Whisper Leak a side-channel attack on Large Language Models</a></li>
<li><a href="#towards-transparent-stance-detection-a-zero-shot-approach-using-implicit-and-explicit-interpretability">Towards Transparent Stance Detection A Zero-Shot Approach Using Implicit and Explicit Interpretability</a></li>
<li><a href="#perfdojo-automated-ml-library-generation-for-heterogeneous-architectures">PerfDojo Automated ML Library Generation for Heterogeneous Architectures</a></li>
<li><a href="#ragboost-efficient-retrieval-augmented-generation-with-accuracy-preserving-context-reuse">RAGBoost Efficient Retrieval-Augmented Generation with Accuracy-Preserving Context Reuse</a></li>
<li><a href="#surgvivqa-temporally-grounded-video-question-answering-for-surgical-scene-understanding">SurgViVQA Temporally-Grounded Video Question Answering for Surgical Scene Understanding</a></li>
<li><a href="#drl-based-robust-multi-timescale-anti-jamming-approaches-under-state-uncertainty">DRL-Based Robust Multi-Timescale Anti-Jamming Approaches under State Uncertainty</a></li>
<li><a href="#umdam-a-unified-data-layout-and-dram-address-mapping-for-heterogenous-npu-pim">UMDAM A Unified Data Layout and DRAM Address Mapping for Heterogenous NPU-PIM</a></li>
<li><a href="#Characterising-Global-Platforms-Centralised,-Decentralised,-Federated,-and-Grassroots">Characterising Global Platforms Centralised, Decentralised, Federated, and Grassroots</a></li>
<li><a href="#provable-separations-between-memorization-and-generalization-in-diffusion-models">Provable Separations between Memorization and Generalization in Diffusion Models</a></li>
<li><a href="#a-quantized-vae-mlp-botnet-detection-model-a-systematic-evaluation-of-quantization-aware-training-and-post-training-quantization-strategies">A Quantized VAE-MLP Botnet Detection Model A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies</a></li>
<li><a href="#ai-as-we-describe-it-how-large-language-models-and-their-applications-in-health-are-represented-across-channels-of-public-discourse">AI as We Describe It How Large Language Models and Their Applications in Health are Represented Across Channels of Public Discourse</a></li>
<li><a href="#large-language-models-require-a-new-form-of-oversight-capability-based-monitoring">Large language models require a new form of oversight capability-based monitoring</a></li>
<li><a href="#snapstream-efficient-long-sequence-decoding-on-dataflow-accelerators">SnapStream Efficient Long Sequence Decoding on Dataflow Accelerators</a></li>
<li><a href="#logicsparse-enabling-engine-free-unstructured-sparsity-for-quantised-deep-learning-accelerators">LogicSparse Enabling Engine-Free Unstructured Sparsity for Quantised Deep-learning Accelerators</a></li>
</ul>
<h2 id="whisper-leak-a-side-channel-attack-on-large-language-models">Whisper Leak a side-channel attack on Large Language Models</h2>
<blockquote>
<p>Authors: Geoff McDonald, Jonathan Bar Or</p>
<p>2025-11-05</p>
<p><a href="http://arxiv.org/abs/2511.03675v1">http://arxiv.org/abs/2511.03675v1</a></p>
</blockquote>
<p>Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) are increasingly deployed in sensitive domains
including healthcare, legal services, and confidential <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>s, where
privacy is paramount. This paper introduces Whisper Leak, a side-channel attack
that infers user prompt topics from encrypted <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> traffic by analyzing packet
size and timing patterns in streaming responses. Despite TLS encryption
protecting content, these metadata patterns leak sufficient information to
enable topic classification. We demonstrate the attack across 28 popular <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s
from major providers, achieving near-perfect classification (often &gt;98% AUPRC)
and high precision even at extreme class imbalance (10,000:1 noise-to-target
ratio). For many models, we achieve 100% precision in identifying sensitive
topics like "money laundering" while recovering 5-20% of target conversations.
This industry-wide vulnerability poses significant risks for users under
network surveillance by ISPs, governments, or local adversaries. We evaluate
three mitigation strategies - random padding, token batching, and packet
injection - finding that while each reduces attack effectiveness, none provides
complete protection. Through responsible disclosure, we have collaborated with
providers to implement initial countermeasures. Our findings underscore the
need for <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> providers to address metadata leakage as AI systems handle
increasingly sensitive information.</p>
<h2 id="towards-transparent-stance-detection-a-zero-shot-approach-using-implicit-and-explicit-interpretability">Towards Transparent Stance Detection A Zero-Shot Approach Using Implicit and Explicit Interpretability</h2>
<blockquote>
<p>Authors: Apoorva Upadhyaya, Wolfgang Nejdl, Marco Fisichella</p>
<p>2025-11-05</p>
<p><a href="http://arxiv.org/abs/2511.03635v1">http://arxiv.org/abs/2511.03635v1</a></p>
</blockquote>
<p>Zero-Shot Stance Detection (ZSSD) identifies the attitude of the post toward
unseen targets. Existing research using contrastive, meta-learning, or data
augmentation suffers from generalizability issues or lack of coherence between
text and target. Recent works leveraging large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) for ZSSD
focus either on improving unseen target-specific knowledge or generating
explanations for stance analysis. However, most of these works are limited by
their over-reliance on explicit reasoning, provide coarse explanations that
lack nuance, and do not explicitly model the reasoning process, making it
difficult to interpret the model's predictions. To address these issues, in our
study, we develop a novel interpretable ZSSD framework, IRIS. We provide an
interpretable understanding of the attitude of the input towards the target
implicitly based on sequences within the text (implicit rationales) and
explicitly based on linguistic measures (explicit rationales). IRIS considers
stance detection as an information retrieval ranking task, understanding the
relevance of implicit rationales for different stances to guide the model
towards correct predictions without requiring the ground-truth of rationales,
thus providing inherent interpretability. In addition, explicit rationales
based on communicative features help <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a> the emotional and cognitive
dimensions of stance, offering an interpretable understanding of the author's
attitude towards the given target. Extensive experiments on the benchmark
datasets of VAST, EZ-STANCE, P-Stance, and RFD using 50%, 30%, and even 10%
training data prove the generalizability of our model, benefiting from the
proposed architecture and interpretable design.</p>
<h2 id="perfdojo-automated-ml-library-generation-for-heterogeneous-architectures">PerfDojo Automated ML Library Generation for Heterogeneous Architectures</h2>
<blockquote>
<p>Authors: Andrei Ivanov, Siyuan Shen, Gioele Gottardo, Marcin Chrapek, Afif Boudaoud, Timo Schneider, Luca Benini, Torsten Hoefler</p>
<p>2025-11-05</p>
<p><a href="http://arxiv.org/abs/2511.03586v1">http://arxiv.org/abs/2511.03586v1</a></p>
</blockquote>
<p>The increasing complexity of machine learning models and the proliferation of
diverse hardware architectures (CPUs, GPUs, accelerators) make achieving
optimal performance a significant challenge. Heterogeneity in instruction sets,
specialized kernel requirements for different data types and model features
(e.g., <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a>, <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a>), and architecture-specific optimizations
complicate performance tuning. Manual optimization is resource-intensive, while
existing automatic approaches often rely on complex hardware-specific
heuristics and uninterpretable intermediate representations, hindering
performance portability. We introduce Perf<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>, a novel automatic optimization
methodology leveraging Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) and Reinforcement Learning
(RL). Central to this is PerfDojo, an environment framing optimization as an RL
game using a human-readable, mathematically-inspired code representation that
guarantees semantic validity through transformations. This allows effective
optimization without prior hardware knowledge, facilitating both human analysis
and RL agent training. We demonstrate Perf<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>'s ability to achieve significant
performance gains across diverse CPU (x86, Arm, RISC-V) and GPU architectures.</p>
<h2 id="ragboost-efficient-retrieval-augmented-generation-with-accuracy-preserving-context-reuse">RAGBoost Efficient Retrieval-Augmented Generation with Accuracy-Preserving Context Reuse</h2>
<blockquote>
<p>Authors: Yinsicheng Jiang, Yeqi Huang, Liang Cheng, Cheng Deng, Xuan Sun, Luo Mai</p>
<p>2025-11-05</p>
<p><a href="http://arxiv.org/abs/2511.03475v1">http://arxiv.org/abs/2511.03475v1</a></p>
</blockquote>
<p>Retrieval-augmented generation (RAG) enhances large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s)
with retrieved context but often suffers from downgraded <a class="glightbox" href="https://img.shields.io/badge/prefill-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/prefill-F08080" /></a> performance as
modern applications demand longer and more complex inputs. Existing caching
techniques either preserve accuracy with low <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> reuse or improve reuse at
the cost of degraded reasoning quality. We present RAGBoost, an efficient RAG
system that achieves high <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> reuse without sacrificing accuracy through
accuracy-pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> context reuse. RAGBoost detects <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a>ping retrieved items
across concurrent sessions and multi-turn interactions, using efficient context
indexing, ordering, and de-duplication to maximize reuse, while lightweight
contextual hints maintain reasoning fidelity. It integrates seamlessly with
existing <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> inference engines and improves their <a class="glightbox" href="https://img.shields.io/badge/prefill-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/prefill-F08080" /></a> performance by 1.5-3X
over state-of-the-art methods, while pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> or even enhancing reasoning
accuracy across diverse RAG and agentic AI workloads. Our code is released at:
https://github.com/Edinburgh-AgenticAI/RAGBoost.</p>
<h2 id="surgvivqa-temporally-grounded-video-question-answering-for-surgical-scene-understanding">SurgViVQA Temporally-Grounded Video Question Answering for Surgical Scene Understanding</h2>
<blockquote>
<p>Authors: Mauro Orazio Drago, Luca Carlini, Pelinsu Celebi Balyemez, Dennis Pierantozzi, Chiara Lena, Cesare Hassan, Danail Stoyanov, Elena De Momi, Sophia Bano, Mobarak I. Hoque</p>
<p>2025-11-05</p>
<p><a href="http://arxiv.org/abs/2511.03325v2">http://arxiv.org/abs/2511.03325v2</a></p>
</blockquote>
<p>Video Question Answering (VideoQA) in the surgical domain aims to enhance
intraoperative understanding by enabling AI models to reason over temporally
coherent events rather than isolated frames. Current approaches are limited to
static image features, and available datasets often lack temporal annotations,
ignoring the dynamics critical for accurate procedural interpretation. We
propose SurgViVQA, a surgical VideoQA model that extends visual reasoning from
static images to dynamic surgical scenes. It uses a Masked Video--Text Encoder
to fuse video and question features, capturing temporal cues such as motion and
tool--tissue interactions, which a fine-tuned large language model (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>) then
<a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>s into coherent answers. To evaluate its performance, we curated
REAL-Colon-VQA, a colonoscopic video dataset that includes motion-related
questions and diagnostic attributes, as well as out-of-template questions with
rephrased or semantically altered formulations to assess model robustness.
Experimental validation on REAL-Colon-VQA and the public EndoVis18-VQA dataset
shows that SurgViVQA outperforms existing image-based VQA benchmark models,
particularly in keyword accuracy, improving over PitVQA by +11\% on
REAL-Colon-VQA and +9\% on EndoVis18-VQA. A perturbation study on the questions
further confirms improved generalizability and robustness to variations in
question phrasing. SurgViVQA and the REAL-Colon-VQA dataset provide a framework
for temporally-aware understanding in surgical VideoQA, enabling AI models to
interpret dynamic procedural contexts more effectively. Code and dataset
available at https://github.com/madratak/SurgViVQA.</p>
<h2 id="drl-based-robust-multi-timescale-anti-jamming-approaches-under-state-uncertainty">DRL-Based Robust Multi-Timescale Anti-Jamming Approaches under State Uncertainty</h2>
<blockquote>
<p>Authors: Haoqin Zhao, Zan Li, Jiangbo Si, Rui Huang, Hang Hu, Tony Q. S. Quek, Naofal Al-Dhahir</p>
<p>2025-11-05</p>
<p><a href="http://arxiv.org/abs/2511.03305v1">http://arxiv.org/abs/2511.03305v1</a></p>
</blockquote>
<p>Owing to the openness of wireless channels, wireless <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> systems
are highly susceptible to malicious jamming. Most existing anti-jamming methods
rely on the assumption of accurate sensing and optimize parameters on a single
timescale. However, such methods overlook two practical issues: mismatched
execution latencies across heterogeneous actions and measurement errors caused
by sensor imperfections. Especially for deep reinforcement learning (DRL)-based
methods, the inherent sensitivity of neural networks implies that even minor
perturbations in the input can mislead the agent into choosing suboptimal
actions, with potentially severe consequences. To ensure reliable wireless
transmission, we establish a multi-timescale decision model that incorporates
state uncertainty. Subsequently, we propose two robust schemes that sustain
performance under bounded sensing errors. First, a Projected Gradient
Descent-assisted Double Deep Q-Network (PGD-DDQN) algorithm is designed, which
derives worst-case perturbations under a norm-bounded error model and applies
PGD during training for robust optimization. Second, a Nonlinear Q-Compression
DDQN (NQC-DDQN) algorithm introduces a nonlinear <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> mechanism that
adaptively contracts Q-value ranges to eliminate action aliasing. Simulation
results indicate that, compared with the perfect-sensing baseline, the proposed
algorithms show only minor degradation in anti-jamming performance while
maintaining robustness under various perturbations, thereby validating their
practicality in imperfect sensing conditions.</p>
<h2 id="umdam-a-unified-data-layout-and-dram-address-mapping-for-heterogenous-npu-pim">UMDAM A Unified Data Layout and DRAM Address Mapping for Heterogenous NPU-PIM</h2>
<blockquote>
<p>Authors: Hai Huang, Xuhong Qiang, Weisheng Zhao, Chenchen Liu</p>
<p>2025-11-05</p>
<p><a href="http://arxiv.org/abs/2511.03293v1">http://arxiv.org/abs/2511.03293v1</a></p>
</blockquote>
<p>Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) are increasingly deployed on edge devices with
Neural Processing Units (NPUs), yet the <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a> phase remains memory-intensive,
limiting performance. Processing-in-Memory (PIM) offers a promising solution,
but co-executing NPU-PIM systems face challenges such as data layout
mismatches, bandwidth loss, and redundant storage. To address these issues, we
propose UMDAM, a unified memory-affinity data layout and DRAM address mapping
scheme tailored for NPU-PIM co-execution. UMDAM employs a column-major,
tile-based layout and a configurable DRAM mapping strategy to ensure
compatibility with NPU computation while maximizing PIM efficiency -- without
introducing extra memory overhead or bandwidth loss. Comprehensive evaluations
on OPT models demonstrate that UMDAM reduces time-to-first-token (TTFT) by up
to 3.0x and time-to-last-token (TTLT) by 2.18x, significantly improving
end-to-end <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> inference efficiency on edge devices.</p>
<h2 id="characterising-global-platforms-centralised-decentralised-federated-and-grassroots">Characterising Global Platforms Centralised, Decentralised, Federated, and Grassroots</h2>
<blockquote>
<p>Authors: Ehud Shapiro</p>
<p>2025-11-05</p>
<p><a href="http://arxiv.org/abs/2511.03286v2">http://arxiv.org/abs/2511.03286v2</a></p>
</blockquote>
<p>Global digital platforms are software systems designed to serve entire
populations, with some already <a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> billions of people. We propose atomic
transactions-based multiagent transition systems and protocols as a formal
framework to study them; introduce essential agents -- minimal sets of agents
the removal of which makes <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> impossible; and show that the
cardinality of essential agents partitions all global platforms into four
classes:
  1. Centralised -- one (the server)
  2. Decentralised -- finite <script type="math/tex">>1</script> (bootstrap nodes)
  3. Federated -- infinite but not universal (all servers)
  4. Grassroots -- universal (all agents)
  Our illustrative formal example is a global social network, for which we
provide centralised, decentralised, federated, and grassroots specifications
via multiagent atomic transactions, and prove they all satisfy the same basic
correctness properties. We discuss informally additional global platforms --
currencies, ``sharing economy'' apps, AI, and more. While this may be the first
characterisation of centralised, decentralised, and federated global platforms,
grassroots platforms have been formally defined previously, but using different
notions. Here, we prove that their original definition implies that all agents
are essential, placing grassroots platforms in a distinct class within the
broader formal context that includes all global platforms. This work provides
the first mathematical framework for classifying any global platform --
existing or imagined -- by providing a multiagent atomic-transactions
specification of it and determining the cardinality of the minimal set of
essential agents in the ensuing multiagent protocol. It thus provides a
unifying mathematical approach for the study of global digital platforms,
perhaps the most important class of computer systems today.</p>
<h2 id="provable-separations-between-memorization-and-generalization-in-diffusion-models">Provable Separations between Memorization and Generalization in Diffusion Models</h2>
<blockquote>
<p>Authors: Zeqi Ye, Qijie Zhu, Molei Tao, Minshuo Chen</p>
<p>2025-11-05</p>
<p><a href="http://arxiv.org/abs/2511.03202v1">http://arxiv.org/abs/2511.03202v1</a></p>
</blockquote>
<p>Diffusion models have achieved remarkable success across diverse domains, but
they remain vulnerable to memorization -- reproducing training data rather than
generating novel outputs. This not only limits their creative potential but
also raises concerns about privacy and safety. While empirical studies have
explored mitigation strategies, theoretical understanding of memorization
remains limited. We address this gap through developing a dual-separation
result via two complementary perspectives: statistical estimation and network
approximation. From the estimation side, we show that the ground-truth score
function does not minimize the empirical denoising loss, creating a separation
that drives memorization. From the approximation side, we prove that
implementing the empirical score function requires network size to scale with
sample size, spelling a separation compared to the more compact network
representation of the ground-truth score function. Guided by these insights, we
develop a <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a>-based method that reduces memorization while maintaining
generation quality in diffusion <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>s.</p>
<h2 id="a-quantized-vae-mlp-botnet-detection-model-a-systematic-evaluation-of-quantization-aware-training-and-post-training-quantization-strategies">A Quantized VAE-MLP Botnet Detection Model A Systematic Evaluation of Quantization-Aware Training and Post-Training Quantization Strategies</h2>
<blockquote>
<p>Authors: Hassan Wasswa, Hussein Abbass, Timothy Lynar</p>
<p>2025-11-05</p>
<p><a href="http://arxiv.org/abs/2511.03201v1">http://arxiv.org/abs/2511.03201v1</a></p>
</blockquote>
<p>In an effort to counter the increasing IoT botnet-based attacks,
state-of-the-art deep learning methods have been proposed and have achieved
impressive detection accuracy. However, their computational intensity restricts
deployment on resource-constrained IoT devices, creating a critical need for
lightweight detection models. A common solution to this challenge is model
<a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> via <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a>. This study proposes a VAE-MLP model framework
where an MLP-based classifier is trained on 8-dimensional latent vectors
derived from the high-dimensional train data using the encoder component of a
pretrained variational autoencoder (VAE). Two widely used <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a>
strategies--Quantization-Aware Training (QAT) and Post-Training Quantization
(PTQ)--are then systematically evaluated in terms of their impact on detection
performance, storage efficiency, and inference latency using two benchmark IoT
botnet datasets--N-BaIoT and CICIoT2022. The results revealed that, with
respect to detection accuracy, the QAT strategy experienced a more noticeable
decline,whereas PTQ incurred only a marginal reduction compared to the original
un<a class="glightbox" href="https://img.shields.io/badge/quantize-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantize-F08080" /></a>d model. Furthermore, PTQ yielded a 6x speedup and 21x reduction in
size, while QAT achieved a 3x speedup and 24x <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a>, demonstrating the
practicality of <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a> for device-level IoT botnet detection.</p>
<h2 id="ai-as-we-describe-it-how-large-language-models-and-their-applications-in-health-are-represented-across-channels-of-public-discourse">AI as We Describe It How Large Language Models and Their Applications in Health are Represented Across Channels of Public Discourse</h2>
<blockquote>
<p>Authors: Jiawei Zhou, Lei Zhang, Mei Li, Benjamin D Horne, Munmun De Choudhury</p>
<p>2025-11-05</p>
<p><a href="http://arxiv.org/abs/2511.03174v1">http://arxiv.org/abs/2511.03174v1</a></p>
</blockquote>
<p>Representation shapes public attitudes and behaviors. With the arrival and
rapid adoption of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s, the way these systems are introduced will negotiate
societal expectations for their role in high-stakes domains like health. Yet it
remains unclear whether current narratives present a balanced view. We analyzed
five prominent discourse channels (news, research press, YouTube, TikTok, and
Reddit) over a two-year period on lexical style, informational content, and
symbolic representation. Discussions were generally positive and episodic, with
positivity increasing over time. Risk <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> was unthorough and often
reduced to information quality incidents, while explanations of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s'
generative nature were rare. Compared with professional outlets, TikTok and
Reddit highlighted wellbeing applications and showed greater variations in tone
and anthropomorphism but little attention to risks. We discuss implications for
public discourse as a diagnostic tool in identifying literacy and governance
gaps, and for <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> and design strategies to support more informed <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>
engagement.</p>
<h2 id="large-language-models-require-a-new-form-of-oversight-capability-based-monitoring">Large language models require a new form of oversight capability-based monitoring</h2>
<blockquote>
<p>Authors: Katherine C. Kellogg, Bingyang Ye, Yifan Hu, Guergana K. Savova, Byron Wallace, Danielle S. Bitterman</p>
<p>2025-11-05</p>
<p><a href="http://arxiv.org/abs/2511.03106v1">http://arxiv.org/abs/2511.03106v1</a></p>
</blockquote>
<p>The rapid adoption of large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) in healthcare has been
accompanied by scrutiny of their oversight. Existing monitoring approaches,
inherited from traditional machine learning (ML), are task-based and founded on
assumed performance degradation arising from dataset drift. In contrast, with
<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s, inevitable model degradation due to changes in populations compared to
the training dataset cannot be assumed, because <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s were not trained for any
specific task in any given population. We therefore propose a new organizing
principle guiding generalist <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> monitoring that is scalable and grounded in
how these models are developed and used in practice: capability-based
monitoring. Capability-based monitoring is motivated by the fact that <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s are
generalist systems whose <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a>ping internal capabilities are reused across
numerous downstream tasks. Instead of evaluating each downstream task
independently, this approach organizes monitoring around shared model
capabilities, such as summarization, reasoning, translation, or safety
guardrails, in order to enable cross-task detection of systemic weaknesses,
long-tail errors, and emergent behaviors that task-based monitoring may miss.
We describe considerations for developers, organizational leaders, and
professional societies for implementing a capability-based monitoring approach.
Ultimately, capability-based monitoring will provide a scalable foundation for
safe, adaptive, and collaborative monitoring of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s and future generalist
artificial intelligence models in healthcare.</p>
<h2 id="snapstream-efficient-long-sequence-decoding-on-dataflow-accelerators">SnapStream Efficient Long Sequence Decoding on Dataflow Accelerators</h2>
<blockquote>
<p>Authors: Jonathan Li, Nasim Farahini, Evgenii Iuliugin, Magnus Vesterlund, Christian Haggstrom, Guangtao Wang, Shubhangi Upasani, Ayush Sachdeva, Rui Li, Faline Fu, Chen Wu, Ayesha Siddiqua, John Long, Tuowen Zhao, Matheen Musaddiq, Hakan Zeffer, Yun Du, Mingran Wang, Qinghua Li, Bo Li, Urmish Thakker, Raghu Prabhakar</p>
<p>2025-11-05</p>
<p><a href="http://arxiv.org/abs/2511.03092v2">http://arxiv.org/abs/2511.03092v2</a></p>
</blockquote>
<p>The proliferation of 100B+ parameter Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) with 100k+
context length support have resulted in increasing demands for on-chip memory
to support large <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a>s. Techniques such as Streaming<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> and Snap<a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>
demonstrate how to control <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> size while maintaining model accuracy. Yet,
these techniques are not commonly used within industrial deployments using
frameworks like v<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> or SGLang. The reason is twofold: on one hand, the static
graphs and continuous batching methodology employed by these frameworks make it
difficult to admit modifications to the standard multi-head attention
algorithm, while on the other hand, the accuracy implications of such
techniques on modern instruction-following and reasoning models are not well
understood, obfuscating the need for implementing these techniques. In this
paper, we explore these accuracy implications on Llama-3.1-8B-Instruct and
DeepSeek-R1, and develop SnapStream, a <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> method that can be
deployed at scale. We demonstrate the efficacy of SnapStream in a 16-way
tensor-parallel deployment of DeepSeek-671B on SambaNova SN40L accelerators
running at 128k context length and up to 1832 tokens per second in a real
production setting. SnapStream enables <script type="math/tex">4\times</script> improved on-chip memory usage
and introduces minimal accuracy degradation on LongBench-v2, AIME24 and
LiveCodeBench. To the best of our knowledge, this is the first implementation
of <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> attention techniques deployed in a production inference system
with static graphs and continuous batching.</p>
<h2 id="logicsparse-enabling-engine-free-unstructured-sparsity-for-quantised-deep-learning-accelerators">LogicSparse Enabling Engine-Free Unstructured Sparsity for Quantised Deep-learning Accelerators</h2>
<blockquote>
<p>Authors: Changhong Li, Biswajit Basu, Shreejith Shanker</p>
<p>2025-11-05</p>
<p><a href="http://arxiv.org/abs/2511.03079v1">http://arxiv.org/abs/2511.03079v1</a></p>
</blockquote>
<p>FPGAs have been shown to be a promising platform for deploying Quantised
Neural Networks (QNNs) with high-speed, low-latency, and energy-efficient
inference. However, the complexity of modern deep-learning models limits the
performance on resource-constrained edge devices. While quantisation and
<a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> alleviate these challenges, unstructured <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a> remains
underexploited due to irregular memory access. This work introduces a framework
that embeds unstructured <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a> into dataflow accelerators, eliminating the
need for dedicated <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> engines and pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> parallelism. A hardware-aware
<a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> strategy is introduced to improve efficiency and design flow further.
On LeNet-5, the framework attains 51.6 x <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> and 1.23 x throughput
improvement using only 5.12% of LUTs, effectively exploiting unstructured
<a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a> for QNN <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a>.</p>
              
  <!-- Giscus  -  notes  -->
<script>
  //  JavaScript  URL 
  if (window.location.pathname.includes('/notes/')) {
    document.write(`
      <div id="giscus-container" style="margin-top: 2rem; padding-top: 1rem; border-top: 1px solid #e1e4e8;">
        <script src="https://giscus.app/client.js"
                data-repo="hustzxd/EfficientPaper"
                data-repo-id="R_kgDOKVYtOg"
                data-category="General"
                data-category-id="DIC_kwDOKVYtOs4CukCv"
                data-mapping="pathname"
                data-strict="0"
                data-reactions-enabled="1"
                data-emit-metadata="0"
                data-input-position="bottom"
                data-theme="light"
                data-lang="zh-CN"
                crossorigin="anonymous"
                async>
        <\/script>
      </div>
    `);
  }
</script>

            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../../baseline_methods_graph/" class="btn btn-neutral float-left" title="Graph"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../2025-11-14/" class="btn btn-neutral float-right" title="2025-11-14">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../../baseline_methods_graph/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../2025-11-14/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-clike.min.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
      <script src="../../js/prism-prototxt.js"></script>
      <script src="../../js/preview.js"></script>
      <script src="../../js/back-to-top.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
</script></body>
</html>
