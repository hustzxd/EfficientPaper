<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>2025-10-17 - Efficient Paper</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.css" rel="stylesheet" />
        <link href="../../css/extra.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "2025-10-17";
        var mkdocs_page_input_path = "weekly_paper/2025-10-17.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
 <link href="../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }</style> <script src="../../assets/javascripts/glightbox.min.js"></script></head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> Efficient Paper
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Paper List</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../cls_year/">By Year</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cls_keyword/">By Keyword</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cls_publication/">By Publication</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cls_institution/">By Institution</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cls_author/">By Author</a>
                  </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../baseline_methods_graph/">Graph</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Weekly Paper</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../2025-09-05/">2025-09-05</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../2025-09-15/">2025-09-15</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../2025-09-19/">2025-09-19</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../2025-09-26/">2025-09-26</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../2025-09-28/">2025-09-28</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../2025-10-09/">2025-10-09</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">2025-10-17</a>
    <ul class="current">
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Lagency</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../lagency/2025-07-25/">2025-07-25</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../lagency/2025-08-01/">2025-08-01</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../lagency/2025-08-08/">2025-08-08</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../lagency/2025-08-15/">2025-08-15</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../lagency/2025-08-22/">2025-08-22</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../lagency/2025-08-29/">2025-08-29</a>
                </li>
    </ul>
                  </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../contributors/">Contributors</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">Efficient Paper</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Weekly Paper</li>
      <li class="breadcrumb-item active">2025-10-17</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
  
                <h1 id="2025-10-17">2025-10-17</h1>
<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#breadcrumbs-reasoning-memory-efficient-reasoning-with-compression-beacons">Breadcrumbs Reasoning Memory-Efficient Reasoning with Compression Beacons</a></li>
<li><a href="#invited-paper-bitmedvit-ternary-quantized-vision-transformer-for-medical-ai-assistants-on-the-edge">Invited Paper BitMedViT Ternary-Quantized Vision Transformer for Medical AI Assistants on the Edge</a></li>
<li><a href="#Don't-Be-Greedy,-Just-Relax!-Pruning-LLMs-via-Frank-Wolfe">Don't Be Greedy, Just Relax! Pruning LLMs via Frank-Wolfe</a></li>
<li><a href="#how-sampling-affects-the-detectability-of-machine-written-texts-a-comprehensive-study">How Sampling Affects the Detectability of Machine-written texts A Comprehensive Study</a></li>
<li><a href="#adaptive-rescheduling-in-prefill-decode-disaggregated-llm-inference">Adaptive Rescheduling in Prefill-Decode Disaggregated LLM Inference</a></li>
<li><a href="#time-series-foundation-models-benchmarking-challenges-and-requirements">Time Series Foundation Models Benchmarking Challenges and Requirements</a></li>
<li><a href="#nosa-native-and-offloadable-sparse-attention">NOSA Native and Offloadable Sparse Attention</a></li>
<li><a href="#dolfin-balancing-stability-and-plasticity-in-federated-continual-learning">DOLFIN Balancing Stability and Plasticity in Federated Continual Learning</a></li>
<li><a href="#steer-moe-efficient-audio-language-alignment-with-a-mixture-of-experts-steering-module">Steer-MoE Efficient Audio-Language Alignment with a Mixture-of-Experts Steering Module</a></li>
<li><a href="#medrek-retrieval-based-editing-for-medical-llms-with-key-aware-prompts">MedREK Retrieval-Based Editing for Medical LLMs with Key-Aware Prompts</a></li>
<li><a href="#Who-Speaks-for-the-Trigger?-Dynamic-Expert-Routing-in-Backdoored-Mixture-of-Experts-Transformers">Who Speaks for the Trigger? Dynamic Expert Routing in Backdoored Mixture-of-Experts Transformers</a></li>
<li><a href="#f-bfq-flexible-block-floating-point-quantization-accelerator-for-llms">F-BFQ Flexible Block Floating-Point Quantization Accelerator for LLMs</a></li>
<li><a href="#Make-an-Offer-They-Can't-Refuse-Grounding-Bayesian-Persuasion-in-Real-World-Dialogues-without-Pre-Commitment">Make an Offer They Can't Refuse Grounding Bayesian Persuasion in Real-World Dialogues without Pre-Commitment</a></li>
<li><a href="#document-intelligence-in-the-era-of-large-language-models-a-survey">Document Intelligence in the Era of Large Language Models A Survey</a></li>
<li><a href="#taming-the-fragility-of-kv-cache-eviction-in-llm-inference">Taming the Fragility of KV Cache Eviction in LLM Inference</a></li>
<li><a href="#chatr1-reinforcement-learning-for-conversational-reasoning-and-retrieval-augmented-question-answering">ChatR1 Reinforcement Learning for Conversational Reasoning and Retrieval Augmented Question Answering</a></li>
<li><a href="#banaserve-unified-kv-cache-and-dynamic-module-migration-for-balancing-disaggregated-llm-serving-in-ai-infrastructure">BanaServe Unified KV Cache and Dynamic Module Migration for Balancing Disaggregated LLM Serving in AI Infrastructure</a></li>
<li><a href="#dscd-large-language-model-detoxification-with-self-constrained-decoding">DSCD Large Language Model Detoxification with Self-Constrained Decoding</a></li>
<li><a href="#a-dimension-keeping-semi-tensor-product-framework-for-compressed-sensing">A Dimension-Keeping Semi-Tensor Product Framework for Compressed Sensing</a></li>
<li><a href="#mirror-speculative-decoding-breaking-the-serial-barrier-in-llm-inference">Mirror Speculative Decoding Breaking the Serial Barrier in LLM Inference</a></li>
<li><a href="#retrieval-in-the-chain-bootstrapping-large-language-models-for-generative-retrieval">Retrieval-in-the-Chain Bootstrapping Large Language Models for Generative Retrieval</a></li>
<li><a href="#neurorvq-multi-scale-eeg-tokenization-for-generative-large-brainwave-models">NeuroRVQ Multi-Scale EEG Tokenization for Generative Large Brainwave Models</a></li>
<li><a href="#neural-approximate-inverse-preconditioners">Neural Approximate Inverse Preconditioners</a></li>
<li><a href="#computationally-efficient-neural-receivers-via-axial-self-attention">Computationally Efficient Neural Receivers via Axial Self-Attention</a></li>
<li><a href="#pruning-cannot-hurt-robustness-certified-trade-offs-in-reinforcement-learning">Pruning Cannot Hurt Robustness Certified Trade-offs in Reinforcement Learning</a></li>
<li><a href="#gaussian-process-implicit-surfaces-as-control-barrier-functions-for-safe-robot-navigation">Gaussian Process Implicit Surfaces as Control Barrier Functions for Safe Robot Navigation</a></li>
<li><a href="#kvcomm-online-cross-context-kv-cache-communication-for-efficient-llm-based-multi-agent-systems">KVCOMM Online Cross-context KV-cache Communication for Efficient LLM-based Multi-agent Systems</a></li>
<li><a href="#what-if--understanding-motion-through-sparse-interactions">What If  Understanding Motion Through Sparse Interactions</a></li>
<li><a href="#carvq-corrective-adaptor-with-group-residual-vector-quantization-for-llm-embedding-compression">CARVQ Corrective Adaptor with Group Residual Vector Quantization for LLM Embedding Compression</a></li>
<li><a href="#enhanced-angle-range-cluster-parameter-estimation-in-full-duplex-isac-systems">Enhanced Angle-Range Cluster Parameter Estimation in Full-Duplex ISAC Systems</a></li>
<li><a href="#Low-Latency,-High-Bandwidth-Streaming-of-Experimental-Data-with-EJFAT">Low Latency, High Bandwidth Streaming of Experimental Data with EJFAT</a></li>
<li><a href="#teaching-language-models-to-faithfully-express-their-uncertainty">Teaching Language Models to Faithfully Express their Uncertainty</a></li>
<li><a href="#smec-rethinking-matryoshka-representation-learning-for-retrieval-embedding-compression">SMEC Rethinking Matryoshka Representation Learning for Retrieval Embedding Compression</a></li>
<li><a href="#evaluating-and-mitigating-llm-as-a-judge-bias-in-communication-systems">Evaluating and Mitigating LLM-as-a-judge Bias in Communication Systems</a></li>
<li><a href="#probing-latent-knowledge-conflict-for-faithful-retrieval-augmented-generation">Probing Latent Knowledge Conflict for Faithful Retrieval-Augmented Generation</a></li>
<li><a href="#videolucy-deep-memory-backtracking-for-long-video-understanding">VideoLucy Deep Memory Backtracking for Long Video Understanding</a></li>
<li><a href="#pricinglogic-evaluating-llms-reasoning-on-complex-tourism-pricing-tasks">PricingLogic Evaluating LLMs Reasoning on Complex Tourism Pricing Tasks</a></li>
<li><a href="#efficient-adaptive-transformer-an-empirical-study-and-reproducible-framework">Efficient Adaptive Transformer An Empirical Study and Reproducible Framework</a></li>
<li><a href="#an-empirical-study-of-reducing-av1-decoder-complexity-and-energy-consumption-via-encoder-parameter-tuning">An Empirical Study of Reducing AV1 Decoder Complexity and Energy Consumption via Encoder Parameter Tuning</a></li>
<li><a href="#curriflow-curriculum-guided-depth-fusion-with-optical-flow-based-temporal-alignment-for-3d-semantic-scene-completion">CurriFlow Curriculum-Guided Depth Fusion with Optical Flow-Based Temporal Alignment for 3D Semantic Scene Completion</a></li>
<li><a href="#traveling-salesman-based-token-ordering-improves-stability-in-homomorphically-encrypted-language-models">Traveling Salesman-Based Token Ordering Improves Stability in Homomorphically Encrypted Language Models</a></li>
<li><a href="#colf-logic-programming-as-infinitary-proof-exploration">CoLF Logic Programming as Infinitary Proof Exploration</a></li>
<li><a href="#reinforced-preference-optimization-for-recommendation">Reinforced Preference Optimization for Recommendation</a></li>
<li><a href="#a-survey-on-parallel-reasoning">A Survey on Parallel Reasoning</a></li>
<li><a href="#fedlodrop-federated-lora-with-dropout-for-generalized-llm-fine-tuning">FedLoDrop Federated LoRA with Dropout for Generalized LLM Fine-tuning</a></li>
<li><a href="#compressibility-measures-complexity-minimum-description-length-meets-singular-learning-theory">Compressibility Measures Complexity Minimum Description Length Meets Singular Learning Theory</a></li>
<li><a href="#geopipe-a-geo-distributed-llm-training-framework-with-enhanced-pipeline-parallelism-in-a-lossless-rdma-enabled-datacenter-optical-transport-network">GeoPipe a Geo-distributed LLM Training Framework with enhanced Pipeline Parallelism in a Lossless RDMA-enabled Datacenter Optical Transport Network</a></li>
<li><a href="#apce-adaptive-progressive-context-expansion-for-long-context-processing">APCE Adaptive Progressive Context Expansion for Long Context Processing</a></li>
<li><a href="#direct-multi-token-decoding">Direct Multi-Token Decoding</a></li>
<li><a href="#flexpipe-adapting-dynamic-llm-serving-through-inflight-pipeline-refactoring-in-fragmented-serverless-clusters">FlexPipe Adapting Dynamic LLM Serving Through Inflight Pipeline Refactoring in Fragmented Serverless Clusters</a></li>
<li><a href="#topological-vibration-analysis-of-elastic-lattices-via-bloch-sphere-mapping">Topological Vibration Analysis of Elastic Lattices via Bloch Sphere Mapping</a></li>
<li><a href="#Indoor-Localization-using-Compact,-Telemetry-Agnostic,-Transfer-Learning-Enabled-Decoder-Only-Transformer">Indoor Localization using Compact, Telemetry-Agnostic, Transfer-Learning Enabled Decoder-Only Transformer</a></li>
<li><a href="#Variational-Mixture-of-Graph-Neural-Experts-for-Alzheimer's-Disease-Biomarker-Recognition-in-EEG-Brain-Networks">Variational Mixture of Graph Neural Experts for Alzheimer's Disease Biomarker Recognition in EEG Brain Networks</a></li>
<li><a href="#qerl-beyond-efficiency----quantization-enhanced-reinforcement-learning-for-llms">QeRL Beyond Efficiency -- Quantization-enhanced Reinforcement Learning for LLMs</a></li>
<li><a href="#scaling-language-centric-omnimodal-representation-learning">Scaling Language-Centric Omnimodal Representation Learning</a></li>
<li><a href="#diffusion-transformers-with-representation-autoencoders">Diffusion Transformers with Representation Autoencoders</a></li>
<li><a href="#hierarchical-qubit-merging-transformer-for-quantum-error-correction">Hierarchical Qubit-Merging Transformer for Quantum Error Correction</a></li>
<li><a href="#Culturally-Aware-Conversations-A-Framework-&amp;-Benchmark-for-LLMs">Culturally-Aware Conversations A Framework &amp; Benchmark for LLMs</a></li>
<li><a href="#situat3dchange-situated-3d-change-understanding-dataset-for-multimodal-large-language-model">Situat3DChange Situated 3D Change Understanding Dataset for Multimodal Large Language Model</a></li>
<li><a href="#relook-vision-grounded-rl-with-a-multimodal-llm-critic-for-agentic-web-coding">ReLook Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web Coding</a></li>
<li><a href="#andesvl-technical-report-an-efficient-mobile-side-multimodal-large-language-model">AndesVL Technical Report An Efficient Mobile-side Multimodal Large Language Model</a></li>
<li><a href="#From-&lt;Answer&gt;-to-&lt;Think&gt;-Multidimensional-Supervision-of-Reasoning-Process-for-LLM-Optimization">From <Answer> to <Think> Multidimensional Supervision of Reasoning Process for LLM Optimization</a></li>
<li><a href="#multi-view-graph-feature-propagation-for-privacy-preservation-and-feature-sparsity">Multi-View Graph Feature Propagation for Privacy Preservation and Feature Sparsity</a></li>
<li><a href="#efficient-llm-inference-over-heterogeneous-edge-networks-with-speculative-decoding">Efficient LLM Inference over Heterogeneous Edge Networks with Speculative Decoding</a></li>
<li><a href="#xquant-achieving-ultra-low-bit-kv-cache-quantization-with-cross-layer-compression">XQuant Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer Compression</a></li>
<li><a href="#The-Curious-Case-of-Factual-(Mis)Alignment-between-LLMs'-Short--and-Long-Form-Answers">The Curious Case of Factual (Mis)Alignment between LLMs' Short- and Long-Form Answers</a></li>
<li><a href="#Discursive-Circuits-How-Do-Language-Models-Understand-Discourse-Relations?">Discursive Circuits How Do Language Models Understand Discourse Relations?</a></li>
<li><a href="#efficient-in-memory-acceleration-of-sparse-block-diagonal-llms">Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs</a></li>
<li><a href="#flow-matching-based-autonomous-driving-planning-with-advanced-interactive-behavior-modeling">Flow Matching-Based Autonomous Driving Planning with Advanced Interactive Behavior Modeling</a></li>
<li><a href="#bit-allocation-transfer-for-perceptual-quality-enhancement-of-vvc-intra-coding">Bit Allocation Transfer for Perceptual Quality Enhancement of VVC Intra Coding</a></li>
<li><a href="#not-all-bits-are-equal-scale-dependent-memory-optimization-strategies-for-reasoning-models">Not All Bits Are Equal Scale-Dependent Memory Optimization Strategies for Reasoning Models</a></li>
<li><a href="#MC#-Mixture-Compressor-for-Mixture-of-Experts-Large-Models">MC# Mixture Compressor for Mixture-of-Experts Large Models</a></li>
<li><a href="#kotox-a-korean-toxic-dataset-for-deobfuscation-and-detoxification">KOTOX A Korean Toxic Dataset for Deobfuscation and Detoxification</a></li>
<li><a href="#The-Social-Cost-of-Intelligence-Emergence,-Propagation,-and-Amplification-of-Stereotypical-Bias-in-Multi-Agent-Systems">The Social Cost of Intelligence Emergence, Propagation, and Amplification of Stereotypical Bias in Multi-Agent Systems</a></li>
<li><a href="#redundancy-as-a-structural-information-principle-for-learning-and-generalization">Redundancy as a Structural Information Principle for Learning and Generalization</a></li>
<li><a href="#awarecompiler-agentic-context-aware-compiler-optimization-via-a-synergistic-knowledge-data-driven-framework">AwareCompiler Agentic Context-Aware Compiler Optimization via a Synergistic Knowledge-Data Driven Framework</a></li>
<li><a href="#fasthmr-accelerating-human-mesh-recovery-via-token-and-layer-merging-with-diffusion-decoding">FastHMR Accelerating Human Mesh Recovery via Token and Layer Merging with Diffusion Decoding</a></li>
<li><a href="#agentic-rag-for-software-testing-with-hybrid-vector-graph-and-multi-agent-orchestration">Agentic RAG for Software Testing with Hybrid Vector-Graph and Multi-Agent Orchestration</a></li>
<li><a href="#a-compressed-code-for-memory-discrimination">A compressed code for memory discrimination</a></li>
<li><a href="#Review-of-Inference-Time-Scaling-Strategies-Reasoning,-Search-and-RAG">Review of Inference-Time Scaling Strategies Reasoning, Search and RAG</a></li>
<li><a href="#adip-adaptive-precision-systolic-array-for-matrix-multiplication-acceleration">ADiP Adaptive Precision Systolic Array for Matrix Multiplication Acceleration</a></li>
<li><a href="#preserving-llm-capabilities-through-calibration-data-curation-from-analysis-to-optimization">Preserving LLM Capabilities through Calibration Data Curation From Analysis to Optimization</a></li>
<li><a href="#large-language-model-empowered-channel-prediction-and-predictive-beamforming-for-leo-satellite-communications">Large Language Model-Empowered Channel Prediction and Predictive Beamforming for LEO Satellite Communications</a></li>
<li><a href="#bitmar-low-bit-multimodal-fusion-with-episodic-memory-for-edge-devices">BitMar Low-Bit Multimodal Fusion with Episodic Memory for Edge Devices</a></li>
<li><a href="#self-supervised-representation-learning-with-id-content-modality-alignment-for-sequential-recommendation">Self-Supervised Representation Learning with ID-Content Modality Alignment for Sequential Recommendation</a></li>
<li><a href="#the-hidden-dna-of-llm-generated-javascript-structural-patterns-enable-high-accuracy-authorship-attribution">The Hidden DNA of LLM-Generated JavaScript Structural Patterns Enable High-Accuracy Authorship Attribution</a></li>
<li><a href="#saser-stego-attacks-on-open-source-llms">SASER Stego attacks on open-source LLMs</a></li>
<li><a href="#anybcq-hardware-efficient-flexible-binary-coded-quantization-for-multi-precision-llms">AnyBCQ Hardware Efficient Flexible Binary-Coded Quantization for Multi-Precision LLMs</a></li>
<li><a href="#when-images-speak-louder-mitigating-language-bias-induced-hallucinations-in-vlms-through-cross-modal-guidance">When Images Speak Louder Mitigating Language Bias-induced Hallucinations in VLMs through Cross-Modal Guidance</a></li>
<li><a href="#nim-neuro-symbolic-ideographic-metalanguage-for-inclusive-communication">NIM Neuro-symbolic Ideographic Metalanguage for Inclusive Communication</a></li>
<li><a href="#robotfleet-an-open-source-framework-for-centralized-multi-robot-task-planning">RobotFleet An Open-Source Framework for Centralized Multi-Robot Task Planning</a></li>
<li><a href="#sp-moe-speculative-decoding-and-prefetching-for-accelerating-moe-based-model-inference">SP-MoE Speculative Decoding and Prefetching for Accelerating MoE-based Model Inference</a></li>
<li><a href="#grounded-ai-for-code-review-resource-efficient-large-model-serving-in-enterprise-pipelines">Grounded AI for Code Review Resource-Efficient Large-Model Serving in Enterprise Pipelines</a></li>
<li><a href="#The-Achilles'-Heel-of-LLMs-How-Altering-a-Handful-of-Neurons-Can-Cripple-Language-Abilities">The Achilles' Heel of LLMs How Altering a Handful of Neurons Can Cripple Language Abilities</a></li>
<li><a href="#ISAAC-Intelligent,-Scalable,-Agile,-and-Accelerated-CPU-Verification-via-LLM-aided-FPGA-Parallelism">ISAAC Intelligent, Scalable, Agile, and Accelerated CPU Verification via LLM-aided FPGA Parallelism</a></li>
<li><a href="#billy-steering-large-language-models-via-merging-persona-vectors-for-creative-generation">BILLY Steering Large Language Models via Merging Persona Vectors for Creative Generation</a></li>
<li><a href="#a-unified-frequency-domain-decomposition-framework-for-interpretable-and-robust-time-series-forecasting">A Unified Frequency Domain Decomposition Framework for Interpretable and Robust Time Series Forecasting</a></li>
<li><a href="#permllm-learnable-channel-permutation-for-nm-sparse-large-language-models">PermLLM Learnable Channel Permutation for NM Sparse Large Language Models</a></li>
<li><a href="#cacheclip-accelerating-rag-with-effective-kv-cache-reuse">CacheClip Accelerating RAG with Effective KV Cache Reuse</a></li>
<li><a href="#lighter-x-an-efficient-and-plug-and-play-strategy-for-graph-based-recommendation-through-decoupled-propagation">Lighter-X An Efficient and Plug-and-play Strategy for Graph-based Recommendation through Decoupled Propagation</a></li>
<li><a href="#P-4DGS-Predictive-4D-Gaussian-Splatting-with-90\times-Compression">P-4DGS Predictive 4D Gaussian Splatting with 90<script type="math/tex">\times</script> Compression</a></li>
<li><a href="#efficient-onboard-vision-language-inference-in-uav-enabled-low-altitude-economy-networks-via-llm-enhanced-optimization">Efficient Onboard Vision-Language Inference in UAV-Enabled Low-Altitude Economy Networks via LLM-Enhanced Optimization</a></li>
<li><a href="#deliberative-dynamics-and-value-alignment-in-llm-debates">Deliberative Dynamics and Value Alignment in LLM Debates</a></li>
<li><a href="#universal-discrete-domain-speech-enhancement">Universal Discrete-Domain Speech Enhancement</a></li>
<li><a href="#conformal-sparsification-for-bandwidth-efficient-edge-cloud-speculative-decoding">Conformal Sparsification for Bandwidth-Efficient Edge-Cloud Speculative Decoding</a></li>
<li><a href="#the-ethics-engine-a-modular-pipeline-for-accessible-psychometric-assessment-of-large-language-models">The Ethics Engine A Modular Pipeline for Accessible Psychometric Assessment of Large Language Models</a></li>
</ul>
<h2 id="breadcrumbs-reasoning-memory-efficient-reasoning-with-compression-beacons">Breadcrumbs Reasoning Memory-Efficient Reasoning with Compression Beacons</h2>
<blockquote>
<p>Authors: Giovanni Monea, Yair Feldman, Shankar Padmanabhan, Kianté Brantley, Yoav Artzi</p>
<p>2025-10-15</p>
<p><a href="http://arxiv.org/abs/2510.13797v1">http://arxiv.org/abs/2510.13797v1</a></p>
</blockquote>
<p>The scalability of large language models for long-context reasoning is
severely constrained by the linear growth of their Transformer key-value <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a>,
which incurs significant memory and computational costs. We posit that as a
model generates reasoning tokens, the informational value of past generated
tokens diminishes, creating an opportunity for <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a>. In this work, we
propose to periodically compress the generation <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> with a learned,
special-purpose token and evict compressed entries. We train the model to
perform this <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> via a modified joint distillation and reinforcement
learning (RL) framework. Our training method minimizes overhead over the
conventional RL process, as it leverages RL outputs for distillation.
Empirically, our method achieves a superior memory-accuracy Pareto frontier
compared to both the model without <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> and training-free
<a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> techniques.</p>
<h2 id="invited-paper-bitmedvit-ternary-quantized-vision-transformer-for-medical-ai-assistants-on-the-edge">Invited Paper BitMedViT Ternary-Quantized Vision Transformer for Medical AI Assistants on the Edge</h2>
<blockquote>
<p>Authors: Mikolaj Walczak, Uttej Kallakuri, Edward Humes, Xiaomin Lin, Tinoosh Mohsenin</p>
<p>2025-10-15</p>
<p><a href="http://arxiv.org/abs/2510.13760v1">http://arxiv.org/abs/2510.13760v1</a></p>
</blockquote>
<p>Vision Transformers (ViTs) have demonstrated strong capabilities in
interpreting complex medical imaging data. However, their significant
computational and memory demands pose challenges for deployment in real-time,
resource-constrained mobile and wearable devices used in clinical environments.
We introduce, BiTMedViT, a new class of Edge ViTs <a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> as medical AI
assistants that perform structured analysis of medical images directly on the
edge. BiTMedViT utilizes ternary- <a class="glightbox" href="https://img.shields.io/badge/quantize-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantize-F08080" /></a>d linear layers tailored for medical
imaging and com- bines a training procedure with multi-query attention,
pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> stability under ternary weights with low-precision activations.
Furthermore, BiTMedViT employs task-aware distillation from a high-capacity
teacher to recover accuracy lost due to extreme <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a>. Lastly, we also
present a pipeline that maps the ternarized ViTs to a custom CUDA kernel for
efficient memory bandwidth utilization and latency reduction on the Jetson Orin
Nano. Finally, BiTMedViT achieves 86% diagnostic accuracy (89% SOTA) on
MedMNIST across 12 datasets, while reducing model size by 43x, memory traffic
by 39x, and enabling 16.8 ms inference at an energy efficiency up to 41x that
of SOTA models at 183.62 GOPs/J on the Orin Nano. Our results demonstrate a
practical and scientifically grounded route for extreme-precision medical
imaging ViTs deployable on the edge, narrowing the gap between algorithmic
advances and deployable clinical tools.</p>
<h2 id="dont-be-greedy-just-relax-pruning-llms-via-frank-wolfe">Don't Be Greedy, Just Relax! Pruning LLMs via Frank-Wolfe</h2>
<blockquote>
<p>Authors: Christophe Roux, Max Zimmer, Alexandre d'Aspremont, Sebastian Pokutta</p>
<p>2025-10-15</p>
<p><a href="http://arxiv.org/abs/2510.13713v1">http://arxiv.org/abs/2510.13713v1</a></p>
</blockquote>
<p>Pruning is a common technique to reduce the compute and storage requirements
of Neural Networks. While conventional approaches typically retrain the model
to recover <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a>-induced performance degradation, state-of-the-art Large
Language Model (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>) <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> methods operate layer-wise, minimizing the
per-layer <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> error on a small calibration dataset to avoid full
retraining, which is considered computationally prohibitive for <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s. However,
finding the optimal <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> mask is a hard combinatorial problem and solving it
to optimality is intractable. Existing methods hence rely on greedy heuristics
that ignore the weight interactions in the <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> objective. In this work, we
instead consider the convex relaxation of these combinatorial constraints and
solve the resulting problem using the Frank-Wolfe (FW) algorithm. Our method
drastically reduces the per-layer <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> error, outperforms strong baselines
on state-of-the-art GPT architectures, and remains memory-efficient. We provide
theoretical justification by showing that, combined with the convergence
guarantees of the FW algorithm, we obtain an approximate solution to the
original combinatorial problem upon rounding the relaxed solution to
integrality.</p>
<h2 id="how-sampling-affects-the-detectability-of-machine-written-texts-a-comprehensive-study">How Sampling Affects the Detectability of Machine-written texts A Comprehensive Study</h2>
<blockquote>
<p>Authors: Matthieu Dubois, François Yvon, Pablo Piantanida</p>
<p>2025-10-15</p>
<p><a href="http://arxiv.org/abs/2510.13681v1">http://arxiv.org/abs/2510.13681v1</a></p>
</blockquote>
<p>As texts generated by Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) are ever more common and
often indistinguishable from human-written content, research on automatic text
detection has attracted growing attention. Many recent detectors report
near-perfect accuracy, often boasting AUROC scores above 99\%. However, these
claims typically assume fixed generation settings, leaving open the question of
how robust such systems are to changes in <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> strategies. In this work, we
systematically examine how sampling-based <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> impacts detectability, with
a focus on how subtle variations in a model's (sub)word-level distribution
affect detection performance. We find that even minor adjustments to <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a>
parameters - such as temperature, top-p, or nucleus sampling - can severely
impair detector accuracy, with AUROC dropping from near-perfect levels to 1\%
in some settings. Our findings expose critical blind spots in current detection
methods and emphasize the need for more comprehensive evaluation protocols. To
facilitate future research, we release a large-scale dataset encompassing 37
<a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> configurations, along with our code and evaluation framework
https://github.com/BaggerOfWords/Sampling-and-Detection</p>
<h2 id="adaptive-rescheduling-in-prefill-decode-disaggregated-llm-inference">Adaptive Rescheduling in Prefill-Decode Disaggregated LLM Inference</h2>
<blockquote>
<p>Authors: Zhibin Wang, Zetao Hong, Xue Li, Zibo Wang, Shipeng Li, Qingkai Meng, Qing Wang, Chengying Huan, Rong Gu, Sheng Zhong, Chen Tian</p>
<p>2025-10-15</p>
<p><a href="http://arxiv.org/abs/2510.13668v1">http://arxiv.org/abs/2510.13668v1</a></p>
</blockquote>
<p>Large Language Model (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>) inference has emerged as a fundamental paradigm.
In real-world scenarios, variations in output length cause severe workload
imbalance in the <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a> phase, particularly for long-output reasoning tasks.
Existing systems, such as PD disaggregation architectures, rely on static
<a class="glightbox" href="https://img.shields.io/badge/prefill-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/prefill-F08080" /></a>-to-<a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a> scheduling, which often results in SLO violations and OOM
failures under evolving <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a> workloads.
  In this paper, we propose ARES, an adaptive <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> rescheduling system
powered by length prediction to anticipate future workloads. Our core
contributions include: (1) A lightweight and continuous <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-native prediction
method that leverages <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> hidden state to model remaining generation length
with high precision (reducing MAE by 49.42%) and low overhead (cutting
predictor parameters by 93.28%); (2) A rescheduling solution in <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a> phase
with : A dynamic balancing mechanism that integrates current and predicted
workloads, reducing P99 TPOT by 74.77% and achieving up to 2.24 times higher
goodput.</p>
<h2 id="time-series-foundation-models-benchmarking-challenges-and-requirements">Time Series Foundation Models Benchmarking Challenges and Requirements</h2>
<blockquote>
<p>Authors: Marcel Meyer, Sascha Kaltenpoth, Kevin Zalipski, Oliver Müller</p>
<p>2025-10-15</p>
<p><a href="http://arxiv.org/abs/2510.13654v1">http://arxiv.org/abs/2510.13654v1</a></p>
</blockquote>
<p>Time Series Foundation Models (TSFMs) represent a new paradigm for time
series forecasting, offering zero-shot forecasting capabilities without the
need for domain-specific pre-training or fine-tuning. However, as with Large
Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s), evaluating TSFMs is tricky, as with ever more extensive
training sets, it becomes more and more challenging to ensure the integrity of
benchmarking data. Our investigation of existing TSFM evaluation highlights
multiple challenges, ranging from the representativeness of the benchmark
datasets, over the lack of spatiotemporal evaluation, to risks of information
leakage due to <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a>ping and obscure datasets, and the memorization of global
patterns caused by external shocks like economic crises or pandemics. Our
findings reveal widespread confusion regarding data partitions, risking
inflated performance estimates and incorrect transfer of global knowledge to
local time series. We argue for the development of robust evaluation
methodologies to prevent pitfalls already observed in <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> and classical time
series benchmarking, and call upon the research community to design new,
principled approaches, such as evaluations on truly out-of-sample future data,
to safeguard the integrity of TSFM assessment.</p>
<h2 id="nosa-native-and-offloadable-sparse-attention">NOSA Native and Offloadable Sparse Attention</h2>
<blockquote>
<p>Authors: Yuxiang Huang, Chaojun Xiao, Xu Han, Zhiyuan Liu</p>
<p>2025-10-15</p>
<p><a href="http://arxiv.org/abs/2510.13602v1">http://arxiv.org/abs/2510.13602v1</a></p>
</blockquote>
<p>Trainable <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> attention has emerged as a promising solution to address the
<a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> efficiency bottleneck of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s in long-context processing,
significantly saving memory accesses while minimally impacting task
performance. However, existing <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> attention methods leave a crucial
limitation unresolved: the size of the key-value (<a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>) <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> remains unreduced,
which constrains on-GPU batch sizes and throttles <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> throughput,
especially in large-scale batched inference. In this paper, we show that
trainable <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> attention naturally exhibits strong locality in token
selection across adjacent <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> steps, thereby enabling <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> offloading
without altering the underlying attention computation. However, the inherent
locality remains insufficient to achieve efficient offloading, as the transfer
of selected <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> pairs between the CPU and GPU continues to dominate the overall
<a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> cost. Building on this insight, we present NOSA, a trainable <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a>
attention framework designed to natively support <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> offloading. NOSA
introduces explicit locality constraints by decomposing token selection into
query-aware and query-agnostic components, thereby reducing <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> transfers while
pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> the same attention computation as used during training. We pretrain
a 1B-parameter model with NOSA and conduct extensive benchmarks, showing that
it preserves near-lossless performance while achieving up to a 2.3x improvement
in <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> throughput compared with the vanilla trainable <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> attention
baseline (Inf<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-V2).</p>
<h2 id="dolfin-balancing-stability-and-plasticity-in-federated-continual-learning">DOLFIN Balancing Stability and Plasticity in Federated Continual Learning</h2>
<blockquote>
<p>Authors: Omayma Moussadek, Riccardo Salami, Simone Calderara</p>
<p>2025-10-15</p>
<p><a href="http://arxiv.org/abs/2510.13567v1">http://arxiv.org/abs/2510.13567v1</a></p>
</blockquote>
<p>Federated continual learning (FCL) enables models to learn new tasks across
multiple distributed clients, protecting privacy and without forgetting
previously acquired knowledge. However, current methods face challenges
balancing performance, privacy preservation, and <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> efficiency. We
introduce a Distributed Online LoRA for Federated INcremental learning method
DOLFIN, a novel approach combining Vision Transformers with low-rank adapters
designed to efficiently and stably learn new tasks in federated environments.
Our method leverages LoRA for minimal <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> overhead and incorporates
DualGradient Projection Memory (DualGPM) to prevent forgetting. Evaluated on
CIFAR-100, ImageNet-R, ImageNet-A, and CUB-200 under two Dirichlet
heterogeneity settings, DOLFIN consistently surpasses six strong baselines in
final average accuracy while matching their memory footprint. Orthogonal
low-rank adapters offer an effective and scalable solution for
privacy-pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> continual learning in federated settings.</p>
<h2 id="steer-moe-efficient-audio-language-alignment-with-a-mixture-of-experts-steering-module">Steer-MoE Efficient Audio-Language Alignment with a Mixture-of-Experts Steering Module</h2>
<blockquote>
<p>Authors: Ruitao Feng, Bixi Zhang, Sheng Liang, Zheng Yuan</p>
<p>2025-10-15</p>
<p><a href="http://arxiv.org/abs/2510.13558v1">http://arxiv.org/abs/2510.13558v1</a></p>
</blockquote>
<p>Aligning pretrained audio encoders and Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) offers a
promising, parameter-efficient path to building powerful multimodal agents.
However, existing methods often require costly full-model finetuning or rely on
static adapters that may lack expressive power. Drawing inspiration from the
Platonic Representation Hypothesis, we introduce SteerMoE, a novel and modular
framework for audio-language alignment. SteerMoE freezes both the audio encoder
and the <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>r, training only a lightweight steering module integrated
within the encoder's layers. This module uses a Mixture-of-Experts (MoE) router
to dynamically select and apply learned steering vectors, progressively
transforming continuous audio representations into a space comprehensible to
the <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>. By operating entirely in the continuous embedding space, our approach
requires no modifications to the <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>'s vocabulary and preserves its advanced
reasoning and agentic capabilities. We demonstrate through experiments on ASR,
audio understanding, and a qualitative function-calling task that SteerMoE
achieves strong performance while remaining highly modular and computationally
efficient, offering a robust new paradigm for developing sophisticated
audio-language systems.</p>
<h2 id="medrek-retrieval-based-editing-for-medical-llms-with-key-aware-prompts">MedREK Retrieval-Based Editing for Medical LLMs with Key-Aware Prompts</h2>
<blockquote>
<p>Authors: Shujun Xia, Haokun Lin, Yichen Wu, Yinan Zhou, Zixuan Li, Zhongwei Wan, Xingrun Xing, Yefeng Zheng, Xiang Li, Caifeng Shan, Zhenan Sun, Quanzheng Li</p>
<p>2025-10-15</p>
<p><a href="http://arxiv.org/abs/2510.13500v1">http://arxiv.org/abs/2510.13500v1</a></p>
</blockquote>
<p><a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s hold great promise for healthcare applications, but the rapid evolution
of medical knowledge and errors in training data often cause them to generate
outdated or inaccurate information, limiting their applicability in high-stakes
clinical practice. Model editing has emerged as a potential remedy without full
retraining. While parameter-based editing often compromises locality and is
thus ill-suited for the medical domain, retrieval-based editing offers a more
viable alternative. However, it still faces two critical challenges: (1)
representation <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a> within the medical knowledge space often causes
inaccurate retrieval and reduces editing accuracy; (2) existing methods are
restricted to single-sample edits, while batch-editing remains largely
unexplored despite its importance for real-world medical applications. To
address these challenges, we first construct MedVersa, \hk{an enhanced
benchmark with broader coverage of medical subjects, designed to evaluate both
single and batch edits under strict locality constraints}. We then propose
MedREK, a retrieval-based editing framework that integrates a shared query-key
module for precise matching with an attention-based prompt encoder for
informative guidance. Experimental results on various medical benchmarks
demonstrate that our MedREK achieves superior performance across different core
metrics and provides the first validated solution for batch-editing in medical
<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s. Our code and dataset are available at
https://github.com/mylittleriver/MedREK.</p>
<h2 id="who-speaks-for-the-trigger-dynamic-expert-routing-in-backdoored-mixture-of-experts-transformers">Who Speaks for the Trigger? Dynamic Expert Routing in Backdoored Mixture-of-Experts Transformers</h2>
<blockquote>
<p>Authors: Xin Zhao, Xiaojun Chen, Bingshan Liu, Haoyu Gao, Zhendong Zhao, Yilong Chen</p>
<p>2025-10-15</p>
<p><a href="http://arxiv.org/abs/2510.13462v1">http://arxiv.org/abs/2510.13462v1</a></p>
</blockquote>
<p>Large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) with Mixture-of-Experts (MoE) architectures
achieve impressive performance and efficiency by dynamically routing inputs to
specialized subnetworks, known as experts. However, this <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> routing
mechanism inherently exhibits task preferences due to expert specialization,
introducing a new and underexplored vulnerability to backdoor attacks. In this
work, we investigate the feasibility and effectiveness of injecting backdoors
into MoE-based <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s by exploiting their inherent expert routing preferences. We
thus propose BadSwitch, a novel backdoor framework that integrates task-coupled
dynamic trigger optimization with a sensitivity-guided Top-S expert tracing
mechanism. Our approach jointly optimizes trigger embeddings during pretraining
while identifying S most sensitive experts, subsequently constraining the Top-K
gating mechanism to these targeted experts. Unlike traditional backdoor attacks
that rely on superficial data poisoning or model editing, BadSwitch primarily
embeds malicious triggers into expert routing paths with strong task affinity,
enabling precise and stealthy model manipulation. Through comprehensive
evaluations across three prominent MoE architectures (Switch Transformer,
QwenMoE, and DeepSeekMoE), we demonstrate that BadSwitch can efficiently hijack
pre-trained models with up to 100% success rate (ASR) while maintaining the
highest clean accuracy (ACC) among all baselines. Furthermore, BadSwitch
exhibits strong resilience against both text-level and model-level defense
mechanisms, achieving 94.07% ASR and 87.18% ACC on the AGNews dataset. Our
analysis of expert activation patterns reveals fundamental insights into MoE
vulnerabilities. We anticipate this work will expose security risks in MoE
systems and contribute to advancing AI safety.</p>
<h2 id="f-bfq-flexible-block-floating-point-quantization-accelerator-for-llms">F-BFQ Flexible Block Floating-Point Quantization Accelerator for LLMs</h2>
<blockquote>
<p>Authors: Jude Haris, José Cano</p>
<p>2025-10-15</p>
<p><a href="http://arxiv.org/abs/2510.13401v1">http://arxiv.org/abs/2510.13401v1</a></p>
</blockquote>
<p>Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) have become increasingly prominent for daily
tasks, from improving sound-totext translation to generating additional frames
for the latest video games. With the help of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> inference frameworks, such as
llama.cpp, which support optimizations such as <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>-caching and <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a>, it
is now easier than ever to deploy <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s on edge devices. Quantization is
fundamental to enable <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s on resource-constrained edge devices, and llama.cpp
utilizes block floating point (BFP) <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a> to drastically reduce the bit
width of weights and input tensors, the memory footprint, and the computational
power required to run <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s. <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s are typically <a class="glightbox" href="https://img.shields.io/badge/quantize-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantize-F08080" /></a>d with mixed BFP
<a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a> across the model layers to reduce the loss of model accuracy due
to <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a>. Therefore, to efficiently accelerate across the layers of
BFP-<a class="glightbox" href="https://img.shields.io/badge/quantize-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantize-F08080" /></a>d <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s, specialized accelerators need to support different BFP
variants without reconfiguration. To address this issue, we propose a Flexible
Block FloatingPoint Quantization (F-BFQ) accelerator, which can dynamically
switch between two BFP <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a> variants and perform matrix multiplication
(MatMul) operations. Our initial F-BFQ accelerator design, deployed on the AMD
Kria board, reduces inference time by 1.4x on average over the Arm NEON-based
CPU execution across three BFP <a class="glightbox" href="https://img.shields.io/badge/quantize-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantize-F08080" /></a>d <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s while achieving 5.2 tokens per
second (~3.9 words per second).</p>
<h2 id="make-an-offer-they-cant-refuse-grounding-bayesian-persuasion-in-real-world-dialogues-without-pre-commitment">Make an Offer They Can't Refuse Grounding Bayesian Persuasion in Real-World Dialogues without Pre-Commitment</h2>
<blockquote>
<p>Authors: Buwei He, Yang Liu, Zhaowei Zhang, Zixia Jia, Huijia Wu, Zhaofeng He, Zilong Zheng, Yipeng Kang</p>
<p>2025-10-15</p>
<p><a href="http://arxiv.org/abs/2510.13387v2">http://arxiv.org/abs/2510.13387v2</a></p>
</blockquote>
<p>Persuasion, a fundamental social capability for humans, remains a challenge
for AI systems such as large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s). Current studies often
overlook the strategic use of information asymmetry in message design or rely
on strong assumptions regarding pre-commitment. In this work, we explore the
application of Bayesian Persuasion (BP) in natural language within single-turn
dialogue settings, to enhance the strategic persuasion capabilities of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s.
Our framework incorporates a commitment-<a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> mechanism, where the
persuader explicitly outlines an information schema by narrating their
potential types (e.g., honest or dishonest), thereby guiding the persuadee in
performing the intended Bayesian belief update. We evaluate two variants of our
approach: Semi-Formal-Natural-Language (SFNL) BP and Fully-Natural-Language
(FNL) BP, benchmarking them against both naive and strong non-BP (NBP)
baselines within a comprehensive evaluation framework. This framework covers a
diverse set of persuadees -- including <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> instances with varying prompts and
fine-tuning and human participants -- across tasks ranging from specially
designed persuasion scenarios to general everyday situations. Experimental
results on <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-based agents reveal three main findings: (1) <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s guided by BP
strategies consistently achieve higher persuasion success rates than NBP
baselines; (2) SFNL exhibits greater credibility and logical coherence, while
FNL shows stronger emotional resonance and robustness in naturalistic
conversations; (3) with supervised fine-tuning, smaller models can attain BP
performance comparable to that of larger models.</p>
<h2 id="document-intelligence-in-the-era-of-large-language-models-a-survey">Document Intelligence in the Era of Large Language Models A Survey</h2>
<blockquote>
<p>Authors: Weishi Wang, Hengchang Hu, Zhijie Zhang, Zhaochen Li, Hongxin Shao, Daniel Dahlmeier</p>
<p>2025-10-15</p>
<p><a href="http://arxiv.org/abs/2510.13366v1">http://arxiv.org/abs/2510.13366v1</a></p>
</blockquote>
<p>Document AI (DAI) has emerged as a vital application area, and is
significantly transformed by the advent of large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s). While
earlier approaches relied on encoder-<a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>r architectures, <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>r-only <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s
have revolutionized DAI, bringing remarkable advancements in understanding and
generation. This survey provides a comprehensive overview of DAI's evolution,
highlighting current research attempts and future prospects of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s in this
field. We explore key advancements and challenges in multimodal, multilingual,
and retrieval-augmented DAI, while also suggesting future research directions,
including agent-based approaches and document-specific foundation models. This
paper aims to provide a structured analysis of the state-of-the-art in DAI and
its implications for both academic and practical applications.</p>
<h2 id="taming-the-fragility-of-kv-cache-eviction-in-llm-inference">Taming the Fragility of KV Cache Eviction in LLM Inference</h2>
<blockquote>
<p>Authors: Yuan Feng, Haoyu Guo, JunLin Lv, S. Kevin Zhou, Xike Xie</p>
<p>2025-10-15</p>
<p><a href="http://arxiv.org/abs/2510.13334v1">http://arxiv.org/abs/2510.13334v1</a></p>
</blockquote>
<p>Large language models have revolutionized natural language processing, yet
their deployment remains hampered by the substantial memory and runtime
overhead of the <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>'s Key-Value <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a>. To mitigate this, recent methods
employ a scoring-aggregation framework to evict unimportant <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> entries,
based on the stability assumption-that a fixed subset of entries remains
consistently important during generation. However, prior work has largely
focused on refining importance indicators for scoring, while defaulting to mean
aggregation due to a faithful trust in the stability assumption. In this work,
we argue that this underlying assumption is inherently fragile, making mean
aggregation highly vulnerable in extreme cases. To counter this, we propose a
simple yet elegant defensive aggregation strategy: a two-step, linear-time
approach that controls worst-case risk, thereby defending against extreme cases
with negligible computational overhead. Embodying this strategy, we propose a
novel <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> eviction method, Defensive<a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> and its extension, Layer-Defensive<a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>,
which incorporates layer-wise budget allocation. Across seven task domains (18
datasets), our methods reduce generation quality loss by 2.3x and 4.3x
respectively, versus the strongest baseline under a 20% <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> size. These
results set new performance benchmarks and pioneer a promising direction for
optimizing <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> eviction against underlying fragility through worst-case risk
management. Our code is available at https://github.com/FFY0/Defensive<a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>.</p>
<h2 id="chatr1-reinforcement-learning-for-conversational-reasoning-and-retrieval-augmented-question-answering">ChatR1 Reinforcement Learning for Conversational Reasoning and Retrieval Augmented Question Answering</h2>
<blockquote>
<p>Authors: Simon Lupart, Mohammad Aliannejadi, Evangelos Kanoulas</p>
<p>2025-10-15</p>
<p><a href="http://arxiv.org/abs/2510.13312v1">http://arxiv.org/abs/2510.13312v1</a></p>
</blockquote>
<p>We present ChatR1, a reasoning framework based on reinforcement learning (RL)
for conversational question answering (CQA). Reasoning plays an important role
in CQA, where user intent evolves across dialogue turns, and utterances are
often underspecified, requiring contextual interpretation, query reformulation,
and dynamic coordination between retrieval and generation. Unlike static
`rewrite, retrieve, and generate' pipelines, ChatR1 interleaves search and
reasoning across turns, enabling exploratory and adaptive behaviors learned
through RL. To address the challenge of <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> and delayed rewards in RL, we
propose an intent-aware reward that provides turn-level feedback by aligning
retrieval and reasoning with evolving user goals. Our proposed ChatR1
demonstrates strong performance on both 3B and 7B model backbones,
outperforming competitive models on five CQA datasets, measured by different
metrics (F1, BERTScore, and <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-as-judge). We include a diverse set of CQA
datasets to cover topic shifts, evolving intents, mixed-initiative dialogues,
and multi-document grounding, testing ChatR1's performance from various
aspects. Ablation studies confirm the effectiveness of the intent-aware reward.
Our analyses further reveal diverse reasoning trajectories and effective use of
the search tool. ChatR1 also generalizes robustly across domains, demonstrating
that RL-based reasoning enables more flexible and context-sensitive behavior
than static CQA pipelines.</p>
<h2 id="banaserve-unified-kv-cache-and-dynamic-module-migration-for-balancing-disaggregated-llm-serving-in-ai-infrastructure">BanaServe Unified KV Cache and Dynamic Module Migration for Balancing Disaggregated LLM Serving in AI Infrastructure</h2>
<blockquote>
<p>Authors: Yiyuan He, Minxian Xu, Jingfeng Wu, Jianmin Hu, Chong Ma, Min Shen, Le Chen, Chengzhong Xu, Lin Qu, Kejiang Ye</p>
<p>2025-10-15</p>
<p><a href="http://arxiv.org/abs/2510.13223v1">http://arxiv.org/abs/2510.13223v1</a></p>
</blockquote>
<p>Large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) are increasingly deployed in AI infrastructure,
driving the need for high throughput, resource efficient <a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> systems.
Disaggregated <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> <a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a>, which separates prompt <a class="glightbox" href="https://img.shields.io/badge/prefill-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/prefill-F08080" /></a> from auto-regressive
<a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>, has emerged as a promising architecture by isolating their
heterogeneous compute and memory demands. However, current <a class="glightbox" href="https://img.shields.io/badge/disaggregate-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/disaggregate-F08080" /></a>d
systems face three key limitations: (i) static resource allocation cannot adapt
to highly dynamic workloads, causing over-provisioning that wastes resources or
under-provisioning that violates service level objectives (SLOs); (ii) inherent
load imbalance between <a class="glightbox" href="https://img.shields.io/badge/prefill-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/prefill-F08080" /></a> and <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a> stages, where <a class="glightbox" href="https://img.shields.io/badge/prefill-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/prefill-F08080" /></a> is
compute-bound and <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a> is memory-bound, causes under-utilization in one tier
while the other becomes a bottleneck; and (iii) prefix <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> aware routing
skews load distribution, as high <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> hit rate <a class="glightbox" href="https://img.shields.io/badge/prefill-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/prefill-F08080" /></a> nodes attract
disproportionately more requests, further degrading balance and efficiency. To
address these issues, we present BanaServe, a dynamic orchestration framework
that continuously rebalances computational and memory resources across <a class="glightbox" href="https://img.shields.io/badge/prefill-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/prefill-F08080" /></a>
and <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a> instances while eliminating hotspots induced by <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a>. BanaServe
introduces layer level weight migration, attention level Key Value Cache (<a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>
Cache) migration, and Global <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> Cache Store sharing with layer wise <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a>ped
transmission, enabling both coarse grained (layer level) and fine grained
(attention level) load redistribution with minimal latency overhead. These
mechanisms allow routers to perform purely load aware scheduling, unconstrained
by <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> placement. Compared to v<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>, BanaServe achieves 1.2x-3.9x higher
throughput with 3.9%-78.4% lower total processing time, and outperforms
DistServe by 1.1x-2.8x in throughput with 1.4%-70.1% latency reduction.</p>
<h2 id="dscd-large-language-model-detoxification-with-self-constrained-decoding">DSCD Large Language Model Detoxification with Self-Constrained Decoding</h2>
<blockquote>
<p>Authors: Ming Dong, Jinkui Zhang, Bolong Zheng, Xinhui Tu, Po Hu, Tingting He</p>
<p>2025-10-15</p>
<p><a href="http://arxiv.org/abs/2510.13183v1">http://arxiv.org/abs/2510.13183v1</a></p>
</blockquote>
<p>Detoxification in large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) remains a significant research
challenge. Existing <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> detoxification methods are all based on external
constraints, which require additional resource overhead and lose generation
fluency. This work proposes Detoxification with Self-Constrained Decoding
(DSCD), a novel method for <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> detoxification without parameter fine-tuning.
DSCD strengthens the inner next-token distribution of the safety layer while
weakening that of hallucination and toxic layers during output generation. This
effectively diminishes toxicity and enhances output safety. DSCD offers
lightweight, high compatibility, and plug-and-play capabilities, readily
integrating with existing detoxification methods for further performance
improvement. Extensive experiments on representative open-source <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s and
public datasets validate DSCD's effectiveness, demonstrating state-of-the-art
(SOTA) performance in both detoxification and generation fluency, with superior
efficiency compared to existing methods. These results highlight DSCD's
potential as a practical and scalable solution for safer <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> deployments.</p>
<h2 id="a-dimension-keeping-semi-tensor-product-framework-for-compressed-sensing">A Dimension-Keeping Semi-Tensor Product Framework for Compressed Sensing</h2>
<blockquote>
<p>Authors: Qi Qi, Abdelhamid Tayebi, Daizhan Cheng, Jun-e Feng</p>
<p>2025-10-15</p>
<p><a href="http://arxiv.org/abs/2510.13180v1">http://arxiv.org/abs/2510.13180v1</a></p>
</blockquote>
<p>In compressed sensing (CS), <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> signals can be reconstructed from
significantly fewer samples than required by the Nyquist-Shannon sampling
theorem. While non-<a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> signals can be <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a>ly represented in appropriate
transformation domains, conventional CS frameworks rely on the incoherence of
the measurement matrix columns to guarantee reconstruction performance. This
paper proposes a novel method termed Dimension-Keeping Semi-Tensor Product
Compressed Sensing (DK-STP-CS), which leverages intra-group correlations while
maintaining inter-group incoherence to enhance the measurement matrix design.
Specifically, the DK-STP algorithm is integrated into the design of the sensing
matrix, enabling dimensionality reduction while pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> signal recovery
capability. For image <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> and reconstruction tasks, the proposed method
achieves notable noise suppression and improves visual fidelity. Experimental
results demonstrate that DK-STP-CS significantly outperforms traditional CS and
STP-CS approaches, as evidenced by higher Peak Signal-to-Noise Ratio (PSNR)
values between the reconstructed and original images. The robustness of
DK-STP-CS is further validated under noisy conditions and varying sampling
rates, highlighting its potential for practical applications in
resource-constrained environments.</p>
<h2 id="mirror-speculative-decoding-breaking-the-serial-barrier-in-llm-inference">Mirror Speculative Decoding Breaking the Serial Barrier in LLM Inference</h2>
<blockquote>
<p>Authors: Nikhil Bhendawade, Kumari Nishu, Arnav Kundu, Chris Bartels, Minsik Cho, Irina Belousova</p>
<p>2025-10-15</p>
<p><a href="http://arxiv.org/abs/2510.13161v1">http://arxiv.org/abs/2510.13161v1</a></p>
</blockquote>
<p>Speculative <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> accelerates <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> inference by using a draft model to look
ahead, but gains are capped by the cost of autoregressive draft generation:
increasing draft size elevates acceptance rates but introduces additional
latency overhead exacerbating the speed-accuracy tradeoff. Prior methods
(Medusa, Hydra, EAGLE) partially reduce draft cost but either degrade
acceptance or introduce overheads that limit scaling. We present Mirror
Speculative Decoding (Mirror-SD), an inference algorithm that breaks the
latency-acceptance tradeoff. Mirror-SD launches branch-complete rollouts from
early-exit signals in parallel with the target model's suffix and explicitly
maps computation across heterogeneous accelerators (GPU and NPU) to exploit
cross-device parallelism. The draft speculates forward continuations for the
target to verify, while the target simultaneously speculates correction paths
for the draft, converting speculation into two complementary execution
pipelines. To further cut draft latency without weakening acceptance semantics,
we add speculative streaming so the draft emits multiple tokens per step. This
dual strategy of parallel heterogeneous execution plus multi-token speculative
streaming pushes speculative <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> toward its ideal regime of high
acceptance with low overhead. On SpecBench with server-scale models from 14B to
66B parameters, Mirror-SD delivers consistent end-to-end gains, achieving
2.8x-5.8x wall-time speedups across diverse tasks and a 30% average relative
improvement over the strongest baseline, EAGLE3.</p>
<h2 id="retrieval-in-the-chain-bootstrapping-large-language-models-for-generative-retrieval">Retrieval-in-the-Chain Bootstrapping Large Language Models for Generative Retrieval</h2>
<blockquote>
<p>Authors: Yingchen zhang, Ruqing zhang, Jiafeng Guo, Wenjun Peng, Sen Li, Fuyu Lv</p>
<p>2025-10-15</p>
<p><a href="http://arxiv.org/abs/2510.13095v1">http://arxiv.org/abs/2510.13095v1</a></p>
</blockquote>
<p>Generative retrieval (GR) is an emerging paradigm that leverages large
language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) to autoregressively generate document identifiers
(docids) relevant to a given query. Prior works have focused on leveraging the
generative capabilities of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s to improve GR, while overlooking that their
reasoning capabilities could likewise help. This raises a key question: Can
explicit reasoning benefit GR? To investigate, we first conduct a preliminary
study where an <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> is prompted to generate free-form chain-of-thought (CoT)
reasoning before performing constrained docid <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a>. Although this method
outperforms standard GR, the generated reasoning tends to be verbose and poorly
aligned with the docid space. These limitations motivate the development of a
reasoning mechanism better tailored to GR.
  Therefore, we propose Reason-for-Retrieval (R4R), a reasoning-augmented
framework for GR that converts free-form CoT reasoning into a compact,
structured format, and iteratively refines the reasoning during the retrieval
process. R4R augments an existing GR method by leveraging a reasoning-capable
<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> that has been instruction-tuned for GR. At inference time, R4R first uses
the <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> to generate an initial structured reasoning; then the same <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>
alternates between (i) constrained <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> with the chosen GR method to
produce candidate docids and (ii) updating the reasoning based on retrieval
results to improve the next round. R4R does not require additional models or
training, and instead a single <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> serves as both the reasoning generator and
the retriever. Extensive experiments on Natural Questions, MS MARCO, and a
real-world item-search benchmark validate the effectiveness of R4R.</p>
<h2 id="neurorvq-multi-scale-eeg-tokenization-for-generative-large-brainwave-models">NeuroRVQ Multi-Scale EEG Tokenization for Generative Large Brainwave Models</h2>
<blockquote>
<p>Authors: Konstantinos Barmpas, Na Lee, Alexandros Koliousis, Yannis Panagakis, Dimitrios A. Adamos, Nikolaos Laskaris, Stefanos Zafeiriou</p>
<p>2025-10-15</p>
<p><a href="http://arxiv.org/abs/2510.13068v1">http://arxiv.org/abs/2510.13068v1</a></p>
</blockquote>
<p>Electroencephalography (EEG) captures neural activity across multiple
temporal and spectral scales, yielding signals that are rich but complex for
representation learning. Recently, EEG foundation models trained to predict
masked signal-tokens have shown promise for learning generalizable
representations. However, their performance is hindered by their signal
tokenization modules. Existing neural tokenizers fail to preserve
high-frequency dynamics, limiting their ability to reconstruct EEG signals with
high fidelity. We introduce NeuroRVQ, a scalable Large Brainwave Model (LBM)
centered on a codebook-based tokenizer. Our tokenizer integrates: (i)
multi-scale feature extraction modules that capture the full frequency neural
spectrum; (ii) hierarchical residual vector <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a> (RVQ) codebooks for
high-resolution encoding; and, (iii) an EEG signal phase- and amplitude-aware
loss function for efficient training. This design enables efficient EEG
<a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> while supporting accurate reconstruction across all frequency
bands, leading to robust generative masked modeling. Our empirical results
demonstrate that NeuroRVQ achieves lower reconstruction error and outperforms
existing LBMs on a variety of downstream tasks. More broadly, NeuroRVQ
tokenizer establishes a strong prior for codebook-based general-purpose
brainwave models, enabling advances in neural <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a>, generative modeling and
multimodal biosignal integration.</p>
<h2 id="neural-approximate-inverse-preconditioners">Neural Approximate Inverse Preconditioners</h2>
<blockquote>
<p>Authors: Tianshi Xu, Rui Peng Li, Yuanzhe Xi</p>
<p>2025-10-14</p>
<p><a href="http://arxiv.org/abs/2510.13034v1">http://arxiv.org/abs/2510.13034v1</a></p>
</blockquote>
<p>In this paper, we propose a data-driven framework for constructing efficient
approximate inverse preconditioners for elliptic partial differential equations
(PDEs) by learning the Green's function of the underlying operator with neural
networks (NNs). The training process integrates four key components: an
adaptive multiscale neural architecture (<script type="math/tex">\alpha</script>MSNN) that captures
hierarchical features across near-, middle-, and far-field regimes; the use of
coarse-grid anchor data to ensure physical identifiability; a
multi-<script type="math/tex">\varepsilon</script> staged training protocol that progressively refines the
Green's function representation across spatial scales; and an <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a>ping
domain decomposition that enables local adaptation while maintaining global
consistency. Once trained, the NN-approximated Green's function is directly
compressed into either a hierarchical (<script type="math/tex">\mathcal{H}</script>-) matrix or a <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a>
matrix-using only the mesh geometry and the network output. This geometric
construction achieves nearly linear complexity in both setup and application
while pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> the spectral properties essential for effective
preconditioning. Numerical experiments on challenging elliptic PDEs demonstrate
that the resulting preconditioners consistently yield fast convergence and
small iteration counts.</p>
<h2 id="computationally-efficient-neural-receivers-via-axial-self-attention">Computationally Efficient Neural Receivers via Axial Self-Attention</h2>
<blockquote>
<p>Authors: SaiKrishna Saketh Yellapragada, Atchutaram K. Kocharlakota, Mário Costa, Esa Ollila, Sergiy A. Vorobyov</p>
<p>2025-10-14</p>
<p><a href="http://arxiv.org/abs/2510.12941v1">http://arxiv.org/abs/2510.12941v1</a></p>
</blockquote>
<p>Deep learning-based neural receivers are redefining physical-layer signal
processing for next-generation wireless systems. We propose an axial
self-attention <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> neural receiver designed for applicability to 6G and
beyond wireless systems, validated through 5G-compliant experimental
configurations, that achieves state-of-the-art block error rate (BLER)
performance with significantly improved computational efficiency. By
factorizing attention operations along temporal and spectral axes, the proposed
architecture reduces the quadratic complexity of conventional multi-head
self-attention from <script type="math/tex">O((TF)^2)</script> to <script type="math/tex">O(T^2F+TF^2)</script>, yielding substantially fewer
total floating-point operations and attention matrix multiplications per
<a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> block compared to global self-attention. Relative to convolutional
neural receiver baselines, the axial neural receiver achieves significantly
lower computational cost with a fraction of the parameters. Experimental
validation under 3GPP Clustered Delay Line (CDL) channels demonstrates
consistent performance gains across varying mobility scenarios. Under
non-line-of-sight CDL-C conditions, the axial neural receiver consistently
outperforms all evaluated receiver architectures, including global
self-attention, convolutional neural receivers, and traditional LS-LMMSE at
10\% BLER with reduced computational complexity per inference. At stringent
reliability targets of 1\% BLER, the axial receiver maintains robust symbol
detection at high user speeds, whereas the traditional LS-LMMSE receiver fails
to converge, underscoring its suitability for ultra-reliable low-latency
(URLLC) <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> in dynamic 6G environments and beyond. These results
establish the axial neural receiver as a structured, scalable, and efficient
framework for AI-Native 6G RAN systems, enabling deployment in
resource-constrained edge environments.</p>
<h2 id="pruning-cannot-hurt-robustness-certified-trade-offs-in-reinforcement-learning">Pruning Cannot Hurt Robustness Certified Trade-offs in Reinforcement Learning</h2>
<blockquote>
<p>Authors: James Pedley, Benjamin Etheridge, Stephen J. Roberts, Francesco Quinzan</p>
<p>2025-10-14</p>
<p><a href="http://arxiv.org/abs/2510.12939v1">http://arxiv.org/abs/2510.12939v1</a></p>
</blockquote>
<p>Reinforcement learning (RL) policies deployed in real-world environments must
remain reliable under adversarial perturbations. At the same time, modern deep
RL agents are heavily over-parameterized, raising costs and fragility concerns.
While <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> has been shown to improve robustness in supervised learning, its
role in adversarial RL remains poorly understood. We develop the first
theoretical framework for certified robustness under <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> in
state-adversarial Markov decision processes (SA-MDPs). For Gaussian and
categorical policies with Lipschitz networks, we prove that element-wise
<a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> can only tighten certified robustness bounds; <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> never makes the
policy less robust. Building on this, we derive a novel three-term regret
decomposition that disentangles clean-task performance, <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a>-induced
performance loss, and robustness gains, exposing a fundamental
performance--robustness frontier. Empirically, we evaluate magnitude and
micro-<a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> schedules on continuous-control benchmarks with strong
policy-aware adversaries. Across tasks, <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> consistently uncovers
reproducible ``sweet spots'' at moderate <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a> levels, where robustness
improves substantially without harming - and sometimes even enhancing - clean
performance. These results position <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> not merely as a <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> tool
but as a structural intervention for robust RL.</p>
<h2 id="gaussian-process-implicit-surfaces-as-control-barrier-functions-for-safe-robot-navigation">Gaussian Process Implicit Surfaces as Control Barrier Functions for Safe Robot Navigation</h2>
<blockquote>
<p>Authors: Mouhyemen Khan, Tatsuya Ibuki, Abhijit Chatterjee</p>
<p>2025-10-14</p>
<p><a href="http://arxiv.org/abs/2510.12919v1">http://arxiv.org/abs/2510.12919v1</a></p>
</blockquote>
<p>Level set methods underpin modern safety techniques such as control barrier
functions (CBFs), while also <a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> as implicit surface representations for
geometric shapes via distance fields. Inspired by these two paradigms, we
propose a unified framework where the implicit surface itself acts as a CBF. We
leverage Gaussian process (GP) implicit surface (GPIS) to represent the safety
boundaries, using safety samples which are derived from sensor measurements to
condition the GP. The GP posterior mean defines the implicit safety surface
(safety belief), while the posterior variance provides a robust safety margin.
Although GPs have favorable properties such as uncertainty estimation and
analytical tractability, they scale cubically with data. To alleviate this
issue, we develop a <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> solution called <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> Gaussian CBFs. To the best of
our knowledge, GPIS have not been explicitly used to synthesize CBFs. We
validate the approach on collision avoidance tasks in two settings: a simulated
7-DOF manipulator operating around the Stanford bunny, and a quadrotor
navigating in 3D around a physical chair. In both cases, Gaussian CBFs (with
and without <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a>) enable safe interaction and collision-free execution of
trajectories that would otherwise intersect the objects.</p>
<h2 id="kvcomm-online-cross-context-kv-cache-communication-for-efficient-llm-based-multi-agent-systems">KVCOMM Online Cross-context KV-cache Communication for Efficient LLM-based Multi-agent Systems</h2>
<blockquote>
<p>Authors: Hancheng Ye, Zhengqi Gao, Mingyuan Ma, Qinsi Wang, Yuzhe Fu, Ming-Yu Chung, Yueqian Lin, Zhijian Liu, Jianyi Zhang, Danyang Zhuo, Yiran Chen</p>
<p>2025-10-14</p>
<p><a href="http://arxiv.org/abs/2510.12872v1">http://arxiv.org/abs/2510.12872v1</a></p>
</blockquote>
<p>Multi-agent large language model (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>) systems are increasingly adopted for
complex language processing tasks that require <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> and coordination
among agents. However, these systems often suffer substantial overhead from
repeated reprocessing of <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a>ping contexts across agents. In typical
pipelines, once an agent receives a message from its predecessor, the full
context-including prior turns-must be reprocessed from scratch, leading to
inefficient processing. While key-value (<a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>) caching is an effective solution
for avoiding redundant computation in single-agent settings where prefixes
remain unchanged, it cannot be directly reused in multi-agent scenarios due to
diverging prefixes introduced by agent-specific context extensions. We identify
that the core challenge lies in the offset variance of <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>-<a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a>s across agents.
To address this, we propose <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>COMM, a training-free framework that enables
efficient <a class="glightbox" href="https://img.shields.io/badge/prefill-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/prefill-F08080" /></a>ing in multi-agent inference by reusing <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>-<a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a>s and aligning
<a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> offsets of <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a>ping contexts under diverse prefix contexts. <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>COMM
estimates and adjusts <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>-<a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a>s for shared content by referencing a pool of
<a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a>d examples-termed anchors-that store observed <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> deviations under
varying prefixes. The anchor pool is maintained and updated online, allowing
dynamic adaptation to distinct user requests and context structures. <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>COMM
achieves over 70% reuse rate across diverse multi-agent workloads, including
retrieval-augmented generation, math reasoning, and collaborative coding tasks,
all without quality degradation. Particularly, when each fully-connected agent
receives 1K input tokens with 512 prefix tokens and 512 output tokens under a
five-agent setting, <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>COMM achieves up to 7.8x speedup compared to the standard
<a class="glightbox" href="https://img.shields.io/badge/prefill-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/prefill-F08080" /></a> pipeline, reducing TTFT from ~430 ms to ~55 ms.</p>
<h2 id="what-if-understanding-motion-through-sparse-interactions">What If  Understanding Motion Through Sparse Interactions</h2>
<blockquote>
<p>Authors: Stefan Andreas Baumann, Nick Stracke, Timy Phan, Björn Ommer</p>
<p>2025-10-14</p>
<p><a href="http://arxiv.org/abs/2510.12777v1">http://arxiv.org/abs/2510.12777v1</a></p>
</blockquote>
<p>Understanding the dynamics of a physical scene involves reasoning about the
diverse ways it can potentially change, especially as a result of local
interactions. We present the Flow Poke Transformer (FPT), a novel framework for
directly predicting the distribution of local motion, conditioned on <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a>
interactions termed "pokes". Unlike traditional methods that typically only
enable dense sampling of a single realization of scene dynamics, FPT provides
an interpretable directly accessible representation of multi-modal scene
motion, its dependency on physical interactions and the inherent uncertainties
of scene dynamics. We also evaluate our model on several downstream tasks to
enable comparisons with prior methods and highlight the flexibility of our
approach. On dense face motion generation, our generic pre-trained model
surpasses specialized baselines. FPT can be fine-tuned in strongly
out-of-distribution tasks such as synthetic datasets to enable significant
improvements over in-domain methods in articulated object motion estimation.
Additionally, predicting explicit motion distributions directly enables our
method to achieve competitive performance on tasks like moving part
segmentation from pokes which further demonstrates the versatility of our FPT.
Code and models are publicly available at
https://compvis.github.io/flow-poke-<a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>.</p>
<h2 id="carvq-corrective-adaptor-with-group-residual-vector-quantization-for-llm-embedding-compression">CARVQ Corrective Adaptor with Group Residual Vector Quantization for LLM Embedding Compression</h2>
<blockquote>
<p>Authors: Dayin Gou, Sanghyun Byun, Nilesh Malpeddi, Gabrielle De Micheli, Prathamesh Vaste, Jacob Song, Woo Seong Chung</p>
<p>2025-10-14</p>
<p><a href="http://arxiv.org/abs/2510.12721v1">http://arxiv.org/abs/2510.12721v1</a></p>
</blockquote>
<p>Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) typically rely on a large number of parameters
for token embedding, leading to substantial storage requirements and memory
footprints. In particular, <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s deployed on edge devices are memory-bound, and
reducing the memory footprint by compressing the embedding layer not only frees
up the memory bandwidth but also speeds up inference. To address this, we
introduce CARVQ, a post-training novel Corrective Adaptor combined with group
Residual Vector Quantization. CARVQ relies on the composition of both linear
and non-linear maps and mimics the original model embedding to compress to
approximately 1.6 bits without requiring specialized hardware to support
lower-bit storage. We test our method on pre-trained <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s such as LLaMA-3.2-1B,
LLaMA-3.2-3B, LLaMA-3.2-3B-Instruct, LLaMA-3.1-8B, Qwen2.5-7B, Qwen2.5-Math-7B
and Phi-4, evaluating on common generative, discriminative, math and reasoning
tasks. We show that in most cases, CARVQ can achieve lower average
bitwidth-per-parameter while maintaining reasonable perplexity and accuracy
compared to scalar <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a>. Our contributions include a novel <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a>
technique that is compatible with state-of-the-art <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a>
methods and can be seamlessly integrated into any hardware supporting 4-bit
memory to reduce the model's memory footprint in memory-constrained devices.
This work demonstrates a crucial step toward the efficient deployment of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s
on edge devices.</p>
<h2 id="enhanced-angle-range-cluster-parameter-estimation-in-full-duplex-isac-systems">Enhanced Angle-Range Cluster Parameter Estimation in Full-Duplex ISAC Systems</h2>
<blockquote>
<p>Authors: Muhammad Talha, Besma Smida, David González G</p>
<p>2025-10-14</p>
<p><a href="http://arxiv.org/abs/2510.12711v1">http://arxiv.org/abs/2510.12711v1</a></p>
</blockquote>
<p>This work studies an integrated sensing and <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> (ISAC) framework
for targets that are spread both in the angle and range domains. We model each
target using a cluster of rays parameterized by a specific density function,
and propose a truncated Multiple Signal Classification (MUSIC) spread (TMS)
algorithm to accurately estimate the parameters of the density function. Unlike
the conventional MUSIC spread (CMS), TMS restricts the signal subspace rank
based on the eigen decomposition of the received-signal autocorrelation. We
also propose a discrete Fourier transform (DFT) based algorithm for estimating
the distance and range spread of each target. Leveraging these estimates, we
then develop a dynamic transmit beamforming algorithm that successfully
illuminates multiple targets while also <a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> multiple downlink (DL) users.
Simulation results demonstrate the superiority of our proposed algorithms over
baseline schemes in both low and high signal-to-noise ratio (SNR) regimes as
well as under a wide angular spread regime.</p>
<h2 id="low-latency-high-bandwidth-streaming-of-experimental-data-with-ejfat">Low Latency, High Bandwidth Streaming of Experimental Data with EJFAT</h2>
<blockquote>
<p>Authors: Ilya Baldin, Michael Goodrich, Vardan Gyurjyan, Graham Heyes, Derek Howard, Yatish Kumar, David Lawrence, Brad Sawatzky, Stacey Sheldon, Carl Timmer</p>
<p>2025-10-14</p>
<p><a href="http://arxiv.org/abs/2510.12597v1">http://arxiv.org/abs/2510.12597v1</a></p>
</blockquote>
<p>Thomas Jefferson National Accelerator Facility (JLab) has partnered with
Energy Sciences Network (ESnet) to define and implement an edge to compute
cluster computational load balancing <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> architecture. The ESnet-JLab
FPGA Accelerated Transport (EJFAT) architecture focuses on FPGA <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> to
address <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a>, fragmentation, UDP packet destination redirection (Network
Address Translation (NAT)) and de<a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> and reassembly.
  EJFAT seamlessly integrates edge and cluster computing to support direct
processing of streamed experimental data. This will directly benefit the JLab
science program as well as data centers of the future that require high
throughput and low latency for both time-critical data acquisition systems and
data center workflows.
  The EJFAT project will be presented along with how it is synergistic with
other DOE activities such as an Integrated Research Infrastructure (IRI), and
recent results using data sources at JLab, an EJFAT LB at ESnet, and
computational cluster resources at Lawrence Berkeley National Laboratory
(LBNL).</p>
<h2 id="teaching-language-models-to-faithfully-express-their-uncertainty">Teaching Language Models to Faithfully Express their Uncertainty</h2>
<blockquote>
<p>Authors: Bryan Eikema, Evgenia Ilia, José G. C. de Souza, Chrysoula Zerva, Wilker Aziz</p>
<p>2025-10-14</p>
<p><a href="http://arxiv.org/abs/2510.12587v1">http://arxiv.org/abs/2510.12587v1</a></p>
</blockquote>
<p>Large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) often miscommunicate their uncertainty: repeated
queries can produce divergent answers, yet generated responses are typically
unhedged or hedged in ways that do not reflect this variability. This conveys
unfaithful information about the uncertain state of the <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s' knowledge,
creating a faithfulness gap that affects even strong <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s. We introduce
Faithful Uncertainty Tuning (FUT): a fine-tuning approach that teaches
instruction-tuned <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s to express uncertainty faithfully without altering their
underlying answer distribution. We construct training data by augmenting model
samples with uncertainty hedges (i.e. verbal cues such as 'possibly' or
'likely') aligned with sample consistency, requiring no supervision beyond the
model and a set of prompts. We evaluate FUT on open-domain question answering
(QA) across multiple models and datasets. Our results show that FUT
substantially reduces the faithfulness gap, while pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> QA accuracy and
introducing minimal semantic distribution shift. Further analyses demonstrate
robustness across <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> strategies, choice of hedgers, and other forms of
uncertainty expression (i.e. numerical). These findings establish FUT as a
simple and effective way to teach <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s to communicate uncertainty faithfully.</p>
<h2 id="smec-rethinking-matryoshka-representation-learning-for-retrieval-embedding-compression">SMEC Rethinking Matryoshka Representation Learning for Retrieval Embedding Compression</h2>
<blockquote>
<p>Authors: Biao Zhang, Lixin Chen, Tong Liu, Bo Zheng</p>
<p>2025-10-14</p>
<p><a href="http://arxiv.org/abs/2510.12474v1">http://arxiv.org/abs/2510.12474v1</a></p>
</blockquote>
<p>Large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) generate high-dimensional embeddings that
capture rich semantic and syntactic information. However, high-dimensional
embeddings exacerbate computational complexity and storage requirements,
thereby hindering practical deployment. To address these challenges, we propose
a novel training framework named Sequential Matryoshka Embedding Compression
(SMEC). This framework introduces the Sequential Matryoshka Representation
Learning(SMRL) method to mitigate gradient variance during training, the
Adaptive Dimension Selection (ADS) module to reduce information degradation
during dimension <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a>, and the Selectable Cross-batch Memory (S-XBM) module
to enhance unsupervised learning between high- and low-dimensional embeddings.
Experiments on image, text, and multimodal datasets demonstrate that SMEC
achieves significant dimensionality reduction while maintaining performance.
For instance, on the BEIR dataset, our approach improves the performance of
compressed <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>2Vec embeddings (256 dimensions) by 1.1 points and 2.7 points
compared to the Matryoshka-Adaptor and Search-Adaptor models, respectively.</p>
<h2 id="evaluating-and-mitigating-llm-as-a-judge-bias-in-communication-systems">Evaluating and Mitigating LLM-as-a-judge Bias in Communication Systems</h2>
<blockquote>
<p>Authors: Jiaxin Gao, Chen Chen, Yanwen Jia, Xueluan Gong, Kwok-Yan Lam, Qian Wang</p>
<p>2025-10-14</p>
<p><a href="http://arxiv.org/abs/2510.12462v1">http://arxiv.org/abs/2510.12462v1</a></p>
</blockquote>
<p>Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) are increasingly being used to autonomously
evaluate the quality of content in <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> systems, e.g., to assess
responses in telecom customer support chatbots. However, the impartiality of
these AI "judges" is not guaranteed, and any biases in their evaluation
criteria could skew outcomes and undermine user trust. In this paper, we
systematically investigate judgment biases in two <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-as-a-judge models (i.e.,
GPT-Judge and JudgeLM) under the point-wise scoring setting, encompassing 11
types of biases that cover both implicit and explicit forms. We observed that
state-of-the-art <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> judges demonstrate robustness to biased inputs, generally
assigning them lower scores than the corresponding clean samples. Providing a
detailed scoring rubric further enhances this robustness. We further found that
fine-tuning an <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> on high-scoring yet biased responses can significantly
degrade its performance, highlighting the risk of training on biased data. We
also discovered that the judged scores correlate with task difficulty: a
challenging dataset like GPQA yields lower average scores, whereas an
open-ended reasoning dataset (e.g., JudgeLM-val) sees higher average scores.
Finally, we proposed four potential mitigation strategies to ensure fair and
reliable AI judging in practical <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> scenarios.</p>
<h2 id="probing-latent-knowledge-conflict-for-faithful-retrieval-augmented-generation">Probing Latent Knowledge Conflict for Faithful Retrieval-Augmented Generation</h2>
<blockquote>
<p>Authors: Linfeng Gao, Baolong Bi, Zheng Yuan, Le Wang, Zerui Chen, Zhimin Wei, Shenghua Liu, Qinggang Zhang, Jinsong Su</p>
<p>2025-10-14</p>
<p><a href="http://arxiv.org/abs/2510.12460v1">http://arxiv.org/abs/2510.12460v1</a></p>
</blockquote>
<p>Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to
enhance the factuality of Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s). However, existing RAG
systems often suffer from an unfaithfulness issue, where the model's response
contradicts evidence from the retrieved context. Existing approaches to
improving contextual faithfulness largely rely on external interventions, such
as prompt engineering, <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> constraints, or reward-based fine-tuning. These
works treat the <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> as a black box and overlook a crucial question: how does
the <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> internally integrate retrieved evidence with its parametric memory,
particularly under knowledge conflicts? To address this gap, we conduct a
probing-based analysis of hidden-state representations in <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s and observe
three findings: knowledge integration occurs hierarchically, conflicts manifest
as latent signals at the sentence level, and irrelevant context is often
amplified when aligned with parametric knowledge. Building on these findings,
we propose CLEAR (Conflict-Localized and Enhanced Attention for RAG), a
framework that (i) decomposes context into fine-grained sentence-level
knowledge, (ii) employs hidden-state probing to localize conflicting knowledge,
and (iii) introduces conflict-aware fine-tuning to guide the model to
accurately integrate retrieved evidence. Extensive experiments across three
benchmarks demonstrate that CLEAR substantially improves both accuracy and
contextual faithfulness, consistently outperforming strong baselines under
diverse conflict conditions. The related resources are available at
https://github.com/LinfengGao/CLEAR.</p>
<h2 id="videolucy-deep-memory-backtracking-for-long-video-understanding">VideoLucy Deep Memory Backtracking for Long Video Understanding</h2>
<blockquote>
<p>Authors: Jialong Zuo, Yongtai Deng, Lingdong Kong, Jingkang Yang, Rui Jin, Yiwei Zhang, Nong Sang, Liang Pan, Ziwei Liu, Changxin Gao</p>
<p>2025-10-14</p>
<p><a href="http://arxiv.org/abs/2510.12422v1">http://arxiv.org/abs/2510.12422v1</a></p>
</blockquote>
<p>Recent studies have shown that agent-based systems leveraging large language
models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) for key information retrieval and integration have emerged as a
promising approach for long video understanding. However, these systems face
two major challenges. First, they typically perform modeling and reasoning on
individual frames, struggling to capture the temporal context of consecutive
frames. Second, to reduce the cost of dense frame-level captioning, they adopt
<a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> frame sampling, which risks discarding crucial information. To overcome
these limitations, we propose VideoLucy, a deep memory backtracking framework
for long video understanding. Inspired by the human recollection process from
coarse to fine, VideoLucy employs a hierarchical memory structure with
progressive granularity. This structure explicitly defines the detail level and
temporal scope of memory at different hierarchical depths. Through an
agent-based iterative backtracking mechanism, VideoLucy systematically mines
video-wide, question-relevant deep memories until sufficient information is
gathered to provide a confident answer. This design enables effective temporal
understanding of consecutive frames while pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> critical details. In
addition, we introduce EgoMem, a new benchmark for long video understanding.
EgoMem is designed to comprehensively evaluate a model's ability to understand
complex events that unfold over time and capture fine-grained details in
extremely long videos. Extensive experiments demonstrate the superiority of
VideoLucy. Built on open-source models, VideoLucy significantly outperforms
state-of-the-art methods on multiple long video understanding benchmarks,
achieving performance even surpassing the latest proprietary models such as
GPT-4o. Our code and dataset will be made publicly at
https://videolucy.github.io</p>
<h2 id="pricinglogic-evaluating-llms-reasoning-on-complex-tourism-pricing-tasks">PricingLogic Evaluating LLMs Reasoning on Complex Tourism Pricing Tasks</h2>
<blockquote>
<p>Authors: Yunuo Liu, Dawei Zhu, Zena Al-Khalili, Dai Cheng, Yanjun Chen, Dietrich Klakow, Wei Zhang, Xiaoyu Shen</p>
<p>2025-10-14</p>
<p><a href="http://arxiv.org/abs/2510.12409v1">http://arxiv.org/abs/2510.12409v1</a></p>
</blockquote>
<p>We present PricingLogic, the first benchmark that probes whether Large
Language Models(<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) can reliably automate tourism-related prices when
multiple, <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a>ping fare rules apply. Travel agencies are eager to offload
this error-prone task onto AI systems; however, deploying <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s without verified
reliability could result in significant financial losses and erode customer
trust. PricingLogic comprises 300 natural-language questions based on booking
requests derived from 42 real-world pricing policies, spanning two levels of
difficulty: (i) basic customer-type pricing and (ii)bundled-tour calculations
involving interacting discounts. Evaluations of a line of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s reveal a steep
performance drop on the harder tier,exposing systematic failures in rule
interpretation and arithmetic reasoning.These results highlight that, despite
their general capabilities, today's <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s remain unreliable in revenue-critical
applications without further safeguards or domain adaptation. Our code and
dataset are available at https://github.com/EIT-NLP/PricingLogic.</p>
<h2 id="efficient-adaptive-transformer-an-empirical-study-and-reproducible-framework">Efficient Adaptive Transformer An Empirical Study and Reproducible Framework</h2>
<blockquote>
<p>Authors: Jan Miller</p>
<p>2025-10-14</p>
<p><a href="http://arxiv.org/abs/2510.12856v1">http://arxiv.org/abs/2510.12856v1</a></p>
</blockquote>
<p>The Efficient Adaptive Transformer (EAT) framework unifies three adaptive
efficiency techniques - progressive token <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a>, <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> attention, and
dynamic early exiting - into a single, reproducible architecture for
input-adaptive inference. EAT provides an open-source benchmarking pipeline
that automates data processing, timing, and ablation across GLUE tasks (SST-2,
QQP, MNLI). Although this empirical study finds that combining these mechanisms
can increase latency in shallow six-layer models, it demonstrates that EAT
achieves slightly higher accuracy than the optimized DistilBERT baseline on
SST-2, illustrating the potential of dynamic computation for latency-sensitive
NLP. The main contribution is the open, end-to-end reproducible framework -
complete with scripts, CSV logging, and analysis utilities - intended to serve
as a community tool for further research on adaptive <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>s.</p>
<h2 id="an-empirical-study-of-reducing-av1-decoder-complexity-and-energy-consumption-via-encoder-parameter-tuning">An Empirical Study of Reducing AV1 Decoder Complexity and Energy Consumption via Encoder Parameter Tuning</h2>
<blockquote>
<p>Authors: Vibhoothi Vibhoothi, Julien Zouein, Shanker Shreejith, Jean-Baptiste Kempf, Anil Kokaram</p>
<p>2025-10-14</p>
<p><a href="http://arxiv.org/abs/2510.12380v1">http://arxiv.org/abs/2510.12380v1</a></p>
</blockquote>
<p>The widespread adoption of advanced video codecs such as AV1 is often
hindered by their high <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> complexity, posing a challenge for
battery-constrained devices. While encoders can be configured to produce
bitstreams that are <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>r-friendly, estimating the <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> complexity and
energy overhead for a given video is non-trivial. In this study, we
systematically analyse the impact of disabling various coding tools and
adjusting coding parameters in two AV1 encoders, libaom-av1 and SVT-AV1. Using
system-level energy measurement tools like RAPL (Running Average Power Limit),
Intel SoC Watch (integrated with VTune profiler), we quantify the resulting
trade-offs between <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> complexity, energy consumption, and <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a>
efficiency for <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> a bitstream. Our results demonstrate that specific
encoder configurations can substantially reduce <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> complexity with
minimal perceptual quality degradation. For libaom-av1, disabling CDEF, an
in-loop filter gives us a mean reduction in <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> cycles by 10%. For
SVT-AV1, using the in-built, fast-<a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>=2 preset achieves a more substantial
24% reduction in <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> cycles. These findings provide strategies for content
providers to lower the energy footprint of AV1 video streaming.</p>
<h2 id="curriflow-curriculum-guided-depth-fusion-with-optical-flow-based-temporal-alignment-for-3d-semantic-scene-completion">CurriFlow Curriculum-Guided Depth Fusion with Optical Flow-Based Temporal Alignment for 3D Semantic Scene Completion</h2>
<blockquote>
<p>Authors: Jinzhou Lin, Jie Zhou, Wenhao Xu, Rongtao Xu, Changwei Wang, Shunpeng Chen, Kexue Fu, Yihua Shao, Li Guo, Shibiao Xu</p>
<p>2025-10-14</p>
<p><a href="http://arxiv.org/abs/2510.12362v1">http://arxiv.org/abs/2510.12362v1</a></p>
</blockquote>
<p>Semantic Scene Completion (SSC) aims to infer complete 3D geometry and
semantics from monocular images, <a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> as a crucial capability for
camera-based perception in autonomous driving. However, existing SSC methods
relying on temporal stacking or depth projection often lack explicit motion
reasoning and struggle with occlusions and noisy depth supervision. We propose
CurriFlow, a novel semantic occupancy prediction framework that integrates
optical flow-based temporal alignment with curriculum-guided depth fusion.
CurriFlow employs a multi-level fusion strategy to align segmentation, visual,
and depth features across frames using pre-trained optical flow, thereby
improving temporal consistency and dynamic object understanding. To enhance
geometric robustness, a curriculum learning mechanism progressively transitions
from <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> yet accurate LiDAR depth to dense but noisy stereo depth during
training, ensuring stable optimization and seamless adaptation to real-world
deployment. Furthermore, semantic priors from the Segment Anything Model (SAM)
provide category-agnostic supervision, strengthening voxel-level semantic
learning and spatial consistency. Experiments on the SemanticKITTI benchmark
demonstrate that CurriFlow achieves state-of-the-art performance with a mean
IoU of 16.9, validating the effectiveness of our motion-guided and
curriculum-aware design for camera-based 3D semantic scene completion.</p>
<h2 id="traveling-salesman-based-token-ordering-improves-stability-in-homomorphically-encrypted-language-models">Traveling Salesman-Based Token Ordering Improves Stability in Homomorphically Encrypted Language Models</h2>
<blockquote>
<p>Authors: Donghwan Rho, Sieun Seo, Hyewon Sung, Chohong Min, Ernest K. Ryu</p>
<p>2025-10-14</p>
<p><a href="http://arxiv.org/abs/2510.12343v1">http://arxiv.org/abs/2510.12343v1</a></p>
</blockquote>
<p>As users increasingly interact with large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) using
private information, secure and encrypted <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> becomes essential.
Homomorphic encryption (HE) provides a principled solution by enabling
computation directly on encrypted data. Although prior work has explored
aspects of running <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s under HE, the challenge of text generation,
particularly next-token prediction, has received limited attention and remains
a key obstacle to practical encrypted interaction. In this work, we propose a
TSP-based token reordering strategy to address the difficulties of encrypted
text generation, together with a post-processing step that further reduces
approximation error. Theoretical analysis and experimental results demonstrate
that our method prevents collapse, improves coherence in generated text, and
preserves data privacy throughout. Overall, our contributions advance the
feasibility of practical and privacy-pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> inference.</p>
<h2 id="colf-logic-programming-as-infinitary-proof-exploration">CoLF Logic Programming as Infinitary Proof Exploration</h2>
<blockquote>
<p>Authors: Zhibo Chen, Frank Pfenning</p>
<p>2025-10-14</p>
<p><a href="http://arxiv.org/abs/2510.12302v1">http://arxiv.org/abs/2510.12302v1</a></p>
</blockquote>
<p>Logical Frameworks such as Automath [de Bruijn, 1968] or LF [Harper et al.,
1993] were originally conceived as metalanguages for the specification of
foundationally uncommitted deductive systems, yielding generic proof checkers.
Their high level of abstraction was soon exploited to also express algorithms
over deductive systems such as theorem provers, type-checkers, evaluators,
compilers, proof <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>s, etc. in the paradigm of
computation-as-proof-construction. This has been realized in languages such as
<script type="math/tex">\lambda</script>-Prolog [Miller et al., 1991] or Elf [Pfenning, 1991] based on
backward chaining, and LolliMon [Lopez et al., 2005] or Celf [Schack-Nielsen
and Schuermann, 2008], which integrated forward chaining. None of these early
frameworks supported the direct expression of infinitary objects or proofs,
which are available in the recently developed CoLF<script type="math/tex">^\omega</script> [Chen, 2023]. In
this work-in-progress report, we sketch an approach to
computation-as-proof-construction over the first-order fragment of
CoLF<script type="math/tex">^\omega</script> (called CoLF<script type="math/tex">^\omega_1</script> ) that already includes infinitary
objects and proofs. A key idea is the interpretation of logic variables as
<a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> channels and computation as concurrent message-passing. This is
realized in a concrete compiler from CoLF<script type="math/tex">^\omega_1</script> to Sax, a
proof-theoretically inspired parallel programming language based on the
proof-reduction in the semi-axiomatic sequent calculus [DeYoung et al., 2020].</p>
<h2 id="reinforced-preference-optimization-for-recommendation">Reinforced Preference Optimization for Recommendation</h2>
<blockquote>
<p>Authors: Junfei Tan, Yuxin Chen, An Zhang, Junguang Jiang, Bin Liu, Ziru Xu, Han Zhu, Jian Xu, Bo Zheng, Xiang Wang</p>
<p>2025-10-14</p>
<p><a href="http://arxiv.org/abs/2510.12211v1">http://arxiv.org/abs/2510.12211v1</a></p>
</blockquote>
<p>Recent breakthroughs in large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) have fundamentally
shifted recommender systems from discriminative to generative paradigms, where
user behavior modeling is achieved by generating target items conditioned on
historical interactions. Yet current generative recommenders still suffer from
two core limitations: the lack of high-quality negative modeling and the
reliance on implicit rewards. Reinforcement learning with verifiable rewards
(RLVR) offers a natural solution by enabling on-policy sampling of harder
negatives and grounding optimization in explicit reward signals. However,
applying RLVR to generative recommenders remains non-trivial. Its unique
generation space often leads to invalid or repetitive items that undermine
sampling efficiency, and ranking supervision is <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> since most items receive
identical zero rewards. To address these challenges, we propose Reinforced
Preference Optimization for Recommendation (ReRe), a reinforcement-based
paradigm tailored to <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-based recommenders, an important direction in
generative recommendation. ReRe incorporates constrained beam search to improve
sampling efficiency and diversify hard negatives, while augmenting rule-based
accuracy rewards with auxiliary ranking rewards for finer-grained supervision.
Extensive experiments on three real-world datasets demonstrate that ReRe
consistently outperforms both traditional and <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-based recommenders in ranking
performance. Further analysis shows that ReRe not only enhances performance
across both base and SFT-initialized models but also generalizes robustly
across different backbone families and scales. Beyond empirical gains, we
systematically investigate the design space of RLVR in recommendation across
generation, sampling strategy, reward modeling, and optimization algorithm,
offering insights for future research.</p>
<h2 id="a-survey-on-parallel-reasoning">A Survey on Parallel Reasoning</h2>
<blockquote>
<p>Authors: Ziqi Wang, Boye Niu, Zipeng Gao, Zhi Zheng, Tong Xu, Linghui Meng, Zhongli Li, Jing Liu, Yilong Chen, Chen Zhu, Hua Wu, Haifeng Wang, Enhong Chen</p>
<p>2025-10-14</p>
<p><a href="http://arxiv.org/abs/2510.12164v1">http://arxiv.org/abs/2510.12164v1</a></p>
</blockquote>
<p>With the increasing capabilities of Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s), parallel
reasoning has emerged as a new inference paradigm that enhances reasoning
robustness by concurrently exploring multiple lines of thought before
converging on a final answer. It has become a significant trend to explore
parallel reasoning to overcome the fragility of standard sequential methods and
improve practical performance. In this paper, we aim to survey and summarize
the progress and challenges of parallel reasoning. We first present a formal
definition of parallel reasoning and clarify its distinction from related
concepts like Chain-of-Thought. Then, we organize and discuss advanced
techniques based on a novel taxonomy, including non-interactive reasoning,
interactive reasoning, and efficiency-focused <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> strategies.
Additionally, we explore various application scenarios, such as solving complex
problems and enhancing the reliability of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> outputs.Finally, we highlight the
core challenges of parallel reasoning and suggest potential directions for
future research. We hope that our work can provide a useful roadmap for
beginners and encourage more research on improving parallel reasoning methods.
Related source can be avaliable in
https://github.com/PPPP-kaqiu/Awesome-Parallel-Reasoning.</p>
<h2 id="fedlodrop-federated-lora-with-dropout-for-generalized-llm-fine-tuning">FedLoDrop Federated LoRA with Dropout for Generalized LLM Fine-tuning</h2>
<blockquote>
<p>Authors: Sijing Xie, Dingzhu Wen, Changsheng You, Qimei Chen, Mehdi Bennis, Kaibin Huang</p>
<p>2025-10-14</p>
<p><a href="http://arxiv.org/abs/2510.12078v1">http://arxiv.org/abs/2510.12078v1</a></p>
</blockquote>
<p>Fine-tuning (FT) large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) is crucial for adapting
general-purpose models to specific tasks, enhancing accuracy and relevance with
minimal resources. To further enhance generalization ability while reducing
training costs, this paper proposes Federated LoRA with Dropout (FedLoDrop), a
new framework that applies dropout to the rows and columns of the trainable
matrix in Federated LoRA. A generalization error bound and convergence analysis
under <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a> regularization are obtained, which elucidate the fundamental
trade-off between underfitting and overfitting. The error bound reveals that a
higher dropout rate increases model <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a>, thereby lowering the upper bound
of pointwise hypothesis stability (PHS). While this reduces the gap between
empirical and generalization errors, it also incurs a higher empirical error,
which, together with the gap, determines the overall generalization error. On
the other hand, though dropout reduces <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> costs, deploying FedLoDrop
at the network edge still faces challenges due to limited network resources. To
address this issue, an optimization problem is formulated to minimize the upper
bound of the generalization error, by jointly optimizing the dropout rate and
resource allocation subject to the latency and per-device energy consumption
constraints. To solve this problem, a branch-and-bound (B\&amp;B)-based method is
proposed to obtain its globally optimal solution. Moreover, to reduce the high
computational complexity of the B\&amp;B-based method, a penalized successive
convex approximation (P-SCA)-based algorithm is proposed to efficiently obtain
its high-quality suboptimal solution. Finally, numerical results demonstrate
the effectiveness of the proposed approach in mitigating overfitting and
improving the generalization capability.</p>
<h2 id="compressibility-measures-complexity-minimum-description-length-meets-singular-learning-theory">Compressibility Measures Complexity Minimum Description Length Meets Singular Learning Theory</h2>
<blockquote>
<p>Authors: Einar Urdshals, Edmund Lau, Jesse Hoogland, Stan van Wingerden, Daniel Murfet</p>
<p>2025-10-14</p>
<p><a href="http://arxiv.org/abs/2510.12077v1">http://arxiv.org/abs/2510.12077v1</a></p>
</blockquote>
<p>We study neural network compressibility by using singular learning theory to
extend the minimum description length (MDL) principle to singular models like
neural networks. Through extensive experiments on the Pythia suite with
<a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a>, factorization, and other <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> techniques, we find that
complexity estimates based on the local learning coefficient (LLC) are closely,
and in some cases, linearly correlated with compressibility. Our results
provide a path toward rigorously evaluating the limits of model <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a>.</p>
<h2 id="geopipe-a-geo-distributed-llm-training-framework-with-enhanced-pipeline-parallelism-in-a-lossless-rdma-enabled-datacenter-optical-transport-network">GeoPipe a Geo-distributed LLM Training Framework with enhanced Pipeline Parallelism in a Lossless RDMA-enabled Datacenter Optical Transport Network</h2>
<blockquote>
<p>Authors: Jun Dai, Xiaorun Wang, Kexiong Fang, Zheng Yang, Yuefeng Ji, Jiawei Zhang</p>
<p>2025-10-14</p>
<p><a href="http://arxiv.org/abs/2510.12064v1">http://arxiv.org/abs/2510.12064v1</a></p>
</blockquote>
<p>The proliferation of Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) with exponentially growing
parameters is making cross-data center (DC) training an inevitable trend.
However, viable strategies for extending single-DC training frameworks to
multi-DC environments remain underdeveloped. We experimentally demonstrate, for
the first time, a high-performance geo-distributed <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s training framework
across multiple DCs interconnected by a lossless, remote direct memory access
(RDMA) enabled Datacenter Optical Transport Network (DC-OTN). An enhanced
pipeline parallelism scheme is implemented within the Ascend full-stack
environment of Huawei, which effectively eliminates the impact of cross-DC
<a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> overhead on training efficiency. The <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a>ped computation and
cross-DC <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> is achieved with constraint cross-DC bandwidth and High
Bandwidth Memory (HBM), reducing computation bubble ratio by up to 78.91%.</p>
<h2 id="apce-adaptive-progressive-context-expansion-for-long-context-processing">APCE Adaptive Progressive Context Expansion for Long Context Processing</h2>
<blockquote>
<p>Authors: Baisub Lee, Sanghyun Byun, Mohanad Odema, Jung Guack, Jacob Song, Woo Seong Chung</p>
<p>2025-10-14</p>
<p><a href="http://arxiv.org/abs/2510.12051v1">http://arxiv.org/abs/2510.12051v1</a></p>
</blockquote>
<p>Deploying useful Long-Context Transformer Models (LCTMs) requires addressing
two key challenges: (1) A growing memory footprint due to quadratic
self-attention and linear <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>-<a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> scaling in memory as sequence length
increases; (2) the ContextRot phenomena where empirical evidence suggests that
<a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> architecture's performance degrades with increasing context length.
Given the shared dependency on the input, a natural question arises: Can we
surgically select the most important input chunks for processing to
synergistically (a) reduce the memory footprint, and (b) mitigate the
ContextRot effects? In this paper, we answer this question in the affirmative
for long-context summarization tasks. We propose APCE as a context-aware
solution to select the most important input chunks through low-dimensional
semantic similarity matching with the current query. By directly operating on
the input, APCE decouples from strict dependency on underlying hardware or CUDA
environments, promising a compatible solution scalable to different deployment
systems. Our empirical evaluations have demonstrated superior or on-par
summarization performance for APCE compared to the full dense baseline using a
fraction (50%-70%) of the input sequence resulting in <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>-<a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> and
self-attention memory efficiency improvements. We hope our findings inspire
further research on context-aware efficiency solutions for LCTMs geared towards
other relevant long-context tasks.</p>
<h2 id="direct-multi-token-decoding">Direct Multi-Token Decoding</h2>
<blockquote>
<p>Authors: Xuan Luo, Weizhi Wang, Xifeng Yan</p>
<p>2025-10-13</p>
<p><a href="http://arxiv.org/abs/2510.11958v1">http://arxiv.org/abs/2510.11958v1</a></p>
</blockquote>
<p>Decoder-only <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>s have become the standard architecture for large
language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) due to their strong performance. Recent studies suggest
that, in pre-trained <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s, early, middle, and late layers may serve distinct
roles: Early layers focus on understanding the input context, middle layers
handle task-specific processing, and late layers convert abstract
representations into output tokens. We hypothesize that once representations
have been processed by the early and middle layers, the resulting hidden states
may encapsulate sufficient information to support the generation of multiple
tokens using only the late layers, eliminating the need to repeatedly traverse
the early and middle layers. We refer to this inference paradigm as Direct
Multi-Token Decoding (DMTD). Unlike speculative <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a>, our method introduces
no additional parameters, auxiliary routines, or post-generation verification.
Despite being trained on a limited dataset, a fine-tuned DMTD Qwen3-4B model
has already demonstrated promising results, achieving up to a 2x speedup with
only minor performance loss. Moreover, as shown in our scaling analysis, its
performance is expected to further improve with larger training datasets.</p>
<h2 id="flexpipe-adapting-dynamic-llm-serving-through-inflight-pipeline-refactoring-in-fragmented-serverless-clusters">FlexPipe Adapting Dynamic LLM Serving Through Inflight Pipeline Refactoring in Fragmented Serverless Clusters</h2>
<blockquote>
<p>Authors: Yanying Lin, Shijie Peng, Chengzhi Lu, Chengzhong Xu, Kejiang Ye</p>
<p>2025-10-13</p>
<p><a href="http://arxiv.org/abs/2510.11938v1">http://arxiv.org/abs/2510.11938v1</a></p>
</blockquote>
<p>Serving Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) in production faces significant
challenges from highly variable request patterns and severe resource
fragmentation in serverless clusters. Current systems rely on static pipeline
configurations that struggle to adapt to dynamic workload conditions, leading
to substantial inefficiencies. We present FlexPipe, a novel system that
dynamically reconfigures pipeline architectures during runtime to address these
fundamental limitations. FlexPipe decomposes models into fine-grained stages
and intelligently adjusts pipeline granularity based on real-time request
pattern analysis, implementing three key innovations: fine-grained model
partitioning with preserved computational graph constraints, inflight pipeline
refactoring with consistent <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> transitions, and topology-aware resource
allocation that navigates GPU fragmentation. Comprehensive evaluation on an
82-GPU cluster demonstrates that FlexPipe achieves up to 8.5x better resource
efficiency while maintaining 38.3% lower latency compared to state-of-the-art
systems, reducing GPU reservation requirements from 75% to 30% of peak
capacity.</p>
<h2 id="topological-vibration-analysis-of-elastic-lattices-via-bloch-sphere-mapping">Topological Vibration Analysis of Elastic Lattices via Bloch Sphere Mapping</h2>
<blockquote>
<p>Authors: Kazi Tahsin Mahmood, M. Arif Hasan</p>
<p>2025-10-13</p>
<p><a href="http://arxiv.org/abs/2510.11930v1">http://arxiv.org/abs/2510.11930v1</a></p>
</blockquote>
<p>Mechanical lattices support topological wave phenomena governed by geometric
phases. We develop a compact Hilbert space description for one-dimensional
elastic chains, expressing intra-cell motion as a normalized superposition of
orthogonal eigenstates and tracking complex amplitudes as trajectories on a
Bloch sphere. For diatomic lattices, this framework makes inversion symmetry
protection explicit: the relative phase between in-phase and out-of-phase modes
is piecewise locked, and the Zak phase is <a class="glightbox" href="https://img.shields.io/badge/quantize-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantize-F08080" /></a>d with band-dependent jumps
at symmetry points. Extending the analysis to triatomic lattices shows that
restoring inversion retains <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a>, whereas breaking it de<a class="glightbox" href="https://img.shields.io/badge/quantize-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantize-F08080" /></a>s the
geometric phase while leaving the spectral origin invariant. Viewing
norm-pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> transformations of the modal coefficient pair as Bloch sphere
rotations, we demonstrate classical analogues of single-qubit logic gates. A
pi-phase rotation about a transverse axis swaps the modal poles, and a
longitudinal-axis phase flip maps balanced superpositions to their conjugates.
These gate-like operations are realized by controlled evolution across
wavenumber space and can be driven or reprogrammed through spatiotemporal
stiffness modulation. Introducing space-time modulation hybridizes carrier and
sideband harmonics, producing continuous phase winding and open-path geometric
phases accumulated along the Floquet trajectory. Across static and modulated
regimes, the framework unifies algebraic and geometric viewpoints, remains
robust to gauge and basis choices, and operates directly on amplitude-phase
data. The results clarify how symmetry, modulation, and topology jointly govern
dispersion, modal mixing, and phase accumulation, providing tools to analyze
and design vibration and acoustic functionalities in engineered structures.</p>
<h2 id="indoor-localization-using-compact-telemetry-agnostic-transfer-learning-enabled-decoder-only-transformer">Indoor Localization using Compact, Telemetry-Agnostic, Transfer-Learning Enabled Decoder-Only Transformer</h2>
<blockquote>
<p>Authors: Nayan Sanjay Bhatia, Pranay Kocheta, Russell Elliott, Harikrishna S. Kuttivelil, Katia Obraczka</p>
<p>2025-10-13</p>
<p><a href="http://arxiv.org/abs/2510.11926v1">http://arxiv.org/abs/2510.11926v1</a></p>
</blockquote>
<p>Indoor Wi-Fi positioning remains a challenging problem due to the high
sensitivity of radio signals to environmental dynamics, channel propagation
characteristics, and hardware heterogeneity. Conventional fingerprinting and
model-based approaches typically require labor-intensive calibration and suffer
rapid performance degradation when devices, channel or deployment conditions
change. In this paper, we introduce Locaris, a <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>r-only large language
model (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>) for indoor localization. Locaris treats each access point (AP)
measurement as a token, enabling the ingestion of raw Wi-Fi telemetry without
pre-processing. By fine-tuning its <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> on different Wi-Fi datasets, Locaris
learns a lightweight and generalizable mapping from raw signals directly to
device location. Our experimental study comparing Locaris with state-of-the-art
methods consistently shows that Locaris matches or surpasses existing
techniques for various types of telemetry. Our results demonstrate that compact
<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s can serve as calibration-free regression models for indoor localization,
offering scalable and robust cross-environment performance in heterogeneous
Wi-Fi deployments. Few-shot adaptation experiments, using only a handful of
calibration points per device, further show that Locaris maintains high
accuracy when applied to previously unseen devices and deployment scenarios.
This yields sub-meter accuracy with just a few hundred samples, robust
performance under missing APs and supports any and all available telemetry. Our
findings highlight the practical viability of Locaris for indoor positioning in
the real-world scenarios, particularly in large-scale deployments where
extensive calibration is infeasible.</p>
<h2 id="variational-mixture-of-graph-neural-experts-for-alzheimers-disease-biomarker-recognition-in-eeg-brain-networks">Variational Mixture of Graph Neural Experts for Alzheimer's Disease Biomarker Recognition in EEG Brain Networks</h2>
<blockquote>
<p>Authors: Jun-En Ding, Anna Zilverstand, Shihao Yang, Albert Chih-Chieh Yang, Feng Liu</p>
<p>2025-10-13</p>
<p><a href="http://arxiv.org/abs/2510.11917v1">http://arxiv.org/abs/2510.11917v1</a></p>
</blockquote>
<p>Dementia disorders such as Alzheimer's disease (AD) and frontotemporal
dementia (FTD) exhibit <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a>ping electrophysiological signatures in EEG that
challenge accurate diagnosis. Existing EEG-based methods are limited by
full-band frequency analysis that hinders precise differentiation of dementia
subtypes and severity stages. We propose a variational mixture of graph neural
experts (VMoGE) that integrates frequency-specific biomarker identification
with structured variational inference for enhanced dementia diagnosis and
staging. VMoGE employs a multi-granularity <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> to extract multi-scale
temporal patterns across four frequency bands, followed by a variational graph
convolutional encoder using Gaussian Markov Random Field priors. Through
structured variational inference and adaptive gating, VMoGE links neural
specialization to physiologically meaningful EEG frequency bands. Evaluated on
two diverse datasets for both subtype classification and severity staging,
VMoGE achieves superior performance with AUC improvements of +4% to +10% over
state-of-the-art methods. Moreover, VMoGE provides interpretable insights
through expert weights that correlate with clinical indicators and spatial
patterns aligned with neuropathological signatures, facilitating EEG biomarker
discovery for comprehensive dementia diagnosis and monitoring.</p>
<h2 id="qerl-beyond-efficiency-quantization-enhanced-reinforcement-learning-for-llms">QeRL Beyond Efficiency -- Quantization-enhanced Reinforcement Learning for LLMs</h2>
<blockquote>
<p>Authors: Wei Huang, Yi Ge, Shuai Yang, Yicheng Xiao, Huizi Mao, Yujun Lin, Hanrong Ye, Sifei Liu, Ka Chun Cheung, Hongxu Yin, Yao Lu, Xiaojuan Qi, Song Han, Yukang Chen</p>
<p>2025-10-13</p>
<p><a href="http://arxiv.org/abs/2510.11696v1">http://arxiv.org/abs/2510.11696v1</a></p>
</blockquote>
<p>We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for
large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s). While RL is essential for <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s' reasoning
capabilities, it is resource-intensive, requiring substantial GPU memory and
long rollout durations. QeRL addresses these issues by combining NVFP4
<a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a> with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL
while reducing memory overhead. Beyond efficiency, our findings show that
<a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a> noise increases policy entropy, enhancing exploration, and
enabling the discovery of better strategies during RL. To further optimize
exploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism,
which dynamically adjusts noise during training. Experiments demonstrate that
QeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is
the first framework to enable RL training of a 32B <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> on a single H100 80GB
GPU, while delivering overall speedups for RL training. It also achieves faster
reward growth and higher final accuracy than 16-bit LoRA and QLoRA, while
matching the performance of full-parameter fine-tuning on mathematical
benchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These
results establish QeRL as an efficient and effective framework for RL training
in <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s.</p>
<h2 id="scaling-language-centric-omnimodal-representation-learning">Scaling Language-Centric Omnimodal Representation Learning</h2>
<blockquote>
<p>Authors: Chenghao Xiao, Hou Pong Chan, Hao Zhang, Weiwen Xu, Mahani Aljunied, Yu Rong</p>
<p>2025-10-13</p>
<p><a href="http://arxiv.org/abs/2510.11693v1">http://arxiv.org/abs/2510.11693v1</a></p>
</blockquote>
<p>Recent multimodal embedding approaches leveraging multimodal large language
models (M<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) fine-tuned with contrastive learning (CL) have shown promising
results, yet the underlying reasons behind their superiority remain
underexplored. This work argues that a crucial advantage of M<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-based
approaches stems from implicit cross-modal alignment achieved during generative
pretraining, where the language <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>r learns to exploit multimodal signals
within a shared representation space for generating unimodal outputs. Through
analysis of anisotropy and kernel similarity structure, we empirically confirm
that latent alignment emerges within M<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> representations, allowing CL to serve
as a lightweight refinement stage. Leveraging this insight, we propose a
Language-Centric Omnimodal Embedding framework, termed LCO-Emb. Extensive
experiments across diverse backbones and benchmarks demonstrate its
effectiveness, achieving state-of-the-art performance across modalities.
Furthermore, we identify a Generation-Representation Scaling Law (GRSL),
showing that the representational capabilities gained through contrastive
refinement scales positively with the M<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>'s generative capabilities. This
suggests that improving generative abilities evolves as an effective paradigm
for enhancing representation quality. We provide a theoretical explanation of
GRSL, which formally links the M<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>'s generative quality to the upper bound on
its representation performance, and validate it on a challenging, low-resource
visual-document retrieval task, showing that continual generative pretraining
before CL can further enhance the potential of a model's embedding
capabilities. Codes, models, and resources are available at
https://github.com/LCO-Embedding/LCO-Embedding.</p>
<h2 id="diffusion-transformers-with-representation-autoencoders">Diffusion Transformers with Representation Autoencoders</h2>
<blockquote>
<p>Authors: Boyang Zheng, Nanye Ma, Shengbang Tong, Saining Xie</p>
<p>2025-10-13</p>
<p><a href="http://arxiv.org/abs/2510.11690v1">http://arxiv.org/abs/2510.11690v1</a></p>
</blockquote>
<p>Latent generative modeling, where a pretrained autoencoder maps pixels into a
latent space for the diffusion process, has become the standard strategy for
Diffusion Transformers (DiT); however, the autoencoder component has barely
evolved. Most DiTs continue to rely on the original VAE encoder, which
introduces several limitations: outdated backbones that compromise
architectural simplicity, low-dimensional latent spaces that restrict
information capacity, and weak representations that result from purely
reconstruction-based training and ultimately limit generative quality. In this
work, we explore replacing the VAE with pretrained representation encoders
(e.g., DINO, SigLIP, MAE) paired with trained <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>rs, forming what we term
Representation Autoencoders (RAEs). These models provide both high-quality
reconstructions and semantically rich latent spaces, while allowing for a
scalable <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>-based architecture. Since these latent spaces are
typically high-dimensional, a key challenge is enabling diffusion <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>s
to operate effectively within them. We analyze the sources of this difficulty,
propose theoretically motivated solutions, and validate them empirically. Our
approach achieves faster convergence without auxiliary representation alignment
losses. Using a DiT variant equipped with a lightweight, wide DDT head, we
achieve strong image generation results on ImageNet: 1.51 FID at 256x256 (no
guidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers
clear advantages and should be the new default for diffusion <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>
training.</p>
<h2 id="hierarchical-qubit-merging-transformer-for-quantum-error-correction">Hierarchical Qubit-Merging Transformer for Quantum Error Correction</h2>
<blockquote>
<p>Authors: Seong-Joon Park, Hee-Youl Kwak, Yongjune Kim</p>
<p>2025-10-13</p>
<p><a href="http://arxiv.org/abs/2510.11593v1">http://arxiv.org/abs/2510.11593v1</a></p>
</blockquote>
<p>For reliable large-scale quantum computation, a quantum error correction
(QEC) scheme must effectively resolve physical errors to protect logical
information. Leveraging recent advances in deep learning, neural network-based
<a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>rs have emerged as a promising approach to enhance the reliability of
QEC. We propose the Hierarchical Qubit-Merging Transformer (HQMT), a novel and
general <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> framework that explicitly leverages the structural graph of
stabilizer codes to learn error correlations across multiple scales. Our
architecture first computes attention locally on structurally related groups of
stabilizers and then systematically merges these qubit-centric representations
to build a global view of the error syndrome. The proposed HQMT achieves
substantially lower logical error rates for surface codes by integrating a
dedicated qubit-merging layer within the <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> architecture. Across
various code distances, HQMT significantly outperforms previous neural
network-based QEC <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>rs as well as a powerful belief propagation with
ordered statistics <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> (BP+OSD) baseline. This hierarchical approach
provides a scalable and effective framework for surface code <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a>,
advancing the realization of reliable quantum computing.</p>
<h2 id="culturally-aware-conversations-a-framework-benchmark-for-llms">Culturally-Aware Conversations A Framework &amp; Benchmark for LLMs</h2>
<blockquote>
<p>Authors: Shreya Havaldar, Sunny Rai, Young-Min Cho, Lyle Ungar</p>
<p>2025-10-13</p>
<p><a href="http://arxiv.org/abs/2510.11563v1">http://arxiv.org/abs/2510.11563v1</a></p>
</blockquote>
<p>Existing benchmarks that measure cultural adaptation in <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s are misaligned
with the actual challenges these models face when interacting with users from
diverse cultural backgrounds. In this work, we introduce the first framework
and benchmark designed to evaluate <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s in realistic, multicultural
conversational settings. Grounded in sociocultural theory, our framework
formalizes how linguistic style - a key element of cultural <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> - is
shaped by situational, relational, and cultural context. We construct a
benchmark dataset based on this framework, annotated by culturally diverse
raters, and propose a new set of desiderata for cross-cultural evaluation in
NLP: conversational framing, stylistic sensitivity, and subjective correctness.
We evaluate today's top <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s on our benchmark and show that these models
struggle with cultural adaptation in a conversational setting.</p>
<h2 id="situat3dchange-situated-3d-change-understanding-dataset-for-multimodal-large-language-model">Situat3DChange Situated 3D Change Understanding Dataset for Multimodal Large Language Model</h2>
<blockquote>
<p>Authors: Ruiping Liu, Junwei Zheng, Yufan Chen, Zirui Wang, Kunyu Peng, Kailun Yang, Jiaming Zhang, Marc Pollefeys, Rainer Stiefelhagen</p>
<p>2025-10-13</p>
<p><a href="http://arxiv.org/abs/2510.11509v1">http://arxiv.org/abs/2510.11509v1</a></p>
</blockquote>
<p>Physical environments and circumstances are fundamentally dynamic, yet
current 3D datasets and evaluation benchmarks tend to concentrate on either
dynamic scenarios or dynamic situations in isolation, resulting in incomplete
comprehension. To overcome these constraints, we introduce Situat3DChange, an
extensive dataset supporting three situation-aware change understanding tasks
following the perception-action model: 121K question-answer pairs, 36K change
descriptions for perception tasks, and 17K rearrangement instructions for the
action task. To construct this large-scale dataset, Situat3DChange leverages
11K human observations of environmental changes to establish shared mental
models and shared situational awareness for human-AI collaboration. These
observations, enriched with egocentric and allocentric perspectives as well as
categorical and coordinate spatial relations, are integrated using an <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> to
support understanding of situated changes. To address the challenge of
comparing pairs of point clouds from the same scene with minor changes, we
propose SCReasoner, an efficient 3D M<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> approach that enables effective point
cloud comparison with minimal parameter overhead and no additional tokens
required for the language <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>r. Comprehensive evaluation on Situat3DChange
tasks highlights both the progress and limitations of M<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s in dynamic scene
and situation understanding. Additional experiments on data scaling and
cross-domain transfer demonstrate the task-agnostic effectiveness of using
Situat3DChange as a training dataset for M<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s.</p>
<h2 id="relook-vision-grounded-rl-with-a-multimodal-llm-critic-for-agentic-web-coding">ReLook Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web Coding</h2>
<blockquote>
<p>Authors: Yuhang Li, Chenchen Zhang, Ruilin Lv, Ao Liu, Ken Deng, Yuanxing Zhang, Jiaheng Liu, Wiggin Zhou, Bo Zhou</p>
<p>2025-10-13</p>
<p><a href="http://arxiv.org/abs/2510.11498v1">http://arxiv.org/abs/2510.11498v1</a></p>
</blockquote>
<p>While Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) excel at algorithmic code generation, they
struggle with front-end development, where correctness is judged on rendered
pixels and interaction. We present ReLook, an agentic, vision-grounded
reinforcement learning framework that empowers an agent to close a robust
generate--diagnose--refine loop by invoking a multimodal <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> (M<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>) as a tool.
During training, the agent uses the M<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-in-the-loop both as a visual
critic--scoring code with screenshots--and as a source of actionable,
vision-grounded feedback; a strict zero-reward rule for invalid renders anchors
renderability and prevents reward hacking. To prevent behavioral collapse, we
introduce Forced Optimization, a strict acceptance rule that admits only
improving revisions, yielding monotonically better trajectories. At inference,
we decouple the critic and run a lightweight, critic-free self-edit cycle,
keeping latency comparable to base <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> while retaining most of the gains.
Across three widely used benchmarks, ReLook consistently outperforms strong
baselines in vision-grounded front-end code generation, highlighting the
benefits of agentic perception, visual rewards, and training-inference
decoupling.</p>
<h2 id="andesvl-technical-report-an-efficient-mobile-side-multimodal-large-language-model">AndesVL Technical Report An Efficient Mobile-side Multimodal Large Language Model</h2>
<blockquote>
<p>Authors: Zhiwei Jin, Xiaohui Song, Nan Wang, Yafei Liu, Chao Li, Xin Li, Ruichen Wang, Zhihao Li, Qi Qi, Long Cheng, Dongze Hao, Quanlong Zheng, Yanhao Zhang, Haobo Ji, Jian Ma, Zhitong Zheng, Zhenyi Lin, Haolin Deng, Xin Zou, Xiaojie Yin, Ruilin Wang, Liankai Cai, Haijing Liu, Yuqing Qiu, Ke Chen, Zixian Li, Chi Xie, Huafei Li, Chenxing Li, Chuangchuang Wang, Kai Tang, Zhiguang Zhu, Kai Tang, Wenmei Gao, Rui Wang, Jun Wu, Chao Liu, Qin Xie, Chen Chen, Haonan Lu</p>
<p>2025-10-13</p>
<p><a href="http://arxiv.org/abs/2510.11496v2">http://arxiv.org/abs/2510.11496v2</a></p>
</blockquote>
<p>In recent years, while cloud-based M<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s such as QwenVL, InternVL, GPT-4o,
Gemini, and Claude Sonnet have demonstrated outstanding performance with
enormous model sizes reaching hundreds of billions of parameters, they
significantly surpass the limitations in memory, power consumption, and
computing capacity of edge devices such as mobile phones. This paper introduces
AndesVL, a suite of mobile-side M<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s with 0.6B to 4B parameters based on
Qwen3's <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> and various visual encoders. We comprehensively outline the model
architectures, training pipeline, and training data of AndesVL, which achieves
first-tier performance across a wide range of open-source benchmarks, including
fields such as text-rich image understanding, reasoning and math, multi-image
comprehension, general VQA, hallucination mitigation, multilingual
understanding, and GUI-related tasks when compared with state-of-the-art models
of a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside
a Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient
task adaptation and model <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> during mobile-side deployment of AndesVL.
Moreover, utilizing our <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> eviction algorithm -- O<a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> -- along with
customized speculative <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> and <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> strategies, we achieve a 6.7x
peak <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> speedup ratio, up to 30.9% memory reduction, and 1.8
bits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We
release all models on https://huggingface.co/OPPOer.</p>
<h2 id="from-to-multidimensional-supervision-of-reasoning-process-for-llm-optimization">From <Answer> to <Think> Multidimensional Supervision of Reasoning Process for LLM Optimization</h2>
<blockquote>
<p>Authors: Beining Wang, Weihang Su, Hongtao Tian, Tao Yang, Yujia Zhou, Ting Yao, Qingyao Ai, Yiqun Liu</p>
<p>2025-10-13</p>
<p><a href="http://arxiv.org/abs/2510.11457v1">http://arxiv.org/abs/2510.11457v1</a></p>
</blockquote>
<p>Improving the multi-step reasoning ability of Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) is
a critical yet challenging task. The dominant paradigm, outcome-supervised
reinforcement learning (RLVR), rewards only correct final answers, often
propagating flawed reasoning and suffering from <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> reward signals. While
process-level reward models (PRMs) provide denser, step-by-step feedback, they
lack generalizability and interpretability, requiring task-specific
segmentation of the reasoning process. To this end, we propose the
Dimension-level Reward Model (DRM), a new supervision framework that bridges
the gap between these two approaches. DRM evaluates the quality of a reasoning
process along three fundamental, complementary, and interpretable dimensions:
Confidence for uncertainty calibration, Relevance for semantic alignment, and
Coherence for logical consistency. Together, these dimensions capture aspects
beyond final answer correctness and enable interpretable assessment without
requiring ground truth answers. Experimental results show that DRM provides
effective supervision signals, guides the optimization of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s and enhances
their reasoning ability. In particular, DRM-supervised training achieves
consistent gains on both in-distribution and out-of-distribution open-domain
tasks, including mathematics, question answering, code execution, and puzzles.
Our findings demonstrate that multidimensional supervision of the reasoning
process can improve the generalized reasoning ability of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s beyond the
training distribution.</p>
<h2 id="multi-view-graph-feature-propagation-for-privacy-preservation-and-feature-sparsity">Multi-View Graph Feature Propagation for Privacy Preservation and Feature Sparsity</h2>
<blockquote>
<p>Authors: Etzion Harari, Moshe Unger</p>
<p>2025-10-13</p>
<p><a href="http://arxiv.org/abs/2510.11347v1">http://arxiv.org/abs/2510.11347v1</a></p>
</blockquote>
<p>Graph Neural Networks (GNNs) have demonstrated remarkable success in node
classification tasks over relational data, yet their effectiveness often
depends on the availability of complete node features. In many real-world
scenarios, however, feature matrices are highly <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> or contain sensitive
information, leading to degraded performance and increased privacy risks.
Furthermore, direct exposure of information can result in unintended data
leakage, enabling adversaries to infer sensitive information. To address these
challenges, we propose a novel Multi-view Feature Propagation (MFP) framework
that enhances node classification under feature <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a> while promoting
privacy preservation. MFP extends traditional Feature Propagation (FP) by
dividing the available features into multiple Gaussian-noised views, each
propagating information independently through the graph topology. The
aggregated representations yield expressive and robust node embeddings. This
framework is novel in two respects: it introduces a mechanism that improves
robustness under extreme <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a>, and it provides a principled way to balance
utility with privacy. Extensive experiments conducted on graph datasets
demonstrate that MFP outperforms state-of-the-art baselines in node
classification while substantially reducing privacy leakage. Moreover, our
analysis demonstrates that propagated outputs serve as alternative imputations
rather than reconstructions of the original features, pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> utility
without compromising privacy. A comprehensive sensitivity analysis further
confirms the stability and practical applicability of MFP across diverse
scenarios. Overall, MFP provides an effective and privacy-aware framework for
graph learning in domains characterized by missing or sensitive features.</p>
<h2 id="efficient-llm-inference-over-heterogeneous-edge-networks-with-speculative-decoding">Efficient LLM Inference over Heterogeneous Edge Networks with Speculative Decoding</h2>
<blockquote>
<p>Authors: Bingjie Zhu, Zhixiong Chen, Liqiang Zhao, Hyundong Shin, Arumugam Nallanathan</p>
<p>2025-10-13</p>
<p><a href="http://arxiv.org/abs/2510.11331v1">http://arxiv.org/abs/2510.11331v1</a></p>
</blockquote>
<p>Large language model (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>) inference at the network edge is a promising
<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> paradigm that leverages distributed edge resources to run inference
near users and enhance privacy. Existing edge-based <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> inference systems
typically adopt autoregressive <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> (AD), which only generates one token
per forward pass. This iterative process, compounded by the limited
computational resources of edge nodes, results in high <a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> latency and
constrains the system's ability to support multiple users under growing
demands.To address these challenges, we propose a speculative <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a>
(SD)-based <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> <a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> framework that deploys small and large models across
heterogeneous edge nodes to collaboratively deliver inference services.
Specifically, the small model rapidly generates draft tokens that the large
model verifies in parallel, enabling multi-token generation per forward pass
and thus reducing <a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> latency. To improve resource utilization of edge
nodes, we incorporate pipeline parallelism to <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a> drafting and verification
across multiple inference tasks. Based on this framework, we analyze and derive
a comprehensive latency model incorporating both <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> and inference
latency. Then, we formulate a joint optimization problem for speculation
length, task batching, and wireless <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> resource allocation to
minimize total <a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> latency. To address this problem, we derive the
closed-form solutions for wireless <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> resource allocation, and
develop a dynamic programming algorithm for joint batching and speculation
control strategies. Experimental results demonstrate that the proposed
framework achieves lower <a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> latency compared to AD-based <a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> systems.
In addition,the proposed joint optimization method delivers up to 44.9% latency
reduction compared to benchmark schemes.</p>
<h2 id="xquant-achieving-ultra-low-bit-kv-cache-quantization-with-cross-layer-compression">XQuant Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer Compression</h2>
<blockquote>
<p>Authors: Haoqi Yang, Yao Yao, Zuchao Li, Baoyuan Qi, Guoming Liu, Hai Zhao</p>
<p>2025-10-13</p>
<p><a href="http://arxiv.org/abs/2510.11236v1">http://arxiv.org/abs/2510.11236v1</a></p>
</blockquote>
<p>Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) have demonstrated remarkable capabilities across
diverse natural language processing tasks. However, their extensive memory
requirements, particularly due to <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> growth during long-text
understanding and generation, present significant challenges for deployment in
resource-constrained environments. Quantization has emerged as a promising
solution to reduce memory consumption while pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> historical information.
We propose XQuant, a training-free and plug-and-play framework that achieves
ultra-low equivalent bit-width <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a>. XQuant introduces two key
innovations: a computationally negligible data-free calibration method and
cross-layer <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a>, enabling <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a> to sub-1.4 bits.
Extensive experiments on TruthfulQA and LongBench demonstrate that XQuant
outperforms state-of-the-art methods (e.g., KIVI-2bit and Asym<a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>-1.5bit) by
achieving lower bit-width while maintaining superior performance, establishing
a better trade-off between memory efficiency and model accuracy.</p>
<h2 id="the-curious-case-of-factual-misalignment-between-llms-short-and-long-form-answers">The Curious Case of Factual (Mis)Alignment between LLMs' Short- and Long-Form Answers</h2>
<blockquote>
<p>Authors: Saad Obaid ul Islam, Anne Lauscher, Goran Glavaš</p>
<p>2025-10-13</p>
<p><a href="http://arxiv.org/abs/2510.11218v1">http://arxiv.org/abs/2510.11218v1</a></p>
</blockquote>
<p>Large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) can correctly answer "When was Einstein born?"
yet fail to provide the same date when writing about Einstein's life revealing
a fundamental inconsistency in how models access factual knowledge across task
complexities. While models display impressive accuracy on factual
question-answering benchmarks, the reliability gap between simple and complex
queries remains poorly understood, eroding their trustworthiness. In this work,
we introduce Short-Long Form Alignment for Factual Question Answering (SLAQ), a
controlled evaluation framework that compares <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s' answers to the same factual
questions asked (a) in isolation (short) vs. (b) integrated into complex
queries (long). Looking at 16 <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s across 600 queries, we find a systematic
misalignment of answers to the corresponding short and long queries. We further
uncover position-dependent accuracy loss and momentum effects where consecutive
correct or incorrect answers create self-reinforcing patterns. Through
mechanistic analysis, we find that aligned facts activate <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a>ping model
internals, and that metrics based on mechanistic similarity can predict
short-long answer alignment with up to 78% accuracy. Our work establishes
factual consistency over query complexity as an important aspect of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s'
trustworthiness and challenges current evaluation practices, which implicitly
assume that good performance for simple factual queries implies reliability in
more complex knowledge-seeking tasks too.</p>
<h2 id="discursive-circuits-how-do-language-models-understand-discourse-relations">Discursive Circuits How Do Language Models Understand Discourse Relations?</h2>
<blockquote>
<p>Authors: Yisong Miao, Min-Yen Kan</p>
<p>2025-10-13</p>
<p><a href="http://arxiv.org/abs/2510.11210v1">http://arxiv.org/abs/2510.11210v1</a></p>
</blockquote>
<p>Which components in <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> language models are responsible for discourse
understanding? We hypothesize that <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> computational graphs, termed as
discursive circuits, control how models process discourse relations. Unlike
simpler tasks, discourse relations involve longer spans and complex reasoning.
To make circuit discovery feasible, we introduce a task called Completion under
Discourse Relation (CuDR), where a model completes a discourse given a
specified relation. To support this task, we construct a corpus of minimal
contrastive pairs tailored for activation patching in circuit discovery.
Experiments show that <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> circuits (<script type="math/tex">\approx 0.2\%</script> of a full GPT-2 model)
recover discourse understanding in the English PDTB-based CuDR task. These
circuits generalize well to unseen discourse frameworks such as RST and SDRT.
Further analysis shows lower layers capture linguistic features such as lexical
semantics and coreference, while upper layers encode discourse-level
abstractions. Feature utility is consistent across frameworks (e.g.,
coreference supports Expansion-like relations).</p>
<h2 id="efficient-in-memory-acceleration-of-sparse-block-diagonal-llms">Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs</h2>
<blockquote>
<p>Authors: João Paulo Cardoso de Lima, Marc Dietrich, Jeronimo Castrillon, Asif Ali Khan</p>
<p>2025-10-13</p>
<p><a href="http://arxiv.org/abs/2510.11192v1">http://arxiv.org/abs/2510.11192v1</a></p>
</blockquote>
<p>Structured <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a> enables deploying large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) on
resource-constrained systems. Approaches like dense-to-<a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> fine-tuning are
particularly compelling, achieving remarkable structured <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a> by reducing
the model size by over 6.7x, while still maintaining acceptable accuracy.
Despite this reduction, <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> inference, especially the <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a> stage being
inherently memory-bound, is extremely expensive on conventional Von-Neumann
architectures. Compute-in-memory (CIM) architectures mitigate this by
performing computations directly in memory, and when paired with <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s,
enable storing and computing the entire model in memory, eliminating the data
movement on the off-chip bus and improving efficiency. Nonetheless, naively
mapping <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> matrices onto CIM arrays leads to poor array utilization and
diminished computational efficiency. In this paper, we present an automated
framework with novel mapping and scheduling strategies to accelerate <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>
inference on CIM accelerators. By exploiting block-diagonal <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a>, our
approach improves CIM array utilization by over 50%, achieving more than 4x
reduction in both memory footprint and the number of required floating-point
operations.</p>
<h2 id="flow-matching-based-autonomous-driving-planning-with-advanced-interactive-behavior-modeling">Flow Matching-Based Autonomous Driving Planning with Advanced Interactive Behavior Modeling</h2>
<blockquote>
<p>Authors: Tianyi Tan, Yinan Zheng, Ruiming Liang, Zexu Wang, Kexin Zheng, Jinliang Zheng, Jianxiong Li, Xianyuan Zhan, Jingjing Liu</p>
<p>2025-10-13</p>
<p><a href="http://arxiv.org/abs/2510.11083v1">http://arxiv.org/abs/2510.11083v1</a></p>
</blockquote>
<p>Modeling interactive driving behaviors in complex scenarios remains a
fundamental challenge for autonomous driving planning. Learning-based
approaches attempt to address this challenge with advanced generative models,
removing the dependency on over-engineered architectures for representation
fusion. However, brute-force implementation by simply stacking <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>
blocks lacks a dedicated mechanism for modeling interactive behaviors that are
common in real driving scenarios. The scarcity of interactive driving data
further exacerbates this problem, leaving conventional imitation learning
methods ill-equipped to capture high-value interactive behaviors. We propose
Flow Planner, which tackles these problems through coordinated innovations in
data modeling, model architecture, and learning scheme. Specifically, we first
introduce fine-grained trajectory tokenization, which decomposes the trajectory
into <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a>ping segments to decrease the complexity of whole trajectory
modeling. With a sophisticatedly designed architecture, we achieve efficient
temporal and spatial fusion of planning and scene information, to better
capture interactive behaviors. In addition, the framework incorporates flow
matching with classifier-free guidance for multi-modal behavior generation,
which dynamically reweights agent interactions during inference to maintain
coherent response strategies, providing a critical boost for interactive
scenario understanding. Experimental results on the large-scale nuPlan dataset
and challenging interactive interPlan dataset demonstrate that Flow Planner
achieves state-of-the-art performance among learning-based approaches while
effectively modeling interactive behaviors in complex driving scenarios.</p>
<h2 id="bit-allocation-transfer-for-perceptual-quality-enhancement-of-vvc-intra-coding">Bit Allocation Transfer for Perceptual Quality Enhancement of VVC Intra Coding</h2>
<blockquote>
<p>Authors: Runyu Yang, Ivan V. Bajić</p>
<p>2025-10-13</p>
<p><a href="http://arxiv.org/abs/2510.10970v2">http://arxiv.org/abs/2510.10970v2</a></p>
</blockquote>
<p>Mainstream image and video coding standards -- including state-of-the-art
codecs like H.266/VVC, AVS3, and AV1 -- adopt a block-based hybrid coding
framework. While this framework facilitates straightforward optimization for
Peak Signal-to-Noise Ratio (PSNR), it struggles to effectively optimize
perceptually-aligned metrics such as Multi-Scale Structural Similarity
(MS-SSIM). To address this challenge, this paper proposes a low-complexity
method to enhance perceptual quality in VVC intra coding by transferring bit
allocation knowledge from end-to-end image <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a>. We introduce a
lightweight model trained with perceptual losses to generate a <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a>
step map. This map implicitly captures block-level perceptual importance,
enabling efficient derivation of a QP map for VVC. Experiments on Kodak and
CLIC datasets demonstrate significant advantages, both in execution time and
perceptual metric performance, with more than 11% BD-rate reduction in terms of
MS-SSIM. Our scheme provides an efficient, practical pathway for perceptual
enhancement of traditional codecs.</p>
<h2 id="not-all-bits-are-equal-scale-dependent-memory-optimization-strategies-for-reasoning-models">Not All Bits Are Equal Scale-Dependent Memory Optimization Strategies for Reasoning Models</h2>
<blockquote>
<p>Authors: Junhyuck Kim, Ethan Ewer, Taehong Moon, Jongho Park, Dimitris Papailiopoulos</p>
<p>2025-10-13</p>
<p><a href="http://arxiv.org/abs/2510.10964v1">http://arxiv.org/abs/2510.10964v1</a></p>
</blockquote>
<p>While 4-bit <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a> has emerged as a memory-optimal choice for
non-reasoning models and zero-shot tasks across scales, we show that this
universal prescription fails for reasoning models, where the <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> rather
than model size can dominate memory. Through systematic experiments across
1,700 inference scenarios on AIME25 and GPQA-Diamond, we find a scale-dependent
trade-off: models with an effective size below 8-bit 4B parameters achieve
better accuracy by allocating memory to more weights rather than longer
generation, while larger models achieve better accuracy by allocating memory to
longer generations. This scale threshold also determines when parallel scaling
becomes memory-efficient and whether <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> eviction outperforms <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>
<a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a>. Our findings show that memory optimization for <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s cannot be
scale-agnostic, while providing principled guidelines: for small reasoning
models, prioritize model capacity over test-time compute, while for larger
ones, maximize test-time compute. Our results suggest that optimizing reasoning
models for deployment requires fundamentally different strategies from those
established for non-reasoning models.</p>
<h2 id="mc-mixture-compressor-for-mixture-of-experts-large-models">MC# Mixture Compressor for Mixture-of-Experts Large Models</h2>
<blockquote>
<p>Authors: Wei Huang, Yue Liao, Yukang Chen, Jianhui Liu, Haoru Tan, Si Liu, Shiming Zhang, Shuicheng Yan, Xiaojuan Qi</p>
<p>2025-10-13</p>
<p><a href="http://arxiv.org/abs/2510.10962v1">http://arxiv.org/abs/2510.10962v1</a></p>
</blockquote>
<p>Mixture-of-Experts (MoE) effectively scales large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) and
vision-language models (VLMs) by increasing capacity through <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> activation.
However, preloading all experts into memory and activating multiple experts per
input introduces significant computational and memory overhead, making the
expert module a major contributor to model size and inference cost. To address
this, we propose MC# (Mixture-Compressor-sharp), a framework that combines
static <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a> and dynamic expert <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> by leveraging the significance
of experts and tokens for aggressive <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> of MoE-<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s/VLMs. To reduce
storage and loading costs, we introduce Pre-Loading Mixed-Precision
Quantization (PMQ), which optimizes bit allocation via linear programming,
balancing expert importance and <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a> error for a Pareto-optimal
trade-off between size and performance. To reduce runtime computation, Online
Top-any Pruning (OTP) uses Gumbel-Softmax sampling to dynamically select a
subset of experts per token, enabling fine-grained control over activation. By
combining PMQ's static bit-width optimization with OTP's dynamic routing, MC#
achieves extreme <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> with minimal accuracy loss. On DeepSeek-VL2, MC#
achieves a 6.2 times weight reduction at 2.57 average bits with only a 1.7%
accuracy drop across five multimodal benchmarks. Additionally, OTP reduces
expert activation over 20% with less than 1% performance degradation,
demonstrating strong potential for efficient MoE-based model deployment.</p>
<h2 id="kotox-a-korean-toxic-dataset-for-deobfuscation-and-detoxification">KOTOX A Korean Toxic Dataset for Deobfuscation and Detoxification</h2>
<blockquote>
<p>Authors: Yejin Lee, Su-Hyeon Kim, Hyundong Jin, Dayoung Kim, Yeonsoo Kim, Yo-Sub Han</p>
<p>2025-10-13</p>
<p><a href="http://arxiv.org/abs/2510.10961v1">http://arxiv.org/abs/2510.10961v1</a></p>
</blockquote>
<p>Toxic content has become an increasingly critical social issue with the rapid
expansion of online <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>. While numerous studies explored methods for
detecting and detoxifying such content, most have focused primarily on English,
leaving low-resource language underrepresented. Consequently, Large Language
Models~(<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) often struggle to identify and neutralize toxic expressions in
these languages. This challenge becomes even more pronounced when user employ
obfuscation techniques to evade detection systems. Therefore, we propose a
\textbf{KOTOX: Korean Toxic Dataset} for deobfuscation and detoxicification to
address this issue. We categorize various obfuscation approaches based on
linguistic characteristics of Korean and define a set of transformation rules
grounded in real-word examples. Using these rules, we construct three dataset
versions (easy, normal, and hard) representing different levels of obfuscation
difficulty. This is the first dataset that simultaneously supports
deobfuscation and detoxification for the Korean language. We expect it to
facilitate better understanding and mitigating of obfuscated toxic content in
<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> for low-resource languages. Our code and data are available at
https://github.com/leeyejin1231/KOTOX.</p>
<h2 id="the-social-cost-of-intelligence-emergence-propagation-and-amplification-of-stereotypical-bias-in-multi-agent-systems">The Social Cost of Intelligence Emergence, Propagation, and Amplification of Stereotypical Bias in Multi-Agent Systems</h2>
<blockquote>
<p>Authors: Thi-Nhung Nguyen, Linhao Luo, Thuy-Trang Vu, Dinh Phung</p>
<p>2025-10-13</p>
<p><a href="http://arxiv.org/abs/2510.10943v1">http://arxiv.org/abs/2510.10943v1</a></p>
</blockquote>
<p>Bias in large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) remains a persistent challenge,
manifesting in stereotyping and unfair treatment across social groups. While
prior research has primarily focused on individual models, the rise of
multi-agent systems (MAS), where multiple <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s collaborate and communicate,
introduces new and largely unexplored dynamics in bias emergence and
propagation. In this work, we present a comprehensive study of stereotypical
bias in MAS, examining how internal specialization, underlying <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s and
inter-agent <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> protocols influence bias robustness, propagation, and
amplification. We simulate social contexts where agents represent different
social groups and evaluate system behavior under various interaction and
adversarial scenarios. Experiments on three bias benchmarks reveal that MAS are
generally less robust than single-agent systems, with bias often emerging early
through in-group favoritism. However, cooperative and debate-based
<a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> can mitigate bias amplification, while more robust underlying
<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s improve overall system stability. Our findings highlight critical factors
shaping fairness and resilience in multi-agent <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> systems.</p>
<h2 id="redundancy-as-a-structural-information-principle-for-learning-and-generalization">Redundancy as a Structural Information Principle for Learning and Generalization</h2>
<blockquote>
<p>Authors: Yuda Bi, Ying Zhu, Vince D Calhoun</p>
<p>2025-10-13</p>
<p><a href="http://arxiv.org/abs/2510.10938v1">http://arxiv.org/abs/2510.10938v1</a></p>
</blockquote>
<p>We present a theoretical framework that extends classical information theory
to finite and structured systems by redefining redundancy as a fundamental
property of information organization rather than inefficiency. In this
framework, redundancy is expressed as a general family of informational
divergences that unifies multiple classical measures, such as mutual
information, chi-squared dependence, and spectral redundancy, under a single
geometric principle. This reveals that these traditional quantities are not
isolated heuristics but projections of a shared redundancy geometry. The theory
further predicts that redundancy is bounded both above and below, giving rise
to an optimal equilibrium that balances over-<a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> (loss of structure)
and over-coupling (collapse). While classical <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> theory favors
minimal redundancy for transmission efficiency, finite and structured systems,
such as those underlying real-world learning, achieve maximal stability and
generalization near this equilibrium. Experiments with masked autoencoders are
used to illustrate and verify this principle: the model exhibits a stable
redundancy level where generalization peaks. Together, these results establish
redundancy as a measurable and tunable quantity that bridges the asymptotic
world of <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> and the finite world of learning.</p>
<h2 id="awarecompiler-agentic-context-aware-compiler-optimization-via-a-synergistic-knowledge-data-driven-framework">AwareCompiler Agentic Context-Aware Compiler Optimization via a Synergistic Knowledge-Data Driven Framework</h2>
<blockquote>
<p>Authors: Hongyu Lin, Haolin Pan, Haoran Luo, Yuchen Li, Kaichun Yao, Libo Zhang, Mingjie Xing, Yanjun Wu</p>
<p>2025-10-13</p>
<p><a href="http://arxiv.org/abs/2510.11759v1">http://arxiv.org/abs/2510.11759v1</a></p>
</blockquote>
<p>Compiler optimization is crucial for enhancing program performance by
transforming the sequence of optimization passes while maintaining correctness.
Despite the promising potential of large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s)-based agent for
software optimization, automating compiler optimization remains challenging due
to: (1) semantic misalignment between abstract program representations and
concrete optimization passes, (2) inefficient interaction mechanisms between
agents and compiler environments, and (3) reward <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a> from the extensive
decision-making process within large optimization spaces. This paper introduces
\textbf{AwareCompiler}, an agentic framework for compiler optimization that
addresses these challenges through three key innovations: structured knowledge
integration and dataset construction, knowledge-driven adaptive pass
generation, and data-driven hybrid training pipeline. Experimental results on
standard benchmarks demonstrate that AwareCompiler significantly outperforms
existing baselines in both performance and efficiency, highlighting the
effectiveness of our synergistic knowledge-data-driven approach. Our code is
publicly available at https://github.com/LHY-24/AwareCompiler.</p>
<h2 id="fasthmr-accelerating-human-mesh-recovery-via-token-and-layer-merging-with-diffusion-decoding">FastHMR Accelerating Human Mesh Recovery via Token and Layer Merging with Diffusion Decoding</h2>
<blockquote>
<p>Authors: Soroush Mehraban, Andrea Iaboni, Babak Taati</p>
<p>2025-10-13</p>
<p><a href="http://arxiv.org/abs/2510.10868v1">http://arxiv.org/abs/2510.10868v1</a></p>
</blockquote>
<p>Recent <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>-based models for 3D Human Mesh Recovery (HMR) have
achieved strong performance but often suffer from high computational cost and
complexity due to deep <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> architectures and redundant tokens. In this
paper, we introduce two HMR-specific merging strategies: Error-Constrained
Layer Merging (ECLM) and Mask-guided Token Merging (Mask-ToMe). ECLM
selectively merges <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> layers that have minimal impact on the Mean Per
Joint Position Error (MPJPE), while Mask-ToMe focuses on merging background
tokens that contribute little to the final prediction. To further address the
potential performance drop caused by merging, we propose a diffusion-based
<a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>r that incorporates temporal context and leverages pose priors learned
from large-scale motion capture datasets. Experiments across multiple
benchmarks demonstrate that our method achieves up to 2.3x speed-up while
slightly improving performance over the baseline.</p>
<h2 id="agentic-rag-for-software-testing-with-hybrid-vector-graph-and-multi-agent-orchestration">Agentic RAG for Software Testing with Hybrid Vector-Graph and Multi-Agent Orchestration</h2>
<blockquote>
<p>Authors: Mohanakrishnan Hariharan, Satish Arvapalli, Seshu Barma, Evangeline Sheela</p>
<p>2025-10-12</p>
<p><a href="http://arxiv.org/abs/2510.10824v1">http://arxiv.org/abs/2510.10824v1</a></p>
</blockquote>
<p>We present an approach to software testing automation using Agentic
Retrieval-Augmented Generation (RAG) systems for Quality Engineering (QE)
artifact creation. We combine autonomous AI agents with hybrid vector-graph
knowledge systems to automate test plan, case, and QE metric generation. Our
approach addresses traditional software testing limitations by leveraging <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s
such as Gemini and Mistral, multi-agent orchestration, and enhanced
contextualization. The system achieves remarkable accuracy improvements from
65% to 94.8% while ensuring comprehensive document traceability throughout the
quality engineering lifecycle. Experimental validation of enterprise Corporate
Systems Engineering and SAP migration projects demonstrates an 85% reduction in
testing timeline, an 85% improvement in test suite efficiency, and projected
35% cost savings, resulting in a 2-month <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> of go-live.</p>
<h2 id="a-compressed-code-for-memory-discrimination">A compressed code for memory discrimination</h2>
<blockquote>
<p>Authors: Dale Zhou, Sharon Mina Noh, Nora C Harhen, Nidhi V Banavar, C. Brock Kirwan, Michael A Yassa, Aaron M Bornstein</p>
<p>2025-10-12</p>
<p><a href="http://arxiv.org/abs/2510.10791v1">http://arxiv.org/abs/2510.10791v1</a></p>
</blockquote>
<p>The ability to discriminate similar visual stimuli is an important index of
memory function. This ability is widely thought to be supported by expanding
the dimensionality of relevant neural codes, such that neural representations
for similar stimuli are maximally distinct, or ``separated.'' An alternative
hypothesis is that discrimination is supported by lossy <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> of visual
inputs, efficiently coding sensory information by discarding seemingly
irrelevant details. A benefit of <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a>, relative to expansion, is that it
allows individuals to retain fewer essential dimensions underlying stimulus
variation -- a process linked to higher-order visual processing -- without
hindering discrimination. Under this hypothesis, pattern separation is
facilitated when more information from similar stimuli can be discarded, rather
than preserved. We test the <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> versus expansion hypotheses by
predicting performance on the canonical mnemonic similarity task. We train
neural networks to compress perceptual and semantic factors of stimuli,
measuring lossiness using the mathematical framework underlying <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a>.
Consistent with the <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> hypothesis, and not the expansion hypothesis,
greater lossiness predicts the ease and performance of lure discrimination,
especially in deeper convolutional network layers that predict higher-order
visual brain activity. We then confirm these predictions across two image sets,
four behavioral datasets, and alternative lossiness metrics. Finally, using
task fMRI, we identify signatures of lossy <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> -- neural dimensionality
reduction and information loss -- in higher-order visual regions V4 and IT and
hippocampal DG/CA3 and CA1 linked to lure discrimination. These results suggest
lossy <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> supports mnemonic discrimination by discarding redundant and
<a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a>ping information.</p>
<h2 id="review-of-inference-time-scaling-strategies-reasoning-search-and-rag">Review of Inference-Time Scaling Strategies Reasoning, Search and RAG</h2>
<blockquote>
<p>Authors: Zhichao Wang, Cheng Wan, Dong Nie</p>
<p>2025-10-12</p>
<p><a href="http://arxiv.org/abs/2510.10787v1">http://arxiv.org/abs/2510.10787v1</a></p>
</blockquote>
<p>The performance gains of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s have historically been driven by scaling up
model size and training data. However, the rapidly diminishing availability of
high-quality training data is introducing a fundamental bottleneck, shifting
the focus of research toward inference-time scaling. This paradigm uses
additional computation at the time of deployment to substantially improve <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>
performance on downstream tasks without costly model re-training. This review
systematically surveys the diverse techniques contributing to this new era of
inference-time scaling, organizing the rapidly evolving field into two
comprehensive perspectives: Output-focused and Input-focused methods.
Output-focused techniques encompass complex, multi-step generation strategies,
including reasoning (e.g., CoT, ToT, ReAct), various search and <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a>
methods (e.g., MCTS, beam search), training for long CoT (e.g., RLVR, GRPO),
and model ensemble methods. Input-focused techniques are primarily categorized
by few-shot and RAG, with RAG as the central focus. The RAG section is further
detailed through a structured examination of query expansion, data, retrieval
and reranker, <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> generation methods, and multi-modal RAG.</p>
<h2 id="adip-adaptive-precision-systolic-array-for-matrix-multiplication-acceleration">ADiP Adaptive Precision Systolic Array for Matrix Multiplication Acceleration</h2>
<blockquote>
<p>Authors: Ahmed J. Abdelmaksoud, Cristian Sestito, Shiwei Wang, Themis Prodromakis</p>
<p>2025-10-12</p>
<p><a href="http://arxiv.org/abs/2510.10623v1">http://arxiv.org/abs/2510.10623v1</a></p>
</blockquote>
<p>Transformers are at the core of modern AI nowadays. They rely heavily on
matrix multiplication and require efficient <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> due to their
substantial memory and computational requirements. Quantization plays a vital
role in reducing memory usage, and can be exploited for computations by
designing reconfigurable architectures that enhance matrix multiplication by
dynamically adjusting the precision. This paper proposes ADiP, a novel
adaptive-precision systolic array architecture designed for efficient matrix
multiplication <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a>.The proposed architecture consists of NxN
adaptive-precision processing elements (PEs) and shared accumulators. ADiP
supports multiple computation modes, including symmetric single-matrix
multiplication as well as asymmetric multi-matrix multiplication with a shared
input matrix, thereby improving data-reuse and PE utilization. In addition,
ADiP maximizes the computational density by adapting to different precisions,
such as 8bitx8bit, 8bitx4bit, and 8bitx2bit. Analytical models are developed
for ADiP architecture, including latency and throughput for versatile
architecture configurations. A comprehensive hardware design space exploration
is demonstrated using 22nm commercial technology, achieving up to a 4x higher
computational throughput. Furthermore, ADiP is evaluated on different
<a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> workloads from GPT-2 Medium, BERT Large, and BitNet-1.58B models,
delivering latency improvement up to 53.6%, and energy improvement up to 24.4%
for BitNet-1.58B MHA workloads. At a 64x64 size with 4096 PEs, ADiP achieves a
peak throughput of 8.192 TOPS, 16.384 TOPS, and 32.768 TOPS for 8bitx8bit,
8bitx4bit, and 8bitx2bit operations, respectively.</p>
<h2 id="preserving-llm-capabilities-through-calibration-data-curation-from-analysis-to-optimization">Preserving LLM Capabilities through Calibration Data Curation From Analysis to Optimization</h2>
<blockquote>
<p>Authors: Bowei He, Lihao Yin, Huiling Zhen, Shuqi Liu, Han Wu, Xiaokun Zhang, Mingxuan Yuan, Chen Ma</p>
<p>2025-10-12</p>
<p><a href="http://arxiv.org/abs/2510.10618v1">http://arxiv.org/abs/2510.10618v1</a></p>
</blockquote>
<p>Post-training <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> has been a widely employed approach to scale down
large language model (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>) and facilitate efficient inference. In various
proposed <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> methods, including <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> and <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a>, calibration
data plays a vital role by informing the weight importance and activation
dynamic ranges. However, how calibration data impacts the <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> capability after
<a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> is less explored. Few of the existing works, though recognizing the
significance of this study, only investigate the language modeling or
commonsense reasoning performance degradation from limited angles, like the
data sources or sample amounts. More systematic research is still needed to
examine the impacts on different <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> capabilities in terms of compositional
properties and domain correspondence of calibration data. In this work, we aim
at bridging this gap and further analyze underlying influencing mechanisms from
the activation pattern perspective. Especially, we explore the calibration
data's impacts on high-level complex reasoning capabilities, like math problem
solving and code generation. Delving into the underlying mechanism, we find
that the representativeness and diversity in activation space more
fundamentally determine the quality of calibration data. Finally, we propose a
calibration data curation framework based on such observations and analysis,
enhancing the performance of existing post-training <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> methods on
pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> critical <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> capabilities. Our code is provided in
\href{https://github.com/BokwaiHo/COLA.git}{Link}.</p>
<h2 id="large-language-model-empowered-channel-prediction-and-predictive-beamforming-for-leo-satellite-communications">Large Language Model-Empowered Channel Prediction and Predictive Beamforming for LEO Satellite Communications</h2>
<blockquote>
<p>Authors: Zhixiong Chen, Hyundong Shin, Arumugam Nallanathan, Jonathon Chambers</p>
<p>2025-10-12</p>
<p><a href="http://arxiv.org/abs/2510.10561v1">http://arxiv.org/abs/2510.10561v1</a></p>
</blockquote>
<p>Accurate channel prediction and effective beamforming are essential for low
Earth orbit (LEO) satellite <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>s to enhance system capacity and
enable high-speed connectivity. Most existing channel prediction and predictive
beamforming methods are limited by model generalization capabilities and
struggle to adapt to time-varying wireless propagation environments. Inspired
by the remarkable generalization and reasoning capabilities of large language
models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s), this work proposes an <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-based channel prediction framework,
namely CP<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>, to forecast future channel state information (CSI) for LEO
satellites based on historical CSI data. In the proposed CP<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>, a dedicated CSI
encoder is designed to map raw CSI data into the textual embedding space,
effectively bridging the modality gap and enabling the <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> to perform reliable
reasoning over CSI data. Additionally, a CSI <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>r is introduced to
simultaneously predict CSI for multiple future time slots, substantially
reducing the computational burden and inference latency associated with the
inherent autoregressive <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> process of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s. Then, instead of training the
<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> from scratch, we adopt a parameter-efficient fine-tuning strategy, i.e.,
LoRA, for CP<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>, where the pretrained <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> remains frozen and trainable low-rank
matrices are injected into each Transformer <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>r layer to enable effective
fine-tuning. Furthermore, we extend CP<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> to directly generate beamforming
strategies for future time slots based on historical CSI data, namely BF<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>.
This extended framework retains the same architecture as CP<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>, while
introducing a dedicated beamforming <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>r to output beamforming strategies.
Finally, extensive simulation results validate the effectiveness of the
proposed approaches in channel prediction and predictive beamforming for LEO
satellite <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>s.</p>
<h2 id="bitmar-low-bit-multimodal-fusion-with-episodic-memory-for-edge-devices">BitMar Low-Bit Multimodal Fusion with Episodic Memory for Edge Devices</h2>
<blockquote>
<p>Authors: Euhid Aman, Esteban Carlin, Hsing-Kuo Pao, Giovanni Beltrame, Ghaluh Indah Permata Sari, Yie-Tarng Chen</p>
<p>2025-10-12</p>
<p><a href="http://arxiv.org/abs/2510.10560v1">http://arxiv.org/abs/2510.10560v1</a></p>
</blockquote>
<p>Cross-attention <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>s and other multimodal vision-language models
excel at grounding and generation; however, their extensive, full-precision
backbones make it challenging to deploy them on edge devices. Memory-augmented
architectures enhance the utilization of past context; however, most works
rarely pair them with aggressive edge-oriented <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a>. We introduce
BitMar, a <a class="glightbox" href="https://img.shields.io/badge/quantize-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantize-F08080" /></a>d multimodal <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> that proposes an external human-like
episodic memory for effective image-text generation on hardware with limited
resources. BitMar utilizes 1.58-bit encoders, one for text (BitNet-style) and
one for vision (DiNOv2-based), to create compact embeddings that are combined
and used to query a fixed-size key-value episodic memory. During vector
retrieval, the BitNet <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>r applies per-layer conditioning, which increases
the contextual relevance of generated content. The <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>r also employs
attention sinks with a sliding-window mechanism to process long or streaming
inputs under tight memory budgets. The combination of per-layer conditioning
and sliding-window attention achieves a strong quality-speed trade-off,
delivering competitive captioning and multimodal understanding at low latency
with a small model footprint. These characteristics make BitMar well-suited for
edge deployment.</p>
<h2 id="self-supervised-representation-learning-with-id-content-modality-alignment-for-sequential-recommendation">Self-Supervised Representation Learning with ID-Content Modality Alignment for Sequential Recommendation</h2>
<blockquote>
<p>Authors: Donglin Zhou, Weike Pan, Zhong Ming</p>
<p>2025-10-12</p>
<p><a href="http://arxiv.org/abs/2510.10556v1">http://arxiv.org/abs/2510.10556v1</a></p>
</blockquote>
<p>Sequential recommendation (SR) models often capture user preferences based on
the historically interacted item IDs, which usually obtain sub-optimal
performance when the interaction history is limited. Content-based sequential
recommendation has recently emerged as a promising direction that exploits
items' textual and visual features to enhance preference learning. However,
there are still three key challenges: (i) how to reduce the semantic gap
between different content modality representations; (ii) how to jointly model
user behavior preferences and content preferences; and (iii) how to design an
effective training strategy to align ID representations and content
representations. To address these challenges, we propose a novel model,
self-supervised representation learning with ID-Content modality alignment,
named SICSRec. Firstly, we propose a <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-driven sample construction method and
develop a supervised fine-tuning approach to align item-level modality
representations. Secondly, we design a novel Transformer-based sequential
model, where an ID-modality sequence encoder captures user behavior
preferences, a content-modality sequence encoder learns user content
preferences, and a mix-modality sequence <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>r grasps the intrinsic
relationship between these two types of preferences. Thirdly, we propose a
two-step training strategy with a content-aware contrastive learning task to
align modality representations and ID representations, which decouples the
training process of content modality dependency and item collaborative
dependency. Extensive experiments conducted on four public video streaming
datasets demonstrate our SICSRec outperforms the state-of-the-art ID-modality
sequential recommenders and content-modality sequential recommenders by 8.04%
on NDCG@5 and 6.62% on NDCD@10 on average, respectively.</p>
<h2 id="the-hidden-dna-of-llm-generated-javascript-structural-patterns-enable-high-accuracy-authorship-attribution">The Hidden DNA of LLM-Generated JavaScript Structural Patterns Enable High-Accuracy Authorship Attribution</h2>
<blockquote>
<p>Authors: Norbert Tihanyi, Bilel Cherif, Richard A. Dubniczky, Mohamed Amine Ferrag, Tamás Bisztray</p>
<p>2025-10-12</p>
<p><a href="http://arxiv.org/abs/2510.10493v1">http://arxiv.org/abs/2510.10493v1</a></p>
</blockquote>
<p>In this paper, we present the first large-scale study exploring whether
JavaScript code generated by Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) can reveal which
model produced it, enabling reliable authorship attribution and model
fingerprinting. With the rapid rise of AI-generated code, attribution is
playing a critical role in detecting vulnerabilities, flagging malicious
content, and ensuring accountability. While AI-vs-human detection usually
treats AI as a single category we show that individual <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s leave unique
stylistic signatures, even among models belonging to the same family or
parameter size. To this end, we introduce <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-NodeJS, a dataset of 50,000
Node.js back-end programs from 20 large language models. Each has four
transformed variants, yielding 250,000 unique JavaScript samples and two
additional representations (JSIR and AST) for diverse research applications.
Using this dataset, we benchmark traditional machine learning classifiers
against fine-tuned Transformer encoders and introduce CodeT5-JSA, a custom
architecture derived from the 770M-parameter CodeT5 model with its <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>r
removed and a modified classification head. It achieves 95.8% accuracy on
five-class attribution, 94.6% on ten-class, and 88.5% on twenty-class tasks,
surpassing other tested models such as BERT, CodeBERT, and Longformer. We
demonstrate that classifiers capture deeper stylistic regularities in program
dataflow and structure, rather than relying on surface-level features. As a
result, attribution remains effective even after mangling, comment removal, and
heavy code transformations. To support open science and reproducibility, we
release the <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-NodeJS dataset, Google Colab training scripts, and all related
materials on GitHub: https://github.com/<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-NodeJS-dataset.</p>
<h2 id="saser-stego-attacks-on-open-source-llms">SASER Stego attacks on open-source LLMs</h2>
<blockquote>
<p>Authors: Ming Tan, Wei Li, Hu Tao, Hailong Ma, Aodi Liu, Qian Chen, Zilong Wang</p>
<p>2025-10-12</p>
<p><a href="http://arxiv.org/abs/2510.10486v1">http://arxiv.org/abs/2510.10486v1</a></p>
</blockquote>
<p>Open-source large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) have demonstrated considerable
dominance over proprietary <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s in resolving neural processing tasks, thanks to
the collaborative and sharing nature. Although full access to source codes,
model parameters, and training data lays the groundwork for transparency, we
argue that such a full-access manner is vulnerable to stego attacks, and their
ill-effects are not fully understood. In this paper, we conduct a systematic
formalization for stego attacks on open-source <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s by enumerating all possible
threat models associated with adversary objectives, knowledge, and
capabilities. Therein, the threat posed by adversaries with internal knowledge,
who inject payloads and triggers during the model sharing phase, is of
practical interest. We go even further and propose the first stego attack on
open-source <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s, dubbed SASER, which wields impacts through identifying
targeted parameters, embedding payloads, injecting triggers, and executing
payloads sequentially. Particularly, SASER enhances the attack robustness
against <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a>-based local deployment by de-quantizing the embedded
payloads. In addition, to achieve stealthiness, SASER devises the
performance-aware importance metric to identify targeted parameters with the
least degradation of model performance. Extensive experiments on LlaMA2-7B and
ChatGLM3-6B, without <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a>, show that the stealth rate of SASER
outperforms existing stego attacks (for general DNNs) by up to 98.1%, while
achieving the same attack success rate (ASR) of 100%. More importantly, SASER
improves ASR on <a class="glightbox" href="https://img.shields.io/badge/quantize-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantize-F08080" /></a>d models from 0 to 100% in all settings. We appeal for
investigations on countermeasures against SASER in view of the significant
attack effectiveness.</p>
<h2 id="anybcq-hardware-efficient-flexible-binary-coded-quantization-for-multi-precision-llms">AnyBCQ Hardware Efficient Flexible Binary-Coded Quantization for Multi-Precision LLMs</h2>
<blockquote>
<p>Authors: Gunho Park, Jeongin Bae, Beomseok Kwon, Byeongwook Kim, Se Jung Kwon, Dongsoo Lee</p>
<p>2025-10-12</p>
<p><a href="http://arxiv.org/abs/2510.10467v1">http://arxiv.org/abs/2510.10467v1</a></p>
</blockquote>
<p>The deployment of large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) is increasingly constrained by
memory and latency bottlenecks, motivating the need for <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a> techniques
that flexibly balance accuracy and efficiency. Recent work has introduced
multi-precision models, which enable inference at multiple precisions within a
single model depending on runtime constraints. To support such flexibility,
<a class="glightbox" href="https://img.shields.io/badge/quantize-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantize-F08080" /></a>d weights are often stored as bit-planes, where hardware efficiency
improves when the compute operates directly at the bit-plane level and
activates only the precision required by each request. In this work, we present
AnyBCQ, a hardware-friendly multi-precision extension of Binary-Coded
Quantization (BCQ) that supports direct bit-plane operations. By representing
weights as binary bit-planes with corresponding scale factors, AnyBCQ enables
bit-plane-level computation and maps naturally to accelerator-friendly,
bit-parallel arithmetic. Our progressive precision expansion mechanism
incrementally refines scaling factors while reusing previously assigned binary
codes, yielding monotonic improvements in accuracy as additional bits are
enabled. We further co-design a specialized kernel that exploits the BCQ
structure to support dynamic per-request precision selection with negligible
overhead. Experiments on recent <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s demonstrate that AnyBCQ significantly
narrows the accuracy drop in the <a class="glightbox" href="https://img.shields.io/badge/low-bit-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/low-bit-F08080" /></a> regime (e.g. 2-bit), remains
competitive at higher precision, and achieves throughput gains of up to 3.0x
over half precision and 1.2x over state-of-the-art multi-precision methods. By
aligning algorithmic flexibility with hardware efficiency, AnyBCQ provides a
practical foundation for multi-precision <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> deployment across diverse
service-level objectives.</p>
<h2 id="when-images-speak-louder-mitigating-language-bias-induced-hallucinations-in-vlms-through-cross-modal-guidance">When Images Speak Louder Mitigating Language Bias-induced Hallucinations in VLMs through Cross-Modal Guidance</h2>
<blockquote>
<p>Authors: Jinjin Cao, Zhiyang Chen, Zijun Wang, Liyuan Ma, Weijian Luo, Guojun Qi</p>
<p>2025-10-12</p>
<p><a href="http://arxiv.org/abs/2510.10466v1">http://arxiv.org/abs/2510.10466v1</a></p>
</blockquote>
<p>Vision-Language Models (VLMs) have shown solid ability for multimodal
understanding of both visual and language contexts. However, existing VLMs
often face severe challenges of hallucinations, meaning that VLMs tend to
generate responses that are only fluent in the language but irrelevant to
images in previous contexts. To address this issue, we analyze how language
bias contributes to hallucinations and then introduce Cross-Modal
Guidance(CMG), a training-free <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> method that addresses the
hallucinations by leveraging the difference between the output distributions of
the original model and the one with degraded visual-language attention. In
practice, we adaptively mask the attention weight of the most influential image
tokens in selected <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> layers to corrupt the visual-language perception
as a concrete type of degradation. Such a degradation-induced <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a>
emphasizes the perception of visual contexts and therefore significantly
reduces language bias without harming the ability of VLMs. In experiment
sections, we conduct comprehensive studies. All results demonstrate the
superior advantages of CMG with neither additional conditions nor training
costs. We also quantitatively show CMG can improve different VLM's performance
on hallucination-specific benchmarks and generalize effectively.</p>
<h2 id="nim-neuro-symbolic-ideographic-metalanguage-for-inclusive-communication">NIM Neuro-symbolic Ideographic Metalanguage for Inclusive Communication</h2>
<blockquote>
<p>Authors: Prawaal Sharma, Poonam Goyal, Navneet Goyal, Vidisha Sharma</p>
<p>2025-10-12</p>
<p><a href="http://arxiv.org/abs/2510.10459v1">http://arxiv.org/abs/2510.10459v1</a></p>
</blockquote>
<p>Digital <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> has become the cornerstone of modern interaction,
enabling rapid, accessible, and interactive exchanges. However, individuals
with lower academic literacy often face significant barriers, exacerbating the
"digital divide". In this work, we introduce a novel, universal ideographic
metalanguage designed as an innovative <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> framework that transcends
academic, linguistic, and cultural boundaries. Our approach leverages
principles of Neuro-symbolic AI, combining neural-based large language models
(<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) enriched with world knowledge and symbolic knowledge heuristics grounded
in the linguistic theory of Natural Semantic Metalanguage (NSM). This enables
the semantic decomposition of complex ideas into simpler, atomic concepts.
Adopting a human-centric, collaborative methodology, we engaged over 200
semi-literate participants in defining the problem, selecting ideographs, and
validating the system. With over 80\% semantic comprehensibility, an accessible
learning curve, and universal adaptability, our system effectively serves
underprivileged populations with limited formal education.</p>
<h2 id="robotfleet-an-open-source-framework-for-centralized-multi-robot-task-planning">RobotFleet An Open-Source Framework for Centralized Multi-Robot Task Planning</h2>
<blockquote>
<p>Authors: Rohan Gupta, Trevor Asbery, Zain Merchant, Abrar Anwar, Jesse Thomason</p>
<p>2025-10-12</p>
<p><a href="http://arxiv.org/abs/2510.10379v1">http://arxiv.org/abs/2510.10379v1</a></p>
</blockquote>
<p>Coordinating heterogeneous robot fleets to achieve multiple goals is
challenging in multi-robot systems. We introduce an open-source and extensible
framework for centralized multi-robot task planning and scheduling that
leverages <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s to enable fleets of heterogeneous robots to accomplish multiple
tasks. RobotFleet provides abstractions for planning, scheduling, and execution
across robots deployed as containerized services to simplify fleet scaling and
management. The framework maintains a shared declarative world state and
two-way <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> for task execution and replanning. By modularizing each
layer of the autonomy stack and using <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s for open-world reasoning, RobotFleet
lowers the barrier to building scalable multi-robot systems. The code can be
found here: https://github.com/therohangupta/robot-fleet.</p>
<h2 id="sp-moe-speculative-decoding-and-prefetching-for-accelerating-moe-based-model-inference">SP-MoE Speculative Decoding and Prefetching for Accelerating MoE-based Model Inference</h2>
<blockquote>
<p>Authors: Liangkun Chen, Zijian Wen, Tian Wu, Xiaoxi Zhang, Chuan Wu</p>
<p>2025-10-11</p>
<p><a href="http://arxiv.org/abs/2510.10302v1">http://arxiv.org/abs/2510.10302v1</a></p>
</blockquote>
<p>The Mixture-of-Experts (MoE) architecture has been widely adopted in large
language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) to reduce computation cost through model <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a>.
Employing speculative <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> (SD) can further accelerate MoE inference by
drafting multiple tokens per step and verifying them in parallel. However,
combining MoE with SD inflates GPU memory and aggravates CPU-GPU bandwidth
contention during multi-token verification. Existing MoE offloading systems are
SD-agnostic and do not address this bottleneck. We present SP-MoE, the first
SD-aware expert-offloading and compute-<a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> pipelining framework.
SP-MoE introduces: (1) speculative expert prefetching that exploits structural
correspondence between the draft and target models to prefetch likely experts
ahead of verification; (2) a cutoff-layer policy that bounds per-layer prefetch
depth based on empirical profiles and an analytical latency model, guaranteeing
just-in-time availability without overfetch; and (3) a pipelined runtime with
asynchronous prefetch threads and batched I/O to hide loading latency.
Extensive experiments demonstrate that SP-MoE achieves a 1.07-3.5 times TPOT
speedup over state-of-the-art methods across diverse datasets, environments,
and MoE-based models.</p>
<h2 id="grounded-ai-for-code-review-resource-efficient-large-model-serving-in-enterprise-pipelines">Grounded AI for Code Review Resource-Efficient Large-Model Serving in Enterprise Pipelines</h2>
<blockquote>
<p>Authors: Sayan Mandal, Hua Jiang</p>
<p>2025-10-11</p>
<p><a href="http://arxiv.org/abs/2510.10290v1">http://arxiv.org/abs/2510.10290v1</a></p>
</blockquote>
<p>Automated code review adoption lags in compliance-heavy settings, where
static analyzers produce high-volume, low-rationale outputs, and naive <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> use
risks hallucination and incurring cost overhead. We present a production system
for grounded, PR-native review that pairs static-analysis findings with
AST-guided context extraction and a single-GPU, on-demand <a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> stack
(<a class="glightbox" href="https://img.shields.io/badge/quantize-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantize-F08080" /></a>d open-weight model, multi-tier caching) to deliver concise
explanations and remediation guidance. Evaluated on safety-oriented C/C++
standards, the approach achieves sub-minute median first-feedback (offline p50
build+<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> 59.8s) while maintaining competitive violation reduction and lower
violation rates versus larger proprietary models. The architecture is
decoupled: teams can adopt the grounding/prompting layer or the <a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> layer
independently. A small internal survey (n=8) provides directional signals of
reduced triage effort and moderate perceived grounding, with participants
reporting fewer human review iterations. We outline operational lessons and
limitations, emphasizing reproducibility, auditability, and pathways to broader
standards and assisted patching.</p>
<h2 id="the-achilles-heel-of-llms-how-altering-a-handful-of-neurons-can-cripple-language-abilities">The Achilles' Heel of LLMs How Altering a Handful of Neurons Can Cripple Language Abilities</h2>
<blockquote>
<p>Authors: Zixuan Qin, Kunlin Lyu, Qingchen Yu, Yifan Sun, Zhaoxin Fan</p>
<p>2025-10-11</p>
<p><a href="http://arxiv.org/abs/2510.10238v1">http://arxiv.org/abs/2510.10238v1</a></p>
</blockquote>
<p>Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) have become foundational tools in natural
language processing, powering a wide range of applications and research. Many
studies have shown that <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s share significant similarities with the human
brain. Recent neuroscience research has found that a small subset of biological
neurons in the human brain are crucial for core cognitive functions, which
raises a fundamental question: do <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s also contain a small subset of critical
neurons? In this paper, we investigate this question by proposing a
Perturbation-based Causal Identification of Critical Neurons method to
systematically locate such critical neurons in <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s. Our findings reveal three
key insights: (1) <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s contain ultra-<a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> critical neuron sets. Disrupting
these critical neurons can cause a 72B-parameter model with over 1.1 billion
neurons to completely collapse, with perplexity increasing by up to 20 orders
of magnitude; (2) These critical neurons are not uniformly distributed, but
tend to concentrate in the outer layers, particularly within the MLP down_proj
components; (3) Performance degradation exhibits sharp phase transitions,
rather than a gradual decline, when these critical neurons are disrupted.
Through comprehensive experiments across diverse model architectures and
scales, we provide deeper analysis of these phenomena and their implications
for <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> robustness and interpretability. These findings can offer guidance for
developing more robust model architectures and improving deployment security in
safety-critical applications.</p>
<h2 id="isaac-intelligent-scalable-agile-and-accelerated-cpu-verification-via-llm-aided-fpga-parallelism">ISAAC Intelligent, Scalable, Agile, and Accelerated CPU Verification via LLM-aided FPGA Parallelism</h2>
<blockquote>
<p>Authors: Jialin Sun, Yuchen Hu, Dean You, Yushu Du, Hui Wang, Xinwei Fang, Weiwei Shan, Nan Guan, Zhe Jiang</p>
<p>2025-10-11</p>
<p><a href="http://arxiv.org/abs/2510.10225v1">http://arxiv.org/abs/2510.10225v1</a></p>
</blockquote>
<p>Functional verification is a critical bottleneck in integrated circuit
development, with CPU verification being especially time-intensive and
labour-consuming. Industrial practice relies on differential testing for CPU
verification, yet faces bottlenecks at nearly each stage of the framework
pipeline: front-end stimulus generation lacks micro-architectural awareness,
yielding low-quality and redundant tests that impede coverage closure and miss
corner cases. Meanwhile, back-end simulation infrastructure, even with FPGA
<a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a>, often stalls on long-running tests and offers limited visibility,
delaying feedback and prolonging the debugging cycle. Here, we present ISAAC, a
full-stack, Large Language Model (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>)-aided CPU verification framework with
FPGA parallelism, from bug categorisation and stimulus generation to simulation
infrastructure. To do so, we presented a multi-agent stimulus engine in ISAAC's
front-end, infused with micro-architectural knowledge and historical bug
patterns, generating highly targeted tests that rapidly achieve coverage goals
and capture elusive corner cases. In ISAAC's back-end, we introduce a
lightweight forward-snapshot mechanism and a decoupled co-simulation
architecture between the Instruction Set Simulator (ISS) and the Design Under
Test (DUT), enabling a single ISS to drive multiple DUTs in parallel. By
eliminating long-tail test bottlenecks and exploiting FPGA parallelism, the
simulation throughput is significantly improved. As a demonstration, we used
ISAAC to verify a mature CPU that has undergone multiple successful tape-outs.
Results show up to 17,536x speed-up over software RTL simulation, while
detecting several previously unknown bugs, two of which are reported in this
paper.</p>
<h2 id="billy-steering-large-language-models-via-merging-persona-vectors-for-creative-generation">BILLY Steering Large Language Models via Merging Persona Vectors for Creative Generation</h2>
<blockquote>
<p>Authors: Tsung-Min Pai, Jui-I Wang, Li-Chun Lu, Shao-Hua Sun, Hung-Yi Lee, Kai-Wei Chang</p>
<p>2025-10-11</p>
<p><a href="http://arxiv.org/abs/2510.10157v1">http://arxiv.org/abs/2510.10157v1</a></p>
</blockquote>
<p>Multi-<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> systems enhance the creativity of large language models by
simulating human collective intelligence but suffer from significant drawbacks,
such as high computational costs and inference latency. To address these
limitations, we propose BILLY (BlendIng persona vectors for Large Language
model creativitY), a training-free framework that captures the benefits of
multi-<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> collaboration, i.e. inducing diverse perspectives and specialized
expertise, within a single model. BILLY operates by extracting and blending
multiple distinct persona vectors directly in the model's activation space. We
steer the model's generation process with this merged vector while inference,
enabling multi-perspective output without explicit multi-<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>. Our
experiments across creativity-oriented benchmarks demonstrate that BILLY
surpasses single model prompting and traditional multi-<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> approaches, while
substantially reducing inference time and computational costs. Our analyses
further reveal that distinct persona vectors can be blended to achieve both
effective control over complementary aspects of generation and greater
interpretability.</p>
<h2 id="a-unified-frequency-domain-decomposition-framework-for-interpretable-and-robust-time-series-forecasting">A Unified Frequency Domain Decomposition Framework for Interpretable and Robust Time Series Forecasting</h2>
<blockquote>
<p>Authors: Cheng He, Xijie Liang, Zengrong Zheng, Patrick P. C. Lee, Xu Huang, Zhaoyi Li, Hong Xie, Defu Lian, Enhong Chen</p>
<p>2025-10-11</p>
<p><a href="http://arxiv.org/abs/2510.10145v1">http://arxiv.org/abs/2510.10145v1</a></p>
</blockquote>
<p>Current approaches for time series forecasting, whether in the time or
frequency domain, predominantly use deep learning models based on linear layers
or <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>s. They often encode time series data in a black-box manner and
rely on trial-and-error optimization solely based on forecasting performance,
leading to limited interpretability and theoretical understanding. Furthermore,
the dynamics in data distribution over time and frequency domains pose a
critical challenge to accurate forecasting. We propose FIRE, a unified
frequency domain decomposition framework that provides a mathematical
abstraction for diverse types of time series, so as to achieve interpretable
and robust time series forecasting. FIRE introduces several key innovations:
(i) independent modeling of amplitude and phase components, (ii) adaptive
learning of weights of frequency basis components, (iii) a targeted loss
function, and (iv) a novel training paradigm for <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> data. Extensive
experiments demonstrate that FIRE consistently outperforms state-of-the-art
models on long-term forecasting benchmarks, achieving superior predictive
performance and significantly enhancing interpretability of time series</p>
<h2 id="permllm-learnable-channel-permutation-for-nm-sparse-large-language-models">PermLLM Learnable Channel Permutation for NM Sparse Large Language Models</h2>
<blockquote>
<p>Authors: Lancheng Zou, Shuo Yin, Zehua Pei, Tsung-Yi Ho, Farzan Farnia, Bei Yu</p>
<p>2025-10-11</p>
<p><a href="http://arxiv.org/abs/2510.10136v1">http://arxiv.org/abs/2510.10136v1</a></p>
</blockquote>
<p>Channel permutation is a powerful technique for enhancing the accuracy of N:M
<a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> models by reordering the channels of weight matrices to prioritize the
retention of important weights. However, traditional channel permutation
methods rely on handcrafted quality metrics, which often fail to accurately
capture the true impact of <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> on model performance. To address this
limitation, we propose Perm<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>, a novel post-training <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> framework that
introduces learnable channel permutation (LCP) for N:M <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a>. LCP leverages
Sinkhorn normalization to transform discrete permutation matrices into
differentiable soft permutation matrices, enabling end-to-end optimization.
Additionally, Perm<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> incorporates an efficient block-wise channel permutation
strategy, which significantly reduces the number of learnable parameters and
computational complexity. Perm<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> seamlessly integrates with existing one-shot
<a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> methods to adaptively optimize channel permutations, effectively
mitigating <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a>-induced errors. Extensive experiments on the LLaMA series,
Qwen, and OPT models demonstrate that Perm<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> achieves superior performance in
optimizing N:M <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> models. The code is available at
https://github.com/lanchengzou/Perm<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>.</p>
<h2 id="cacheclip-accelerating-rag-with-effective-kv-cache-reuse">CacheClip Accelerating RAG with Effective KV Cache Reuse</h2>
<blockquote>
<p>Authors: Bin Yang, Qiuyu Leng, Jun Zeng, Zhenhua Wu</p>
<p>2025-10-11</p>
<p><a href="http://arxiv.org/abs/2510.10129v1">http://arxiv.org/abs/2510.10129v1</a></p>
</blockquote>
<p>Retrieval-Augmented Generation (RAG) systems suffer from severe
time-to-first-token (TTFT) bottlenecks due to long input sequences. Existing <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>
<a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> reuse methods face a fundamental trade-off: prefix caching requires
identical prefixes that rarely occur in RAG scenarios, while direct
precomputation sacrifices quality due to missing inter-chunk attention and
repeated attention sinks. Recent methods like APE and CacheBlend partially
address these issues but remain inadequate for robust RAG applications. This
paper presents CacheClip, a novel framework that achieves both fast TTFT and
high generation quality. Our key insight is that small auxiliary <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s exhibit
similar last-layer attention distributions to primary <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s (the target model
for generation), enabling efficient identification of tokens critical for
restoring inter-chunk attention, thereby significantly improving response
quality on cross-chunk reasoning tasks. CacheClip integrates three techniques:
(1) auxiliary-model-guided token selection for selective <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a>
recomputation, where the auxiliary model is finetuned to improve selection
accuracy, (2) shared prefixes to eliminate redundant attention sinks, and (3)
grouping strategy to maintain local coherence during partial <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> <a class="glightbox" href="https://img.shields.io/badge/cache-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/cache-F08080" /></a> updates.
Experiments show CacheClip retains up to 94.8% and 85.0% of full-attention
performance on NIAH and LongBench, outperforming APE and CacheBlend by 25.2%
and 35.1% on NIAH (with reomp% = 20%). Meanwhile, CacheClip accelerates <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>
inference by up to 1.92x in <a class="glightbox" href="https://img.shields.io/badge/prefill-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/prefill-F08080" /></a> time, providing a practical solution to the
efficiency-quality trade-off in RAG systems.</p>
<h2 id="lighter-x-an-efficient-and-plug-and-play-strategy-for-graph-based-recommendation-through-decoupled-propagation">Lighter-X An Efficient and Plug-and-play Strategy for Graph-based Recommendation through Decoupled Propagation</h2>
<blockquote>
<p>Authors: Yanping Zheng, Zhewei Wei, Frank de Hoog, Xu Chen, Hongteng Xu, Yuhang Ye, Jiadeng Huang</p>
<p>2025-10-11</p>
<p><a href="http://arxiv.org/abs/2510.10105v1">http://arxiv.org/abs/2510.10105v1</a></p>
</blockquote>
<p>Graph Neural Networks (GNNs) have demonstrated remarkable effectiveness in
recommendation systems. However, conventional graph-based recommenders, such as
LightGCN, require maintaining embeddings of size <script type="math/tex">d</script> for each node, resulting
in a parameter complexity of <script type="math/tex">\mathcal{O}(n \times d)</script>, where <script type="math/tex">n</script> represents
the total number of users and items. This scaling pattern poses significant
challenges for deployment on large-scale graphs encountered in real-world
applications. To address this scalability limitation, we propose
\textbf{Lighter-X}, an efficient and modular framework that can be seamlessly
integrated with existing GNN-based recommender architectures. Our approach
substantially reduces both parameter size and computational complexity while
pre<a class="glightbox" href="https://img.shields.io/badge/serving-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/serving-FF8C00" /></a> the theoretical guarantees and empirical performance of the base
models, thereby enabling practical deployment at scale. Specifically, we
analyze the original structure and inherent redundancy in their parameters,
identifying opportunities for optimization. Based on this insight, we propose
an efficient <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> scheme for the <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> adjacency structure and
high-dimensional embedding matrices, achieving a parameter complexity of
<script type="math/tex">\mathcal{O}(h \times d)</script>, where <script type="math/tex">h \ll n</script>. Furthermore, the model is optimized
through a decoupled framework, reducing computational complexity during the
training process and enhancing scalability. Extensive experiments demonstrate
that Lighter-X achieves comparable performance to baseline models with
significantly fewer parameters. In particular, on large-scale interaction
graphs with millions of edges, we are able to attain even better results with
only 1\% of the parameter over LightGCN.</p>
<h2 id="p-4dgs-predictive-4d-gaussian-splatting-with-90times-compression">P-4DGS Predictive 4D Gaussian Splatting with 90<script type="math/tex">\times</script> Compression</h2>
<blockquote>
<p>Authors: Henan Wang, Hanxin Zhu, Xinliang Gong, Tianyu He, Xin Li, Zhibo Chen</p>
<p>2025-10-11</p>
<p><a href="http://arxiv.org/abs/2510.10030v1">http://arxiv.org/abs/2510.10030v1</a></p>
</blockquote>
<p>3D Gaussian Splatting (3DGS) has garnered significant attention due to its
superior scene representation fidelity and real-time rendering performance,
especially for dynamic 3D scene reconstruction (\textit{i.e.}, 4D
reconstruction). However, despite achieving promising results, most existing
algorithms overlook the substantial temporal and spatial redundancies inherent
in dynamic scenes, leading to prohibitive memory consumption. To address this,
we propose P-4DGS, a novel dynamic 3DGS representation for compact 4D scene
modeling. Inspired by intra- and inter-frame prediction techniques commonly
used in video <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a>, we first design a 3D anchor point-based
spatial-temporal prediction module to fully exploit the spatial-temporal
correlations across different 3D Gaussian primitives. Subsequently, we employ
an adaptive <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a> strategy combined with context-based entropy coding to
further reduce the size of the 3D anchor points, thereby achieving enhanced
<a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> efficiency. To evaluate the rate-distortion performance of our
proposed P-4DGS in comparison with other dynamic 3DGS representations, we
conduct extensive experiments on both synthetic and real-world datasets.
Experimental results demonstrate that our approach achieves state-of-the-art
reconstruction quality and the fastest rendering speed, with a remarkably low
storage footprint (around \textbf{1MB} on average), achieving up to
\textbf{40<script type="math/tex">\times</script>} and \textbf{90<script type="math/tex">\times</script>} <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> on synthetic and
real-world scenes, respectively.</p>
<h2 id="efficient-onboard-vision-language-inference-in-uav-enabled-low-altitude-economy-networks-via-llm-enhanced-optimization">Efficient Onboard Vision-Language Inference in UAV-Enabled Low-Altitude Economy Networks via LLM-Enhanced Optimization</h2>
<blockquote>
<p>Authors: Yang Li, Ruichen Zhang, Yinqiu Liu, Guangyuan Liu, Dusit Niyato, Abbas Jamalipour, Xianbin Wang, Dong In Kim</p>
<p>2025-10-11</p>
<p><a href="http://arxiv.org/abs/2510.10028v1">http://arxiv.org/abs/2510.10028v1</a></p>
</blockquote>
<p>The rapid advancement of Low-Altitude Economy Networks (LAENets) has enabled
a variety of applications, including aerial surveillance, environmental
sensing, and semantic data collection. To support these scenarios, unmanned
aerial vehicles (UAVs) equipped with onboard vision-language models (VLMs)
offer a promising solution for real-time multimodal inference. However,
ensuring both inference accuracy and <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> efficiency remains a
significant challenge due to limited onboard resources and dynamic network
conditions. In this paper, we first propose a UAV-enabled LAENet system model
that jointly captures UAV mobility, user-UAV <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>, and the onboard
visual question answering (VQA) pipeline. Based on this model, we formulate a
mixed-integer non-convex optimization problem to minimize task latency and
power consumption under user-specific accuracy constraints. To solve the
problem, we design a hierarchical optimization framework composed of two parts:
(i) an Alternating Resolution and Power Optimization (ARPO) algorithm for
resource allocation under accuracy constraints, and (ii) a Large Language
Model-augmented Reinforcement Learning Approach (LLaRA) for adaptive UAV
trajectory optimization. The large language model (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>) serves as an expert in
refining reward design of reinforcement learning in an offline fashion,
introducing no additional latency in real-time decision-making. Numerical
results demonstrate the efficacy of our proposed framework in improving
inference performance and <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> efficiency under dynamic LAENet
conditions.</p>
<h2 id="deliberative-dynamics-and-value-alignment-in-llm-debates">Deliberative Dynamics and Value Alignment in LLM Debates</h2>
<blockquote>
<p>Authors: Pratik S. Sachdeva, Tom van Nuenen</p>
<p>2025-10-11</p>
<p><a href="http://arxiv.org/abs/2510.10002v1">http://arxiv.org/abs/2510.10002v1</a></p>
</blockquote>
<p>As large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) are increasingly deployed in sensitive
everyday contexts - offering personal advice, mental health support, and moral
guidance - understanding their elicited values in navigating complex moral
reasoning is essential. Most evaluations study this sociotechnical alignment
through single-turn prompts, but it is unclear if these findings extend to
multi-turn settings where values emerge through dialogue, revision, and
consensus. We address this gap using <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> debate to examine deliberative
dynamics and value alignment in multi-turn settings by prompting subsets of
three models (GPT-4.1, Claude 3.7 Sonnet, and Gemini 2.0 Flash) to collectively
assign blame in 1,000 everyday dilemmas from Reddit's "Am I the Asshole"
community. We use both synchronous (parallel responses) and round-robin
(sequential responses) formats to test order effects and verdict revision. Our
findings show striking behavioral differences. In the synchronous setting, GPT
showed strong inertia (0.6-3.1% revision rates) while Claude and Gemini were
far more flexible (28-41%). Value patterns also diverged: GPT emphasized
personal autonomy and direct <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>, while Claude and Gemini prioritized
empathetic dialogue. Certain values proved especially effective at driving
verdict changes. We further find that deliberation format had a strong impact
on model behavior: GPT and Gemini stood out as highly conforming relative to
Claude, with their verdict behavior strongly shaped by order effects. These
results show how deliberation format and model-specific behaviors shape moral
reasoning in multi-turn interactions, underscoring that sociotechnical
alignment depends on how systems structure dialogue as much as on their
outputs.</p>
<h2 id="universal-discrete-domain-speech-enhancement">Universal Discrete-Domain Speech Enhancement</h2>
<blockquote>
<p>Authors: Fei Liu, Yang Ai, Ye-Xin Lu, Rui-Chen Zheng, Hui-Peng Du, Zhen-Hua Ling</p>
<p>2025-10-11</p>
<p><a href="http://arxiv.org/abs/2510.09974v1">http://arxiv.org/abs/2510.09974v1</a></p>
</blockquote>
<p>In real-world scenarios, speech signals are inevitably corrupted by various
types of interference, making speech enhancement (SE) a critical task for
robust speech processing. However, most existing SE methods only handle a
limited range of distortions, such as additive noise, reverberation, or band
limitation, while the study of SE under multiple simultaneous distortions
remains limited. This gap affects the generalization and practical usability of
SE methods in real-world environments.To address this gap, this paper proposes
a novel Universal Discrete-domain SE model called UDSE.Unlike regression-based
SE models that directly predict clean speech waveform or continuous features,
UDSE redefines SE as a discrete-domain classification task, instead predicting
the clean discrete tokens <a class="glightbox" href="https://img.shields.io/badge/quantize-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantize-F08080" /></a>d by the residual vector <a class="glightbox" href="https://img.shields.io/badge/quantize-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantize-F08080" /></a>r (RVQ) of a
pre-trained neural speech codec.Specifically, UDSE first extracts global
features from the degraded speech. Guided by these global features, the clean
token prediction for each VQ follows the rules of RVQ, where the prediction of
each VQ relies on the results of the preceding ones. Finally, the predicted
clean tokens from all VQs are <a class="glightbox" href="https://img.shields.io/badge/decode-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decode-F08080" /></a>d to reconstruct the clean speech waveform.
During training, the UDSE model employs a teacher-forcing strategy, and is
optimized with cross-entropy loss. Experimental results confirm that the
proposed UDSE model can effectively enhance speech degraded by various
conventional and unconventional distortions, e.g., additive noise,
reverberation, band limitation, clipping, phase distortion, and <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a>
distortion, as well as their combinations. These results demonstrate the
superior universality and practicality of UDSE compared to advanced
regression-based SE methods.</p>
<h2 id="conformal-sparsification-for-bandwidth-efficient-edge-cloud-speculative-decoding">Conformal Sparsification for Bandwidth-Efficient Edge-Cloud Speculative Decoding</h2>
<blockquote>
<p>Authors: Payel Bhattacharjee, Fengwei Tian, Meiyu Zhong, Guangyi Zhang, Osvaldo Simeone, Ravi Tandon</p>
<p>2025-10-11</p>
<p><a href="http://arxiv.org/abs/2510.09942v1">http://arxiv.org/abs/2510.09942v1</a></p>
</blockquote>
<p>Edge-cloud speculative <a class="glightbox" href="https://img.shields.io/badge/decoding-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/decoding-F08080" /></a> (SD) accelerates inference by having a
cloud-based large language model (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>) that verifies draft tokens generated by
a resource-constrained small language model (SLM) at the edge. A central
bottleneck is the limited bandwidth of the edge-cloud link, which necessitates
efficient <a class="glightbox" href="https://img.shields.io/badge/compression-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/compression-FF8C00" /></a> of draft token distributions. We first derive an
information-theoretic bound that decomposes the token rejection rate into
contributions from SLM-<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> distribution mismatch and from <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a>
distortion. Guided by this analysis, we propose the Sparse Quantize-and-Sample
SD (SQS-SD) framework, which exploits distributional <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a> through
structured sparsification and lattice-based <a class="glightbox" href="https://img.shields.io/badge/quantization-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/quantization-F08080" /></a>. Within this
framework, K-SQS applies fixed top-K truncation, while C-SQS adaptively adjusts
the retained token set via online conformal prediction to ensure bounded
deviation from the dense distribution. Empirical results confirm that both
approaches improve end-to-end latency and rejection rates in complimentary
operating regimes.</p>
<h2 id="the-ethics-engine-a-modular-pipeline-for-accessible-psychometric-assessment-of-large-language-models">The Ethics Engine A Modular Pipeline for Accessible Psychometric Assessment of Large Language Models</h2>
<blockquote>
<p>Authors: Jake Van Clief, Constantine Kyritsopoulos</p>
<p>2025-10-11</p>
<p><a href="http://arxiv.org/abs/2510.11742v1">http://arxiv.org/abs/2510.11742v1</a></p>
</blockquote>
<p>As Large Language Models increasingly mediate human <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> and
decision-making, understanding their value expression becomes critical for
research across disciplines. This work presents the Ethics Engine, a modular
Python pipeline that transforms psychometric assessment of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s from a
technically complex endeavor into an accessible research tool. The pipeline
demonstrates how thoughtful infrastructure design can expand participation in
AI research, enabling investigators across cognitive science, political
psychology, education, and other fields to study value expression in language
models. Recent adoption by University of Edinburgh researchers studying
authoritarianism validates its research utility, processing over 10,000 AI
responses across multiple models and contexts. We argue that such tools
fundamentally change the landscape of AI research by lowering technical
barriers while maintaining scientific rigor. As <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s increasingly serve as
cognitive infrastructure, their embedded values shape millions of daily
interactions. Without systematic measurement of these value expressions, we
deploy systems whose moral influence remains uncharted. The Ethics Engine
enables the rigorous assessment necessary for informed governance of these
influential technologies.</p>
              
  <!-- Giscus 评论系统 - 只在 notes 文件夹下显示 -->
<script>
  // 使用 JavaScript 来判断 URL 路径
  if (window.location.pathname.includes('/notes/')) {
    document.write(`
      <div id="giscus-container" style="margin-top: 2rem; padding-top: 1rem; border-top: 1px solid #e1e4e8;">
        <script src="https://giscus.app/client.js"
                data-repo="hustzxd/EfficientPaper"
                data-repo-id="R_kgDOKVYtOg"
                data-category="General"
                data-category-id="DIC_kwDOKVYtOs4CukCv"
                data-mapping="pathname"
                data-strict="0"
                data-reactions-enabled="1"
                data-emit-metadata="0"
                data-input-position="bottom"
                data-theme="light"
                data-lang="zh-CN"
                crossorigin="anonymous"
                async>
        <\/script>
      </div>
    `);
  }
</script>

            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../2025-10-09/" class="btn btn-neutral float-left" title="2025-10-09"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../lagency/2025-07-25/" class="btn btn-neutral float-right" title="2025-07-25">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../2025-10-09/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../lagency/2025-07-25/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-clike.min.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
      <script src="../../js/prism-prototxt.js"></script>
      <script src="../../js/preview.js"></script>
      <script src="../../js/back-to-top.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
</script></body>
</html>
