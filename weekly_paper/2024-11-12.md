# 2024-11-12

# Table of Contents
* [TreeCoders Trees of Transformers](#TreeCoders-Trees-of-Transformers)
* [The Super Weight in Large Language Models](#The-Super-Weight-in-Large-Language-Models)
* [Fast and Robust Contextual Node Representation Learning over Dynamic Graphs](#Fast-and-Robust-Contextual-Node-Representation-Learning-over-Dynamic-Graphs)
* [SCAR Sparse Conditioned Autoencoders for Concept Detection and Steering in LLMs](#SCAR-Sparse-Conditioned-Autoencoders-for-Concept-Detection-and-Steering-in-LLMs)
* [Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training](#Zeroth-Order-Adaptive-Neuron-Alignment-Based-Pruning-without-Re-Training)
* [A Hierarchical Compression Technique for 3D Gaussian Splatting Compression](#A-Hierarchical-Compression-Technique-for-3D-Gaussian-Splatting-Compression)
* [MapSAM Adapting Segment Anything Model for Automated Feature Detection in Historical Maps](#MapSAM-Adapting-Segment-Anything-Model-for-Automated-Feature-Detection-in-Historical-Maps)
* [SPARTAN A Sparse Transformer Learning Local Causation](#SPARTAN-A-Sparse-Transformer-Learning-Local-Causation)
* [Spatially Constrained Transformer with Efficient Global Relation Modelling for Spatio-Temporal Prediction](#Spatially-Constrained-Transformer-with-Efficient-Global-Relation-Modelling-for-Spatio-Temporal-Prediction)
* [HarmLevelBench Evaluating Harm-Level Compliance and the Impact of Quantization on Model Alignment](#HarmLevelBench-Evaluating-Harm-Level-Compliance-and-the-Impact-of-Quantization-on-Model-Alignment)
* [White-Box Diffusion Transformer for single-cell RNA-seq generation](#White-Box-Diffusion-Transformer-for-single-cell-RNA-seq-generation)
* [The First Prompt Counts the Most! An Evaluation of Large Language Models on Iterative Example-based Code Generation](#The-First-Prompt-Counts-the-Most!-An-Evaluation-of-Large-Language-Models-on-Iterative-Example-based-Code-Generation)
* [Towards the Proximity Conjecture on Group-Labeled Matroids](#Towards-the-Proximity-Conjecture-on-Group-Labeled-Matroids)
* [Quantum Homotopy Analysis Method with Secondary Linearization for Nonlinear Partial Differential Equations](#Quantum-Homotopy-Analysis-Method-with-Secondary-Linearization-for-Nonlinear-Partial-Differential-Equations)
* [Anchor Attention, Small Cache Code Generation with Large Language Models](#Anchor-Attention,-Small-Cache-Code-Generation-with-Large-Language-Models)
* [Few-shot Semantic Learning for Robust Multi-Biome 3D Semantic Mapping in Off-Road Environments](#Few-shot-Semantic-Learning-for-Robust-Multi-Biome-3D-Semantic-Mapping-in-Off-Road-Environments)
* [Randomized Black-Box PIT for Small Depth +-Regular Non-commutative Circuits](#Randomized-Black-Box-PIT-for-Small-Depth-+-Regular-Non-commutative-Circuits)
* [Time-delayed Dynamic Mode Decomposition for families of periodic trajectories in Cislunar Space](#Time-delayed-Dynamic-Mode-Decomposition-for-families-of-periodic-trajectories-in-Cislunar-Space)
* [Generalized Principal Component Analysis for Large-dimensional Matrix Factor Model](#Generalized-Principal-Component-Analysis-for-Large-dimensional-Matrix-Factor-Model)
* [A Hybrid Approach for COVID-19 Detection Combining Wasserstein GAN with Transfer Learning](#A-Hybrid-Approach-for-COVID-19-Detection-Combining-Wasserstein-GAN-with-Transfer-Learning)
* [SplatFormer Point Transformer for Robust 3D Gaussian Splatting](#SplatFormer-Point-Transformer-for-Robust-3D-Gaussian-Splatting)
* [Monitoring-feedback induced entanglement relaxations in a tilted free fermionic chain](#Monitoring-feedback-induced-entanglement-relaxations-in-a-tilted-free-fermionic-chain)
* [Optimizing Large Language Models through Quantization A Comparative Analysis of PTQ and QAT Techniques](#Optimizing-Large-Language-Models-through-Quantization-A-Comparative-Analysis-of-PTQ-and-QAT-Techniques)
* [Validation of an LLM-based Multi-Agent Framework for Protein Engineering in Dry Lab and Wet Lab](#Validation-of-an-LLM-based-Multi-Agent-Framework-for-Protein-Engineering-in-Dry-Lab-and-Wet-Lab)
* [TourSynbio-Search A Large Language Model Driven Agent Framework for Unified Search Method for Protein Engineering](#TourSynbio-Search-A-Large-Language-Model-Driven-Agent-Framework-for-Unified-Search-Method-for-Protein-Engineering)
* [Recycled Attention Efficient inference for long-context language models](#Recycled-Attention-Efficient-inference-for-long-context-language-models)
* [Autoregressive Adaptive Hypergraph Transformer for Skeleton-based Activity Recognition](#Autoregressive-Adaptive-Hypergraph-Transformer-for-Skeleton-based-Activity-Recognition)
* [SSSD Simply-Scalable Speculative Decoding](#SSSD-Simply-Scalable-Speculative-Decoding)
* [AcceLLM Accelerating LLM Inference using Redundancy for Load Balancing and Data Locality](#AcceLLM-Accelerating-LLM-Inference-using-Redundancy-for-Load-Balancing-and-Data-Locality)
* [FGGP Fixed-Rate Gradient-First Gradual Pruning](#FGGP-Fixed-Rate-Gradient-First-Gradual-Pruning)
* [SASWISE-UE Segmentation and Synthesis with Interpretable Scalable Ensembles for Uncertainty Estimation](#SASWISE-UE-Segmentation-and-Synthesis-with-Interpretable-Scalable-Ensembles-for-Uncertainty-Estimation)
* [SpecHub Provable Acceleration to Multi-Draft Speculative Decoding](#SpecHub-Provable-Acceleration-to-Multi-Draft-Speculative-Decoding)


## TreeCoders Trees of Transformers

>Authors: Pierre Colonna D'Istria, Abdulrahman Altahhan

>2024-11-11

> http://arxiv.org/abs/2411.07218v1

In this paper, we introduce TreeCoders, a novel family of transformer trees.
We moved away from traditional linear transformers to complete k-ary trees.
Transformer blocks serve as nodes, and generic classifiers learn to select the
best child and route the sequence of tokens to a specific leaf. The selectors,
moved outside the transformer blocks, allow for the use of a variety of
architecture without further modifications. Furthermore, our proposed
architecture supports **sparse** node activation due to the logarithmic complexity
of a tree search. We validate our idea by testing a series of decoder-only tree
transformers, achieving competitive results across a diverse range of language
datasets. Our study demonstrates that the proposed tree transformer model
outperforms a size-equivalent linear transformer model 76\% of the time over a
wide range of tree architectures. Furthermore, our proposed model naturally
lends itself to distributed implementation.


## The Super Weight in Large Language Models

>Authors: Mengxia Yu, De Wang, Qi Shan, Colorado Reed, Alvin Wan

>2024-11-11

> http://arxiv.org/abs/2411.07191v1

Recent works have shown a surprising result: a small fraction of Large
Language Model (LLM) parameter outliers are disproportionately important to the
quality of the model. LLMs contain billions of parameters, so these small
fractions, such as 0.01%, translate to hundreds of thousands of parameters. In
this work, we present an even more surprising finding: Pruning as few as a
single parameter can destroy an LLM's ability to generate text -- increasing
perplexity by 3 orders of magnitude and reducing zero-shot accuracy to
guessing. We propose a data-free method for identifying such parameters, termed
super weights, using a single forward pass through the model. We additionally
find that these super weights induce correspondingly rare and large activation
outliers, termed super activations. When preserved with high precision, super
activations can improve simple round-to-nearest **quantization** to become
competitive with state-of-the-art methods. For weight **quantization**, we
similarly find that by preserving the super weight and clipping other weight
outliers, round-to-nearest **quantization** can scale to much larger block sizes
than previously considered. To facilitate further research into super weights,
we provide an index of super weight coordinates for common, openly available
LLMs.


## Fast and Robust Contextual Node Representation Learning over Dynamic Graphs

>Authors: Xingzhi Guo, Silong Wang, Baojian Zhou, Yanghua Xiao, Steven Skiena

>2024-11-11

> http://arxiv.org/abs/2411.07123v1

Real-world graphs grow rapidly with edge and vertex insertions over time,
motivating the problem of efficiently maintaining robust node representation
over evolving graphs. Recent efficient GNNs are designed to decouple recursive
message passing from the learning process, and favor Personalized PageRank
(PPR) as the underlying feature propagation mechanism. However, most PPR-based
GNNs are designed for static graphs, and efficient PPR maintenance remains as
an open problem. Further, there is surprisingly little theoretical
justification for the choice of PPR, despite its impressive empirical
performance.
  In this paper, we are inspired by the recent PPR formulation as an explicit
$\ell_1$-regularized optimization problem and propose a unified dynamic graph
learning framework based on **sparse** node-wise attention. We also present a set
of desired properties to justify the choice of PPR in STOA GNNs, and serves as
the guideline for future node attention designs. Meanwhile, we take advantage
of the PPR-equivalent optimization formulation and employ the proximal gradient
method (ISTA) to improve the efficiency of PPR-based GNNs upto 6 times.
Finally, we instantiate a simple-yet-effective model (\textsc{GoPPE}) with
robust positional encodings by maximizing PPR previously used as attention. The
model performs comparably to or better than the STOA baselines and greatly
outperforms when the initial node attributes are noisy during graph evolution,
demonstrating the effectiveness and robustness of \textsc{GoPPE}.


## SCAR Sparse Conditioned Autoencoders for Concept Detection and Steering in LLMs

>Authors: Ruben Härle, Felix Friedrich, Manuel Brack, Björn Deiseroth, Patrick Schramowski, Kristian Kersting

>2024-11-11

> http://arxiv.org/abs/2411.07122v1

Large Language Models (LLMs) have demonstrated remarkable capabilities in
generating human-like text, but their output may not be aligned with the user
or even produce harmful content. This paper presents a novel approach to detect
and steer concepts such as toxicity before generation. We introduce the Sparse
Conditioned Autoencoder (SCAR), a single trained module that extends the
otherwise untouched LLM. SCAR ensures full steerability, towards and away from
concepts (e.g., toxic content), without compromising the quality of the model's
text generation on standard evaluation benchmarks. We demonstrate the effective
application of our approach through a variety of concepts, including toxicity,
safety, and writing style alignment. As such, this work establishes a robust
framework for controlling LLM generations, ensuring their ethical and safe
deployment in real-world applications.


## Zeroth-Order Adaptive Neuron Alignment Based Pruning without Re-Training

>Authors: Elia Cunegatti, Leonardo Lucio Custode, Giovanni Iacca

>2024-11-11

> http://arxiv.org/abs/2411.07066v1

Network **pruning** is a set of computational techniques that aim to reduce a
given model's computational cost by removing a subset of its parameters while
having minimal impact on performance. Throughout the last decade, the most
widely used **pruning** paradigm has focused on **pruning** and re-training, which
nowadays is inconvenient due to the vast amount of pre-trained models, which
are in any case too expensive to re-train. In this paper, we exploit functional
information from dense pre-trained models, i.e., their activations, to obtain
**sparse** models that maximize the activations' alignment w.r.t. their
corresponding dense models. Hence, we propose \textsc{NeuroAl}, a \emph{top-up}
algorithm that can be used on top of any given **pruning** algorithm for LLMs, that
modifies the block-wise and row-wise **sparsity** ratios to maximize the
\emph{neuron alignment} among activations. Moreover, differently from existing
methods, our approach adaptively selects the best parameters for the block-wise
and row-wise **sparsity** ratios w.r.t. to the model and the desired **sparsity**
(given as input), and requires \emph{no re-training}. We test our method on 4
different LLM families and 3 different **sparsity** ratios, showing how it
consistently outperforms the latest state-of-the-art techniques. The code is
available at https://github.com/eliacunegatti/NeuroAL.


## A Hierarchical Compression Technique for 3D Gaussian Splatting Compression

>Authors: He Huang, Wenjie Huang, Qi Yang, Yiling Xu, Zhu li

>2024-11-11

> http://arxiv.org/abs/2411.06976v1

3D Gaussian Splatting (GS) demonstrates excellent rendering quality and
generation speed in novel view synthesis. However, substantial data size poses
challenges for storage and transmission, making 3D GS compression an essential
technology. Current 3D GS compression research primarily focuses on developing
more compact scene representations, such as converting explicit 3D GS data into
implicit forms. In contrast, compression of the GS data itself has hardly been
explored. To address this gap, we propose a Hierarchical GS Compression (HGSC)
technique. Initially, we prune unimportant Gaussians based on importance scores
derived from both global and local significance, effectively reducing
redundancy while maintaining visual quality. An Octree structure is used to
compress 3D positions. Based on the 3D GS Octree, we implement a hierarchical
attribute compression strategy by employing a KD-tree to partition the 3D GS
into multiple blocks. We apply farthest point sampling to select anchor
primitives within each block and others as non-anchor primitives with varying
Levels of Details (LoDs). Anchor primitives serve as reference points for
predicting non-anchor primitives across different LoDs to reduce spatial
redundancy. For anchor primitives, we use the region adaptive hierarchical
transform to achieve near-lossless compression of various attributes. For
non-anchor primitives, each is predicted based on the k-nearest anchor
primitives. To further minimize prediction errors, the reconstructed LoD and
anchor primitives are combined to form new anchor primitives to predict the
next LoD. Our method notably achieves superior compression quality and a
significant data size reduction of over 4.5 times compared to the
state-of-the-art compression method on small scenes datasets.


## MapSAM Adapting Segment Anything Model for Automated Feature Detection in Historical Maps

>Authors: Xue Xia, Daiwei Zhang, Wenxuan Song, Wei Huang, Lorenz Hurni

>2024-11-11

> http://arxiv.org/abs/2411.06971v1

Automated feature detection in historical maps can significantly accelerate
the reconstruction of the geospatial past. However, this process is often
constrained by the time-consuming task of manually digitizing sufficient
high-quality training data. The emergence of visual foundation models, such as
the Segment Anything Model (SAM), offers a promising solution due to their
remarkable generalization capabilities and rapid adaptation to new data
distributions. Despite this, directly applying SAM in a zero-shot manner to
historical map segmentation poses significant challenges, including poor
recognition of certain geospatial features and a reliance on input prompts,
which limits its ability to be fully automated. To address these challenges, we
introduce MapSAM, a parameter-efficient fine-tuning strategy that adapts SAM
into a prompt-free and versatile solution for various downstream historical map
segmentation tasks. Specifically, we employ Weight-Decomposed Low-Rank
Adaptation (DoRA) to integrate domain-specific knowledge into the image
encoder. Additionally, we develop an automatic prompt generation process,
eliminating the need for manual input. We further enhance the positional prompt
in SAM, transforming it into a higher-level positional-semantic prompt, and
modify the cross-attention mechanism in the mask decoder with masked attention
for more effective feature aggregation. The proposed MapSAM framework
demonstrates promising performance across two distinct historical map
segmentation tasks: one focused on linear features and the other on areal
features. Experimental results show that it adapts well to various features,
even when fine-tuned with extremely limited data (e.g. 10 shots).


## SPARTAN A Sparse Transformer Learning Local Causation

>Authors: Anson Lei, Ingmar Posner, Bernhard Schölkopf

>2024-11-11

> http://arxiv.org/abs/2411.06890v1

Causal structures play a central role in world models that flexibly adapt to
changes in the environment. While recent works motivate the benefits of
discovering local causal graphs for dynamics modelling, in this work we
demonstrate that accurately capturing these relationships in complex settings
remains challenging for the current state-of-the-art. To remedy this
shortcoming, we postulate that **sparsity** is a critical ingredient for the
discovery of such local causal structures. To this end we present the SPARse
TrANsformer World model (SPARTAN), a Transformer-based world model that learns
local causal structures between entities in a scene. By applying **sparsity**
regularisation on the attention pattern between object-factored tokens, SPARTAN
identifies **sparse** local causal models that accurately predict future object
states. Furthermore, we extend our model to capture **sparse** interventions with
unknown targets on the dynamics of the environment. This results in a highly
interpretable world model that can efficiently adapt to changes. Empirically,
we evaluate SPARTAN against the current state-of-the-art in object-centric
world models on observation-based environments and demonstrate that our model
can learn accurate local causal graphs and achieve significantly improved
few-shot adaptation to changes in the dynamics of the environment as well as
robustness against removing irrelevant distractors.


## Spatially Constrained Transformer with Efficient Global Relation Modelling for Spatio-Temporal Prediction

>Authors: Ashutosh Sao, Simon Gottschalk

>2024-11-11

> http://arxiv.org/abs/2411.06836v1

Accurate spatio-temporal prediction is crucial for the sustainable
development of smart cities. However, current approaches often struggle to
capture important spatio-temporal relationships, particularly overlooking
global relations among distant city regions. Most existing techniques
predominantly rely on Convolutional Neural Networks (CNNs) to capture global
relations. However, CNNs exhibit neighbourhood bias, making them insufficient
for capturing distant relations. To address this limitation, we propose
ST-SampleNet, a novel transformer-based architecture that combines CNNs with
self-attention mechanisms to capture both local and global relations
effectively. Moreover, as the number of regions increases, the quadratic
complexity of self-attention becomes a challenge. To tackle this issue, we
introduce a lightweight region sampling strategy that prunes non-essential
regions and enhances the efficiency of our approach. Furthermore, we introduce
a spatially constrained position embedding that incorporates spatial
neighbourhood information into the self-attention mechanism, aiding in semantic
interpretation and improving the performance of ST-SampleNet. Our experimental
evaluation on three real-world datasets demonstrates the effectiveness of
ST-SampleNet. Additionally, our efficient variant achieves a 40% reduction in
computational costs with only a marginal compromise in performance,
approximately 1%.


## HarmLevelBench Evaluating Harm-Level Compliance and the Impact of Quantization on Model Alignment

>Authors: Yannis Belkhiter, Giulio Zizzo, Sergio Maffeis

>2024-11-11

> http://arxiv.org/abs/2411.06835v1

With the introduction of the transformers architecture, LLMs have
revolutionized the NLP field with ever more powerful models. Nevertheless,
their development came up with several challenges. The exponential growth in
computational power and reasoning capabilities of language models has
heightened concerns about their security. As models become more powerful,
ensuring their safety has become a crucial focus in research. This paper aims
to address gaps in the current literature on jailbreaking techniques and the
evaluation of LLM vulnerabilities. Our contributions include the creation of a
novel dataset designed to assess the harmfulness of model outputs across
multiple harm levels, as well as a focus on fine-grained harm-level analysis.
Using this framework, we provide a comprehensive benchmark of state-of-the-art
jailbreaking attacks, specifically targeting the Vicuna 13B v1.5 model.
Additionally, we examine how **quantization** techniques, such as AWQ and GPTQ,
influence the alignment and robustness of models, revealing trade-offs between
enhanced robustness with regards to transfer attacks and potential increases in
vulnerability on direct ones. This study aims to demonstrate the influence of
harmful input queries on the complexity of jailbreaking techniques, as well as
to deepen our understanding of LLM vulnerabilities and improve methods for
assessing model robustness when confronted with harmful content, particularly
in the context of compression strategies.


## White-Box Diffusion Transformer for single-cell RNA-seq generation

>Authors: Zhuorui Cui, Shengze Dong, Ding Liu

>2024-11-11

> http://arxiv.org/abs/2411.06785v1

As a powerful tool for characterizing cellular subpopulations and cellular
heterogeneity, single cell RNA sequencing (scRNA-seq) technology offers
advantages of high throughput and multidimensional analysis. However, the
process of data acquisition is often constrained by high cost and limited
sample availability. To overcome these limitations, we propose a hybrid model
based on Diffusion model and White-Box transformer that aims to generate
synthetic and biologically plausible scRNA-seq data. Diffusion model
progressively introduce noise into the data and then recover the original data
through a denoising process, a forward and reverse process that is particularly
suitable for generating complex data distributions. White-Box transformer is a
deep learning architecture that emphasizes mathematical interpretability. By
minimizing the encoding rate of the data and maximizing the **sparsity** of the
representation, it not only reduces the computational burden, but also provides
clear insight into underlying structure. Our White-Box Diffusion Transformer
combines the generative capabilities of Diffusion model with the mathematical
interpretability of White-Box transformer. Through experiments using six
different single-cell RNA-Seq datasets, we visualize both generated and real
data using t-SNE dimensionality reduction technique, as well as quantify
similarity between generated and real data using various metrics to demonstrate
comparable performance of White-Box Diffusion Transformer and Diffusion
Transformer in generating scRNA-seq data alongside significant improvements in
training efficiency and resource utilization. Our code is available at
https://github.com/lingximamo/White-Box-Diffusion-Transformer


## The First Prompt Counts the Most! An Evaluation of Large Language Models on Iterative Example-based Code Generation

>Authors: Yingjie Fu, Bozhou Li, Linyi Li, Wentao Zhang, Tao Xie

>2024-11-11

> http://arxiv.org/abs/2411.06774v1

The capabilities of Large Language Models (LLMs) in code generation,
particularly for implementing target functionalities from natural language
descriptions, have been extensively studied. As an alternative form of natural
language, input-output examples (I/O examples) provide an accessible,
unambiguous, and flexible way to describe functionalities, but the diversity,
**sparse**ness, and incompleteness of I/O examples also place challenges on
understanding and implementing requirements. Therefore, generating code from
input-output examples (i.e., example-based code generation) provides a new
perspective, allowing us to evaluate LLMs' capability to infer target
functionalities from limited information and to process new-form requirements.
However, related research about LLMs in example-based code generation remains
largely unexplored. To fill this gap, this paper presents the first
comprehensive study on example-based code generation using LLMs. To address the
incorrectness caused by the incompleteness of I/O examples, we adopt an
iterative evaluation framework and formalize the objective of example-based
code generation as two sequential sub-objectives: generating code conforming to
given examples and generating code that successfully implements the target
functionalities from (iteratively) given examples. We assess six
state-of-the-art LLMs using a new benchmark of 168 diverse target
functionalities. The results demonstrate that when requirements were described
using iterative I/O examples rather than natural language, the LLMs' score
decreased by over 60%, indicating that example-based code generation remains
challenging for the evaluated LLMs. More interestingly, the vast majority (even
over 95%) of successfully implemented functionalities are achieved in the first
round of iterations, suggesting that the LLMs struggle to effectively utilize
the iteratively supplemented requirements.


## Towards the Proximity Conjecture on Group-Labeled Matroids

>Authors: Dániel Garamvölgyi, Ryuhei Mizutani, Taihei Oki, Tamás Schwarcz, Yutaro Yamaguchi

>2024-11-11

> http://arxiv.org/abs/2411.06771v1

Consider a matroid $M$ whose ground set is equipped with a labeling to an
abelian group. A basis of $M$ is called $F$-avoiding if the sum of the labels
of its elements is not in a forbidden label set $F$. H\"orsch, Imolay,
Mizutani, Oki, and Schwarcz (2024) conjectured that if an $F$-avoiding basis
exists, then any basis can be transformed into an $F$-avoiding basis by
exchanging at most $|F|$ elements. This proximity conjecture is known to hold
for certain specific groups; in the case where $|F| \le 2$; or when the matroid
is subsequence-interchangeably base orderable (SIBO), which is a weakening of
the so-called strongly base orderable (SBO) property.
  In this paper, we settle the proximity conjecture for **sparse** paving matroids
or in the case where $|F| \le 4$. Related to the latter result, we present the
first known example of a non-SIBO matroid. We further address the setting of
multiple group-label constraints, showing proximity results for the cases of
two labelings, SIBO matroids, matroids representable over a fixed, finite
field, and **sparse** paving matroids.


## Quantum Homotopy Analysis Method with Secondary Linearization for Nonlinear Partial Differential Equations

>Authors: Cheng Xue, Xiao-Fan Xu, Xi-Ning Zhuang, Tai-Ping Sun, Yun-Jie Wang, Ming-Yang Tan, Chuang-Chao Ye, Huan-Yu Liu, Yu-Chun Wu, Zhao-Yun Chen, Guo-Ping Guo

>2024-11-11

> http://arxiv.org/abs/2411.06759v1

Nonlinear partial differential equations (PDEs) are crucial for modeling
complex fluid dynamics and are foundational to many computational fluid
dynamics (CFD) applications. However, solving these nonlinear PDEs is
challenging due to the vast computational resources they demand, highlighting
the pressing need for more efficient computational methods. Quantum computing
offers a promising but technically challenging approach to solving nonlinear
PDEs. Recently, Liao proposed a framework that leverages quantum computing to
accelerate the solution of nonlinear PDEs based on the homotopy analysis method
(HAM), a semi-analytical technique that transforms nonlinear PDEs into a series
of linear PDEs. However, the no-cloning theorem in quantum computing poses a
major limitation, where directly applying quantum simulation to each HAM step
results in exponential complexity growth with the HAM truncation order. This
study introduces a "secondary linearization" approach that maps the whole HAM
process into a system of linear PDEs, allowing for a one-time solution using
established quantum PDE solvers. Our method preserves the exponential speedup
of quantum linear PDE solvers while ensuring that computational complexity
increases only polynomially with the HAM truncation order. We demonstrate the
efficacy of our approach by applying it to the Burgers' equation and the
Korteweg-de Vries (KdV) equation. Our approach provides a novel pathway for
transforming nonlinear PDEs into linear PDEs, with potential applications to
fluid dynamics. This work thus lays the foundation for developing quantum
algorithms capable of solving the Navier-Stokes equations, ultimately offering
a promising route to accelerate their solutions using quantum computing.


## Anchor Attention, Small Cache Code Generation with Large Language Models

>Authors: Xiangyu Zhang, Yu Zhou, Guang Yang, Harald C. Gall, Taolue Chen

>2024-11-11

> http://arxiv.org/abs/2411.06680v1

The development of large language models (LLMs) has revolutionized automated
code generation. However, their high demand of computation resources has
hindered a broader deployment and raised environmental concerns. A common
strategy for diminishing computational demands is to cache Key-Value (**KV**)
states from the attention mechanism which is adopted predominately by
mainstream LLMs. It can mitigate the need of repeated attention computations,
but brings significant memory overhead. Current practices in NLP often use
**sparse** attention which may, unfortunately, lead to substantial inaccuracies, or
hallucinations, in code generation tasks. In this paper, we analyze the
attention weights distribution within code generation models via an empirical
study, uncovering a **sparsity** pattern, i.e., the aggregation of information at
specific anchor points. Based on this observation, we propose a novel approach,
AnchorCoder, which features token-wise anchor attention designed to extract and
compress the contextual information, and layer-wise anchor attention enabling
cross-layer communication to mitigate the issue of excessive superposition
caused by the compression. The extensive experiments across multiple benchmark
datasets confirm the effectiveness of AnchorCoder, which can consistently
achieve a significant (at least 70%) reduction in **KV** cache requirements, while
preserving the majority of model's performance.


## Few-shot Semantic Learning for Robust Multi-Biome 3D Semantic Mapping in Off-Road Environments

>Authors: Deegan Atha, Xianmei Lei, Shehryar Khattak, Anna Sabel, Elle Miller, Aurelio Noca, Grace Lim, Jeffrey Edlund, Curtis Padgett, Patrick Spieler

>2024-11-10

> http://arxiv.org/abs/2411.06632v1

Off-road environments pose significant perception challenges for high-speed
autonomous navigation due to unstructured terrain, degraded sensing conditions,
and domain-shifts among biomes. Learning semantic information across these
conditions and biomes can be challenging when a large amount of ground truth
data is required. In this work, we propose an approach that leverages a
pre-trained Vision Transformer (ViT) with fine-tuning on a small (<500 images),
**sparse** and coarsely labeled (<30% pixels) multi-biome dataset to predict 2D
semantic segmentation classes. These classes are fused over time via a novel
range-based metric and aggregated into a 3D semantic voxel map. We demonstrate
zero-shot out-of-biome 2D semantic segmentation on the Yamaha (52.9 mIoU) and
Rellis (55.5 mIoU) datasets along with few-shot coarse **sparse** labeling with
existing data for improved segmentation performance on Yamaha (66.6 mIoU) and
Rellis (67.2 mIoU). We further illustrate the feasibility of using a voxel map
with a range-based semantic fusion approach to handle common off-road hazards
like pop-up hazards, overhangs, and water features.


## Randomized Black-Box PIT for Small Depth +-Regular Non-commutative Circuits

>Authors: G V Sumukha Bharadwaj, S Raja

>2024-11-10

> http://arxiv.org/abs/2411.06569v1

In this paper, we address the black-box polynomial identity testing (PIT)
problem for non-commutative polynomials computed by $+$-regular circuits, a
class of homogeneous circuits introduced by Arvind, Joglekar, Mukhopadhyay, and
Raja (STOC 2017, Theory of Computing 2019). These circuits can compute
polynomials with a number of monomials that are doubly exponential in the
circuit size. They gave an efficient randomized PIT algorithm for +-regular
circuits of depth 3 and posed the problem of developing an efficient black-box
PIT for higher depths as an open problem.
  We present a randomized black-box polynomial-time algorithm for +-regular
circuits of any constant depth. Specifically, our algorithm runs in
$s^{O(d^2)}$ time, where $s$ and $d$ represent the size and the depth of the
$+$-regular circuit, respectively. Our approach combines several key techniques
in a novel way. We employ a nondeterministic substitution automaton that
transforms the polynomial into a structured form and utilizes polynomial
sparsification along with commutative transformations to maintain non-zeroness.
Additionally, we introduce matrix composition, coefficient modification via the
automaton, and multi-entry outputs--methods that have not previously been
applied in the context of black-box PIT. Together, these techniques enable us
to effectively handle exponential degrees and doubly exponential **sparsity** in
non-commutative settings, enabling polynomial identity testing for higher-depth
circuits. Our work resolves an open problem from \cite{AJMR19}.
  In particular, we show that if $f$ is a non-zero non-commutative polynomial
in $n$ variables over the field $F$, computed by a depth-$d$ $+$-regular
circuit of size $s$, then $f$ cannot be a polynomial identity for the matrix
algebra $\mathbb{M}_{N}(F)$, where $N= s^{O(d^2)}$ and the size of the field
$F$ depending on the degree of $f$.


## Time-delayed Dynamic Mode Decomposition for families of periodic trajectories in Cislunar Space

>Authors: Sriram Narayanan, Mohamed Naveed Gul Mohamed, Indranil Nayak, Suman Chakravorty, Mrinal Kumar

>2024-11-10

> http://arxiv.org/abs/2411.06511v1

In recent years, the development of the Lunar Gateway and Artemis missions
has renewed interest in lunar exploration, including both manned and unmanned
missions. This interest necessitates accurate initial orbit determination (IOD)
and orbit prediction (OP) in this domain, which faces significant challenges
such as severe nonlinearity, sensitivity to initial conditions, large
state-space volume, and **sparse**, faint, and unreliable measurements. This paper
explores the capability of data-driven Koopman operator-based approximations
for OP in these scenarios. Three stable periodic trajectories from distinct
cislunar families are analyzed. The analysis includes theoretical justification
for using a linear time-invariant system as the data-driven surrogate. This
theoretical framework is supported by experimental validation. Furthermore, the
accuracy is assessed by comparing the spectral content captured to period
estimates derived from the fast Fourier transform (FFT) and Poincare-like
sections.


## Generalized Principal Component Analysis for Large-dimensional Matrix Factor Model

>Authors: Yong He, Yujie Hou, Haixia Liu, Yalin Wang

>2024-11-10

> http://arxiv.org/abs/2411.06423v1

Matrix factor models have been growing popular dimension reduction tools for
large-dimensional matrix time series. However, the heteroscedasticity of the
idiosyncratic components has barely received any attention. Starting from the
pseudo likelihood function, this paper introduces a Generalized Principal
Component Analysis (GPCA) method for matrix factor model which takes the
heteroscedasticity into account. Theoretically, we first derive the asymptotic
distribution of the GPCA estimators by assuming the separable covariance
matrices are known in advance. We then propose adaptive thresholding estimators
for the separable covariance matrices and show that this would not alter the
asymptotic distribution of the GPCA estimators under certain regular **sparsity**
conditions in the high-dimensional covariance matrix estimation literature. The
GPCA estimators are shown to be more efficient than the state-of-the-art
methods under certain heteroscedasticity conditions. Thorough numerical studies
are conducted to demonstrate the superiority of our method over the existing
approaches. Analysis of a financial portfolio dataset illustrates the empirical
usefulness of the proposed method.


## A Hybrid Approach for COVID-19 Detection Combining Wasserstein GAN with Transfer Learning

>Authors: Sumera Rounaq, Shahid Munir Shah, Mahmoud Aljawarneh, Sarah Khan, Ghulam Muhammad

>2024-11-10

> http://arxiv.org/abs/2411.06397v1

COVID-19 is extremely contagious and its rapid growth has drawn attention
towards its early diagnosis. Early diagnosis of COVID-19 enables healthcare
professionals and government authorities to break the chain of transition and
flatten the epidemic curve. With the number of cases accelerating across the
developed world, COVID-19 induced Viral Pneumonia cases is a big challenge.
Overlapping of COVID-19 cases with Viral Pneumonia and other lung infections
with limited dataset and long training hours is a serious problem to cater.
Limited amount of data often results in over-fitting models and due to this
reason, model does not predict generalized results. To fill this gap, we
proposed GAN-based approach to synthesize images which later fed into the deep
learning models to classify images of COVID-19, Normal, and Viral Pneumonia.
Specifically, customized Wasserstein GAN is proposed to generate 19% more Chest
X-ray images as compare to the real images. This expanded dataset is then used
to train four proposed deep learning models: VGG-16, ResNet-50, GoogLeNet and
MNAST. The result showed that expanded dataset utilized deep learning models to
deliver high classification accuracies. In particular, VGG-16 achieved highest
accuracy of 99.17% among all four proposed schemes. Rest of the models like
ResNet-50, GoogLeNet and MNAST delivered 93.9%, 94.49% and 97.75% testing
accuracies respectively. Later, the efficiency of these models is compared with
the state of art models on the basis of accuracy. Further, our proposed models
can be applied to address the issue of scant datasets for any problem of image
analysis.


## SplatFormer Point Transformer for Robust 3D Gaussian Splatting

>Authors: Yutong Chen, Marko Mihajlovic, Xiyi Chen, Yiming Wang, Sergey Prokudin, Siyu Tang

>2024-11-10

> http://arxiv.org/abs/2411.06390v1

3D Gaussian Splatting (3DGS) has recently transformed photorealistic
reconstruction, achieving high visual fidelity and real-time performance.
However, rendering quality significantly deteriorates when test views deviate
from the camera angles used during training, posing a major challenge for
applications in immersive free-viewpoint rendering and navigation. In this
work, we conduct a comprehensive evaluation of 3DGS and related novel view
synthesis methods under out-of-distribution (OOD) test camera scenarios. By
creating diverse test cases with synthetic and real-world datasets, we
demonstrate that most existing methods, including those incorporating various
regularization techniques and data-driven priors, struggle to generalize
effectively to OOD views. To address this limitation, we introduce SplatFormer,
the first point transformer model specifically designed to operate on Gaussian
splats. SplatFormer takes as input an initial 3DGS set optimized under limited
training views and refines it in a single forward pass, effectively removing
potential artifacts in OOD test views. To our knowledge, this is the first
successful application of point transformers directly on 3DGS sets, surpassing
the limitations of previous multi-scene training methods, which could handle
only a restricted number of input views during inference. Our model
significantly improves rendering quality under extreme novel views, achieving
state-of-the-art performance in these challenging scenarios and outperforming
various 3DGS regularization techniques, multi-scene models tailored for **sparse**
view synthesis, and diffusion-based frameworks.


## Monitoring-feedback induced entanglement relaxations in a tilted free fermionic chain

>Authors: Xuyang Huang, Han-Ze Li, Yu-Jun Zhao, Shuo Liu, Jian-Xin Zhong

>2024-11-10

> http://arxiv.org/abs/2411.06332v1

Recent years have witnessed measurement-induced entanglement phase
transitions attracting significant attention, yet the feedback-induced skin
effect in tilted fields remains understudied. This work investigates
entanglement relaxation and dynamical transitions in a fermionic chain subject
to generalized monitoring and an applied tilted field. We show that the
feedback-induced skin effect persists with tilting, though relaxation time
increases. For small tilts, bipartite entanglement exhibits a widened
metastable phase, transitioning from a logarithmic-law to an area-law phase. In
contrast, strong tilts accelerate relaxation, leading to a single area-law
phase. Moreover, due to the unique properties of the tilted chain, we examine
the impact of the boundary on entanglement relaxation. This work not only
deepens our understanding of the role of feedback in measurement-induced
entanglement phase transitions but also enhances our comprehension of the
non-Hermitian skin effect beyond the no-click limit.


## Optimizing Large Language Models through Quantization A Comparative Analysis of PTQ and QAT Techniques

>Authors: Jahid Hasan

>2024-11-09

> http://arxiv.org/abs/2411.06084v1

This paper presents a comprehensive analysis of **quantization** techniques for
optimizing Large Language Models (LLMs), specifically focusing on Post-Training
Quantization (PTQ) and Quantization-Aware Training (QAT). Through empirical
evaluation across models ranging from 10M to 1B parameters, we demonstrate that
**quantization** can achieve up to 68% reduction in model size while maintaining
performance within 6% of full-precision baselines when utilizing our proposed
scaling factor {\gamma}. Our experiments show that INT8 **quantization** delivers a
40% reduction in computational cost and power consumption, while INT4
**quantization** further improves these metrics by 60%. We introduce a novel
theoretical framework for mixed-precision **quantization**, deriving optimal bit
allocation strategies based on layer sensitivity and weight variance. Hardware
efficiency evaluations on edge devices reveal that our **quantization** approach
enables up to 2.4x throughput improvement for INT8 and 3x for INT4, with 60%
power reduction compared to full-precision models.


## Validation of an LLM-based Multi-Agent Framework for Protein Engineering in Dry Lab and Wet Lab

>Authors: Zan Chen, Yungeng Liu, Yu Guang Wang, Yiqing Shen

>2024-11-09

> http://arxiv.org/abs/2411.06029v1

Recent advancements in Large Language Models (LLMs) have enhanced efficiency
across various domains, including protein engineering, where they offer
promising opportunities for dry lab and wet lab experiment workflow automation.
Previous work, namely TourSynbio-Agent, integrates a protein-specialized
multimodal LLM (i.e. TourSynbio-7B) with domain-specific deep learning (DL)
models to streamline both computational and experimental protein engineering
tasks. While initial validation demonstrated TourSynbio-7B's fundamental
protein property understanding, the practical effectiveness of the complete
TourSynbio-Agent framework in real-world applications remained unexplored. This
study presents a comprehensive validation of TourSynbio-Agent through five
diverse case studies spanning both computational (dry lab) and experimental
(wet lab) protein engineering. In three computational case studies, we evaluate
the TourSynbio-Agent's capabilities in mutation prediction, protein folding,
and protein design. Additionally, two wet-lab validations demonstrate
TourSynbio-Agent's practical utility: engineering P450 proteins with up to 70%
improved selectivity for steroid 19-hydroxylation, and developing reductases
with 3.7x enhanced catalytic efficiency for alcohol conversion. Our findings
from the five case studies establish that TourSynbio-Agent can effectively
automate complex protein engineering workflows through an intuitive
conversational interface, potentially accelerating scientific discovery in
protein engineering.


## TourSynbio-Search A Large Language Model Driven Agent Framework for Unified Search Method for Protein Engineering

>Authors: Yungeng Liu, Zan Chen, Yu Guang Wang, Yiqing Shen

>2024-11-09

> http://arxiv.org/abs/2411.06024v1

The exponential growth in protein-related databases and scientific
literature, combined with increasing demands for efficient biological
information retrieval, has created an urgent need for unified and accessible
search methods in protein engineering research. We present TourSynbio-Search, a
novel bioinformatics search agent framework powered by the TourSynbio-7B
protein multimodal large language model (LLM), designed to address the growing
challenges of information retrieval across rapidly expanding protein databases
and corresponding online research literature. The agent's dual-module
architecture consists of PaperSearch and ProteinSearch components, enabling
comprehensive exploration of both scientific literature and protein data across
multiple biological databases. At its core, TourSynbio-Search employs an
intelligent agent system that interprets natural language queries, optimizes
search parameters, and executes search operations across major platforms
including UniProt, PDB, ArXiv, and BioRxiv. The agent's ability to process
intuitive natural language queries reduces technical barriers, allowing
researchers to efficiently access and analyze complex biological data without
requiring extensive bioinformatics expertise. Through detailed case studies in
literature retrieval and protein structure visualization, we demonstrate
TourSynbio-Search's effectiveness in streamlining biological information
retrieval and enhancing research productivity. This framework represents an
advancement in bridging the accessibility gap between complex biological
databases and researchers, potentially accelerating progress in protein
engineering applications. Our codes are available at:
https://github.com/tsynbio/Toursynbio-Search


## Recycled Attention Efficient inference for long-context language models

>Authors: Fangyuan Xu, Tanya Goyal, Eunsol Choi

>2024-11-08

> http://arxiv.org/abs/2411.05787v1

Generating long sequences of tokens given a long-context input imposes a
heavy computational burden for large language models (LLMs). One of the
computational bottleneck comes from computing attention over a long sequence of
input at each generation step. In this paper, we propose Recycled Attention, an
inference-time method which alternates between full context attention and
attention over a subset of input tokens. When performing partial attention, we
recycle the attention pattern of a previous token that has performed full
attention and attend only to the top K most attended tokens, reducing the cost
of data movement and attention computation. Compared to previously proposed
inference-time **acceleration** method which attends only to local context or
tokens with high accumulative attention scores, our approach flexibly chooses
tokens that are relevant to the current decoding step. We evaluate our methods
on RULER, a suite of tasks designed to comprehensively evaluate long-context
abilities, and long-context language modeling tasks. Applying our method to
off-the-shelf LLMs achieves comparable speedup to baselines which only consider
local context while improving the performance by 2x. We further explore two
ideas to improve performance-efficiency trade-offs: (1) dynamically decide when
to perform recycled or full attention step based on the query similarities and
(2) continued pre-training the model with Recycled Attention.


## Autoregressive Adaptive Hypergraph Transformer for Skeleton-based Activity Recognition

>Authors: Abhisek Ray, Ayush Raj, Maheshkumar H. Kolekar

>2024-11-08

> http://arxiv.org/abs/2411.05692v1

Extracting multiscale contextual information and higher-order correlations
among skeleton sequences using Graph Convolutional Networks (GCNs) alone is
inadequate for effective action classification. Hypergraph convolution
addresses the above issues but cannot harness the long-range dependencies.
Transformer proves to be effective in capturing these dependencies and making
complex contextual features accessible. We propose an Autoregressive Adaptive
HyperGraph Transformer (AutoregAd-HGformer) model for in-phase (autoregressive
and discrete) and out-phase (adaptive) hypergraph generation. The vector
**quantize**d in-phase hypergraph equipped with powerful autoregressive learned
priors produces a more robust and informative representation suitable for
hyperedge formation. The out-phase hypergraph generator provides a
model-agnostic hyperedge learning technique to align the attributes with input
skeleton embedding. The hybrid (supervised and unsupervised) learning in
AutoregAd-HGformer explores the action-dependent feature along spatial,
temporal, and channel dimensions. The extensive experimental results and
ablation study indicate the superiority of our model over state-of-the-art
hypergraph architectures on NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets.


## SSSD Simply-Scalable Speculative Decoding

>Authors: Michele Marzollo, Jiawei Zhuang, Niklas Roemer, Lorenz K. Müller, Lukas Cavigelli

>2024-11-08

> http://arxiv.org/abs/2411.05894v1

Over the past year, Speculative Decoding has gained popularity as a technique
for accelerating Large Language Model inference. While several methods have
been introduced, most struggle to deliver satisfactory performance at batch
sizes typical for data centers ($\geq 8$) and often involve significant
deployment complexities. In this work, we offer a theoretical explanation of
how Speculative Decoding can be effectively utilized with larger batch sizes.
We also introduce a method that integrates seamlessly into existing systems
without additional training or the complexity of deploying a small LLM. In a
continuous batching setting, we achieve a 4x increase in throughput without any
latency impact for short context generation, and a 1.7-2x improvement in both
latency and throughput for longer contexts.


## AcceLLM Accelerating LLM Inference using Redundancy for Load Balancing and Data Locality

>Authors: Ilias Bournias, Lukas Cavigelli, Georgios Zacharopoulos

>2024-11-08

> http://arxiv.org/abs/2411.05555v1

Large Language Model (LLM) inference on large-scale systems is expected to
dominate future cloud infrastructures. Efficient LLM inference in cloud
environments with numerous AI accelerators is challenging, necessitating
extensive optimizations for optimal performance. Current systems batch prefill
and decoding to boost throughput but encounter latency issues, while others
disaggregate these phases, leading to resource underutilization. We propose
AcceLLM, a novel method addressing latency and load balancing, inspired by the
cache data management. It strategically utilizes redundant data to enhance
inference via load balancing and optimal hardware use. Simulated evaluations on
Nvidia H100 GPU and Huawei Ascend 910B2 show AcceLLM surpasses state-of-the-art
systems up to 30% in latency and efficiency, handling diverse workloads
effectively.


## FGGP Fixed-Rate Gradient-First Gradual Pruning

>Authors: Lingkai Zhu, Can Deniz Bezek, Orcun Goksel

>2024-11-08

> http://arxiv.org/abs/2411.05500v1

In recent years, the increasing size of deep learning models and their
growing demand for computational resources have drawn significant attention to
the practice of **pruning** neural networks, while aiming to preserve their
accuracy. In unstructured gradual **pruning**, which sparsifies a network by
gradually removing individual network parameters until a targeted network
**sparsity** is reached, recent works show that both gradient and weight magnitudes
should be considered. In this work, we show that such mechanism, e.g., the
order of prioritization and selection criteria, is essential. We introduce a
gradient-first magnitude-next strategy for choosing the parameters to prune,
and show that a fixed-rate subselection criterion between these steps works
better, in contrast to the annealing approach in the literature. We validate
this on CIFAR-10 dataset, with multiple randomized initializations on both
VGG-19 and ResNet-50 network backbones, for **pruning** targets of 90, 95, and 98%
**sparsity** and for both initially dense and 50% **sparse** networks. Our proposed
fixed-rate gradient-first gradual **pruning** (FGGP) approach outperforms its
state-of-the-art alternatives in most of the above experimental settings, even
occasionally surpassing the upperbound of corresponding dense network results,
and having the highest ranking across the considered experimental settings.


## SASWISE-UE Segmentation and Synthesis with Interpretable Scalable Ensembles for Uncertainty Estimation

>Authors: Weijie Chen, Alan McMillan

>2024-11-08

> http://arxiv.org/abs/2411.05324v1

This paper introduces an efficient sub-model ensemble framework aimed at
enhancing the interpretability of medical deep learning models, thus increasing
their clinical applicability. By generating uncertainty maps, this framework
enables end-users to evaluate the reliability of model outputs. We developed a
strategy to develop diverse models from a single well-trained checkpoint,
facilitating the training of a model family. This involves producing multiple
outputs from a single input, fusing them into a final output, and estimating
uncertainty based on output disagreements. Implemented using U-Net and UNETR
models for segmentation and synthesis tasks, this approach was tested on CT
body segmentation and MR-CT synthesis datasets. It achieved a mean Dice
coefficient of 0.814 in segmentation and a Mean Absolute Error of 88.17 HU in
synthesis, improved from 89.43 HU by **pruning**. Additionally, the framework was
evaluated under corruption and undersampling, maintaining correlation between
uncertainty and error, which highlights its robustness. These results suggest
that the proposed approach not only maintains the performance of well-trained
models but also enhances interpretability through effective uncertainty
estimation, applicable to both convolutional and transformer models in a range
of imaging tasks.


## SpecHub Provable Acceleration to Multi-Draft Speculative Decoding

>Authors: Ryan Sun, Tianyi Zhou, Xun Chen, Lichao Sun

>2024-11-08

> http://arxiv.org/abs/2411.05289v1

Large Language Models (LLMs) have become essential in advancing natural
language processing (NLP) tasks, but their sequential token generation limits
inference speed. Multi-Draft Speculative Decoding (MDSD) offers a promising
solution by using a smaller draft model to generate multiple token sequences,
which the target LLM verifies in parallel. However, current heuristic
approaches, such as Recursive Rejection Sampling (RRS), suffer from low
acceptance rates in subsequent drafts, limiting the advantages of using
multiple drafts. Meanwhile, Optimal Transport with Membership Cost (OTM) can
theoretically improve acceptance rates, but its computational cost is too high
for real-time use. We present SpecHub, a novel, efficient sampling-verification
method for MDSD that improves acceptance rates with only linear computational
overhead. By simplifying the OTM problem into a compact Linear Programming
model, SpecHub significantly reduces computational complexity. It further
accelerates sampling by leveraging a **sparse** joint distribution, focusing
computation on high-probability token sequences. In extensive experiments,
Spechub consistently generates 0.05-0.27 and 0.02-0.16 more tokens per step
than RRS and RRS without replacement. We attach our code at
\url{https://github.com/MasterGodzilla/Speculative_decoding_OT}.

