<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../../img/favicon.ico" />
    <title>2025-07-25 - Efficient Paper</title>
    <link rel="stylesheet" href="../../../css/theme.css" />
    <link rel="stylesheet" href="../../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.css" rel="stylesheet" />
        <link href="../../../css/extra.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "2025-07-25";
        var mkdocs_page_input_path = "weekly_paper/legacy/2025-07-25.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
 <link href="../../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }</style> <script src="../../../assets/javascripts/glightbox.min.js"></script></head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../../.." class="icon icon-home"> Efficient Paper
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../../search/">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Paper List</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../../cls_year/">By Year</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../cls_keyword/">By Keyword</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../cls_publication/">By Publication</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../cls_institution/">By Institution</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../cls_author/">By Author</a>
                  </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../../baseline_methods_graph/">Graph</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Weekly Paper</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../../2025-11-07/">2025-11-07</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../2025-11-14/">2025-11-14</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../2025-11-21/">2025-11-21</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../2025-11-28/">2025-11-28</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../2025-12-05/">2025-12-05</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../2025-12-12/">2025-12-12</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../2025-12-19/">2025-12-19</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../2025-12-26/">2025-12-26</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" >Legacy</a>
    <ul class="current">
                <li class="toctree-l2 current"><a class="reference internal current" href="#">2025-07-25</a>
    <ul class="current">
    </ul>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../2025-08-01/">2025-08-01</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../2025-08-08/">2025-08-08</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../2025-08-15/">2025-08-15</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../2025-08-22/">2025-08-22</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../2025-08-29/">2025-08-29</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../2025-09-05/">2025-09-05</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../2025-09-15/">2025-09-15</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../2025-09-19/">2025-09-19</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../2025-09-26/">2025-09-26</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../2025-09-28/">2025-09-28</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../2025-10-09/">2025-10-09</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../2025-10-17/">2025-10-17</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../2025-10-24/">2025-10-24</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../2025-10-31/">2025-10-31</a>
                </li>
    </ul>
                  </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../../contributors/">Contributors</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../..">Efficient Paper</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../.." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Weekly Paper</li>
          <li class="breadcrumb-item">Legacy</li>
      <li class="breadcrumb-item active">2025-07-25</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
  
                <h1 id="2025-07-25">2025-07-25</h1>
<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#Explainable-Mapper-Charting-LLM-Embedding-Spaces-Using-Perturbation-Based-Explanation-and-Verification-Agents">Explainable Mapper Charting LLM Embedding Spaces Using Perturbation-Based Explanation and Verification Agents</a></li>
<li><a href="#Not-All-Features-Deserve-Attention-Graph-Guided-Dependency-Learning-for-Tabular-Data-Generation-with-Language-Models">Not All Features Deserve Attention Graph-Guided Dependency Learning for Tabular Data Generation with Language Models</a></li>
<li><a href="#Enhanced-Velocity-Adaptive-Scheme-Joint-Fair-Access-and-Age-of-Information-Optimization-in-Vehicular-Networks">Enhanced Velocity-Adaptive Scheme Joint Fair Access and Age of Information Optimization in Vehicular Networks</a></li>
<li><a href="#StyleAdaptedLM-Enhancing-Instruction-Following-Models-with-Efficient-Stylistic-Transfer">StyleAdaptedLM Enhancing Instruction Following Models with Efficient Stylistic Transfer</a></li>
<li><a href="#Assemble-Your-Crew-Automatic-Multi-agent-Communication-Topology-Design-via-Autoregressive-Graph-Generation">Assemble Your Crew Automatic Multi-agent Communication Topology Design via Autoregressive Graph Generation</a></li>
<li><a href="#Prune&amp;Comp-Free-Lunch-for-Layer-Pruned-LLMs-via-Iterative-Pruning-with-Magnitude-Compensation">Prune&amp;Comp Free Lunch for Layer-Pruned LLMs via Iterative Pruning with Magnitude Compensation</a></li>
<li><a href="#SpecASR-Accelerating-LLM-based-Automatic-Speech-Recognition-via-Speculative-Decoding">SpecASR Accelerating LLM-based Automatic Speech Recognition via Speculative Decoding</a></li>
<li><a href="#ICWLM-A-Multi-Task-Wireless-Large-Model-via-In-Context-Learning">ICWLM A Multi-Task Wireless Large Model via In-Context Learning</a></li>
<li><a href="#NeuralDB-Scaling-Knowledge-Editing-in-LLMs-to-100,000-Facts-with-Neural-KV-Database">NeuralDB Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database</a></li>
<li><a href="#Who-Attacks,-and-Why?-Using-LLMs-to-Identify-Negative-Campaigning-in-18M-Tweets-across-19-Countries">Who Attacks, and Why? Using LLMs to Identify Negative Campaigning in 18M Tweets across 19 Countries</a></li>
<li><a href="#R-Stitch-Dynamic-Trajectory-Stitching-for-Efficient-Reasoning">R-Stitch Dynamic Trajectory Stitching for Efficient Reasoning</a></li>
<li><a href="#EFS-Evolutionary-Factor-Searching-for-Sparse-Portfolio-Optimization-Using-Large-Language-Models">EFS Evolutionary Factor Searching for Sparse Portfolio Optimization Using Large Language Models</a></li>
<li><a href="#LLM-Meets-the-Sky-Heuristic-Multi-Agent-Reinforcement-Learning-for-Secure-Heterogeneous-UAV-Networks">LLM Meets the Sky Heuristic Multi-Agent Reinforcement Learning for Secure Heterogeneous UAV Networks</a></li>
<li><a href="#Resilient-Multi-Agent-Negotiation-for-Medical-Supply-ChainsIntegrating-LLMs-and-Blockchain-for-Transparent-Coordination">Resilient Multi-Agent Negotiation for Medical Supply ChainsIntegrating LLMs and Blockchain for Transparent Coordination</a></li>
<li><a href="#Reinforcement-Learning-Fine-Tunes-a-Sparse-Subnetwork-in-Large-Language-Models">Reinforcement Learning Fine-Tunes a Sparse Subnetwork in Large Language Models</a></li>
<li><a href="#LoRA-is-All-You-Need-for-Safety-Alignment-of-Reasoning-LLMs">LoRA is All You Need for Safety Alignment of Reasoning LLMs</a></li>
<li><a href="#Parallelism-Meets-Adaptiveness-Scalable-Documents-Understanding-in-Multi-Agent-LLM-Systems">Parallelism Meets Adaptiveness Scalable Documents Understanding in Multi-Agent LLM Systems</a></li>
<li><a href="#Beyond-Context-Limits-Subconscious-Threads-for-Long-Horizon-Reasoning">Beyond Context Limits Subconscious Threads for Long-Horizon Reasoning</a></li>
<li><a href="#Collaborative-Inference-and-Learning-between-Edge-SLMs-and-Cloud-LLMs-A-Survey-of-Algorithms,-Execution,-and-Open-Challenges">Collaborative Inference and Learning between Edge SLMs and Cloud LLMs A Survey of Algorithms, Execution, and Open Challenges</a></li>
<li><a href="#ACT-Bridging-the-Gap-in-Code-Translation-through-Synthetic-Data-Generation-&amp;-Adaptive-Training">ACT Bridging the Gap in Code Translation through Synthetic Data Generation &amp; Adaptive Training</a></li>
<li><a href="#Mamba-OTR-a-Mamba-based-Solution-for-Online-Take-and-Release-Detection-from-Untrimmed-Egocentric-Video">Mamba-OTR a Mamba-based Solution for Online Take and Release Detection from Untrimmed Egocentric Video</a></li>
<li><a href="#CompLeak-Deep-Learning-Model-Compression-Exacerbates-Privacy-Leakage">CompLeak Deep Learning Model Compression Exacerbates Privacy Leakage</a></li>
<li><a href="#Time-to-Split-Exploring-Data-Splitting-Strategies-for-Offline-Evaluation-of-Sequential-Recommenders">Time to Split Exploring Data Splitting Strategies for Offline Evaluation of Sequential Recommenders</a></li>
<li><a href="#Reducing-GPU-Memory-Fragmentation-via-Spatio-Temporal-Planning-for-Efficient-Large-Scale-Model-Training">Reducing GPU Memory Fragmentation via Spatio-Temporal Planning for Efficient Large-Scale Model Training</a></li>
<li><a href="#Benchmarking-LLM-Privacy-Recognition-for-Social-Robot-Decision-Making">Benchmarking LLM Privacy Recognition for Social Robot Decision Making</a></li>
<li><a href="#TorchAO-PyTorch-Native-Training-to-Serving-Model-Optimization">TorchAO PyTorch-Native Training-to-Serving Model Optimization</a></li>
<li><a href="#On-the-transferability-of-Sparse-Autoencoders-for-interpreting-compressed-models">On the transferability of Sparse Autoencoders for interpreting compressed models</a></li>
<li><a href="#Just-Ask-for-Music-(JAM)-Multimodal-and-Personalized-Natural-Language-Music-Recommendation">Just Ask for Music (JAM) Multimodal and Personalized Natural Language Music Recommendation</a></li>
<li><a href="#Reservoir-Computing-as-a-Language-Model">Reservoir Computing as a Language Model</a></li>
<li><a href="#Who-Leads-in-the-Shadows?-ERGM-and-Centrality-Analysis-of-Congressional-Democrats-on-Bluesky">Who Leads in the Shadows? ERGM and Centrality Analysis of Congressional Democrats on Bluesky</a></li>
<li><a href="#Transformer-based-Deep-Learning-Model-for-Joint-Routing-and-Scheduling-with-Varying-Electric-Vehicle-Numbers">Transformer-based Deep Learning Model for Joint Routing and Scheduling with Varying Electric Vehicle Numbers</a></li>
<li><a href="#Metaphor-and-Large-Language-Models-When-Surface-Features-Matter-More-than-Deep-Understanding">Metaphor and Large Language Models When Surface Features Matter More than Deep Understanding</a></li>
<li><a href="#Scaling-Decentralized-Learning-with-FLock">Scaling Decentralized Learning with FLock</a></li>
<li><a href="#IM-Chat-A-Multi-agent-LLM-based-Framework-for-Knowledge-Transfer-in-Injection-Molding-Industry">IM-Chat A Multi-agent LLM-based Framework for Knowledge Transfer in Injection Molding Industry</a></li>
<li><a href="#CHADET-Cross-Hierarchical-Attention-for-Depth-Completion-Using-Unsupervised-Lightweight-Transformer">CHADET Cross-Hierarchical-Attention for Depth-Completion Using Unsupervised Lightweight Transformer</a></li>
<li><a href="#Beyond-Visual-Line-of-Sight-UAVs-with-Edge-AI,-Connected-LLMs,-and-VR-for-Autonomous-Aerial-Intelligence">Beyond Visual Line of Sight UAVs with Edge AI, Connected LLMs, and VR for Autonomous Aerial Intelligence</a></li>
<li><a href="#From-Neurons-to-Semantics-Evaluating-Cross-Linguistic-Alignment-Capabilities-of-Large-Language-Models-via-Neurons-Alignment">From Neurons to Semantics Evaluating Cross-Linguistic Alignment Capabilities of Large Language Models via Neurons Alignment</a></li>
<li><a href="#Sparse-Autoencoder-guided-Supervised-Finetuning-to-Mitigate-Unexpected-Code-Switching-in-LLMs">Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs</a></li>
<li><a href="#Tiny-language-models">Tiny language models</a></li>
<li><a href="#An-Evaluation-of-DUSt3R/MASt3R/VGGT-3D-Reconstruction-on-Photogrammetric-Aerial-Blocks">An Evaluation of DUSt3R/MASt3R/VGGT 3D Reconstruction on Photogrammetric Aerial Blocks</a></li>
<li><a href="#LeAdQA-LLM-Driven-Context-Aware-Temporal-Grounding-for-Video-Question-Answering">LeAdQA LLM-Driven Context-Aware Temporal Grounding for Video Question Answering</a></li>
<li><a href="#CXR-TFT-Multi-Modal-Temporal-Fusion-Transformer-for-Predicting-Chest-X-ray-Trajectories">CXR-TFT Multi-Modal Temporal Fusion Transformer for Predicting Chest X-ray Trajectories</a></li>
<li><a href="#GRACE-Generative-Recommendation-via-Journey-Aware-Sparse-Attention-on-Chain-of-Thought-Tokenization">GRACE Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization</a></li>
<li><a href="#Spatial-Temporal-Transformer-with-Curriculum-Learning-for-EEG-Based-Emotion-Recognition">Spatial-Temporal Transformer with Curriculum Learning for EEG-Based Emotion Recognition</a></li>
<li><a href="#Enabling-Efficient-Hardware-Acceleration-of-Hybrid-Vision-Transformer-(ViT)-Networks-at-the-Edge">Enabling Efficient Hardware Acceleration of Hybrid Vision Transformer (ViT) Networks at the Edge</a></li>
<li><a href="#Linear-Relational-Decoding-of-Morphology-in-Language-Models">Linear Relational Decoding of Morphology in Language Models</a></li>
<li><a href="#KinForm-Kinetics-Informed-Feature-Optimised-Representation-Models-for-Enzyme-k_{cat}-and-K_{M}-Prediction">KinForm Kinetics Informed Feature Optimised Representation Models for Enzyme <script type="math/tex">k_{cat}</script> and <script type="math/tex">K_{M}</script> Prediction</a></li>
<li><a href="#Agentic-Satellite-Augmented-Low-Altitude-Economy-and-Terrestrial-Networks-A-Survey-on-Generative-Approaches">Agentic Satellite-Augmented Low-Altitude Economy and Terrestrial Networks A Survey on Generative Approaches</a></li>
<li><a href="#Efficient-LLM-Inference-Bandwidth,-Compute,-Synchronization,-and-Capacity-are-all-you-need">Efficient LLM Inference Bandwidth, Compute, Synchronization, and Capacity are all you need</a></li>
<li><a href="#Characterizing-Communication-Patterns-in-Distributed-Large-Language-Model-Inference">Characterizing Communication Patterns in Distributed Large Language Model Inference</a></li>
<li><a href="#DPMT-Dual-Process-Multi-scale-Theory-of-Mind-Framework-for-Real-time-Human-AI-Collaboration">DPMT Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration</a></li>
<li><a href="#KROMA-Ontology-Matching-with-Knowledge-Retrieval-and-Large-Language-Models">KROMA Ontology Matching with Knowledge Retrieval and Large Language Models</a></li>
<li><a href="#LoopServe-An-Adaptive-Dual-phase-LLM-Inference-Acceleration-System-for-Multi-Turn-Dialogues">LoopServe An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues</a></li>
</ul>
<h2 id="explainable-mapper-charting-llm-embedding-spaces-using-perturbation-based-explanation-and-verification-agents">Explainable Mapper Charting LLM Embedding Spaces Using Perturbation-Based Explanation and Verification Agents</h2>
<blockquote>
<p>Authors: Xinyuan Yan, Rita Sevastjanova, Sinie van der Ben, Mennatallah El-Assady, Bei Wang</p>
<p>2025-07-24</p>
<p>http://arxiv.org/abs/2507.18607v1</p>
</blockquote>
<p>Large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) produce high-dimensional embeddings that capture
rich semantic and syntactic relationships between words, sentences, and
concepts. Investigating the topological structures of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> embedding spaces via
mapper graphs enables us to understand their underlying structures.
Specifically, a mapper graph summarizes the topological structure of the
embedding space, where each node represents a topological neighborhood
(containing a cluster of embeddings), and an edge connects two nodes if their
corresponding neighborhoods <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a>. However, manually exploring these
embedding spaces to uncover encoded linguistic properties requires considerable
human effort. To address this challenge, we introduce a framework for
semi-automatic annotation of these embedding properties. To organize the
exploration process, we first define a taxonomy of explorable elements within a
mapper graph such as nodes, edges, paths, components, and trajectories. The
annotation of these elements is executed through two types of customizable
<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-based agents that employ perturbation techniques for scalable and automated
analysis. These agents help to explore and explain the characteristics of
mapper elements and verify the robustness of the generated explanations. We
instantiate the framework within a visual analytics workspace and demonstrate
its effectiveness through case studies. In particular, we replicate findings
from prior research on BERT's embedding properties across various layers of its
architecture and provide further observations into the linguistic properties of
topological neighborhoods.</p>
<h2 id="not-all-features-deserve-attention-graph-guided-dependency-learning-for-tabular-data-generation-with-language-models">Not All Features Deserve Attention Graph-Guided Dependency Learning for Tabular Data Generation with Language Models</h2>
<blockquote>
<p>Authors: Zheyu Zhang, Shuo Yang, Bardh Prenkaj, Gjergji Kasneci</p>
<p>2025-07-24</p>
<p>http://arxiv.org/abs/2507.18504v1</p>
</blockquote>
<p>Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) have shown strong potential for tabular data
generation by modeling textualized feature-value pairs. However, tabular data
inherently exhibits <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> feature-level dependencies, where many feature
interactions are structurally insignificant. This creates a fundamental
mismatch as <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s' self-attention mechanism inevitably distributes focus across
all pairs, diluting attention on critical relationships, particularly in
datasets with complex dependencies or semantically ambiguous features. To
address this limitation, we propose GraDe (Graph-Guided Dependency Learning), a
novel method that explicitly integrates <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> dependency graphs into <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s'
attention mechanism. GraDe employs a lightweight dynamic graph learning module
guided by externally extracted functional dependencies, prioritizing key
feature interactions while suppressing irrelevant ones. Our experiments across
diverse real-world datasets demonstrate that GraDe outperforms existing
<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-based approaches by up to 12% on complex datasets while achieving
competitive results with state-of-the-art approaches in synthetic data quality.
Our method is minimally intrusive yet effective, offering a practical solution
for structure-aware tabular data modeling with <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s.</p>
<h2 id="enhanced-velocity-adaptive-scheme-joint-fair-access-and-age-of-information-optimization-in-vehicular-networks">Enhanced Velocity-Adaptive Scheme Joint Fair Access and Age of Information Optimization in Vehicular Networks</h2>
<blockquote>
<p>Authors: Xiao Xu, Qiong Wu, Pingyi Fan, Kezhi Wang, Nan Cheng, Wen Chen, Khaled B. Letaief</p>
<p>2025-07-24</p>
<p>http://arxiv.org/abs/2507.18328v1</p>
</blockquote>
<p>In this paper, we consider the fair access problem and the Age of Information
(AoI) under 5G New Radio (NR) Vehicle-to-Infrastructure (V2I) Mode 2 in
vehicular networks. Specifically, vehicles follow Mode 2 to communicate with
Roadside Units (RSUs) to obtain accurate data for driving
assistance.Nevertheless, vehicles often have different velocity when they are
moving in adjacent lanes, leading to difference in RSU dwelltime and
<a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> duration. This results in unfair access to network resources,
potentially influencing driving safety. To ensure the freshness of received
data, the AoI should be analyzed. Mode 2 introduces a novel preemption
mechanism, necessitating simultaneous optimization of fair access and AoI to
guarantee timely and relevant data delivery. We propose a joint optimization
framework for vehicular network, defining a fairness index and employing
Stochastic Hybrid Systems (SHS) to model AoI under preemption mechanism. By
adaptively adjusting the selection window of Semi-Persistent Scheduling (SPS)
in Mode 2, we address the optimization of fairness and AoI. We apply a large
language model (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>)-Based Multi-objective Evolutionary Algorithm Based on
Decomposition (MOEA/D) to solve this problem. Simulation results demonstrate
the effectiveness of our scheme in balancing fair access and minimizing AoI.</p>
<h2 id="styleadaptedlm-enhancing-instruction-following-models-with-efficient-stylistic-transfer">StyleAdaptedLM Enhancing Instruction Following Models with Efficient Stylistic Transfer</h2>
<blockquote>
<p>Authors: Pritika Ramu, Apoorv Saxena, Meghanath M Y, Varsha Sankar, Debraj Basu</p>
<p>2025-07-24</p>
<p>http://arxiv.org/abs/2507.18294v1</p>
</blockquote>
<p>Adapting <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s to specific stylistic characteristics, like brand voice or
authorial tones, is crucial for enterprise <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> but challenging to
achieve from corpora which lacks instruction-response formatting without
compromising instruction adherence. We introduce StyleAdaptedLM, a framework
that efficiently transfers stylistic traits to instruction-following models
using Low-Rank Adaptation (LoRA). LoRA adapters are first trained on a base
model with diverse unstructured stylistic corpora, then merged with a separate
instruction-following model. This enables robust stylistic customization
without paired data or sacrificing task performance. Experiments across
multiple datasets and models demonstrate improved stylistic consistency while
preserving instruction adherence, with human evaluations confirming
brand-specific convention uptake. StyleAdaptedLM offers an efficient path for
stylistic personalization in <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s.</p>
<h2 id="assemble-your-crew-automatic-multi-agent-communication-topology-design-via-autoregressive-graph-generation">Assemble Your Crew Automatic Multi-agent Communication Topology Design via Autoregressive Graph Generation</h2>
<blockquote>
<p>Authors: Shiyuan Li, Yixin Liu, Qingsong Wen, Chengqi Zhang, Shirui Pan</p>
<p>2025-07-24</p>
<p>http://arxiv.org/abs/2507.18224v1</p>
</blockquote>
<p>Multi-agent systems (MAS) based on large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) have emerged
as a powerful solution for dealing with complex problems across diverse
domains. The effectiveness of MAS is critically dependent on its collaboration
topology, which has become a focal point for automated design research.
However, existing approaches are fundamentally constrained by their reliance on
a template graph modification paradigm with a predefined set of agents and
hard-coded interaction structures, significantly limiting their adaptability to
task-specific requirements. To address these limitations, we reframe MAS design
as a conditional autoregressive graph generation task, where both the system
composition and structure are designed jointly. We propose ARG-Designer, a
novel autoregressive model that operationalizes this paradigm by constructing
the collaboration graph from scratch. Conditioned on a natural language task
query, ARG-Designer sequentially and dynamically determines the required number
of agents, selects their appropriate roles from an extensible pool, and
establishes the optimal <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> links between them. This generative
approach creates a customized topology in a flexible and extensible manner,
precisely tailored to the unique demands of different tasks. Extensive
experiments across six diverse benchmarks demonstrate that ARG-Designer not
only achieves state-of-the-art performance but also enjoys significantly
greater token efficiency and enhanced extensibility. The source code of
ARG-Designer is available at https://github.com/Shiy-Li/ARG-Designer.</p>
<h2 id="prunecomp-free-lunch-for-layer-pruned-llms-via-iterative-pruning-with-magnitude-compensation">Prune&amp;Comp Free Lunch for Layer-Pruned LLMs via Iterative Pruning with Magnitude Compensation</h2>
<blockquote>
<p>Authors: Xinrui Chen, Hongxing Zhang, Fanyi Zeng, Yongxian Wei, Yizhi Wang, Xitong Ling, Guanghao Li, Chun Yuan</p>
<p>2025-07-24</p>
<p>http://arxiv.org/abs/2507.18212v1</p>
</blockquote>
<p>Layer <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> has emerged as a promising technique for compressing large
language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) while achieving <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> proportional to the <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a>
ratio. In this work, we identify that removing any layer induces a significant
magnitude gap in hidden states, resulting in substantial performance
degradation. To address this issue, we propose Prune&amp;Comp, a novel
plug-and-play layer <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> scheme that leverages magnitude compensation to
mitigate such gaps in a training-free manner. Specifically, we first estimate
the magnitude gap caused by layer removal and then eliminate this gap by
rescaling the remaining weights offline, with zero runtime overhead incurred.
We further demonstrate the advantages of Prune&amp;Comp through an iterative
<a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> strategy. When integrated with an iterative prune-and-compensate loop,
Prune&amp;Comp consistently enhances existing layer <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> metrics. For instance,
when 5 layers of LLaMA-3-8B are pruned using the prevalent block influence
metric, Prune&amp;Comp nearly halves the perplexity and retains 93.19\% of the
original model's question-answering performance, outperforming the baseline by
4.01%.</p>
<h2 id="specasr-accelerating-llm-based-automatic-speech-recognition-via-speculative-decoding">SpecASR Accelerating LLM-based Automatic Speech Recognition via Speculative Decoding</h2>
<blockquote>
<p>Authors: Linye Wei, Shuzhang Zhong, Songqiang Xu, Runsheng Wang, Ru Huang, Meng Li</p>
<p>2025-07-24</p>
<p>http://arxiv.org/abs/2507.18181v1</p>
</blockquote>
<p>Large language model (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>)-based automatic speech recognition (ASR) has
recently attracted a lot of attention due to its high recognition accuracy and
enhanced multi-dialect support. However, the high decoding latency of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s
challenges the real-time ASR requirements. Although speculative decoding has
been explored for better decoding efficiency, they usually ignore the key
characteristics of the ASR task and achieve limited speedup. To further reduce
the real-time ASR latency, in this paper, we propose a novel speculative
decoding framework specialized for ASR, dubbed SpecASR. SpecASR is developed
based on our core observation that ASR decoding is audio-conditioned, which
results in high output alignment between small and large ASR models, even given
output mismatches in intermediate decoding steps. Therefore, SpecASR features
an adaptive draft sequence generation process that dynamically modifies the
draft sequence length to maximize the token acceptance length. SpecASR further
proposes a draft sequence recycling strategy that reuses the previously
generated draft sequence to reduce the draft ASR model latency. Moreover, a
two-pass <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> token tree generation algorithm is also proposed to balance the
latency of draft and target ASR models. With extensive experimental results, we
demonstrate SpecASR achieves 3.04x-3.79x and 1.25x-1.84x speedup over the
baseline autoregressive decoding and speculative decoding, respectively,
without any loss in recognition accuracy.</p>
<h2 id="icwlm-a-multi-task-wireless-large-model-via-in-context-learning">ICWLM A Multi-Task Wireless Large Model via In-Context Learning</h2>
<blockquote>
<p>Authors: Yuxuan Wen, Xiaoming Chen, Maojun Zhang, Zhaoyang Zhang</p>
<p>2025-07-24</p>
<p>http://arxiv.org/abs/2507.18167v1</p>
</blockquote>
<p>The rapid evolution of wireless <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> technologies, particularly
massive multiple-input multiple-output (mMIMO) and millimeter-wave (mmWave),
introduces significant network complexity and computational demands.
Significant research efforts have been made to improve physical layer
performance by resorting to deep learning (DL) methods, which, however, are
usually task-specific and struggle with data scarcity and generalization. To
address these challenges, we propose a novel In-Context Wireless Large Model
(ICWLM), a wireless-native foundation model designed for simultaneous
multi-task learning at the physical layer. Unlike conventional methods that
adapt wireless data to pre-trained large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s), ICWLM is
trained directly on large-scale, mixed wireless datasets from scratch. It
jointly solves multiple classical physical layer problems, including multi-user
precoding (sum-rate maximization and max-min SINR) and channel prediction. A
key innovation of ICWLM is its utilization of in-context learning (ICL),
enabling the model to adapt to varying system configurations and channel
conditions with minimal demonstration pairs, eliminating the need for extensive
retraining. Furthermore, we employ the Dynamic Weight Averaging (DWA) algorithm
to dynamically balance the individual task losses during multi-task training,
ensuring efficient and stable learning across diverse objectives. Extensive
simulation results demonstrate that ICWLM achieves competitive performance
compared to task-specific methods while exhibiting remarkable generalization
capabilities to unseen system configurations. This work offers a promising
paradigm for developing unified and adaptive AI models for future wireless
networks, potentially reducing deployment complexity and enhancing intelligent
resource management.</p>
<h2 id="neuraldb-scaling-knowledge-editing-in-llms-to-100000-facts-with-neural-kv-database">NeuralDB Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database</h2>
<blockquote>
<p>Authors: Weizhi Fei, Hao Shi, Jing Xu, Jingchen Peng, Jiazheng Li, Jingzhao Zhang, Bo Bai, Wei Han, Zhenyuan Chen, Xueyan Niu</p>
<p>2025-07-24</p>
<p>http://arxiv.org/abs/2507.18028v1</p>
</blockquote>
<p>Efficiently editing knowledge stored in large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) enables
model updates without large-scale training. One possible solution is
Locate-and-Edit (L\&amp;E), allowing simultaneous modifications of a massive number
of facts. However, such editing may compromise the general abilities of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s
and even result in forgetting edited facts when scaling up to thousands of
edits. In this paper, we model existing linear L\&amp;E methods as querying a
Key-Value (<a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>) database. From this perspective, we then propose NeuralDB, an
editing framework that explicitly represents the edited facts as a neural <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>
database equipped with a non-linear gated retrieval module, % In particular,
our gated module only operates when inference involves the edited facts,
effectively preserving the general abilities of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s. Comprehensive experiments
involving the editing of 10,000 facts were conducted on the ZsRE and
CounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results
demonstrate that NeuralDB not only excels in editing efficacy, generalization,
specificity, fluency, and consistency, but also preserves overall performance
across six representative text understanding and generation tasks. Further
experiments indicate that NeuralDB maintains its effectiveness even when scaled
to 100,000 facts (\textbf{50x} more than in prior work).</p>
<h2 id="who-attacks-and-why-using-llms-to-identify-negative-campaigning-in-18m-tweets-across-19-countries">Who Attacks, and Why? Using LLMs to Identify Negative Campaigning in 18M Tweets across 19 Countries</h2>
<blockquote>
<p>Authors: Victor Hartman, Petter TÃ¶rnberg</p>
<p>2025-07-23</p>
<p>http://arxiv.org/abs/2507.17636v1</p>
</blockquote>
<p>Negative campaigning is a central feature of political competition, yet
empirical research has been limited by the high cost and limited scalability of
existing classification methods. This study makes two key contributions. First,
it introduces zero-shot Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) as a novel approach for
cross-lingual classification of negative campaigning. Using benchmark datasets
in ten languages, we demonstrate that <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s achieve performance on par with
native-speaking human coders and outperform conventional supervised machine
learning approaches. Second, we leverage this novel method to conduct the
largest cross-national study of negative campaigning to date, analyzing 18
million tweets posted by parliamentarians in 19 European countries between 2017
and 2022. The results reveal consistent cross-national patterns: governing
parties are less likely to use negative messaging, while ideologically extreme
and populist parties -- particularly those on the radical right -- engage in
significantly higher levels of negativity. These findings advance our
understanding of how party-level characteristics shape strategic <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>
in multiparty systems. More broadly, the study demonstrates the potential of
<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s to enable scalable, transparent, and replicable research in political
<a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> across linguistic and cultural contexts.</p>
<h2 id="r-stitch-dynamic-trajectory-stitching-for-efficient-reasoning">R-Stitch Dynamic Trajectory Stitching for Efficient Reasoning</h2>
<blockquote>
<p>Authors: Zhuokun Chen, Zeren Chen, Jiahao He, Mingkui Tan, Jianfei Cai, Bohan Zhuang</p>
<p>2025-07-23</p>
<p>http://arxiv.org/abs/2507.17307v1</p>
</blockquote>
<p>Chain-of-thought (CoT) reasoning enhances the problem-solving capabilities of
large language models by encouraging step-by-step intermediate reasoning during
inference. While effective, CoT introduces substantial computational overhead
due to its reliance on autoregressive decoding over long token sequences.
Existing <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> strategies either reduce sequence length through early
stopping or compressive reward designs, or improve decoding speed via
speculative decoding with smaller models. However, speculative decoding suffers
from limited speedup when the agreement between small and large models is low,
and fails to exploit the potential advantages of small models in producing
concise intermediate reasoning. In this paper, we present R-Stitch, a
token-level, confidence-based hybrid decoding framework that accelerates CoT
inference by switching between a small language model (SLM) and a large
language model (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>) along the reasoning trajectory. R-Stitch uses the SLM to
generate tokens by default and delegates to the <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> only when the SLM's
confidence falls below a threshold. This design avoids full-sequence rollback
and selectively invokes the <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> on uncertain steps, preserving both efficiency
and answer quality. R-Stitch is model-agnostic, training-free, and compatible
with standard decoding pipelines. Experiments on math reasoning benchmarks
demonstrate that R-Stitch achieves up to 85\% reduction in inference latency
with negligible accuracy drop, highlighting its practical effectiveness in
accelerating CoT reasoning.</p>
<h2 id="efs-evolutionary-factor-searching-for-sparse-portfolio-optimization-using-large-language-models">EFS Evolutionary Factor Searching for Sparse Portfolio Optimization Using Large Language Models</h2>
<blockquote>
<p>Authors: Haochen Luo, Yuan Zhang, Chen Liu</p>
<p>2025-07-23</p>
<p>http://arxiv.org/abs/2507.17211v1</p>
</blockquote>
<p>Sparse portfolio optimization is a fundamental yet challenging problem in
quantitative finance, since traditional approaches heavily relying on
historical return statistics and static objectives can hardly adapt to dynamic
market regimes. To address this issue, we propose Evolutionary Factor Search
(EFS), a novel framework that leverages large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) to
automate the generation and evolution of alpha factors for <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> portfolio
construction. By reformulating the asset selection problem as a top-m ranking
task guided by <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-generated factors, EFS incorporates an evolutionary feedback
loop to iteratively refine the factor pool based on performance. Extensive
experiments on five Fama-French benchmark datasets and three real-market
datasets (US50, HSI45 and CSI300) demonstrate that EFS significantly
outperforms both statistical-based and optimization-based baselines, especially
in larger asset universes and volatile conditions. Comprehensive ablation
studies validate the importance of prompt composition, factor diversity, and
<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> backend choice. Our results highlight the promise of language-guided
evolution as a robust and interpretable paradigm for portfolio optimization
under structural constraints.</p>
<h2 id="llm-meets-the-sky-heuristic-multi-agent-reinforcement-learning-for-secure-heterogeneous-uav-networks">LLM Meets the Sky Heuristic Multi-Agent Reinforcement Learning for Secure Heterogeneous UAV Networks</h2>
<blockquote>
<p>Authors: Lijie Zheng, Ji He, Shih Yu Chang, Yulong Shen, Dusit Niyato</p>
<p>2025-07-23</p>
<p>http://arxiv.org/abs/2507.17188v1</p>
</blockquote>
<p>This work tackles the physical layer security (PLS) problem of maximizing the
secrecy rate in heterogeneous UAV networks (HetUAVNs) under propulsion energy
constraints. Unlike prior studies that assume uniform UAV capabilities or
overlook energy-security trade-offs, we consider a realistic scenario where
UAVs with diverse payloads and computation resources collaborate to serve
ground terminals in the presence of eavesdroppers. To manage the complex
coupling between UAV motion and <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>, we propose a hierarchical
optimization framework. The inner layer uses a semidefinite relaxation
(SDR)-based S2DC algorithm combining penalty functions and difference-of-convex
(d.c.) programming to solve the secrecy precoding problem with fixed UAV
positions. The outer layer introduces a Large Language Model (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>)-guided
heuristic multi-agent reinforcement learning approach (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-HeMARL) for
trajectory optimization. <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-HeMARL efficiently incorporates expert heuristics
policy generated by the <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>, enabling UAVs to learn energy-aware,
security-driven trajectories without the inference overhead of real-time <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>
calls. The simulation results show that our method outperforms existing
baselines in secrecy rate and energy efficiency, with consistent robustness
across varying UAV swarm sizes and random seeds.</p>
<h2 id="resilient-multi-agent-negotiation-for-medical-supply-chainsintegrating-llms-and-blockchain-for-transparent-coordination">Resilient Multi-Agent Negotiation for Medical Supply ChainsIntegrating LLMs and Blockchain for Transparent Coordination</h2>
<blockquote>
<p>Authors: Mariam ALMutairi, Hyungmin Kim</p>
<p>2025-07-23</p>
<p>http://arxiv.org/abs/2507.17134v1</p>
</blockquote>
<p>Global health emergencies, such as the COVID-19 pandemic, have exposed
critical weaknesses in traditional medical supply chains, including
inefficiencies in resource allocation, lack of transparency, and poor
adaptability to dynamic disruptions. This paper presents a novel hybrid
framework that integrates blockchain technology with a decentralized, large
language model (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>) powered multi-agent negotiation system to enhance the
resilience and accountability of medical supply chains during crises. In this
system, autonomous agents-representing manufacturers, distributors, and
healthcare institutions-engage in structured, context-aware negotiation and
decision-making processes facilitated by <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s, enabling rapid and ethical
allocation of scarce medical resources. The off-chain agent layer supports
adaptive reasoning and local decision-making, while the on-chain blockchain
layer ensures immutable, transparent, and auditable enforcement of decisions
via smart contracts. The framework also incorporates a formal cross-layer
<a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> protocol to bridge decentralized negotiation with institutional
enforcement. A simulation environment emulating pandemic scenarios evaluates
the system's performance, demonstrating improvements in negotiation efficiency,
fairness of allocation, supply chain responsiveness, and auditability. This
research contributes an innovative approach that synergizes blockchain trust
guarantees with the adaptive intelligence of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-driven agents, providing a
robust and scalable solution for critical supply chain coordination under
uncertainty.</p>
<h2 id="reinforcement-learning-fine-tunes-a-sparse-subnetwork-in-large-language-models">Reinforcement Learning Fine-Tunes a Sparse Subnetwork in Large Language Models</h2>
<blockquote>
<p>Authors: Andrii Balashov</p>
<p>2025-07-23</p>
<p>http://arxiv.org/abs/2507.17107v1</p>
</blockquote>
<p>Reinforcement learning (RL) is a key post-pretraining step for aligning large
language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) with complex tasks and human preferences. While it is
often assumed that RL fine-tuning requires updating most of a model's
parameters, we challenge this assumption with a surprising finding: RL
fine-tuning consistently modifies only a small subnetwork (typically 5-30% of
weights), leaving most parameters unchanged. We call this phenomenon RL-induced
parameter update <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a>. It arises naturally, without any <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a>
constraints or parameter-efficient tuning, and appears across multiple RL
algorithms (e.g., PPO, DPO, SimPO, PRIME) and model families (e.g., OpenAI,
Meta, and open-source <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s). Moreover, the subnetworks updated by RL show
substantial <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a> across different seeds, datasets, and algorithms-far
exceeding chance-suggesting a partially transferable structure in the
pretrained model. We show that fine-tuning only this <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> subnetwork recovers
full model performance and yields parameters nearly identical to the fully
fine-tuned model. Our analysis suggests this <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a> emerges because RL
operates near the model's original distribution, requiring only targeted
changes. KL penalties, gradient clipping, and on-policy dynamics have limited
effect on the <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a> pattern. These findings shed new light on how RL adapts
models: not by shifting all weights, but by focusing training on a small,
consistently updated subnetwork. This insight enables more efficient RL methods
and reframes <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a> through the lens of the lottery ticket hypothesis.</p>
<h2 id="lora-is-all-you-need-for-safety-alignment-of-reasoning-llms">LoRA is All You Need for Safety Alignment of Reasoning LLMs</h2>
<blockquote>
<p>Authors: Yihao Xue, Baharan Mirzasoleiman</p>
<p>2025-07-22</p>
<p>http://arxiv.org/abs/2507.17075v1</p>
</blockquote>
<p>Reasoning <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s have demonstrated remarkable breakthroughs in solving complex
problems that were previously out of reach. To ensure <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s do not assist with
harmful requests, safety alignment fine-tuning is necessary in the
post-training phase. However, safety alignment fine-tuning has recently been
shown to significantly degrade reasoning abilities, a phenomenon known as the
"Safety Tax". In this work, we show that using LoRA for SFT on refusal datasets
effectively aligns the model for safety without harming its reasoning
capabilities. This is because restricting the safety weight updates to a
low-rank space minimizes the interference with the reasoning weights. Our
extensive experiments across four benchmarks covering math, science, and coding
show that this approach produces highly safe <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s -- with safety levels
comparable to full-model fine-tuning -- without compromising their reasoning
abilities. Additionally, we observe that LoRA induces weight updates with
smaller <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a> with the initial weights compared to full-model fine-tuning. We
also explore methods that further reduce such <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a> -- via regularization or
during weight merging -- and observe some improvement on certain tasks. We hope
this result motivates designing approaches that yield more consistent
improvements in the reasoning-safety trade-off.</p>
<h2 id="parallelism-meets-adaptiveness-scalable-documents-understanding-in-multi-agent-llm-systems">Parallelism Meets Adaptiveness Scalable Documents Understanding in Multi-Agent LLM Systems</h2>
<blockquote>
<p>Authors: Chengxuan Xia, Qianye Wu, Sixuan Tian, Yilun Hao</p>
<p>2025-07-22</p>
<p>http://arxiv.org/abs/2507.17061v1</p>
</blockquote>
<p>Large language model (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>) agents have shown increasing promise for
collaborative task completion. However, existing multi-agent frameworks often
rely on static workflows, fixed roles, and limited inter-agent <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>,
reducing their effectiveness in open-ended, high-complexity domains. This paper
proposes a coordination framework that enables adaptiveness through three core
mechanisms: dynamic task routing, bidirectional feedback, and parallel agent
evaluation. The framework allows agents to reallocate tasks based on confidence
and workload, exchange structured critiques to iteratively improve outputs, and
crucially compete on high-ambiguity subtasks with evaluator-driven selection of
the most suitable result. We instantiate these principles in a modular
architecture and demonstrate substantial improvements in factual coverage,
coherence, and efficiency over static and partially adaptive baselines. Our
findings highlight the benefits of incorporating both adaptiveness and
structured competition in multi-agent <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> systems.</p>
<h2 id="beyond-context-limits-subconscious-threads-for-long-horizon-reasoning">Beyond Context Limits Subconscious Threads for Long-Horizon Reasoning</h2>
<blockquote>
<p>Authors: Hongyin Luo, Nathaniel Morgan, Tina Li, Derek Zhao, Ai Vy Ngo, Philip Schroeder, Lijie Yang, Assaf Ben-Kish, Jack O'Brien, James Glass</p>
<p>2025-07-22</p>
<p>http://arxiv.org/abs/2507.16784v1</p>
</blockquote>
<p>To break the context limits of large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) that bottleneck
reasoning accuracy and efficiency, we propose the Thread Inference Model (TIM),
a family of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s trained for recursive and decompositional problem solving, and
TIMRUN, an inference runtime enabling long-horizon structured reasoning beyond
context limits. Together, TIM hosted on TIMRUN supports virtually unlimited
working memory and multi-hop tool calls within a single language model
inference, overcoming output limits, positional-embedding constraints, and
GPU-memory bottlenecks. Performance is achieved by modeling natural language as
reasoning trees measured by both length and depth instead of linear sequences.
The reasoning trees consist of tasks with thoughts, recursive subtasks, and
conclusions based on the concept we proposed in Schroeder et al, 2025. During
generation, we maintain a working memory that retains only the key-value states
of the most relevant context tokens, selected by a rule-based subtask-<a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a>
mechanism, enabling reuse of positional embeddings and GPU memory pages
throughout reasoning. Experimental results show that our system sustains high
inference throughput, even when manipulating up to 90% of the <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> cache in GPU
memory. It also delivers accurate reasoning on mathematical tasks and handles
information retrieval challenges that require long-horizon reasoning and
multi-hop tool use.</p>
<h2 id="collaborative-inference-and-learning-between-edge-slms-and-cloud-llms-a-survey-of-algorithms-execution-and-open-challenges">Collaborative Inference and Learning between Edge SLMs and Cloud LLMs A Survey of Algorithms, Execution, and Open Challenges</h2>
<blockquote>
<p>Authors: Senyao Li, Haozhao Wang, Wenchao Xu, Rui Zhang, Song Guo, Jingling Yuan, Xian Zhong, Tianwei Zhang, Ruixuan Li</p>
<p>2025-07-22</p>
<p>http://arxiv.org/abs/2507.16731v1</p>
</blockquote>
<p>As large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) evolve, deploying them solely in the cloud or
compressing them for edge devices has become inadequate due to concerns about
latency, privacy, cost, and personalization. This survey explores a
collaborative paradigm in which cloud-based <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s and edge-deployed small
language models (SLMs) cooperate across both inference and training. We present
a unified taxonomy of edge-cloud collaboration strategies. For inference, we
categorize approaches into task assignment, task division, and mixture-based
collaboration at both task and token granularity, encompassing adaptive
scheduling, resource-aware offloading, speculative decoding, and modular
routing. For training, we review distributed adaptation techniques, including
parameter alignment, <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a>, bidirectional distillation, and
small-model-guided optimization. We further summarize datasets, benchmarks, and
deployment cases, and highlight privacy-preserving methods and vertical
applications. This survey provides the first systematic foundation for <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-SLM
collaboration, bridging system and algorithm co-design to enable efficient,
scalable, and trustworthy edge-cloud intelligence.</p>
<h2 id="act-bridging-the-gap-in-code-translation-through-synthetic-data-generation-adaptive-training">ACT Bridging the Gap in Code Translation through Synthetic Data Generation &amp; Adaptive Training</h2>
<blockquote>
<p>Authors: Shreya Saxena, Siva Prasad, Zishan Ahmad, Vishal Vaddina</p>
<p>2025-07-22</p>
<p>http://arxiv.org/abs/2507.16478v1</p>
</blockquote>
<p>Code translation is a crucial process in software development and migration
projects, enabling interoperability between different programming languages and
enhancing software adaptability and thus longevity. Traditional automated
translation methods rely heavily on handcrafted transformation rules, which
often lack flexibility and scalability. Meanwhile, advanced language models
present promising alternatives but are often limited by proprietary, API-based
implementations that raise concerns over data security and reliance. In this
paper, we present Auto-Train for Code Translation (ACT), an innovative
framework that aims to improve code translation capabilities by enabling
in-house finetuning of open-source Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s). ACT's
automated pipeline significantly boosts the performance of these models,
narrowing the gap between open-source accessibility and the high performance of
closed-source solutions. Central to ACT is its synthetic data generation
module, which builds extensive, high-quality datasets from initial code
samples, incorporating unit tests to ensure functional accuracy and diversity.
ACT's evaluation framework incorporates execution-level checks, offering a
comprehensive assessment of translation quality. A key feature in ACT is its
controller module, which manages the entire pipeline by dynamically adjusting
hyperparameters, orchestrating iterative data generation, and finetuning based
on real-time evaluations. This enables ACT to intelligently optimize when to
continue training, generate additional targeted training data, or stop the
process. Our results demonstrate that ACT consistently enhances the
effectiveness of open-source models, offering businesses and developers a
secure and reliable alternative. Additionally, applying our data generation
pipeline to industry-scale migration projects has led to a notable increase in
developer <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a>.</p>
<h2 id="mamba-otr-a-mamba-based-solution-for-online-take-and-release-detection-from-untrimmed-egocentric-video">Mamba-OTR a Mamba-based Solution for Online Take and Release Detection from Untrimmed Egocentric Video</h2>
<blockquote>
<p>Authors: Alessandro Sebastiano Catinello, Giovanni Maria Farinella, Antonino Furnari</p>
<p>2025-07-22</p>
<p>http://arxiv.org/abs/2507.16342v1</p>
</blockquote>
<p>This work tackles the problem of Online detection of Take and Release (OTR)
of an object in untrimmed egocentric videos. This task is challenging due to
severe label imbalance, with temporally <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> positive annotations, and the
need for precise temporal predictions. Furthermore, methods need to be
computationally efficient in order to be deployed in real-world online
settings. To address these challenges, we propose Mamba-OTR, a model based on
the Mamba architecture. Mamba-OTR is designed to exploit temporal recurrence
during inference while being trained on short video clips. To address label
imbalance, our training pipeline incorporates the focal loss and a novel
regularization scheme that aligns model predictions with the evaluation metric.
Extensive experiments on EPIC-KITCHENS-100, the comparisons with
<a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>-based approach, and the evaluation of different training and test
schemes demonstrate the superiority of Mamba-OTR in both accuracy and
efficiency. These finding are particularly evident when evaluating full-length
videos or high frame-rate sequences, even when trained on short video snippets
for computational convenience. The proposed Mamba-OTR achieves a noteworthy
mp-mAP of 45.48 when operating in a sliding-window fashion, and 43.35 in
streaming mode, versus the 20.32 of a vanilla <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> and 25.16 of a
vanilla Mamba, thus providing a strong baseline for OTR. We will publicly
release the source code of Mamba-OTR to support future research.</p>
<h2 id="compleak-deep-learning-model-compression-exacerbates-privacy-leakage">CompLeak Deep Learning Model Compression Exacerbates Privacy Leakage</h2>
<blockquote>
<p>Authors: Na Li, Yansong Gao, Hongsheng Hu, Boyu Kuang, Anmin Fu</p>
<p>2025-07-22</p>
<p>http://arxiv.org/abs/2507.16872v1</p>
</blockquote>
<p>Model compression is crucial for minimizing memory storage and accelerating
inference in deep learning (DL) models, including recent foundation models like
large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s). Users can access different compressed model
versions according to their resources and budget. However, while existing
compression operations primarily focus on optimizing the trade-off between
resource efficiency and model performance, the privacy risks introduced by
compression remain overlooked and insufficiently understood.
  In this work, through the lens of membership inference attack (MIA), we
propose CompLeak, the first privacy risk evaluation framework examining three
widely used compression configurations that are <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a>, quantization, and
weight clustering supported by the commercial model compression framework of
Google's TensorFlow-Lite (TF-Lite) and Facebook's PyTorch Mobile. CompLeak has
three variants, given available access to the number of compressed models and
original model. CompLeakNR starts by adopting existing MIA methods to attack a
single compressed model, and identifies that different compressed models
influence members and non-members differently. When the original model and one
compressed model are available, CompLeakSR leverages the compressed model as a
reference to the original model and uncovers more privacy by combining meta
information (e.g., confidence vector) from both models. When multiple
compressed models are available with/without accessing the original model,
CompLeakMR innovatively exploits privacy leakage info from multiple compressed
versions to substantially signify the overall privacy leakage. We conduct
extensive experiments on seven diverse model architectures (from ResNet to
foundation models of BERT and GPT-2), and six image and textual benchmark
datasets.</p>
<h2 id="time-to-split-exploring-data-splitting-strategies-for-offline-evaluation-of-sequential-recommenders">Time to Split Exploring Data Splitting Strategies for Offline Evaluation of Sequential Recommenders</h2>
<blockquote>
<p>Authors: Danil Gusak, Anna Volodkevich, Anton Klenitskiy, Alexey Vasilev, Evgeny Frolov</p>
<p>2025-07-22</p>
<p>http://arxiv.org/abs/2507.16289v1</p>
</blockquote>
<p>Modern sequential recommender systems, ranging from lightweight
<a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>-based variants to large language models, have become increasingly
prominent in academia and industry due to their strong performance in the
next-item prediction task. Yet common evaluation protocols for sequential
recommendations remain insufficiently developed: they often fail to reflect the
corresponding recommendation task accurately, or are not aligned with
real-world scenarios.
  Although the widely used leave-one-out split matches next-item prediction, it
permits the <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a> between training and test periods, which leads to temporal
leakage and unrealistically long test horizon, limiting real-world relevance.
Global temporal splitting addresses these issues by evaluating on distinct
future periods. However, its applications to sequential recommendations remain
loosely defined, particularly in terms of selecting target interactions and
constructing a validation subset that provides necessary consistency between
validation and test metrics.
  In this paper, we demonstrate that evaluation outcomes can vary significantly
across splitting strategies, influencing model rankings and practical
deployment decisions. To improve reproducibility in both academic and
industrial settings, we systematically compare different splitting strategies
for sequential recommendations across multiple datasets and established
baselines. Our findings show that prevalent splits, such as leave-one-out, may
be insufficiently aligned with more realistic evaluation strategies. Code:
https://github.com/monkey0head/time-to-split</p>
<h2 id="reducing-gpu-memory-fragmentation-via-spatio-temporal-planning-for-efficient-large-scale-model-training">Reducing GPU Memory Fragmentation via Spatio-Temporal Planning for Efficient Large-Scale Model Training</h2>
<blockquote>
<p>Authors: Zixiao Huang, Junhao Hu, Hao Lin, Chunyang Zhu, Yueran Tang, Quanlu Zhang, Zhen Guo, Zhenhua Li, Shengen Yan, Zhenhua Zhu, Guohao Dai, Yu Wang</p>
<p>2025-07-22</p>
<p>http://arxiv.org/abs/2507.16274v1</p>
</blockquote>
<p>The rapid scaling of large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) has significantly increased
GPU memory pressure, which is further aggravated by training optimization
techniques such as virtual pipeline and recomputation that disrupt tensor
lifespans and introduce considerable memory fragmentation. Default GPU memory
allocators of popular deep learning frameworks like PyTorch use online
strategies without knowledge of tensor lifespans, which can waste up to 43\% of
memory and cause out-of-memory errors, rendering optimization techniques
ineffective or even unusable.
  To address this, we introduce STWeaver, a GPU memory allocator for deep
learning frameworks that reduces fragmentation by exploiting the spatial and
temporal regularity in memory allocation behaviors of training workloads.
STWeaver introduces a novel paradigm that combines offline planning with online
allocation. The offline planning leverages spatio-temporal regularities to
generate a near-optimal allocation plan, while the online allocation handles
complex and dynamic models such as Mixture-of-Experts (MoE). Built as a
pluggable PyTorch allocator, STWeaver reduces fragmentation ratio on average by
79.2\% (up to 100\%) across both dense and <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> models, with negligible
overhead. This enables more efficient, high-throughput training configurations
and improves performance by up to 32.5\%.</p>
<h2 id="benchmarking-llm-privacy-recognition-for-social-robot-decision-making">Benchmarking LLM Privacy Recognition for Social Robot Decision Making</h2>
<blockquote>
<p>Authors: Dakota Sullivan, Shirley Zhang, Jennica Li, Heather Kirkorian, Bilge Mutlu, Kassem Fawaz</p>
<p>2025-07-22</p>
<p>http://arxiv.org/abs/2507.16124v1</p>
</blockquote>
<p>Social robots are embodied agents that interact with people while following
human <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> norms. These robots interact using verbal and non-verbal
cues, and share the physical environments of people. While social robots have
previously utilized rule-based systems or probabilistic models for user
interaction, the rapid evolution of large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) presents new
opportunities to develop <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-empowered social robots for enhanced human-robot
interaction. To fully realize these capabilities, however, robots need to
collect data such as audio, fine-grained images, video, and locations. As a
result, <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s often process sensitive personal information, particularly within
home environments. Given the tension between utility and privacy risks,
evaluating how current <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s manage sensitive data is critical. Specifically, we
aim to explore the extent to which out-of-the-box <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s are privacy-aware in the
context of household social robots. In this study, we present a set of
privacy-relevant scenarios crafted through the lens of Contextual Integrity
(CI). We first survey users' privacy preferences regarding in-home social robot
behaviors and then examine how their privacy orientation affects their choices
of these behaviors (N = 450). We then provide the same set of scenarios and
questions to state-of-the-art <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s (N = 10) and find that the agreement between
humans and <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s is low. To further investigate the capabilities of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s as a
potential privacy controller, we implement four additional prompting strategies
and compare their results. Finally, we discuss the implications and potential
of AI privacy awareness in human-robot interaction.</p>
<h2 id="torchao-pytorch-native-training-to-serving-model-optimization">TorchAO PyTorch-Native Training-to-Serving Model Optimization</h2>
<blockquote>
<p>Authors: Andrew Or, Apurva Jain, Daniel Vega-Myhre, Jesse Cai, Charles David Hernandez, Zhenrui Zheng, Driss Guessous, Vasiliy Kuznetsov, Christian Puhrsch, Mark Saroufim, Supriya Rao, Thien Tran, Aleksandar SamardÅ¾iÄ</p>
<p>2025-07-21</p>
<p>http://arxiv.org/abs/2507.16099v1</p>
</blockquote>
<p>We present TorchAO, a PyTorch-native model optimization framework leveraging
quantization and <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a> to provide an end-to-end, training-to-serving
workflow for AI models. TorchAO supports a variety of popular model
optimization techniques, including FP8 quantized training, quantization-aware
training (QAT), post-training quantization (PTQ), and 2:4 <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a>, and
leverages a novel tensor subclass abstraction to represent a variety of
widely-used, backend agnostic low precision data types, including INT4, INT8,
FP8, MXFP4, MXFP6, and MXFP8. TorchAO integrates closely with the broader
ecosystem at each step of the model optimization pipeline, from pre-training
(TorchTitan) to fine-tuning (TorchTune, Axolotl) to serving (HuggingFace, v<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>,
SGLang, ExecuTorch), connecting an otherwise fragmented space in a single,
unified workflow. TorchAO has enabled recent launches of the quantized Llama
3.2 1B/3B and LlamaGuard3-8B models and is open-source at
https://github.com/pytorch/ao/.</p>
<h2 id="on-the-transferability-of-sparse-autoencoders-for-interpreting-compressed-models">On the transferability of Sparse Autoencoders for interpreting compressed models</h2>
<blockquote>
<p>Authors: Suchit Gupte, Vishnu Kabir Chhabra, Mohammad Mahdi Khalili</p>
<p>2025-07-21</p>
<p>http://arxiv.org/abs/2507.15977v1</p>
</blockquote>
<p>Modern <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s face inference efficiency challenges due to their scale. To
address this, many compression methods have been proposed, such as <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> and
quantization. However, the effect of compression on a model's interpretability
remains elusive. While several model interpretation approaches exist, such as
circuit discovery, Sparse Autoencoders (SAEs) have proven particularly
effective in decomposing a model's activation space into its feature basis. In
this work, we explore the differences in SAEs for the original and compressed
models. We find that SAEs trained on the original model can interpret the
compressed model albeit with slight performance degradation compared to the
trained SAE on the compressed model. Furthermore, simply <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> the original
SAE itself achieves performance comparable to training a new SAE on the pruned
model. This finding enables us to mitigate the extensive training costs of
SAEs.</p>
<h2 id="just-ask-for-music-jam-multimodal-and-personalized-natural-language-music-recommendation">Just Ask for Music (JAM) Multimodal and Personalized Natural Language Music Recommendation</h2>
<blockquote>
<p>Authors: Alessandro B. Melchiorre, Elena V. Epure, Shahed Masoudian, Gustavo Escobedo, Anna Hausberger, Manuel Moussallam, Markus Schedl</p>
<p>2025-07-21</p>
<p>http://arxiv.org/abs/2507.15826v1</p>
</blockquote>
<p>Natural language interfaces offer a compelling approach for music
recommendation, enabling users to express complex preferences conversationally.
While Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) show promise in this direction, their
scalability in recommender systems is limited by high costs and latency.
Retrieval-based approaches using smaller language models mitigate these issues
but often rely on single-modal item representations, overlook long-term user
preferences, and require full model retraining, posing challenges for
real-world deployment. In this paper, we present JAM (Just Ask for Music), a
lightweight and intuitive framework for natural language music recommendation.
JAM models user-query-item interactions as vector translations in a shared
latent space, inspired by knowledge graph embedding methods like TransE. To
capture the complexity of music and user intent, JAM aggregates multimodal item
features via cross-attention and <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> mixture-of-experts. We also introduce
JAMSessions, a new dataset of over 100k user-query-item triples with anonymized
user/item embeddings, uniquely combining conversational queries and user
long-term preferences. Our results show that JAM provides accurate
recommendations, produces intuitive representations suitable for practical use
cases, and can be easily integrated with existing music recommendation stacks.</p>
<h2 id="reservoir-computing-as-a-language-model">Reservoir Computing as a Language Model</h2>
<blockquote>
<p>Authors: Felix KÃ¶ster, Atsushi Uchida</p>
<p>2025-07-21</p>
<p>http://arxiv.org/abs/2507.15779v1</p>
</blockquote>
<p>Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>) have dominated the science and media landscape
duo to their impressive performance on processing large chunks of data and
produce human-like levels of text. Nevertheless, their huge energy demand and
slow processing still a bottleneck for further increasing quality while also
making the models accessible to everyone. To solve this bottleneck, we will
investigate how reservoir computing performs on natural text processing, which
could enable fast and energy efficient hardware implementations. Studies
investigating the use of reservoir computing as a language model remain <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a>.
In this paper, we compare three distinct approaches for character-level
language modeling, two different reservoir computing approaches, where only an
output layer is trainable, and the well-known <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>-based architectures,
which fully learn an attention-based sequence representation. We explore the
performance, computational cost and prediction accuracy for both paradigms by
equally varying the number of trainable parameters for all models. Using a
consistent pipeline for all three approaches, we demonstrate that <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>s
excel in prediction quality, whereas reservoir computers remain highly
efficient reducing the training and inference speed. Furthermore, we
investigate two types of reservoir computing: a traditional reservoir with a
static linear readout, and an attention-enhanced reservoir that dynamically
adapts its output weights via an attention mechanism. Our findings underline
how these paradigms scale and offer guidelines to balance resource constraints
with performance.</p>
<h2 id="who-leads-in-the-shadows-ergm-and-centrality-analysis-of-congressional-democrats-on-bluesky">Who Leads in the Shadows? ERGM and Centrality Analysis of Congressional Democrats on Bluesky</h2>
<blockquote>
<p>Authors: Gordon Hew, Ian McCulloh</p>
<p>2025-07-21</p>
<p>http://arxiv.org/abs/2507.16858v1</p>
</blockquote>
<p>Following the 2024 U.S. presidential election, Democratic lawmakers and their
supporters increasingly migrated from mainstream social media plat-forms like X
(formerly Twitter) to decentralized alternatives such as Bluesky. This study
investigates how Congressional Democrats use Bluesky to form networks of
influence and disseminate political messaging in a platform environment that
lacks algorithmic amplification. We employ a mixed-methods approach that
combines social network analysis, expo-nential random graph modeling (ERGM),
and <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>-based topic mod-eling (BERTopic) to analyze follows, mentions,
reposts, and discourse pat-terns among 182 verified Democratic members of
Congress. Our findings show that while party leaders such as Hakeem Jeffries
and Elizabeth War-ren dominate visibility metrics, overlooked figures like
Marcy Kaptur, Donald Beyer, and Dwight Evans occupy structurally central
positions, suggesting latent influence within the digital party ecosystem. ERGM
re-sults reveal significant homophily along ideological, state, and leadership
lines, with Senate leadership exhibiting lower connectivity. Topic analysis
identifies both shared themes (e.g., reproductive rights, foreign conflicts)
and subgroup-specific issues, with The Squad showing the most distinct
discourse profile. These results demonstrate the potential of decentralized
platforms to reshape intra-party <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> dynamics and highlight the need
for continued computational research on elite political behavior in emerging
digital environments.</p>
<h2 id="transformer-based-deep-learning-model-for-joint-routing-and-scheduling-with-varying-electric-vehicle-numbers">Transformer-based Deep Learning Model for Joint Routing and Scheduling with Varying Electric Vehicle Numbers</h2>
<blockquote>
<p>Authors: Jun Kang Yap, Vishnu Monn Baskaran, Wen Shan Tan, Ze Yang Ding, Hao Wang, David L. Dowe</p>
<p>2025-07-21</p>
<p>http://arxiv.org/abs/2507.15385v1</p>
</blockquote>
<p>The growing integration of renewable energy sources in modern power systems
has introduced significant operational challenges due to their intermittent and
uncertain outputs. In recent years, mobile energy storage systems (ESSs) have
emerged as a popular flexible resource for mitigating these challenges.
Compared to stationary ESSs, mobile ESSs offer additional spatial flexibility,
enabling cost-effective energy delivery through the transportation network.
However, the widespread deployment of mobile ESSs is often hindered by the high
investment cost, which has motivated researchers to investigate utilising more
readily available alternatives, such as electric vehicles (EVs) as mobile
energy storage units instead. Hence, we explore this opportunity with a
MIP-based day-ahead electric vehicle joint routing and scheduling problem in
this work. However, solving the problem in a practical setting can often be
computationally intractable since the existence of binary variables makes it
combinatorial challenging. Therefore, we proposed to simplify the problem's
solution process for a MIP solver by <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> the solution search space with a
<a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>-based deep learning (DL) model. This is done by training the model
to rapidly predict the optimal binary solutions. In addition, unlike many
existing DL approaches that assume fixed problem structures, the proposed model
is designed to accommodate problems with EV fleets of any sizes. This
flexibility is essential since frequent re-training can introduce significant
computational overhead. We evaluated the approach with simulations on the IEEE
33-bus system coupled with the Nguyen-Dupuis transportation network.</p>
<h2 id="metaphor-and-large-language-models-when-surface-features-matter-more-than-deep-understanding">Metaphor and Large Language Models When Surface Features Matter More than Deep Understanding</h2>
<blockquote>
<p>Authors: Elisa Sanchez-Bayona, Rodrigo Agerri</p>
<p>2025-07-21</p>
<p>http://arxiv.org/abs/2507.15357v1</p>
</blockquote>
<p>This paper presents a comprehensive evaluation of the capabilities of Large
Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) in metaphor interpretation across multiple datasets,
tasks, and prompt configurations. Although metaphor processing has gained
significant attention in Natural Language Processing (NLP), previous research
has been limited to single-dataset evaluations and specific task settings,
often using artificially constructed data through lexical replacement. We
address these limitations by conducting extensive experiments using diverse
publicly available datasets with inference and metaphor annotations, focusing
on Natural Language Inference (NLI) and Question Answering (QA) tasks. The
results indicate that <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s' performance is more influenced by features like
lexical <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a> and sentence length than by metaphorical content, demonstrating
that any alleged emergent abilities of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s to understand metaphorical language
are the result of a combination of surface-level features, in-context learning,
and linguistic knowledge. This work provides critical insights into the current
capabilities and limitations of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s in processing figurative language,
highlighting the need for more realistic evaluation frameworks in metaphor
interpretation tasks. Data and code are publicly available.</p>
<h2 id="scaling-decentralized-learning-with-flock">Scaling Decentralized Learning with FLock</h2>
<blockquote>
<p>Authors: Zehua Cheng, Rui Sun, Jiahao Sun, Yike Guo</p>
<p>2025-07-21</p>
<p>http://arxiv.org/abs/2507.15349v1</p>
</blockquote>
<p>Fine-tuning the large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) are prevented by the deficiency
of centralized control and the massive computing and <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> overhead on
the decentralized schemes. While the typical standard federated learning (FL)
supports data privacy, the central server requirement creates a single point of
attack and vulnerability to poisoning attacks. Generalizing the result in this
direction to 70B-parameter models in the heterogeneous, trustless environments
has turned out to be a huge, yet unbroken bottleneck. This paper introduces
FLock, a decentralized framework for secure and efficient collaborative <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>
fine-tuning. Integrating a blockchain-based trust layer with economic
incentives, FLock replaces the central aggregator with a secure, auditable
protocol for cooperation among untrusted parties. We present the first
empirical validation of fine-tuning a 70B <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> in a secure, multi-domain,
decentralized setting. Our experiments show the FLock framework defends against
backdoor poisoning attacks that compromise standard FL optimizers and fosters
synergistic knowledge transfer. The resulting models show a &gt;68% reduction in
adversarial attack success rates. The global model also demonstrates superior
cross-domain generalization, outperforming models trained in isolation on their
own specialized data.</p>
<h2 id="im-chat-a-multi-agent-llm-based-framework-for-knowledge-transfer-in-injection-molding-industry">IM-Chat A Multi-agent LLM-based Framework for Knowledge Transfer in Injection Molding Industry</h2>
<blockquote>
<p>Authors: Junhyeong Lee, Joon-Young Kim, Heekyu Kim, Inhyo Lee, Seunghwa Ryu</p>
<p>2025-07-21</p>
<p>http://arxiv.org/abs/2507.15268v1</p>
</blockquote>
<p>The injection molding industry faces critical challenges in preserving and
transferring field knowledge, particularly as experienced workers retire and
multilingual barriers hinder effective <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>. This study introduces
IM-Chat, a multi-agent framework based on large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s),
designed to facilitate knowledge transfer in injection molding. IM-Chat
integrates both limited documented knowledge (e.g., troubleshooting tables,
manuals) and extensive field data modeled through a data-driven process
condition generator that infers optimal manufacturing settings from
environmental inputs such as temperature and humidity, enabling robust and
context-aware task resolution. By adopting a retrieval-augmented generation
(RAG) strategy and tool-calling agents within a modular architecture, IM-Chat
ensures adaptability without the need for fine-tuning. Performance was assessed
across 100 single-tool and 60 hybrid tasks for GPT-4o, GPT-4o-mini, and
GPT-3.5-turbo by domain experts using a 10-point rubric focused on relevance
and correctness, and was further supplemented by automated evaluation using
GPT-4o guided by a domain-adapted instruction prompt. The evaluation results
indicate that more capable models tend to achieve higher accuracy, particularly
in complex, tool-integrated scenarios. Overall, these findings demonstrate the
viability of multi-agent <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> systems for industrial knowledge workflows and
establish IM-Chat as a scalable and generalizable approach to AI-assisted
decision support in manufacturing.</p>
<h2 id="chadet-cross-hierarchical-attention-for-depth-completion-using-unsupervised-lightweight-transformer">CHADET Cross-Hierarchical-Attention for Depth-Completion Using Unsupervised Lightweight Transformer</h2>
<blockquote>
<p>Authors: Kevin Christiansen Marsim, Jinwoo Jeon, Yeeun Kim, Myeongwoo Jeong, Hyun Myung</p>
<p>2025-07-21</p>
<p>http://arxiv.org/abs/2507.15189v1</p>
</blockquote>
<p>Depth information which specifies the distance between objects and current
position of the robot is essential for many robot tasks such as navigation.
Recently, researchers have proposed depth completion frameworks to provide
dense depth maps that offer comprehensive information about the surrounding
environment. However, existing methods show significant trade-offs between
computational efficiency and accuracy during inference. The substantial memory
and computational requirements make them unsuitable for real-time applications,
highlighting the need to improve the completeness and accuracy of depth
information while improving processing speed to enhance robot performance in
various tasks. To address these challenges, in this paper, we propose
CHADET(cross-hierarchical-attention depth-completion <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>), a
lightweight depth-completion network that can generate accurate dense depth
maps from RGB images and <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> depth points. For each pair, its feature is
extracted from the depthwise blocks and passed to the equally lightweight
<a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>-based decoder. In the decoder, we utilize the novel
cross-hierarchical-attention module that refines the image features from the
depth information. Our approach improves the quality and reduces memory usage
of the depth map prediction, as validated in both KITTI, NYUv2, and VOID
datasets.</p>
<h2 id="beyond-visual-line-of-sight-uavs-with-edge-ai-connected-llms-and-vr-for-autonomous-aerial-intelligence">Beyond Visual Line of Sight UAVs with Edge AI, Connected LLMs, and VR for Autonomous Aerial Intelligence</h2>
<blockquote>
<p>Authors: Andres Navarro, Carlos de Quinto, JosÃ© Alberto HernÃ¡ndez</p>
<p>2025-07-20</p>
<p>http://arxiv.org/abs/2507.15049v1</p>
</blockquote>
<p>Unmanned Aerial Vehicles are reshaping Non-Terrestrial Networks by acting as
agile, intelligent nodes capable of advanced analytics and instantaneous
situational awareness. This article introduces a budget-friendly quadcopter
platform that unites 5G <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>s, edge-based processing, and AI to tackle
core challenges in NTN scenarios. Outfitted with a panoramic camera, robust
onboard computation, and <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s, the drone system delivers seamless object
recognition, contextual analysis, and immersive operator experiences through
virtual reality VR technology. Field evaluations confirm the platform's ability
to process visual streams with low latency and sustain robust 5G links. Adding
<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s further streamlines operations by extracting actionable insights and
refining collected data for decision support. Demonstrated use cases, including
emergency response, infrastructure assessment, and environmental surveillance,
underscore the system's adaptability in demanding contexts.</p>
<h2 id="from-neurons-to-semantics-evaluating-cross-linguistic-alignment-capabilities-of-large-language-models-via-neurons-alignment">From Neurons to Semantics Evaluating Cross-Linguistic Alignment Capabilities of Large Language Models via Neurons Alignment</h2>
<blockquote>
<p>Authors: Chongxuan Huang, Yongshi Ye, Biao Fu, Qifeng Su, Xiaodong Shi</p>
<p>2025-07-20</p>
<p>http://arxiv.org/abs/2507.14900v2</p>
</blockquote>
<p>Large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) have demonstrated remarkable multilingual
capabilities, however, how to evaluate cross-lingual alignment remains
underexplored. Existing alignment benchmarks primarily focus on sentence
embeddings, but prior research has shown that neural models tend to induce a
non-smooth representation space, which impact of semantic alignment evaluation
on low-resource languages. Inspired by neuroscientific findings that similar
information activates <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a>ping neuronal regions, we propose a novel Neuron
State-Based Cross-Lingual Alignment (NeuronXA) to assess the cross-lingual a
lignment capabilities of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s, which offers a more semantically grounded
approach to assess cross-lingual alignment. We evaluate NeuronXA on several
prominent multilingual <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s (LLaMA, Qwen, Mistral, GLM, and OLMo) across two
transfer tasks and three multilingual benchmarks. The results demonstrate that
with only 100 parallel sentence pairs, NeuronXA achieves a Pearson correlation
of 0.9556 with downstream tasks performance and 0.8514 with transferability.
These findings demonstrate NeuronXA's effectiveness in assessing both
cross-lingual alignment and transferability, even with a small dataset. This
highlights its potential to advance cross-lingual alignment research and to
improve the semantic understanding of multilingual <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s.</p>
<h2 id="sparse-autoencoder-guided-supervised-finetuning-to-mitigate-unexpected-code-switching-in-llms">Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs</h2>
<blockquote>
<p>Authors: Boyi Deng, Yu Wan, Baosong Yang, Fei Huang, Wenjie Wang, Fuli Feng</p>
<p>2025-07-20</p>
<p>http://arxiv.org/abs/2507.14894v1</p>
</blockquote>
<p>Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) have impressive multilingual capabilities, but
they suffer from unexpected code-switching, also known as language mixing,
which involves switching to unexpected languages in the model response. This
problem leads to poor readability and degrades the usability of model
responses. However, existing work on this issue lacks a mechanistic analysis
and shows limited effectiveness. In this paper, we first provide an in-depth
analysis of unexpected code-switching using <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> autoencoders and find that
when <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s switch to a language, the features of that language exhibit excessive
pre-activation values. Based on our findings, we propose <script type="math/tex">\textbf{S}</script>parse
<script type="math/tex">\textbf{A}</script>utoencoder-guided <script type="math/tex">\textbf{S}</script>upervised
<script type="math/tex">\textbf{F}</script>ine<script type="math/tex">\textbf{t}</script>uning (SASFT), which teaches <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s to maintain
appropriate pre-activation values of specific language features during
training. Experiments on five models across three languages demonstrate that
SASFT consistently reduces unexpected code-switching by more than 50\% compared
to standard supervised fine-tuning, with complete elimination in four cases.
Moreover, SASFT maintains or even improves the models' performance on six
multilingual benchmarks, showing its effectiveness in addressing code-switching
while preserving multilingual capabilities.</p>
<h2 id="tiny-language-models">Tiny language models</h2>
<blockquote>
<p>Authors: Ronit D. Gross, Yarden Tzach, Tal Halevi, Ella Koresh, Ido Kanter</p>
<p>2025-07-20</p>
<p>http://arxiv.org/abs/2507.14871v2</p>
</blockquote>
<p>A prominent achievement of natural language processing (NLP) is its ability
to understand and generate meaningful human language. This capability relies on
complex feedforward <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> block architectures pre-trained on large
language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s). However, <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> pre-training is currently feasible only
for a few dominant companies due to the immense computational resources
required, limiting broader research participation. This creates a critical need
for more accessible alternatives. In this study, we explore whether tiny
language models (TLMs) exhibit the same key qualitative features of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s. We
demonstrate that TLMs exhibit a clear performance gap between pre-trained and
non-pre-trained models across classification tasks, indicating the
effectiveness of pre-training, even at a tiny scale. The performance gap
increases with the size of the pre-training dataset and with greater <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a>
between tokens in the pre-training and classification datasets. Furthermore,
the classification accuracy achieved by a pre-trained deep TLM architecture can
be replicated through a soft committee of multiple, independently pre-trained
shallow architectures, enabling low-latency TLMs without affecting
classification accuracy. Our results are based on pre-training BERT-6 and
variants of BERT-1 on subsets of the Wikipedia dataset and evaluating their
performance on FewRel, AGNews, and DBPedia classification tasks. Future
research on TLM is expected to further illuminate the mechanisms underlying
NLP, especially given that its biologically inspired models suggest that TLMs
may be sufficient for children or adolescents to develop language. The data and
code that support the findings of this study are openly available on
https://github.com/Rg32601/Tiny-Language-Models .</p>
<h2 id="an-evaluation-of-dust3rmast3rvggt-3d-reconstruction-on-photogrammetric-aerial-blocks">An Evaluation of DUSt3R/MASt3R/VGGT 3D Reconstruction on Photogrammetric Aerial Blocks</h2>
<blockquote>
<p>Authors: Xinyi Wu, Steven Landgraf, Markus Ulrich, Rongjun Qin</p>
<p>2025-07-20</p>
<p>http://arxiv.org/abs/2507.14798v1</p>
</blockquote>
<p>State-of-the-art 3D computer vision algorithms continue to advance in
handling <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a>, unordered image sets. Recently developed foundational models
for 3D reconstruction, such as Dense and Unconstrained Stereo 3D Reconstruction
(DUSt3R), Matching and Stereo 3D Reconstruction (MASt3R), and Visual Geometry
Grounded Transformer (VGGT), have attracted attention due to their ability to
handle very <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> image <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a>s. Evaluating DUSt3R/MASt3R/VGGT on typical
aerial images matters, as these models may handle extremely low image <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a>s,
stereo occlusions, and textureless regions. For redundant collections, they can
accelerate 3D reconstruction by using extremely sparsified image sets. Despite
tests on various computer vision benchmarks, their potential on photogrammetric
aerial blocks remains unexplored. This paper conducts a comprehensive
evaluation of the pre-trained DUSt3R/MASt3R/VGGT models on the aerial blocks of
the UseGeo dataset for pose estimation and dense 3D reconstruction. Results
show these methods can accurately reconstruct dense point clouds from very
<a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> image sets (fewer than 10 images, up to 518 pixels resolution), with
completeness gains up to +50% over COLMAP. VGGT also demonstrates higher
computational efficiency, scalability, and more reliable camera pose
estimation. However, all exhibit limitations with high-resolution images and
large sets, as pose reliability declines with more images and geometric
complexity. These findings suggest <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>-based methods cannot fully
replace traditional SfM and MVS, but offer promise as complementary approaches,
especially in challenging, low-resolution, and <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> scenarios.</p>
<h2 id="leadqa-llm-driven-context-aware-temporal-grounding-for-video-question-answering">LeAdQA LLM-Driven Context-Aware Temporal Grounding for Video Question Answering</h2>
<blockquote>
<p>Authors: Xinxin Dong, Baoyun Peng, Haokai Ma, Yufei Wang, Zixuan Dong, Fei Hu, Xiaodong Wang</p>
<p>2025-07-20</p>
<p>http://arxiv.org/abs/2507.14784v1</p>
</blockquote>
<p>Video Question Answering (VideoQA) requires identifying <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> critical
moments in long videos and reasoning about their causal relationships to answer
semantically complex questions. While recent advances in multimodal learning
have improved alignment and fusion, current approaches remain limited by two
prevalent but fundamentally flawed strategies: (1) task-agnostic sampling
indiscriminately processes all frames, overwhelming key events with irrelevant
content; and (2) heuristic retrieval captures superficial patterns but misses
causal-temporal structures needed for complex reasoning. To address these
challenges, we introduce LeAdQA, an innovative approach that bridges these gaps
through synergizing causal-aware query refinement with fine-grained visual
grounding. Our method first leverages <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s to reformulate question-option
pairs, resolving causal ambiguities and sharpening temporal focus. These
refined queries subsequently direct a temporal grounding model to precisely
retrieve the most salient segments, complemented by an adaptive fusion
mechanism dynamically integrating the evidence to maximize relevance. The
integrated visual-textual cues are then processed by an M<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> to generate
accurate, contextually-grounded answers. Experiments on NExT-QA, IntentQA, and
NExT-GQA demonstrate that our method's precise visual grounding substantially
enhances the understanding of video-question relationships, achieving
state-of-the-art (SOTA) performance on complex reasoning tasks while
maintaining computational efficiency.</p>
<h2 id="cxr-tft-multi-modal-temporal-fusion-transformer-for-predicting-chest-x-ray-trajectories">CXR-TFT Multi-Modal Temporal Fusion Transformer for Predicting Chest X-ray Trajectories</h2>
<blockquote>
<p>Authors: Mehak Arora, Ayman Ali, Kaiyuan Wu, Carolyn Davis, Takashi Shimazui, Mahmoud Alwakeel, Victor Moas, Philip Yang, Annette Esper, Rishikesan Kamaleswaran</p>
<p>2025-07-19</p>
<p>http://arxiv.org/abs/2507.14766v1</p>
</blockquote>
<p>In intensive care units (ICUs), patients with complex clinical conditions
require vigilant monitoring and prompt interventions. Chest X-rays (CXRs) are a
vital diagnostic tool, providing insights into clinical trajectories, but their
irregular acquisition limits their utility. Existing tools for CXR
interpretation are constrained by cross-sectional analysis, failing to capture
temporal dynamics. To address this, we introduce CXR-TFT, a novel multi-modal
framework that integrates temporally <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> CXR imaging and radiology reports
with high-frequency clinical data, such as vital signs, laboratory values, and
respiratory flow sheets, to predict the trajectory of CXR findings in
critically ill patients. CXR-TFT leverages latent embeddings from a vision
encoder that are temporally aligned with hourly clinical data through
interpolation. A <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> model is then trained to predict CXR embeddings at
each hour, conditioned on previous embeddings and clinical measurements. In a
retrospective study of 20,000 ICU patients, CXR-TFT demonstrated high accuracy
in forecasting abnormal CXR findings up to 12 hours before they became
radiographically evident. This predictive capability in clinical data holds
significant potential for enhancing the management of time-sensitive conditions
like acute respiratory distress syndrome, where early intervention is crucial
and diagnoses are often delayed. By providing distinctive temporal resolution
in prognostic CXR analysis, CXR-TFT offers actionable 'whole patient' insights
that can directly improve clinical outcomes.</p>
<h2 id="grace-generative-recommendation-via-journey-aware-sparse-attention-on-chain-of-thought-tokenization">GRACE Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization</h2>
<blockquote>
<p>Authors: Luyi Ma, Wanjia Zhang, Kai Zhao, Abhishek Kulkarni, Lalitesh Morishetti, Anjana Ganesh, Ashish Ranjan, Aashika Padmanabhan, Jianpeng Xu, Jason Cho, Praveen Kanumala, Kaushiki Nag, Sumit Dutta, Kamiya Motwani, Malay Patel, Evren Korpeoglu, Sushant Kumar, Kannan Achan</p>
<p>2025-07-19</p>
<p>http://arxiv.org/abs/2507.14758v1</p>
</blockquote>
<p>Generative models have recently demonstrated strong potential in
multi-behavior recommendation systems, leveraging the expressive power of
<a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>s and tokenization to generate personalized item sequences. However,
their adoption is hindered by (1) the lack of explicit information for token
reasoning, (2) high computational costs due to quadratic attention complexity
and dense sequence representations after tokenization, and (3) limited
multi-scale modeling over user history. In this work, we propose GRACE
(Generative Recommendation via journey-aware <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> Attention on
Chain-of-thought tokEnization), a novel generative framework for multi-behavior
sequential recommendation. GRACE introduces a hybrid Chain-of-Thought (CoT)
tokenization method that encodes user-item interactions with explicit
attributes from product knowledge graphs (e.g., category, brand, price) over
semantic tokenization, enabling interpretable and behavior-aligned generation.
To address the inefficiency of standard attention, we design a Journey-Aware
Sparse Attention (JSA) mechanism, which selectively attends to compressed,
intra-, inter-, and current-context segments in the tokenized sequence.
Experiments on two real-world datasets show that GRACE significantly
outperforms state-of-the-art baselines, achieving up to +106.9% HR@10 and
+106.7% NDCG@10 improvement over the state-of-the-art baseline on the Home
domain, and +22.1% HR@10 on the Electronics domain. GRACE also reduces
attention computation by up to 48% with long sequences.</p>
<h2 id="spatial-temporal-transformer-with-curriculum-learning-for-eeg-based-emotion-recognition">Spatial-Temporal Transformer with Curriculum Learning for EEG-Based Emotion Recognition</h2>
<blockquote>
<p>Authors: Xuetao Lin, Tianhao Peng, Peihong Dai, Yu Liang, Wenjun Wu</p>
<p>2025-07-19</p>
<p>http://arxiv.org/abs/2507.14698v1</p>
</blockquote>
<p>EEG-based emotion recognition plays an important role in developing adaptive
brain-computer <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> systems, yet faces two fundamental challenges in
practical implementations: (1) effective integration of non-stationary
spatial-temporal neural patterns, (2) robust adaptation to dynamic emotional
intensity variations in real-world scenarios. This paper proposes SST-CL, a
novel framework integrating spatial-temporal <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>s with curriculum
learning. Our method introduces two core components: a spatial encoder that
models inter-channel relationships and a temporal encoder that captures
multi-scale dependencies through windowed attention mechanisms, enabling
simultaneous extraction of spatial correlations and temporal dynamics from EEG
signals. Complementing this architecture, an intensity-aware curriculum
learning strategy progressively guides training from high-intensity to
low-intensity emotional states through dynamic sample scheduling based on a
dual difficulty assessment. Comprehensive experiments on three benchmark
datasets demonstrate state-of-the-art performance across various emotional
intensity levels, with ablation studies confirming the necessity of both
architectural components and the curriculum learning mechanism.</p>
<h2 id="enabling-efficient-hardware-acceleration-of-hybrid-vision-transformer-vit-networks-at-the-edge">Enabling Efficient Hardware Acceleration of Hybrid Vision Transformer (ViT) Networks at the Edge</h2>
<blockquote>
<p>Authors: Joren Dumoulin, Pouya Houshmand, Vikram Jain, Marian Verhelst</p>
<p>2025-07-19</p>
<p>http://arxiv.org/abs/2507.14651v1</p>
</blockquote>
<p>Hybrid vision <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>s combine the elements of conventional neural
networks (NN) and vision <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>s (ViT) to enable lightweight and accurate
detection. However, several challenges remain for their efficient deployment on
resource-constrained edge devices. The hybrid models suffer from a widely
diverse set of NN layer types and large intermediate data tensors, hampering
efficient hardware <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a>. To enable their execution at the edge, this
paper proposes innovations across the hardware-scheduling stack: a.) At the
lowest level, a configurable PE array supports all hybrid ViT layer types; b.)
temporal loop re-ordering within one layer, enabling hardware support for
normalization and softmax layers, minimizing on-chip data transfers; c.)
further scheduling optimization employs layer fusion across inverted bottleneck
layers to drastically reduce off-chip memory transfers. The resulting
accelerator is implemented in 28nm CMOS, achieving a peak energy efficiency of
1.39 TOPS/W at 25.6 GMACs/s.</p>
<h2 id="linear-relational-decoding-of-morphology-in-language-models">Linear Relational Decoding of Morphology in Language Models</h2>
<blockquote>
<p>Authors: Eric Xia, Jugal Kalita</p>
<p>2025-07-19</p>
<p>http://arxiv.org/abs/2507.14640v1</p>
</blockquote>
<p>A two-part affine approximation has been found to be a good approximation for
<a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> computations over certain subject object relations. Adapting the
Bigger Analogy Test Set, we show that the linear transformation Ws, where s is
a middle layer representation of a subject token and W is derived from model
derivatives, is also able to accurately reproduce final object states for many
relations. This linear technique is able to achieve 90% faithfulness on
morphological relations, and we show similar findings multi-lingually and
across models. Our findings indicate that some conceptual relationships in
language models, such as morphology, are readily interpretable from latent
space, and are <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a>ly encoded by cross-layer linear transformations.</p>
<h2 id="kinform-kinetics-informed-feature-optimised-representation-models-for-enzyme-k_cat-and-k_m-prediction">KinForm Kinetics Informed Feature Optimised Representation Models for Enzyme <script type="math/tex">k_{cat}</script> and <script type="math/tex">K_{M}</script> Prediction</h2>
<blockquote>
<p>Authors: Saleh Alwer, Ronan Fleming</p>
<p>2025-07-19</p>
<p>http://arxiv.org/abs/2507.14639v1</p>
</blockquote>
<p>Kinetic parameters such as the turnover number (<script type="math/tex">k_{cat}</script>) and Michaelis
constant (<script type="math/tex">K_{\mathrm{M}}</script>) are essential for modelling enzymatic activity but
experimental data remains limited in scale and diversity. Previous methods for
predicting enzyme kinetics typically use mean-pooled residue embeddings from a
single protein language model to represent the protein. We present KinForm, a
machine learning framework designed to improve predictive accuracy and
generalisation for kinetic parameters by optimising protein feature
representations. KinForm combines several residue-level embeddings
(Evolutionary Scale Modeling Cambrian, Evolutionary Scale Modeling 2, and
ProtT5-XL-UniRef50), taken from empirically selected intermediate <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>
layers and applies weighted pooling based on per-residue binding-site
probability. To counter the resulting high dimensionality, we apply
dimensionality reduction using principal--component analysis (PCA) on
concatenated protein features, and rebalance the training data via a
similarity-based oversampling strategy. KinForm outperforms baseline methods on
two benchmark datasets. Improvements are most pronounced in low sequence
similarity bins. We observe improvements from binding-site probability pooling,
intermediate-layer selection, PCA, and oversampling of low-identity proteins.
We also find that removing sequence <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a> between folds provides a more
realistic evaluation of generalisation and should be the standard over random
splitting when benchmarking kinetic prediction models.</p>
<h2 id="agentic-satellite-augmented-low-altitude-economy-and-terrestrial-networks-a-survey-on-generative-approaches">Agentic Satellite-Augmented Low-Altitude Economy and Terrestrial Networks A Survey on Generative Approaches</h2>
<blockquote>
<p>Authors: Xiaozheng Gao, Yichen Wang, Bosen Liu, Xiao Zhou, Ruichen Zhang, Jiacheng Wang, Dusit Niyato, Dong In Kim, Abbas Jamalipour, Chau Yuen, Jianping An, Kai Yang</p>
<p>2025-07-19</p>
<p>http://arxiv.org/abs/2507.14633v1</p>
</blockquote>
<p>The development of satellite-augmented low-altitude economy and terrestrial
networks (SLAETNs) demands intelligent and autonomous systems that can operate
reliably across heterogeneous, dynamic, and mission-critical environments. To
address these challenges, this survey focuses on enabling agentic artificial
intelligence (AI), that is, artificial agents capable of perceiving, reasoning,
and acting, through generative AI (GAI) and large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s). We
begin by introducing the architecture and characteristics of SLAETNs, and
analyzing the challenges that arise in integrating satellite, aerial, and
terrestrial components. Then, we present a model-driven foundation by
systematically reviewing five major categories of generative models:
variational autoencoders (VAEs), generative adversarial networks (GANs),
generative diffusion models (GDMs), <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>-based models (TBMs), and <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s.
Moreover, we provide a comparative analysis to highlight their generative
mechanisms, capabilities, and deployment trade-offs within SLAETNs. Building on
this foundation, we examine how these models empower agentic functions across
three domains: <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> enhancement, security and privacy protection, and
intelligent satellite tasks. Finally, we outline key future directions for
building scalable, adaptive, and trustworthy generative agents in SLAETNs. This
survey aims to provide a unified understanding and actionable reference for
advancing agentic AI in next-generation integrated networks.</p>
<h2 id="efficient-llm-inference-bandwidth-compute-synchronization-and-capacity-are-all-you-need">Efficient LLM Inference Bandwidth, Compute, Synchronization, and Capacity are all you need</h2>
<blockquote>
<p>Authors: Michael Davies, Neal Crago, Karthikeyan Sankaralingam, Christos Kozyrakis</p>
<p>2025-07-18</p>
<p>http://arxiv.org/abs/2507.14397v1</p>
</blockquote>
<p>This paper presents a limit study of <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>-based large language model
(<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>) inference, focusing on the fundamental performance bottlenecks imposed by
memory bandwidth, memory capacity, and synchronization overhead in distributed
inference systems. We develop a hardware-agnostic performance model that
abstracts away implementation details, enabling the analysis of a wide range of
current and near-future hardware technologies. Our analysis spans from current
HBM3 memory technology used in AI accelerators like GPUs and TPUs to systems
based on advanced HBM4 and advanced 3D-stacked DRAM technology. It also covers
SRAM-based designs and scaling techniques from distributed clusters with
varying numbers of chips to wafer-scale integration. Our key findings for
auto-regressive decoding are: i) serving <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s requires 100s of GB per server to
serve a model instance; ii) high memory bandwidth is critical for high per-user
throughput; iii) exposed synchronization latencies to achieve collective
<a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> must be around 1us else they make the memory bandwidth
ineffective; iv) DRAM-based designs have a fundamental advantage in terms of
system-level efficiency as measured in throughput per cost or watt; and v)
hardware designs can easily reach 2000+ user token/sec but getting to 10,000+
tokens/sec will need smaller models, smaller context, or other forms of
algorithmic advances. This study provides valuable insights into the
fundamental performance limits of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> inference, highlighting the potential
benefits of future hardware advancements and guiding the optimization of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>
deployment strategies.</p>
<h2 id="characterizing-communication-patterns-in-distributed-large-language-model-inference">Characterizing Communication Patterns in Distributed Large Language Model Inference</h2>
<blockquote>
<p>Authors: Lang Xu, Kaushik Kandadi Suresh, Quentin Anthony, Nawras Alnaasan, Dhabaleswar K. Panda</p>
<p>2025-07-18</p>
<p>http://arxiv.org/abs/2507.14392v1</p>
</blockquote>
<p>Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) built on <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> architectures have
transformed natural language processing, achieving remarkable performance
across diverse applications. While distributed inference frameworks enable
practical deployment of these models, inter-GPU <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> creates
significant performance constraints that limit service quality in real-world
systems. This paper investigates <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> dynamics in distributed <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>
serving-analyzing how various parallelization approaches coordinate data
exchange between GPU workers during inference. We study dense <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>-based
models as representative examples of contemporary architectures widely used in
operational deployments. Our work combines detailed profiling measurements with
predictive analytical models to characterize <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> behavior across
different parallelization configurations. Results show that tensor parallelism
incurs substantial network overhead but delivers superior response times for
brief sequences, pipeline parallelism minimizes data transfer requirements
while increasing total latency, and combined approaches demand careful tuning
to achieve balanced performance. These insights offer practical recommendations
for selecting appropriate parallelization schemes in production <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> services
and identify key opportunities for optimizing inference frameworks and
<a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> infrastructure.</p>
<h2 id="dpmt-dual-process-multi-scale-theory-of-mind-framework-for-real-time-human-ai-collaboration">DPMT Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration</h2>
<blockquote>
<p>Authors: Xiyun Li, Yining Ding, Yuhua Jiang, Yunlong Zhao, Runpeng Xie, Shuang Xu, Yuanhua Ni, Yiqin Yang, Bo Xu</p>
<p>2025-07-18</p>
<p>http://arxiv.org/abs/2507.14088v1</p>
</blockquote>
<p>Real-time human-artificial intelligence (AI) collaboration is crucial yet
challenging, especially when AI agents must adapt to diverse and unseen human
behaviors in dynamic scenarios. Existing large language model (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>) agents
often fail to accurately model the complex human mental characteristics such as
domain intentions, especially in the absence of direct <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>. To
address this limitation, we propose a novel dual process multi-scale theory of
mind (DPMT) framework, drawing inspiration from cognitive science dual process
theory. Our DPMT framework incorporates a multi-scale theory of mind (ToM)
module to facilitate robust human partner modeling through mental
characteristic reasoning. Experimental results demonstrate that DPMT
significantly enhances human-AI collaboration, and ablation studies further
validate the contributions of our multi-scale ToM in the slow system.</p>
<h2 id="kroma-ontology-matching-with-knowledge-retrieval-and-large-language-models">KROMA Ontology Matching with Knowledge Retrieval and Large Language Models</h2>
<blockquote>
<p>Authors: Lam Nguyen, Erika Barcelos, Roger French, Yinghui Wu</p>
<p>2025-07-18</p>
<p>http://arxiv.org/abs/2507.14032v1</p>
</blockquote>
<p>Ontology Matching (OM) is a cornerstone task of semantic interoperability,
yet existing systems often rely on handcrafted rules or specialized models with
limited adaptability. We present KROMA, a novel OM framework that harnesses
Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) within a Retrieval-Augmented Generation (RAG)
pipeline to dynamically enrich the semantic context of OM tasks with
structural, lexical, and definitional knowledge. To optimize both performance
and efficiency, KROMA integrates a bisimilarity-based concept matching and a
lightweight ontology refinement step, which prune candidate concepts and
substantially reduce the <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> overhead from invoking <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s. Through
experiments on multiple benchmark datasets, we show that integrating knowledge
retrieval with context-augmented <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s significantly enhances ontology matching,
outperforming both classic OM systems and cutting-edge <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-based approaches
while keeping <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> overhead comparable. Our study highlights the
feasibility and benefit of the proposed optimization techniques (targeted
knowledge retrieval, prompt enrichment, and ontology refinement) for ontology
matching at scale.</p>
<h2 id="loopserve-an-adaptive-dual-phase-llm-inference-acceleration-system-for-multi-turn-dialogues">LoopServe An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues</h2>
<blockquote>
<p>Authors: Haoyang Li, Zhanchao Xu, Yiming Li, Xuejia Chen, Darian Li, Anxin Tian, Qingfa Xiao, Cheng Deng, Jun Wang, Qing Li, Lei Chen, Mingxuan Yuan</p>
<p>2025-07-18</p>
<p>http://arxiv.org/abs/2507.13681v1</p>
</blockquote>
<p>Multi-turn dialogues are essential in many real-world applications of large
language models, such as chatbots and virtual assistants. As conversation
histories become longer, existing large language models face increasing
computational and memory challenges, which hinder their ability to provide
efficient and responsive interactions. Most current <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> methods either
compress the context or optimize key value caching, but they often rely on
fixed or position-based heuristics that do not adapt well to the dynamic and
unpredictable patterns found in actual multi-turn conversations. In this paper,
we present LoopServe, an adaptive dual-phase inference <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> framework
for large language models in multi-turn dialogues. LoopServe introduces two
main innovations. First, it performs online sparsification during the
prefilling phase by dynamically selecting the most important parts of the
attention matrix for each new input. Second, it uses progressive key value
compression during decoding by adaptively maintaining a relevant and efficient
cache based on the most recently generated output tokens. We also propose a
\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s}{new
benchmark} with eleven multi-turn datasets that reflect realistic query
positions and conversational dependencies. Extensive experiments demonstrate
that LoopServe consistently achieves superior effectiveness compared to
existing baselines and significantly accelerates <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> inference across a wide
range of long-context dialogue tasks.</p>
              
  <!-- Giscus è¯è®ºç³»ç» - åªå¨ notes æä»¶å¤¹ä¸æ¾ç¤º -->
<script>
  // ä½¿ç¨ JavaScript æ¥å¤æ­ URL è·¯å¾
  if (window.location.pathname.includes('/notes/')) {
    document.write(`
      <div id="giscus-container" style="margin-top: 2rem; padding-top: 1rem; border-top: 1px solid #e1e4e8;">
        <script src="https://giscus.app/client.js"
                data-repo="hustzxd/EfficientPaper"
                data-repo-id="R_kgDOKVYtOg"
                data-category="General"
                data-category-id="DIC_kwDOKVYtOs4CukCv"
                data-mapping="pathname"
                data-strict="0"
                data-reactions-enabled="1"
                data-emit-metadata="0"
                data-input-position="bottom"
                data-theme="light"
                data-lang="zh-CN"
                crossorigin="anonymous"
                async>
        <\/script>
      </div>
    `);
  }
</script>

            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../../2025-12-26/" class="btn btn-neutral float-left" title="2025-12-26"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../2025-08-01/" class="btn btn-neutral float-right" title="2025-08-01">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../../2025-12-26/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../2025-08-01/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../../..";</script>
    <script src="../../../js/theme_extra.js"></script>
    <script src="../../../js/theme.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-clike.min.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
      <script src="../../../js/prism-prototxt.js"></script>
      <script src="../../../js/preview.js"></script>
      <script src="../../../js/back-to-top.js"></script>
      <script src="../../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
</script></body>
</html>
