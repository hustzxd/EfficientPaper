# 2025-03-21

# Table of Contents
* [FP4DiT Towards Effective Floating Point Quantization for Diffusion Transformers](#FP4DiT-Towards-Effective-Floating-Point-Quantization-for-Diffusion-Transformers)
* [An extensive simulation study evaluating the interaction of resampling techniques across multiple causal discovery contexts](#An-extensive-simulation-study-evaluating-the-interaction-of-resampling-techniques-across-multiple-causal-discovery-contexts)
* [Brunovsky Riccati Recursion for Linear Model Predictive Control](#Brunovsky-Riccati-Recursion-for-Linear-Model-Predictive-Control)
* [Kolmogorov-Arnold Network for Transistor Compact Modeling](#Kolmogorov-Arnold-Network-for-Transistor-Compact-Modeling)
* [Optimizing Retrieval Strategies for Financial Question Answering Documents in Retrieval-Augmented Generation Systems](#Optimizing-Retrieval-Strategies-for-Financial-Question-Answering-Documents-in-Retrieval-Augmented-Generation-Systems)
* [A Spectral Approach to Optimal Control of the Fokker-Planck Equation](#A-Spectral-Approach-to-Optimal-Control-of-the-Fokker-Planck-Equation)
* [xMOD Cross-Modal Distillation for 2D/3D Multi-Object Discovery from 2D motion](#xMOD-Cross-Modal-Distillation-for-2D/3D-Multi-Object-Discovery-from-2D-motion)
* [QEA An Accelerator for Quantum Circuit Simulation with Resources Efficiency and Flexibility](#QEA-An-Accelerator-for-Quantum-Circuit-Simulation-with-Resources-Efficiency-and-Flexibility)
* [Communication-Efficient Distributed On-Device LLM Inference Over Wireless Networks](#Communication-Efficient-Distributed-On-Device-LLM-Inference-Over-Wireless-Networks)
* [Exploring the Limits of KV Cache Compression in Visual Autoregressive Transformers](#Exploring-the-Limits-of-KV-Cache-Compression-in-Visual-Autoregressive-Transformers)
* [ClimateGS Real-Time Climate Simulation with 3D Gaussian Style Transfer](#ClimateGS-Real-Time-Climate-Simulation-with-3D-Gaussian-Style-Transfer)
* [Fake Runs, Real Fixes -- Analyzing xPU Performance Through Simulation](#Fake-Runs,-Real-Fixes----Analyzing-xPU-Performance-Through-Simulation)
* [NeCTAr A Heterogeneous RISC-V SoC for Language Model Inference in Intel 16](#NeCTAr-A-Heterogeneous-RISC-V-SoC-for-Language-Model-Inference-in-Intel-16)
* [SplatVoxel History-Aware Novel View Streaming without Temporal Training](#SplatVoxel-History-Aware-Novel-View-Streaming-without-Temporal-Training)
* [Towards More Economical Context-Augmented LLM Generation by Reusing Stored KV Cache](#Towards-More-Economical-Context-Augmented-LLM-Generation-by-Reusing-Stored-KV-Cache)
* [Deeply Supervised Flow-Based Generative Models](#Deeply-Supervised-Flow-Based-Generative-Models)
* [Engineering Scientific Assistants using Interactive Structured Induction of Programs](#Engineering-Scientific-Assistants-using-Interactive-Structured-Induction-of-Programs)
* [Sequence Analysis Using the Bezier Curve](#Sequence-Analysis-Using-the-Bezier-Curve)
* [Retrospective A CORDIC Based Configurable Activation Function for NN Applications](#Retrospective-A-CORDIC-Based-Configurable-Activation-Function-for-NN-Applications)
* [Quantization-Free Autoregressive Action Transformer](#Quantization-Free-Autoregressive-Action-Transformer)
* [The EXOD search for faint transients in XMM-Newton observations. Part II](#The-EXOD-search-for-faint-transients-in-XMM-Newton-observations.-Part-II)
* [SCJD Sparse Correlation and Joint Distillation for Efficient 3D Human Pose Estimation](#SCJD-Sparse-Correlation-and-Joint-Distillation-for-Efficient-3D-Human-Pose-Estimation)
* [Growing a Twig to Accelerate Large Vision-Language Models](#Growing-a-Twig-to-Accelerate-Large-Vision-Language-Models)
* [Fast Autoregressive Video Generation with Diagonal Decoding](#Fast-Autoregressive-Video-Generation-with-Diagonal-Decoding)
* [A-SCoRe Attention-based Scene Coordinate Regression for wide-ranging scenarios](#A-SCoRe-Attention-based-Scene-Coordinate-Regression-for-wide-ranging-scenarios)
* [COMMConcentrated Margin Maximization for Robust Document-Level Relation Extraction](#COMMConcentrated-Margin-Maximization-for-Robust-Document-Level-Relation-Extraction)
* [Enabling Inclusive Systematic Reviews Incorporating Preprint Articles with Large Language Model-Driven Evaluations](#Enabling-Inclusive-Systematic-Reviews-Incorporating-Preprint-Articles-with-Large-Language-Model-Driven-Evaluations)
* [Automatic MILP Model Construction for Multi-Robot Task Allocation and Scheduling Based on Large Language Models](#Automatic-MILP-Model-Construction-for-Multi-Robot-Task-Allocation-and-Scheduling-Based-on-Large-Language-Models)
* [SMILE a Scale-aware Multiple Instance Learning Method for Multicenter STAS Lung Cancer Histopathology Diagnosis](#SMILE-a-Scale-aware-Multiple-Instance-Learning-Method-for-Multicenter-STAS-Lung-Cancer-Histopathology-Diagnosis)
* [Mitigating KV Cache Competition to Enhance User Experience in LLM Inference](#Mitigating-KV-Cache-Competition-to-Enhance-User-Experience-in-LLM-Inference)
* [Single Sparse Graph Enhanced Expectation Propagation Algorithm Design for Uplink MIMO-SCMA](#Single-Sparse-Graph-Enhanced-Expectation-Propagation-Algorithm-Design-for-Uplink-MIMO-SCMA)
* [MaTVLM Hybrid Mamba-Transformer for Efficient Vision-Language Modeling](#MaTVLM-Hybrid-Mamba-Transformer-for-Efficient-Vision-Language-Modeling)
* [Scale Efficient Training for Large Datasets](#Scale-Efficient-Training-for-Large-Datasets)
* [Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning](#Mitigating-Visual-Forgetting-via-Take-along-Visual-Conditioning-for-Multi-modal-Long-CoT-Reasoning)
* [LIMCA LLM for Automating Analog In-Memory Computing Architecture Design Exploration](#LIMCA-LLM-for-Automating-Analog-In-Memory-Computing-Architecture-Design-Exploration)
* [$φ$-Decoding Adaptive Foresight Sampling for Balanced Inference-Time Exploration and Exploitation](#$φ$-Decoding-Adaptive-Foresight-Sampling-for-Balanced-Inference-Time-Exploration-and-Exploitation)
* [A Design of Denser-Graph-Frequency Graph Fourier Frames for Graph Signal Analysis](#A-Design-of-Denser-Graph-Frequency-Graph-Fourier-Frames-for-Graph-Signal-Analysis)
* [Unprecedented superionicity of ultra-low barrier in A0.5CoO2 (A=Li, Zn)](#Unprecedented-superionicity-of-ultra-low-barrier-in-A0.5CoO2-(A=Li,-Zn))
* [Lifting the Veil on Visual Information Flow in MLLMs Unlocking Pathways to Faster Inference](#Lifting-the-Veil-on-Visual-Information-Flow-in-MLLMs-Unlocking-Pathways-to-Faster-Inference)
* [ClusComp A Simple Paradigm for Model Compression and Efficient Finetuning](#ClusComp-A-Simple-Paradigm-for-Model-Compression-and-Efficient-Finetuning)
* [HERMES High-Performance RISC-V Memory Hierarchy for ML Workloads](#HERMES-High-Performance-RISC-V-Memory-Hierarchy-for-ML-Workloads)
* [ROMA a Read-Only-Memory-based Accelerator for QLoRA-based On-Device LLM](#ROMA-a-Read-Only-Memory-based-Accelerator-for-QLoRA-based-On-Device-LLM)
* [ML-SpecQD Multi-Level Speculative Decoding with Quantized Drafts](#ML-SpecQD-Multi-Level-Speculative-Decoding-with-Quantized-Drafts)
* [Verification Learning Make Unsupervised Neuro-Symbolic System Feasible](#Verification-Learning-Make-Unsupervised-Neuro-Symbolic-System-Feasible)
* [Safeguarding LLM Embeddings in End-Cloud Collaboration via Entropy-Driven Perturbation](#Safeguarding-LLM-Embeddings-in-End-Cloud-Collaboration-via-Entropy-Driven-Perturbation)
* [Efficient noise tailoring and detection of hypergraph states using Clifford circuits](#Efficient-noise-tailoring-and-detection-of-hypergraph-states-using-Clifford-circuits)
* [ACT360 An Efficient 360-Degree Action Detection and Summarization Framework for Mission-Critical Training and Debriefing](#ACT360-An-Efficient-360-Degree-Action-Detection-and-Summarization-Framework-for-Mission-Critical-Training-and-Debriefing)
* [Edge dependence of the Josephson current in the quantum Hall regime](#Edge-dependence-of-the-Josephson-current-in-the-quantum-Hall-regime)
* [TransDiff Diffusion-Based Method for Manipulating Transparent Objects Using a Single RGB-D Image](#TransDiff-Diffusion-Based-Method-for-Manipulating-Transparent-Objects-Using-a-Single-RGB-D-Image)
* [Survival Analysis with Machine Learning for Predicting Li-ion Battery Remaining Useful Life](#Survival-Analysis-with-Machine-Learning-for-Predicting-Li-ion-Battery-Remaining-Useful-Life)
* [LLM-Mediated Guidance of MARL Systems](#LLM-Mediated-Guidance-of-MARL-Systems)
* [Single-Carrier Waveform Design for Joint Sensing and Communication](#Single-Carrier-Waveform-Design-for-Joint-Sensing-and-Communication)
* [SAUCE Selective Concept Unlearning in Vision-Language Models with Sparse Autoencoders](#SAUCE-Selective-Concept-Unlearning-in-Vision-Language-Models-with-Sparse-Autoencoders)
* [BFANet Revisiting 3D Semantic Segmentation with Boundary Feature Analysis](#BFANet-Revisiting-3D-Semantic-Segmentation-with-Boundary-Feature-Analysis)
* [CAKE Cascading and Adaptive KV Cache Eviction with Layer Preferences](#CAKE-Cascading-and-Adaptive-KV-Cache-Eviction-with-Layer-Preferences)
* [LazyMAR Accelerating Masked Autoregressive Models via Feature Caching](#LazyMAR-Accelerating-Masked-Autoregressive-Models-via-Feature-Caching)
* [Enhancing Visual Representation with Textual Semantics Textual Semantics-Powered Prototypes for Heterogeneous Federated Learning](#Enhancing-Visual-Representation-with-Textual-Semantics-Textual-Semantics-Powered-Prototypes-for-Heterogeneous-Federated-Learning)
* [When neural implant meets multimodal LLM A dual-loop system for neuromodulation and naturalistic neuralbehavioral research](#When-neural-implant-meets-multimodal-LLM-A-dual-loop-system-for-neuromodulation-and-naturalistic-neuralbehavioral-research)
* [Leveraging Vision Capabilities of Multimodal LLMs for Automated Data Extraction from Plots](#Leveraging-Vision-Capabilities-of-Multimodal-LLMs-for-Automated-Data-Extraction-from-Plots)
* [Changing Base Without Losing Pace A GPU-Efficient Alternative to MatMul in DNNs](#Changing-Base-Without-Losing-Pace-A-GPU-Efficient-Alternative-to-MatMul-in-DNNs)
* [Bridging Textual-Collaborative Gap through Semantic Codes for Sequential Recommendation](#Bridging-Textual-Collaborative-Gap-through-Semantic-Codes-for-Sequential-Recommendation)
* [PLM Efficient Peripheral Language Models Hardware-Co-Designed for Ubiquitous Computing](#PLM-Efficient-Peripheral-Language-Models-Hardware-Co-Designed-for-Ubiquitous-Computing)
* [SFMNet Sparse Focal Modulation for 3D Object Detection](#SFMNet-Sparse-Focal-Modulation-for-3D-Object-Detection)
* [Diffusion Dynamics Models with Generative State Estimation for Cloth Manipulation](#Diffusion-Dynamics-Models-with-Generative-State-Estimation-for-Cloth-Manipulation)
* [Fraesormer Learning Adaptive Sparse Transformer for Efficient Food Recognition](#Fraesormer-Learning-Adaptive-Sparse-Transformer-for-Efficient-Food-Recognition)
* [Key, Value, Compress A Systematic Exploration of KV Cache Compression Techniques](#Key,-Value,-Compress-A-Systematic-Exploration-of-KV-Cache-Compression-Techniques)
* [Tensor Convolutional Network for Higher-Order Interaction Prediction in Sparse Tensors](#Tensor-Convolutional-Network-for-Higher-Order-Interaction-Prediction-in-Sparse-Tensors)
* [ASMA-Tune Unlocking LLMs' Assembly Code Comprehension via Structural-Semantic Instruction Tuning](#ASMA-Tune-Unlocking-LLMs'-Assembly-Code-Comprehension-via-Structural-Semantic-Instruction-Tuning)
* [BioMamba Leveraging Spectro-Temporal Embedding in Bidirectional Mamba for Enhanced Biosignal Classification](#BioMamba-Leveraging-Spectro-Temporal-Embedding-in-Bidirectional-Mamba-for-Enhanced-Biosignal-Classification)
* [Similarity-Aware Token Pruning Your VLM but Faster](#Similarity-Aware-Token-Pruning-Your-VLM-but-Faster)
* [Multi-View Node Pruning for Accurate Graph Representation](#Multi-View-Node-Pruning-for-Accurate-Graph-Representation)
* [Text Compression for Efficient Language Generation](#Text-Compression-for-Efficient-Language-Generation)
* [TransiT Transient Transformer for Non-line-of-sight Videography](#TransiT-Transient-Transformer-for-Non-line-of-sight-Videography)
* [When Do Transformers Outperform Feedforward and Recurrent Networks? A Statistical Perspective](#When-Do-Transformers-Outperform-Feedforward-and-Recurrent-Networks?-A-Statistical-Perspective)
* [PrivacyScalpel Enhancing LLM Privacy via Interpretable Feature Intervention with Sparse Autoencoders](#PrivacyScalpel-Enhancing-LLM-Privacy-via-Interpretable-Feature-Intervention-with-Sparse-Autoencoders)
* [FastVID Dynamic Density Pruning for Fast Video Large Language Models](#FastVID-Dynamic-Density-Pruning-for-Fast-Video-Large-Language-Models)
* [Learnable Group Transform Enhancing Genotype-to-Phenotype Prediction for Rice Breeding with Small, Structured Datasets](#Learnable-Group-Transform-Enhancing-Genotype-to-Phenotype-Prediction-for-Rice-Breeding-with-Small,-Structured-Datasets)
* [Multi-constraint Graph Partitioning Problems Via Recursive Bipartition Algorithm Based on Subspace Minimization Conjugate Gradient Method](#Multi-constraint-Graph-Partitioning-Problems-Via-Recursive-Bipartition-Algorithm-Based-on-Subspace-Minimization-Conjugate-Gradient-Method)
* [Towards Extreme Pruning of LLMs with Plug-and-Play Mixed Sparsity](#Towards-Extreme-Pruning-of-LLMs-with-Plug-and-Play-Mixed-Sparsity)
* [X-EcoMLA Upcycling Pre-Trained Attention into MLA for Efficient and Extreme KV Compression](#X-EcoMLA-Upcycling-Pre-Trained-Attention-into-MLA-for-Efficient-and-Extreme-KV-Compression)
* [Don't Forget It! Conditional Sparse Autoencoder Clamping Works for Unlearning](#Don't-Forget-It!-Conditional-Sparse-Autoencoder-Clamping-Works-for-Unlearning)
* [Modeling and Optimization for Flexible Cylindrical Arrays-Enabled Wireless Communications](#Modeling-and-Optimization-for-Flexible-Cylindrical-Arrays-Enabled-Wireless-Communications)
* [Limits of KV Cache Compression for Tensor Attention based Autoregressive Transformers](#Limits-of-KV-Cache-Compression-for-Tensor-Attention-based-Autoregressive-Transformers)
* [Quantifying Interpretability in CLIP Models with Concept Consistency](#Quantifying-Interpretability-in-CLIP-Models-with-Concept-Consistency)
* [DeepSeek Powered Solid Dosage Formulation Design and Development](#DeepSeek-Powered-Solid-Dosage-Formulation-Design-and-Development)
* [Stochastic resolution of identity to CC2 for large systems Oscillator strength and ground state gradient calculations](#Stochastic-resolution-of-identity-to-CC2-for-large-systems-Oscillator-strength-and-ground-state-gradient-calculations)
* [Exploration of metastable A-site-ordered perovskites (Ca,Ba)FeO3-δ by computationally-guided multi-step synthesis](#Exploration-of-metastable-A-site-ordered-perovskites-(Ca,Ba)FeO3-δ-by-computationally-guided-multi-step-synthesis)


## FP4DiT Towards Effective Floating Point Quantization for Diffusion Transformers

>Authors: Ruichen Chen, Keith G. Mills, Di Niu

>2025-03-19

> http://arxiv.org/abs/2503.15465v1

Diffusion Models (DM) have revolutionized the text-to-image visual generation
process. However, the large computational cost and model footprint of DMs
hinders practical deployment, especially on edge devices. Post-training
**quantization** (PTQ) is a lightweight method to alleviate these burdens without
the need for training or fine-tuning. While recent DM PTQ methods achieve W4A8
on integer-based PTQ, two key limitations remain: First, while most existing DM
PTQ methods evaluate on classical DMs like Stable Diffusion XL, 1.5 or earlier,
which use convolutional U-Nets, newer Diffusion Transformer (DiT) models like
the PixArt series, Hunyuan and others adopt fundamentally different transformer
backbones to achieve superior image synthesis. Second, integer (INT)
**quantization** is prevailing in DM PTQ but doesn't align well with the network
weight and activation distribution, while Floating-Point Quantization (FPQ) is
still under-investigated, yet it holds the potential to better align the weight
and activation distributions in **low-bit** settings for DiT. In response, we
introduce FP4DiT, a PTQ method that leverages FPQ to achieve W4A6 **quantization**.
Specifically, we extend and generalize the Adaptive Rounding PTQ technique to
adequately calibrate weight **quantization** for FPQ and demonstrate that DiT
activations depend on input patch data, necessitating robust online activation
**quantization** techniques. Experimental results demonstrate that FP4DiT
outperforms integer-based PTQ at W4A6 and W4A8 precision and generates
convincing visual content on PixArt-$\alpha$, PixArt-$\Sigma$ and Hunyuan in
terms of several T2I metrics such as HPSv2 and CLIP.


## An extensive simulation study evaluating the interaction of resampling techniques across multiple causal discovery contexts

>Authors: Ritwick Banerjee, Bryan Andrews, Erich Kummerfeld

>2025-03-19

> http://arxiv.org/abs/2503.15436v1

Despite the accelerating presence of exploratory causal analysis in modern
science and medicine, the available non-experimental methods for validating
causal models are not well characterized. One of the most popular methods is to
evaluate the stability of model features after resampling the data, similar to
resampling methods for estimating confidence intervals in statistics. Many
aspects of this approach have received little to no attention, however, such as
whether the choice of resampling method should depend on the sample size,
algorithms being used, or algorithm tuning parameters. We present theoretical
results proving that certain resampling methods closely emulate the assignment
of specific values to algorithm tuning parameters. We also report the results
of extensive simulation experiments, which verify the theoretical result and
provide substantial data to aid researchers in further characterizing
resampling in the context of causal discovery analysis. Together, the
theoretical work and simulation results provide specific guidance on how
resampling methods and tuning parameters should be selected in practice.


## Brunovsky Riccati Recursion for Linear Model Predictive Control

>Authors: Shaohui Yang, Toshiyuki Ohtsuka, Colin N. Jones

>2025-03-19

> http://arxiv.org/abs/2503.15271v1

In almost all algorithms for Model Predictive Control (MPC), the most
time-consuming step is to solve some form of Linear Quadratic (LQ) Optimal
Control Problem (OCP) repeatedly. The commonly recognized best option for this
is a Riccati recursion based solver, which has a time complexity of
$\mathcal{O}(N(n_x^3 + n_x^2 n_u + n_x n_u^2 + n_u^3))$. In this paper, we
propose a novel \textit{Brunovsky Riccati Recursion} algorithm to solve LQ OCPs
for Linear Time Invariant (LTI) systems. The algorithm transforms the system
into Brunovsky form, formulates a new LQ cost (and constraints, if any) in
Brunovsky coordinates, performs the Riccati recursion there, and converts the
solution back. Due to the **sparsity** (block-diagonality and zero-one pattern per
block) of Brunovsky form and the data parallelism introduced in the cost,
constraints, and solution transformations, the time complexity of the new
method is greatly reduced to $\mathcal{O}(n_x^3 + N(n_x^2 n_u + n_x n_u^2 +
n_u^3))$ if $N$ threads/cores are available for parallel computing.


## Kolmogorov-Arnold Network for Transistor Compact Modeling

>Authors: Rodion Novkin, Hussam Amrouch

>2025-03-19

> http://arxiv.org/abs/2503.15209v1

Neural network (NN)-based transistor compact modeling has recently emerged as
a transformative solution for accelerating device modeling and SPICE circuit
simulations. However, conventional NN architectures, despite their widespread
adoption in state-of-the-art methods, primarily function as black-box problem
solvers. This lack of interpretability significantly limits their capacity to
extract and convey meaningful insights into learned data patterns, posing a
major barrier to their broader adoption in critical modeling tasks. This work
introduces, for the first time, Kolmogorov-Arnold network (KAN) for the
transistor - a groundbreaking NN architecture that seamlessly integrates
interpretability with high precision in physics-based function modeling. We
systematically evaluate the performance of KAN and Fourier KAN for FinFET
compact modeling, benchmarking them against the golden industry-standard
compact model and the widely used MLP architecture. Our results reveal that KAN
and FKAN consistently achieve superior prediction accuracy for critical figures
of merit, including gate current, drain charge, and source charge. Furthermore,
we demonstrate and improve the unique ability of KAN to derive symbolic
formulas from learned data patterns - a capability that not only enhances
interpretability but also facilitates in-depth transistor analysis and
optimization. This work highlights the transformative potential of KAN in
bridging the gap between interpretability and precision in NN-driven transistor
compact modeling. By providing a robust and transparent approach to transistor
modeling, KAN represents a pivotal advancement for the semiconductor industry
as it navigates the challenges of advanced technology scaling.


## Optimizing Retrieval Strategies for Financial Question Answering Documents in Retrieval-Augmented Generation Systems

>Authors: Sejong Kim, Hyunseo Song, Hyunwoo Seo, Hyunjun Kim

>2025-03-19

> http://arxiv.org/abs/2503.15191v1

Retrieval-Augmented Generation (RAG) has emerged as a promising framework to
mitigate hallucinations in Large Language Models (LLMs), yet its overall
performance is dependent on the underlying retrieval system. In the finance
domain, documents such as 10-K reports pose distinct challenges due to
domain-specific vocabulary and multi-hierarchical tabular data. In this work,
we introduce an efficient, end-to-end RAG pipeline that enhances retrieval for
financial documents through a three-phase approach: pre-retrieval, retrieval,
and post-retrieval. In the pre-retrieval phase, various query and corpus
preprocessing techniques are employed to enrich input data. During the
retrieval phase, we fine-tuned state-of-the-art (SOTA) embedding models with
domain-specific knowledge and implemented a hybrid retrieval strategy that
combines dense and **sparse** representations. Finally, the post-retrieval phase
leverages Direct Preference Optimization (DPO) training and document selection
methods to further refine the results. Evaluations on seven financial question
answering datasets-FinDER, FinQABench, FinanceBench, TATQA, FinQA, ConvFinQA,
and MultiHiertt-demonstrate substantial improvements in retrieval performance,
leading to more accurate and contextually appropriate generation. These
findings highlight the critical role of tailored retrieval techniques in
advancing the effectiveness of RAG systems for financial applications. A fully
replicable pipeline is available on GitHub:
https://github.com/seohyunwoo-0407/GAR.


## A Spectral Approach to Optimal Control of the Fokker-Planck Equation

>Authors: Dante Kalise, Lucas M. Moschen, Grigorios A. Pavliotis, Urbain Vaes

>2025-03-19

> http://arxiv.org/abs/2503.15125v1

In this paper, we present a spectral optimal control framework for
Fokker-Planck equations based on the standard ground state transformation that
maps the Fokker-Planck operator to a Schrodinger operator. Our primary
objective is to accelerate convergence toward the (unique) steady state. To
fulfill this objective, a gradient-based iterative algorithm with Pontryagin's
maximum principle and Barzilai-Borwein update is developed to compute
time-dependent controls. Numerical experiments on two-dimensional
ill-conditioned normal distributions and double-well potentials demonstrate
that our approach effectively targets slow-decaying modes, thus increasing the
spectral gap.


## xMOD Cross-Modal Distillation for 2D/3D Multi-Object Discovery from 2D motion

>Authors: Saad Lahlali, Sandra Kara, Hejer Ammar, Florian Chabot, Nicolas Granger, Hervé Le Borgne, Quoc-Cuong Pham

>2025-03-19

> http://arxiv.org/abs/2503.15022v1

Object discovery, which refers to the task of localizing objects without
human annotations, has gained significant attention in 2D image analysis.
However, despite this growing interest, it remains under-explored in 3D data,
where approaches rely exclusively on 3D motion, despite its several challenges.
In this paper, we present a novel framework that leverages advances in 2D
object discovery which are based on 2D motion to exploit the advantages of such
motion cues being more flexible and generalizable and to bridge the gap between
2D and 3D modalities. Our primary contributions are twofold: (i) we introduce
DIOD-3D, the first baseline for multi-object discovery in 3D data using 2D
motion, incorporating scene completion as an auxiliary task to enable dense
object localization from **sparse** input data; (ii) we develop xMOD, a cross-modal
training framework that integrates 2D and 3D data while always using 2D motion
cues. xMOD employs a teacher-student training paradigm across the two
modalities to mitigate confirmation bias by leveraging the domain gap. During
inference, the model supports both RGB-only and point cloud-only inputs.
Additionally, we propose a late-fusion technique tailored to our pipeline that
further enhances performance when both modalities are available at inference.
We evaluate our approach extensively on synthetic (TRIP-PD) and challenging
real-world datasets (KITTI and Waymo). Notably, our approach yields a
substantial performance improvement compared with the 2D object discovery
state-of-the-art on all datasets with gains ranging from +8.7 to +15.1 in F1@50
score. The code is available at https://github.com/CEA-LIST/xMOD


## QEA An Accelerator for Quantum Circuit Simulation with Resources Efficiency and Flexibility

>Authors: Van Duy Tran, Tuan Hai Vu, Vu Trung Duong Le, Hoai Luan Pham, Yasuhiko Nakashima

>2025-03-19

> http://arxiv.org/abs/2503.14951v1

The area of quantum circuit simulation has attracted a lot of attention in
recent years. However, due to the exponentially increasing computational costs,
assessing and validating these models on large datasets poses significant
obstacles. Despite plenty of research in quantum simulation, issues such as
memory management, system adaptability, and execution efficiency remain
unresolved. In this study, we introduce QEA, a state vector-based hardware
accelerator that overcomes these difficulties with four key improvements:
optimized memory allocation management, open PE, flexible ALU, and simplified
CX swapper. To evaluate QEA's capabilities, we implemented and evaluated it on
the AMD Alveo U280 board, which uses only 0.534 W of power. Experimental
results show that QEA is extremely flexible, supporting a wide range of quantum
circuits, has excellent fidelity, making it appropriate for standard quantum
emulators, and outperforms powerful CPUs and related works up to 153.16x better
in terms of normalized gate speed. This study has considerable potential as a
useful approach for quantum emulators in future works.


## Communication-Efficient Distributed On-Device LLM Inference Over Wireless Networks

>Authors: Kai Zhang, Hengtao He, Shenghui Song, Jun Zhang, Khaled B. Letaief

>2025-03-19

> http://arxiv.org/abs/2503.14882v1

Large language models (LLMs) have demonstrated remarkable success across
various application domains, but their enormous sizes and computational demands
pose significant challenges for deployment on resource-constrained edge
devices. To address this issue, we propose a novel distributed on-device LLM
inference framework that leverages tensor parallelism to partition the neural
network tensors (e.g., weight matrices) of one LLM across multiple edge devices
for collaborative inference. A key challenge in tensor parallelism is the
frequent all-reduce operations for aggregating intermediate layer outputs
across participating devices, which incurs significant communication overhead.
To alleviate this bottleneck, we propose an over-the-air computation (AirComp)
approach that harnesses the analog superposition property of wireless
multiple-access channels to perform fast all-reduce steps. To utilize the
heterogeneous computational capabilities of edge devices and mitigate
communication distortions, we investigate a joint model assignment and
transceiver optimization problem to minimize the average transmission error.
The resulting mixed-timescale stochastic non-convex optimization problem is
intractable, and we propose an efficient two-stage algorithm to solve it.
Moreover, we prove that the proposed algorithm converges almost surely to a
stationary point of the original problem. Comprehensive simulation results will
show that the proposed framework outperforms existing benchmark schemes,
achieving up to 5x inference speed **acceleration** and improving inference
accuracy.


## Exploring the Limits of KV Cache Compression in Visual Autoregressive Transformers

>Authors: Bo Chen, Xiaoyu Li, Yekun Ke, Yingyu Liang, Zhenmei Shi, Zhao Song

>2025-03-19

> http://arxiv.org/abs/2503.14881v1

A fundamental challenge in Visual Autoregressive models is the substantial
memory overhead required during inference to store previously generated
representations. Despite various attempts to mitigate this issue through
compression techniques, prior works have not explicitly formalized the problem
of **KV**-cache compression in this context. In this work, we take the first step
in formally defining the **KV**-cache compression problem for Visual Autoregressive
transformers. We then establish a fundamental negative result, proving that any
mechanism for sequential visual token generation under attention-based
architectures must use at least $\Omega(n^2 d)$ memory, when $d = \Omega(\log
n)$, where $n$ is the number of tokens generated and $d$ is the embedding
dimensionality. This result demonstrates that achieving truly sub-quadratic
memory usage is impossible without additional structural constraints. Our proof
is constructed via a reduction from a computational lower bound problem,
leveraging randomized embedding techniques inspired by dimensionality reduction
principles. Finally, we discuss how **sparsity** priors on visual representations
can influence memory efficiency, presenting both impossibility results and
potential directions for mitigating memory overhead.


## ClimateGS Real-Time Climate Simulation with 3D Gaussian Style Transfer

>Authors: Yuezhen Xie, Meiying Zhang, Qi Hao

>2025-03-19

> http://arxiv.org/abs/2503.14845v1

Adverse climate conditions pose significant challenges for autonomous
systems, demanding reliable perception and decision-making across diverse
environments. To better simulate these conditions, physically-based NeRF
rendering methods have been explored for their ability to generate realistic
scene representations. However, these methods suffer from slow rendering speeds
and long preprocessing times, making them impractical for real-time testing and
user interaction. This paper presents ClimateGS, a novel framework integrating
3D Gaussian representations with physical simulation to enable real-time
climate effects rendering. The novelty of this work is threefold: 1) developing
a linear transformation for 3D Gaussian photorealistic style transfer, enabling
direct modification of spherical harmonics across bands for efficient and
consistent style adaptation; 2) developing a joint training strategy for 3D
style transfer, combining supervised and self-supervised learning to accelerate
convergence while preserving original scene details; 3) developing a real-time
rendering method for climate simulation, integrating physics-based effects with
3D Gaussian to achieve efficient and realistic rendering. We evaluate ClimateGS
on MipNeRF360 and Tanks and Temples, demonstrating real-time rendering with
comparable or superior visual quality to SOTA 2D/3D methods, making it suitable
for interactive applications.


## Fake Runs, Real Fixes -- Analyzing xPU Performance Through Simulation

>Authors: Ioannis Zarkadas, Amanda Tomlinson, Asaf Cidon, Baris Kasikci, Ofir Weisse

>2025-03-18

> http://arxiv.org/abs/2503.14781v1

As models become larger, ML accelerators are a scarce resource whose
performance must be continually optimized to improve efficiency. Existing
performance analysis tools are coarse grained, and fail to capture model
performance at the machine-code level. In addition, these tools often do not
provide specific recommendations for optimizations. We present xPU-Shark, a
fine-grained methodology for analyzing ML models at the machine-code level that
provides actionable optimization suggestions. Our core insight is to use a
hardware-level simulator, an artifact of the hardware design process that we
can re-purpose for performance analysis. xPU-Shark captures traces from
production deployments running on accelerators and replays them in a modified
microarchitecture simulator to gain low-level insights into the model's
performance. We implement xPU-Shark for our in-house accelerator and used it to
analyze the performance of several of our production LLMs, revealing several
previously-unknown microarchitecture inefficiencies. Leveraging these insights,
we optimize a common communication collective by up to 15% and reduce token
generation latency by up to 4.1%.


## NeCTAr A Heterogeneous RISC-V SoC for Language Model Inference in Intel 16

>Authors: Viansa Schmulbach, Jason Kim, Ethan Gao, Lucy Revina, Nikhil Jha, Ethan Wu, Borivoje Nikolic

>2025-03-18

> http://arxiv.org/abs/2503.14708v1

This paper introduces NeCTAr (Near-Cache Transformer Accelerator), a 16nm
heterogeneous multicore RISC-V SoC for **sparse** and dense machine learning
kernels with both near-core and near-memory accelerators. A prototype chip runs
at 400MHz at 0.85V and performs matrix-vector multiplications with 109 GOPs/W.
The effectiveness of the design is demonstrated by running inference on a
**sparse** language model, ReLU-Llama.


## SplatVoxel History-Aware Novel View Streaming without Temporal Training

>Authors: Yiming Wang, Lucy Chai, Xuan Luo, Michael Niemeyer, Manuel Lagunas, Stephen Lombardi, Siyu Tang, Tiancheng Sun

>2025-03-18

> http://arxiv.org/abs/2503.14698v1

We study the problem of novel view streaming from **sparse**-view videos, which
aims to generate a continuous sequence of high-quality, temporally consistent
novel views as new input frames arrive. However, existing novel view synthesis
methods struggle with temporal coherence and visual fidelity, leading to
flickering and inconsistency. To address these challenges, we introduce
history-awareness, leveraging previous frames to reconstruct the scene and
improve quality and stability. We propose a hybrid splat-voxel feed-forward
scene reconstruction approach that combines Gaussian Splatting to propagate
information over time, with a hierarchical voxel grid for temporal fusion.
Gaussian primitives are efficiently warped over time using a motion graph that
extends 2D tracking models to 3D motion, while a **sparse** voxel transformer
integrates new temporal observations in an error-aware manner. Crucially, our
method does not require training on multi-view video datasets, which are
currently limited in size and diversity, and can be directly applied to
**sparse**-view video streams in a history-aware manner at inference time. Our
approach achieves state-of-the-art performance in both static and streaming
scene reconstruction, effectively reducing temporal artifacts and visual
artifacts while running at interactive rates (15 fps with 350ms delay) on a
single H100 GPU. Project Page: https://19reborn.github.io/SplatVoxel/


## Towards More Economical Context-Augmented LLM Generation by Reusing Stored KV Cache

>Authors: Hanchen Li, Yuhan Liu, Yihua Cheng, Kuntai Du, Junchen Jiang

>2025-03-18

> http://arxiv.org/abs/2503.14647v1

Across large language model (LLM) applications, we observe an emerging trend
for reusing **KV** caches to save the prefill delays of processing repeated input
texts in different LLM inputs. This has led to a broad design space, including
colocating stored **KV** caches with (or close to) GPUs to various **KV** cache
compression. However, a key question remains unanswered: can these delay
reductions also be economically favorable? Specifically, we ask whether a
developer can use public cloud services to store precomputed **KV** caches and
reuse them to save delay without incurring more costs in terms of compute,
storage, and network. To answer this question, we propose an validated
analytical model for the cloud cost (in compute, storage, and network) of
storing and reusing **KV** caches based on various workload parameters, such as
reuse frequency, generated text lengths, model sizes, etc. Preliminary results
show that **KV** cache reusing is able to save both delay and cloud cost across a
range of workloads with long context. And we call more efforts on building more
economical context augmented LLM by **KV** cache reusing.


## Deeply Supervised Flow-Based Generative Models

>Authors: Inkyu Shin, Chenglin Yang, Liang-Chieh Chen

>2025-03-18

> http://arxiv.org/abs/2503.14494v1

Flow based generative models have charted an impressive path across multiple
visual generation tasks by adhering to a simple principle: learning velocity
representations of a linear interpolant. However, we observe that training
velocity solely from the final layer output underutilizes the rich inter layer
representations, potentially impeding model convergence. To address this
limitation, we introduce DeepFlow, a novel framework that enhances velocity
representation through inter layer communication. DeepFlow partitions
transformer layers into balanced branches with deep supervision and inserts a
lightweight Velocity Refiner with Acceleration (VeRA) block between adjacent
branches, which aligns the intermediate velocity features within transformer
blocks. Powered by the improved deep supervision via the internal velocity
alignment, DeepFlow converges 8 times faster on ImageNet with equivalent
performance and further reduces FID by 2.6 while halving training time compared
to previous flow based models without a classifier free guidance. DeepFlow also
outperforms baselines in text to image generation tasks, as evidenced by
evaluations on MSCOCO and zero shot GenEval.


## Engineering Scientific Assistants using Interactive Structured Induction of Programs

>Authors: Shraddha Surana, Ashwin Srinivasan

>2025-03-18

> http://arxiv.org/abs/2503.14488v1

We are interested in the construction of software that can act as scientific
assistants to domain specialists. It is expected that such assistants will be
needed to accelerate the identification of ways to address complex problems
requiring urgent solutions. In this paper, our focus is not on a specific
scientific problem, but on the software-engineering of such 'science
accelerators'. Recent developments in 'No Code' techniques would seem to
suggest that scientist can simply hypothesise solutions simply by conversing
with a large language model (LLM). However, for complex scientific problems,
this seems unlikely given the current state of LLM technology. What does appear
feasible is that a software engineer can use LLMs to rapidly construct programs
for use by a domain-specialist, including the specialist's requirements
expressed in natural language. We propose the design of an interactive form of
'structured' inductive programming in which a software-engineer and an LLM
collaboratively construct an 'assistant' for a scientific data analysis. The
paper describes a simple implementation called iStrucInd that adapts a '2-way
Intelligibility' protocol to implement the interaction between the software
engineer and the LLM. We test the tool on two different non-trivial scientific
data analysis tasks. Specifically, we compare the system constructed by
iStrucInd against systems constructed manually and by Low Code/No Code methods
along dimensions of: (a) program performance; (b) program quality; and (c)
programming effort. The results show iStrucInd allows a software engineer to
develop better programs faster suggesting interactive structured induction can
play a useful role in the rapid construction of scientific assistants.


## Sequence Analysis Using the Bezier Curve

>Authors: Taslim Murad, Sarwan Ali, Murray Patterson

>2025-03-18

> http://arxiv.org/abs/2503.14574v1

The analysis of sequences (e.g., protein, DNA, and SMILES string) is
essential for disease diagnosis, biomaterial engineering, genetic engineering,
and drug discovery domains. Conventional analytical methods focus on
transforming sequences into numerical representations for applying machine
learning/deep learning-based sequence characterization. However, their efficacy
is constrained by the intrinsic nature of deep learning (DL) models, which tend
to exhibit suboptimal performance when applied to tabular data. An alternative
group of methodologies endeavors to convert biological sequences into image
forms by applying the concept of Chaos Game Representation (CGR). However, a
noteworthy drawback of these methods lies in their tendency to map individual
elements of the sequence onto a relatively small subset of designated pixels
within the generated image. The resulting **sparse** image representation may not
adequately encapsulate the comprehensive sequence information, potentially
resulting in suboptimal predictions. In this study, we introduce a novel
approach to transform sequences into images using the B\'ezier curve concept
for element mapping. Mapping the elements onto a curve enhances the sequence
information representation in the respective images, hence yielding better
DL-based classification performance. We employed different sequence datasets to
validate our system by using different classification tasks, and the results
illustrate that our B\'ezier curve method is able to achieve good performance
for all the tasks.


## Retrospective A CORDIC Based Configurable Activation Function for NN Applications

>Authors: Omkar Kokane, Gopal Raut, Salim Ullah, Mukul Lokhande, Adam Teman, Akash Kumar, Santosh Kumar Vishvakarma

>2025-03-18

> http://arxiv.org/abs/2503.14354v1

A CORDIC-based configuration for the design of Activation Functions (AF) was
previously suggested to accelerate ASIC hardware design for
resource-constrained systems by providing functional reconfigurability. Since
its introduction, this new approach for neural network **acceleration** has gained
widespread popularity, influencing numerous designs for activation functions in
both academic and commercial AI processors. In this retrospective analysis, we
explore the foundational aspects of this initiative, summarize key developments
over recent years, and introduce the DA-VINCI AF tailored for the evolving
needs of AI applications. This new generation of dynamically configurable and
precision-adjustable activation function cores promise greater adaptability for
a range of activation functions in AI workloads, including Swish, SoftMax,
SeLU, and GeLU, utilizing the Shift-and-Add CORDIC technique. The previously
presented design has been optimized for MAC, Sigmoid, and Tanh functionalities
and incorporated into ReLU AFs, culminating in an accumulative NEURIC compute
unit. These enhancements position NEURIC as a fundamental component in the
resource-efficient vector engine for the realization of AI accelerators that
focus on DNNs, RNNs/LSTMs, and Transformers, achieving a quality of results
(QoR) of 98.5%.


## Quantization-Free Autoregressive Action Transformer

>Authors: Ziyad Sheebaelhamd, Michael Tschannen, Michael Muehlebach, Claire Vernade

>2025-03-18

> http://arxiv.org/abs/2503.14259v1

Current transformer-based imitation learning approaches introduce discrete
action representations and train an autoregressive transformer decoder on the
resulting latent code. However, the initial **quantization** breaks the continuous
structure of the action space thereby limiting the capabilities of the
generative model. We propose a **quantization**-free method instead that leverages
Generative Infinite-Vocabulary Transformers (GIVT) as a direct, continuous
policy parametrization for autoregressive transformers. This simplifies the
imitation learning pipeline while achieving state-of-the-art performance on a
variety of popular simulated robotics tasks. We enhance our policy roll-outs by
carefully studying sampling algorithms, further improving the results.


## The EXOD search for faint transients in XMM-Newton observations. Part II

>Authors: Norman Khan, Erwan Quintin, Natalie A. Webb, Robbie Webbe, Maitrayee Gupta, Inés Pastor-Marazuela, Florent Castellani, Axel D. Schwope, Iris Traulsen, Ada Nebot

>2025-03-18

> http://arxiv.org/abs/2503.14208v2

The XMM-Newton observatory has accumulated a vast archive of over 17,000
X-ray observations over the last 25 years. However, the standard data
processing pipelines may fail to detect certain types of transient X-ray
sources due to their short-lived or dim nature. Identifying these transient
sources is important for understanding the full range of temporal X-ray
behaviour, as well as understanding the types of sources that could be
routinely detected by future missions such as Athena. This work aims to
reprocess XMM-Newton archival observations using newly developed dedicated
software in order to identify neglected and missed transient X-ray sources that
were not detected by the existing pipeline. We use a new approach that builds
upon previous methodologies, by transforming event lists into data cubes, which
are then searched for transient variability in short time windows. Our method
enhances the detection capabilities in the Poisson regime by accounting for the
statistical properties of **sparse** count rates, and allowing for transient search
in previously discarded periods of high background activity. Our reprocessing
efforts identified 32,247 variable sources at the 3-sigma level and 4,083
sources at the 5-sigma level in 12,926 XMM archival observations. We highlight
four noteworthy sources: A candidate quasi-periodic eruption (QPE), a new
magnetar candidate, a previously undetected Galactic hard X-ray burst and a
possible X-ray counterpart to a Galactic radio pulsar. Our method demonstrates
a new, fast, and effective way to process event list data from XMM-Newton,
which is efficient in finding rapid outburst-like or eclipsing behaviour. This
technique can be adapted for use with future telescopes, such as Athena, and
can be generalised to other photon counting instruments operating in the
low-count Poisson regime.


## SCJD Sparse Correlation and Joint Distillation for Efficient 3D Human Pose Estimation

>Authors: Weihong Chen, Xuemiao Xu, Haoxin Yang, Yi Xie, Peng Xiao, Cheng Xu, Huaidong Zhang, Pheng-Ann Heng

>2025-03-18

> http://arxiv.org/abs/2503.14097v1

Existing 3D Human Pose Estimation (HPE) methods achieve high accuracy but
suffer from computational overhead and slow inference, while knowledge
distillation methods fail to address spatial relationships between joints and
temporal correlations in multi-frame inputs. In this paper, we propose Sparse
Correlation and Joint Distillation (SCJD), a novel framework that balances
efficiency and accuracy for 3D HPE. SCJD introduces Sparse Correlation Input
Sequence Downsampling to reduce redundancy in student network inputs while
preserving inter-frame correlations. For effective knowledge transfer, we
propose Dynamic Joint Spatial Attention Distillation, which includes Dynamic
Joint Embedding Distillation to enhance the student's feature representation
using the teacher's multi-frame context feature, and Adjacent Joint Attention
Distillation to improve the student network's focus on adjacent joint
relationships for better spatial understanding. Additionally, Temporal
Consistency Distillation aligns the temporal correlations between teacher and
student networks through upsampling and global supervision. Extensive
experiments demonstrate that SCJD achieves state-of-the-art performance. Code
is available at https://github.com/wileychan/SCJD.


## Growing a Twig to Accelerate Large Vision-Language Models

>Authors: Zhenwei Shao, Mingyang Wang, Zhou Yu, Wenwen Pan, Yan Yang, Tao Wei, Hongyuan Zhang, Ning Mao, Wei Chen, Jun Yu

>2025-03-18

> http://arxiv.org/abs/2503.14075v1

Large vision-language models (VLMs) have demonstrated remarkable capabilities
in open-world multimodal understanding, yet their high computational overheads
pose great challenges for practical deployment. Some recent works have proposed
methods to accelerate VLMs by **pruning** redundant visual tokens guided by the
attention maps of VLM's early layers. Despite the success of these token
**pruning** methods, they still suffer from two major shortcomings: (i)
considerable accuracy drop due to insensitive attention signals in early
layers, and (ii) limited speedup when generating long responses (e.g., 30
tokens). To address the limitations above, we present TwigVLM -- a simple and
general architecture by growing a lightweight twig upon an early layer of the
base VLM. Compared with most existing VLM **acceleration** methods purely based on
visual token **pruning**, our TwigVLM not only achieves better accuracy retention
by employing a twig-guided token **pruning** (TTP) strategy, but also yields higher
generation speed by utilizing a self-speculative decoding (SSD) strategy.
Taking LLaVA-1.5-7B as the base VLM, experimental results show that TwigVLM
preserves 96% of the original performance after **pruning** 88.9% of visual tokens
and achieves 154% speedup in generating long responses, delivering
significantly better performance in terms of both accuracy and speed over the
state-of-the-art VLM **acceleration** methods. Code will be made publicly
available.


## Fast Autoregressive Video Generation with Diagonal Decoding

>Authors: Yang Ye, Junliang Guo, Haoyu Wu, Tianyu He, Tim Pearce, Tabish Rashid, Katja Hofmann, Jiang Bian

>2025-03-18

> http://arxiv.org/abs/2503.14070v1

Autoregressive Transformer models have demonstrated impressive performance in
video generation, but their sequential token-by-token decoding process poses a
major bottleneck, particularly for long videos represented by tens of thousands
of tokens. In this paper, we propose Diagonal Decoding (DiagD), a training-free
inference **acceleration** algorithm for autoregressively pre-trained models that
exploits spatial and temporal correlations in videos. Our method generates
tokens along diagonal paths in the spatial-temporal token grid, enabling
parallel decoding within each frame as well as partially overlapping across
consecutive frames. The proposed algorithm is versatile and adaptive to various
generative models and tasks, while providing flexible control over the
trade-off between inference speed and visual quality. Furthermore, we propose a
cost-effective finetuning strategy that aligns the attention patterns of the
model with our decoding order, further mitigating the training-inference gap on
small-scale models. Experiments on multiple autoregressive video generation
models and datasets demonstrate that DiagD achieves up to $10\times$ speedup
compared to naive sequential decoding, while maintaining comparable visual
fidelity.


## A-SCoRe Attention-based Scene Coordinate Regression for wide-ranging scenarios

>Authors: Huy-Hoang Bui, Bach-Thuan Bui, Quang-Vinh Tran, Yasuyuki Fujii, Joo-Ho Lee

>2025-03-18

> http://arxiv.org/abs/2503.13982v1

Visual localization is considered to be one of the crucial parts in many
robotic and vision systems. While state-of-the art methods that relies on
feature matching have proven to be accurate for visual localization, its
requirements for storage and compute are burdens. Scene coordinate regression
(SCR) is an alternative approach that remove the barrier for storage by
learning to map 2D pixels to 3D scene coordinates. Most popular SCR use
Convolutional Neural Network (CNN) to extract 2D descriptor, which we would
argue that it miss the spatial relationship between pixels. Inspired by the
success of vision transformer architecture, we present a new SCR architecture,
called A-ScoRe, an Attention-based model which leverage attention on descriptor
map level to produce meaningful and high-semantic 2D descriptors. Since the
operation is performed on descriptor map, our model can work with multiple data
modality whether it is a dense or **sparse** from depth-map, SLAM to
Structure-from-Motion (SfM). This versatility allows A-SCoRe to operate in
different kind of environments, conditions and achieve the level of flexibility
that is important for mobile robots. Results show our methods achieve
comparable performance with State-of-the-art methods on multiple benchmark
while being light-weighted and much more flexible. Code and pre-trained models
are public in our repository: https://github.com/ais-lab/A-SCoRe.


## COMMConcentrated Margin Maximization for Robust Document-Level Relation Extraction

>Authors: Zhichao Duan, Tengyu Pan, Zhenyu Li, Xiuxing Li, Jianyong Wang

>2025-03-18

> http://arxiv.org/abs/2503.13885v1

Document-level relation extraction (DocRE) is the process of identifying and
extracting relations between entities that span multiple sentences within a
document. Due to its realistic settings, DocRE has garnered increasing research
attention in recent years. Previous research has mostly focused on developing
sophisticated encoding models to better capture the intricate patterns between
entity pairs. While these advancements are undoubtedly crucial, an even more
foundational challenge lies in the data itself. The complexity inherent in
DocRE makes the labeling process prone to errors, compounded by the extreme
**sparsity** of positive relation samples, which is driven by both the limited
availability of positive instances and the broad diversity of positive relation
types. These factors can lead to biased optimization processes, further
complicating the task of accurate relation extraction. Recognizing these
challenges, we have developed a robust framework called \textit{\textbf{COMM}}
to better solve DocRE. \textit{\textbf{COMM}} operates by initially employing
an instance-aware reasoning method to dynamically capture pertinent information
of entity pairs within the document and extract relational features. Following
this, \textit{\textbf{COMM}} takes into account the distribution of relations
and the difficulty of samples to dynamically adjust the margins between
prediction logits and the decision threshold, a process we call Concentrated
Margin Maximization. In this way, \textit{\textbf{COMM}} not only enhances the
extraction of relevant relational features but also boosts DocRE performance by
addressing the specific challenges posed by the data. Extensive experiments and
analysis demonstrate the versatility and effectiveness of
\textit{\textbf{COMM}}, especially its robustness when trained on low-quality
data (achieves \textgreater 10\% performance gains).


## Enabling Inclusive Systematic Reviews Incorporating Preprint Articles with Large Language Model-Driven Evaluations

>Authors: Rui Yang, Jiayi Tong, Haoyuan Wang, Hui Huang, Ziyang Hu, Peiyu Li, Nan Liu, Christopher J. Lindsell, Michael J. Pencina, Yong Chen, Chuan Hong

>2025-03-18

> http://arxiv.org/abs/2503.13857v2

Background. Systematic reviews in comparative effectiveness research require
timely evidence synthesis. Preprints accelerate knowledge dissemination but
vary in quality, posing challenges for systematic reviews.
  Methods. We propose AutoConfidence (automated confidence assessment), an
advanced framework for predicting preprint publication, which reduces reliance
on manual curation and expands the range of predictors, including three key
advancements: (1) automated data extraction using natural language processing
techniques, (2) semantic embeddings of titles and abstracts, and (3) large
language model (LLM)-driven evaluation scores. Additionally, we employed two
prediction models: a random forest classifier for binary outcome and a survival
cure model that predicts both binary outcome and publication risk over time.
  Results. The random forest classifier achieved AUROC 0.692 with LLM-driven
scores, improving to 0.733 with semantic embeddings and 0.747 with article
usage metrics. The survival cure model reached AUROC 0.716 with LLM-driven
scores, improving to 0.731 with semantic embeddings. For publication risk
prediction, it achieved a concordance index of 0.658, increasing to 0.667 with
semantic embeddings.
  Conclusion. Our study advances the framework for preprint publication
prediction through automated data extraction and multiple feature integration.
By combining semantic embeddings with LLM-driven evaluations, AutoConfidence
enhances predictive performance while reducing manual annotation burden. The
framework has the potential to facilitate systematic incorporation of preprint
articles in evidence-based medicine, supporting researchers in more effective
evaluation and utilization of preprint resources.


## Automatic MILP Model Construction for Multi-Robot Task Allocation and Scheduling Based on Large Language Models

>Authors: Mingming Peng, Zhendong Chen, Jie Yang, Jin Huang, Zhengqi Shi, Qihao Liu, Xinyu Li, Liang Gao

>2025-03-18

> http://arxiv.org/abs/2503.13813v1

With the accelerated development of Industry 4.0, intelligent manufacturing
systems increasingly require efficient task allocation and scheduling in
multi-robot systems. However, existing methods rely on domain expertise and
face challenges in adapting to dynamic production constraints. Additionally,
enterprises have high privacy requirements for production scheduling data,
which prevents the use of cloud-based large language models (LLMs) for solution
development. To address these challenges, there is an urgent need for an
automated modeling solution that meets data privacy requirements. This study
proposes a knowledge-augmented mixed integer linear programming (MILP)
automated formulation framework, integrating local LLMs with domain-specific
knowledge bases to generate executable code from natural language descriptions
automatically. The framework employs a knowledge-guided
DeepSeek-R1-Distill-Qwen-32B model to extract complex spatiotemporal
constraints (82% average accuracy) and leverages a supervised fine-tuned
Qwen2.5-Coder-7B-Instruct model for efficient MILP code generation (90% average
accuracy). Experimental results demonstrate that the framework successfully
achieves automatic modeling in the aircraft skin manufacturing case while
ensuring data privacy and computational efficiency. This research provides a
low-barrier and highly reliable technical path for modeling in complex
industrial scenarios.


## SMILE a Scale-aware Multiple Instance Learning Method for Multicenter STAS Lung Cancer Histopathology Diagnosis

>Authors: Liangrui Pan, Xiaoyu Li, Yutao Dou, Qiya Song, Jiadi Luo, Qingchun Liang, Shaoliang Peng

>2025-03-18

> http://arxiv.org/abs/2503.13799v1

Spread through air spaces (STAS) represents a newly identified aggressive
pattern in lung cancer, which is known to be associated with adverse prognostic
factors and complex pathological features. Pathologists currently rely on time
consuming manual assessments, which are highly subjective and prone to
variation. This highlights the urgent need for automated and precise diag
nostic solutions. 2,970 lung cancer tissue slides are comprised from multiple
centers, re-diagnosed them, and constructed and publicly released three lung
cancer STAS datasets: STAS CSU (hospital), STAS TCGA, and STAS CPTAC. All STAS
datasets provide corresponding pathological feature diagnoses and related
clinical data. To address the bias, **sparse** and heterogeneous nature of STAS, we
propose an scale-aware multiple instance learning(SMILE) method for STAS
diagnosis of lung cancer. By introducing a scale-adaptive attention mechanism,
the SMILE can adaptively adjust high attention instances, reducing
over-reliance on local regions and promoting consistent detection of STAS
lesions. Extensive experiments show that SMILE achieved competitive diagnostic
results on STAS CSU, diagnosing 251 and 319 STAS samples in CPTAC
andTCGA,respectively, surpassing clinical average AUC. The 11 open baseline
results are the first to be established for STAS research, laying the
foundation for the future expansion, interpretability, and clinical integration
of computational pathology technologies. The datasets and code are available at
https://anonymous.4open.science/r/IJCAI25-1DA1.


## Mitigating KV Cache Competition to Enhance User Experience in LLM Inference

>Authors: Haiying Shen, Tanmoy Sen

>2025-03-17

> http://arxiv.org/abs/2503.13773v1

In Large Language Model (LLM) serving, the **KV**-cache (**KV**C) bottleneck causes
high tail Time-to-First-Token (TTFT) and Time-Between-Tokens (TBT), impairing
user experience, particularly in time-sensitive applications. However,
satisfying both TTFT and TBT service-level objectives (SLOs) is challenging. To
address this, we propose a system, named CacheOPT for mitigating **KV** Cache
competition, based on key insights from our measurements, incorporating novel
components. First, it estimates a request's output length, bounding the
deviation with a high specified probability, adjusted based on the request
arrival rate. Second, it allocates the estimated **KV**C demand to a request, and
reuses other requests' allocated **KV**C to avoid preemptions while reducing
waiting time. Third, it proactively allocates **KV**C before instead of at the time
a request exhausts its allocation and reserves **KV**C globally to prevent
preemptions. Fourth, it chooses a request that has long TBT SLO, long job
remaining time and short preemption time to preempt. Fifth, it selects the
shortest-latency strategy between swapping and recomputation for preemptions.
Experiments show that CacheOPT achieves up to 3.29$\times$ and 2.83$\times$
lower tail TBT and tail TTFT, 47\% and 53\% higher TTFT and TBT SLO
attainments, and supports up to 1.58$\times$ higher request arrival rate than
the state-of-the-art methods.


## Single Sparse Graph Enhanced Expectation Propagation Algorithm Design for Uplink MIMO-SCMA

>Authors: Qu Luo, Jing Zhu, Gaojie Chen, Pei Xiao, Rahim Tafazolli

>2025-03-17

> http://arxiv.org/abs/2503.13681v1

Sparse code multiple access (SCMA) and multiple input multiple output (MIMO)
are considered as two efficient techniques to provide both massive connectivity
and high spectrum efficiency for future machine-type wireless networks. This
paper proposes a single **sparse** graph (SSG) enhanced expectation propagation
algorithm (EPA) receiver, referred to as SSG-EPA, for uplink MIMO-SCMA systems.
Firstly, we reformulate the **sparse** codebook mapping process using a linear
encoding model, which transforms the variable nodes (VNs) of SCMA from
symbol-level to bit-level VNs. Such transformation facilitates the integration
of the VNs of SCMA and low-density parity-check (LDPC), thereby emerging the
SCMA and LDPC graphs into a SSG. Subsequently, to further reduce the detection
complexity, the message propagation between SCMA VNs and function nodes (FNs)
are designed based on EPA principles. Different from the existing iterative
detection and decoding (IDD) structure, the proposed EPA-SSG allows a
simultaneously detection and decoding at each iteration, and eliminates the use
of interleavers, de-interleavers, symbol-to-bit, and bit-to-symbol LLR
transformations. Simulation results show that the proposed SSG-EPA achieves
better error rate performance compared to the state-of-the-art schemes.


## MaTVLM Hybrid Mamba-Transformer for Efficient Vision-Language Modeling

>Authors: Yingyue Li, Bencheng Liao, Wenyu Liu, Xinggang Wang

>2025-03-17

> http://arxiv.org/abs/2503.13440v2

With the advancement of RNN models with linear complexity, the quadratic
complexity challenge of transformers has the potential to be overcome. Notably,
the emerging Mamba-2 has demonstrated competitive performance, bridging the gap
between RNN models and transformers. However, due to sequential processing and
vanishing gradients, RNN models struggle to capture long-range dependencies,
limiting contextual understanding. This results in slow convergence, high
resource demands, and poor performance on downstream understanding and complex
reasoning tasks. In this work, we present a hybrid model MaTVLM by substituting
a portion of the transformer decoder layers in a pre-trained VLM with Mamba-2
layers. Leveraging the inherent relationship between attention and Mamba-2, we
initialize Mamba-2 with corresponding attention weights to accelerate
convergence. Subsequently, we employ a single-stage distillation process, using
the pre-trained VLM as the teacher model to transfer knowledge to the MaTVLM,
further enhancing convergence speed and performance. Furthermore, we
investigate the impact of differential distillation loss within our training
framework. We evaluate the MaTVLM on multiple benchmarks, demonstrating
competitive performance against the teacher model and existing VLMs while
surpassing both Mamba-based VLMs and models of comparable parameter scales.
Remarkably, the MaTVLM achieves up to 3.6x faster inference than the teacher
model while reducing GPU memory consumption by 27.5%, all without compromising
performance. Code and models are released at http://github.com/hustvl/MaTVLM.


## Scale Efficient Training for Large Datasets

>Authors: Qing Zhou, Junyu Gao, Qi Wang

>2025-03-17

> http://arxiv.org/abs/2503.13385v1

The rapid growth of dataset scales has been a key driver in advancing deep
learning research. However, as dataset scale increases, the training process
becomes increasingly inefficient due to the presence of low-value samples,
including excessive redundant samples, overly challenging samples, and
inefficient easy samples that contribute little to model improvement.To address
this challenge, we propose Scale Efficient Training (SeTa) for large datasets,
a dynamic sample **pruning** approach that losslessly reduces training time. To
remove low-value samples, SeTa first performs random **pruning** to eliminate
redundant samples, then clusters the remaining samples according to their
learning difficulty measured by loss. Building upon this clustering, a sliding
window strategy is employed to progressively remove both overly challenging and
inefficient easy clusters following an easy-to-hard curriculum.We conduct
extensive experiments on large-scale synthetic datasets, including ToCa, SS1M,
and ST+MJ, each containing over 3 million samples.SeTa reduces training costs
by up to 50\% while maintaining or improving performance, with minimal
degradation even at 70\% cost reduction. Furthermore, experiments on various
scale real datasets across various backbones (CNNs, Transformers, and Mambas)
and diverse tasks (instruction tuning, multi-view stereo, geo-localization,
composed image retrieval, referring image segmentation) demonstrate the
powerful effectiveness and universality of our approach. Code is available at
https://github.com/mrazhou/SeTa.


## Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning

>Authors: Hai-Long Sun, Zhun Sun, Houwen Peng, Han-Jia Ye

>2025-03-17

> http://arxiv.org/abs/2503.13360v1

Recent advancements in Large Language Models (LLMs) have demonstrated
enhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting
to advanced, product-oriented solutions like OpenAI o1. During our
re-implementation of this model, we noticed that in multimodal tasks requiring
visual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to
maintain focus on the visual information, in other words, MLLMs suffer from a
gradual decline in attention to visual information as reasoning progresses,
causing text-over-relied outputs. To investigate this, we ablate image inputs
during long-chain reasoning. Concretely, we truncate the reasoning process
midway, then re-complete the reasoning process with the input image removed. We
observe only a ~2% accuracy drop on MathVista's test-hard subset, revealing the
model's textual outputs dominate the following reasoning process. Motivated by
this, we propose Take-along Visual Conditioning (TVC), a strategy that shifts
image input to critical reasoning stages and compresses redundant visual tokens
via dynamic **pruning**. This methodology helps the model retain attention to the
visual components throughout the reasoning. Our approach achieves
state-of-the-art performance on average across five mathematical reasoning
benchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in
enhancing multimodal reasoning systems.


## LIMCA LLM for Automating Analog In-Memory Computing Architecture Design Exploration

>Authors: Deepak Vungarala, Md Hasibul Amin, Pietro Mercati, Arnob Ghosh, Arman Roohi, Ramtin Zand, Shaahin Angizi

>2025-03-17

> http://arxiv.org/abs/2503.13301v1

Resistive crossbars enabling analog In-Memory Computing (IMC) have emerged as
a promising architecture for Deep Neural Network (DNN) **acceleration**, offering
high memory bandwidth and in-situ computation. However, the manual,
knowledge-intensive design process and the lack of high-quality circuit
netlists have significantly constrained design space exploration and
optimization to behavioral system-level tools. In this work, we introduce
LIMCA, a novel fine-tune-free Large Language Model (LLM)-driven framework for
automating the design and evaluation of IMC crossbar architectures. Unlike
traditional approaches, LIMCA employs a No-Human-In-Loop (NHIL) automated
pipeline to generate and validate circuit netlists for SPICE simulations,
eliminating manual intervention. LIMCA systematically explores the IMC design
space by leveraging a structured dataset and LLM-based performance evaluation.
Our experimental results on MNIST classification demonstrate that LIMCA
successfully generates crossbar designs achieving $\geq$96% accuracy while
maintaining a power consumption $\leq$3W, making this the first work in
LLM-assisted IMC design space exploration. Compared to existing frameworks,
LIMCA provides an automated, scalable, and hardware-aware solution, reducing
design exploration time while ensuring user-constrained performance trade-offs.


## $φ$-Decoding Adaptive Foresight Sampling for Balanced Inference-Time Exploration and Exploitation

>Authors: Fangzhi Xu, Hang Yan, Chang Ma, Haiteng Zhao, Jun Liu, Qika Lin, Zhiyong Wu

>2025-03-17

> http://arxiv.org/abs/2503.13288v1

Inference-time optimization scales computation to derive deliberate reasoning
steps for effective performance. While previous search-based strategies address
the short-sightedness of auto-regressive generation, the vast search space
leads to excessive exploration and insufficient exploitation. To strike an
efficient balance to derive the optimal step, we frame the decoding strategy as
foresight sampling, leveraging simulated future steps to obtain globally
optimal step estimation. Built on it, we propose a novel decoding strategy,
named $\phi$-Decoding. To provide a precise and expressive estimation of step
value, $\phi$-Decoding approximates two distributions via foresight and
clustering. Sampling from the joint distribution, the optimal steps can be
selected for exploitation. To support adaptive computation allocation, we
propose in-width and in-depth **pruning** strategies, featuring a light-weight
solution to achieve inference efficiency. Extensive experiments across seven
benchmarks show $\phi$-Decoding outperforms strong baselines in both
performance and efficiency. Additional analysis demonstrates its generalization
across various LLMs and scalability across a wide range of computing budgets.
The code will be released at https://github.com/xufangzhi/phi-Decoding, and the
open-source PyPI package is coming soon.


## A Design of Denser-Graph-Frequency Graph Fourier Frames for Graph Signal Analysis

>Authors: Kaito Nitani, Seisuke Kyochi

>2025-03-17

> http://arxiv.org/abs/2503.13164v1

This paper introduces a design method for densergraph-frequency graph Fourier
frames (DGFFs) to enhance graph signal processing and analysis. The graph
Fourier transform (GFT) enables us to analyze graph signals in the graph
spectral domain and facilitates various graph signal processing tasks, such as
filtering, sampling and reconstruction, denoising, and so on. However, the
conventional GFT faces two significant limitations. First, unlike the discrete
Fourier transform and its variants (such as discrete cosine transforms), the
graph frequencies of the derived graph Fourier basis (GFB) from a given graph
tend to be unevenly distributed or localized, which leads to biased spectral
analysis. Second, the GFB used in GFT does not provide an efficient **sparse**
representation of graph signals compared to overcomplete systems like frames.
To overcome these challenges, we propose adding oscillating vectors with
intermediate graph frequencies between the original vectors in the GFB for both
undirected and directed graphs, constructing GFFs with densergraph frequencies.
The resulting DGFFs are expected to enable more accurate graph signal analysis.
Furthermore, we propose a graph filtering method based on the DGFFs. In
experiments, we apply the DGFFs to practical applications such as graph signal
recovery, demonstrating superior performance compared to existing GFBs.


## Unprecedented superionicity of ultra-low barrier in A0.5CoO2 (A=Li, Zn)

>Authors: Xuechen Wang, Yaxin Gao, Menghao Wu

>2025-03-17

> http://arxiv.org/abs/2503.13159v1

The ion conductivity of a solid-state ion conductor generally increases
exponentially upon reduction in ion migration barrier. For prevalent cathode
material LiCoO2, the room-temperature ion conductivity and migration barrier
are respectively around 10-4 S/cm and 0.3 eV. In this paper, through
first-principles calculations we predict the existence of 1D superionicity as
the Li ions in O2 LiCoO2 are transformed to Zn0.5CoO2 or Li0.5CoO2 via
cation-exchange reaction or deintercalation. The ion migration barriers
(0.01-0.02 eV) even lower than room-temperature ~kBT are reduced by more than
an order of magnitude compared with LiCoO2, which are facilitated by facile
transition of mobile ions between two coordination configurations. The
room-temperature ion conductivity is estimated to be over 50 S/cm, enhanced by
2-3 orders of magnitude compared with current highest reported value. Such
unprecedented superionicity may also exist in other similar layered ion
conductors, which may render technical advances and exotic effects such as
ultrafast ion batteries and **quantize**d ferroelectricity.


## Lifting the Veil on Visual Information Flow in MLLMs Unlocking Pathways to Faster Inference

>Authors: Hao Yin, Guangzong Si, Zilei Wang

>2025-03-17

> http://arxiv.org/abs/2503.13108v1

Multimodal large language models (MLLMs) improve performance on
vision-language tasks by integrating visual features from pre-trained vision
encoders into large language models (LLMs). However, how MLLMs process and
utilize visual information remains unclear. In this paper, a shift in the
dominant flow of visual information is uncovered: (1) in shallow layers, strong
interactions are observed between image tokens and instruction tokens, where
most visual information is injected into instruction tokens to form cross-modal
semantic representations; (2) in deeper layers, image tokens primarily interact
with each other, aggregating the remaining visual information to optimize
semantic representations within visual modality. Based on these insights, we
propose Hierarchical Modality-Aware Pruning (HiMAP), a plug-and-play inference
**acceleration** method that dynamically prunes image tokens at specific layers,
reducing computational costs by approximately 65% without sacrificing
performance. Our findings offer a new understanding of visual information
processing in MLLMs and provide a state-of-the-art solution for efficient
inference.


## ClusComp A Simple Paradigm for Model Compression and Efficient Finetuning

>Authors: Baohao Liao, Christian Herold, Seyyed Hadi Hashemi, Stefan Vasilev, Shahram Khadivi, Christof Monz

>2025-03-17

> http://arxiv.org/abs/2503.13089v1

As large language models (LLMs) scale, model compression is crucial for edge
deployment and accessibility. Weight-only **quantization** reduces model size but
suffers from performance degradation at lower bit widths. Moreover, standard
finetuning is incompatible with **quantize**d models, and alternative methods often
fall short of full finetuning. In this paper, we propose ClusComp, a simple yet
effective compression paradigm that clusters weight matrices into codebooks and
finetunes them block-by-block. ClusComp (1) achieves superior performance in
2-4 bit **quantization**, (2) pushes compression to 1-bit while outperforming
ultra-**low-bit** methods with minimal finetuning, and (3) enables efficient
finetuning, even surpassing existing **quantization**-based approaches and rivaling
full FP16 finetuning. Notably, ClusComp supports compression and finetuning of
70B LLMs on a single A6000-48GB GPU.


## HERMES High-Performance RISC-V Memory Hierarchy for ML Workloads

>Authors: Pranav Suryadevara

>2025-03-17

> http://arxiv.org/abs/2503.13064v1

The growth of machine learning (ML) workloads has underscored the importance
of efficient memory hierarchies to address bandwidth, latency, and scalability
challenges. HERMES focuses on optimizing memory subsystems for RISC-V
architectures to meet the computational needs of ML models such as CNNs, RNNs,
and Transformers. This project explores state-of-the-art techniques such as
advanced prefetching, tensor-aware caching, and hybrid memory models. The
cornerstone of HERMES is the integration of shared L3 caches with fine-grained
coherence protocols and specialized pathways to deep learning accelerators like
Gemmini. Simulation tools like Gem5 and DRAMSim2 are used to evaluate baseline
performance and scalability under representative ML workloads. The findings of
this study highlight the design choices and anticipated challenges, paving the
way for low-latency scalable memory operations for ML applications.


## ROMA a Read-Only-Memory-based Accelerator for QLoRA-based On-Device LLM

>Authors: Wenqiang Wang, Yijia Zhang, Zikai Zhang, Guanting Huo, Hao Liang, Shijie Cao, Ningyi Xu

>2025-03-17

> http://arxiv.org/abs/2503.12988v1

As large language models (LLMs) demonstrate powerful capabilities, deploying
them on edge devices has become increasingly crucial, offering advantages in
privacy and real-time interaction. QLoRA has emerged as the standard approach
for on-device LLMs, leveraging **quantize**d models to reduce memory and
computational costs while utilizing LoRA for task-specific adaptability. In
this work, we propose ROMA, a QLoRA accelerator with a hybrid storage
architecture that uses ROM for **quantize**d base models and SRAM for LoRA weights
and **KV** cache. Our insight is that the **quantize**d base model is stable and
converged, making it well-suited for ROM storage. Meanwhile, LoRA modules offer
the flexibility to adapt to new data without requiring updates to the base
model. To further reduce the area cost of ROM, we introduce a novel B-ROM
design and integrate it with the compute unit to form a fused cell for
efficient use of chip resources. ROMA can effectively store both a 4-bit 3B and
a 2-bit 8B LLaMA model entirely on-chip, achieving a notable generation speed
exceeding 20,000 tokens/s without requiring external memory.


## ML-SpecQD Multi-Level Speculative Decoding with Quantized Drafts

>Authors: Evangelos Georganas, Dhiraj Kalamkar, Alexander Kozlov, Alexander Heinecke

>2025-03-17

> http://arxiv.org/abs/2503.13565v1

Speculative decoding (SD) has emerged as a method to accelerate LLM inference
without sacrificing any accuracy over the 16-bit model inference. In a typical
SD setup, the idea is to use a full-precision, small, fast model as "draft" to
generate the next few tokens and use the "target" large model to verify the
draft-generated tokens. The efficacy of this method heavily relies on the
acceptance ratio of the draft-generated tokens and the relative token
throughput of the draft versus the target model. Nevertheless, an efficient SD
pipeline requires pre-training and aligning the draft model to the target
model, making it impractical for LLM inference in a plug-and-play fashion. In
this work, we propose using MXFP4 models as drafts in a plug-and-play fashion
since the MXFP4 Weight-Only-Quantization (WOQ) merely direct-casts the BF16
target model weights to MXFP4. In practice, our plug-and-play solution gives
speedups up to 2x over the BF16 baseline. Then we pursue an opportunity for
further **acceleration**: the MXFP4 draft token generation itself can be
accelerated via speculative decoding by using yet another smaller draft. We
call our method ML-SpecQD: Multi-Level Speculative Decoding with Quantized
Drafts since it recursively applies speculation for accelerating the
draft-token generation. Combining Multi-Level Speculative Decoding with MXFP4
Quantized Drafts we outperform state-of-the-art speculative decoding, yielding
speedups up to 2.72x over the BF16 baseline.


## Verification Learning Make Unsupervised Neuro-Symbolic System Feasible

>Authors: Lin-Han Jia, Wen-Chao Hu, Jie-Jing Shao, Lan-Zhe Guo, Yu-Feng Li

>2025-03-17

> http://arxiv.org/abs/2503.12917v1

The current Neuro-Symbolic (NeSy) Learning paradigm suffers from an
over-reliance on labeled data. If we completely disregard labels, it leads to
less symbol information, a larger solution space, and more shortcuts-issues
that current Nesy systems cannot resolve. This paper introduces a novel
learning paradigm, Verification Learning (VL), which addresses this challenge
by transforming the label-based reasoning process in Nesy into a label-free
verification process. VL achieves excellent learning results solely by relying
on unlabeled data and a function that verifies whether the current predictions
conform to the rules. We formalize this problem as a Constraint Optimization
Problem (COP) and propose a Dynamic combinatorial Sorting (DCS) algorithm that
accelerates the solution by reducing verification attempts, effectively
lowering computational costs to the level of a Constraint Satisfaction Problem
(CSP). To further enhance performance, we introduce a prior alignment method to
address potential shortcuts. Our theoretical analysis points out which tasks in
Nesy systems can be completed without labels and explains why rules can replace
infinite labels, such as in addition, for some tasks, while for others, like
Sudoku, the rules have no effect. We validate the proposed framework through
several fully unsupervised tasks including addition, sort, match, and chess,
each showing significant performance and efficiency improvements.


## Safeguarding LLM Embeddings in End-Cloud Collaboration via Entropy-Driven Perturbation

>Authors: Shuaifan Jin, Xiaoyi Pang, Zhibo Wang, He Wang, Jiacheng Du, Jiahui Hu, Kui Ren

>2025-03-17

> http://arxiv.org/abs/2503.12896v1

Recent studies improve on-device language model (LM) inference through
end-cloud collaboration, where the end device retrieves useful information from
cloud databases to enhance local processing, known as Retrieval-Augmented
Generation (RAG). Typically, to retrieve information from the cloud while
safeguarding privacy, the end device transforms original data into embeddings
with a local embedding model. However, the recently emerging Embedding
Inversion Attacks (EIAs) can still recover the original data from text
embeddings (e.g., training a recovery model to map embeddings back to original
texts), posing a significant threat to user privacy. To address this risk, we
propose EntroGuard, an entropy-driven perturbation-based embedding privacy
protection method, which can protect the privacy of text embeddings while
maintaining retrieval accuracy during the end-cloud collaboration.
Specifically, to defeat various EIAs, we perturb the embeddings to increase the
entropy of the recovered text in the common structure of recovery models, thus
steering the embeddings toward meaningless texts rather than original sensitive
texts during the recovery process. To maintain retrieval performance in the
cloud, we constrain the perturbations within a bound, applying the strategy of
reducing them where redundant and increasing them where **sparse**. Moreover,
EntroGuard can be directly integrated into end devices without requiring any
modifications to the embedding model. Extensive experimental results
demonstrate that EntroGuard can reduce the risk of privacy leakage by up to 8
times at most with negligible loss of retrieval performance compared to
existing privacy-preserving methods.


## Efficient noise tailoring and detection of hypergraph states using Clifford circuits

>Authors: Guedong Park, Jinzhao Sun, Hyunseok Jeong

>2025-03-17

> http://arxiv.org/abs/2503.12870v1

Hypergraph states are important magic resources for realizing universal
quantum computation and diverse non-local physical phenomena. However, noise
detection for such states is challenging due to their large dimension and
entanglement. This work proposes an efficient Clifford circuit-based scheme for
tailoring and detecting noise in third-ordered hypergraph states generated by
CCZ, CZ, and Z gates. The core part of our scheme is converting the noisy input
state into a diagonal form and obtaining the convolution equation of noise rate
via Clifford circuits. The depth of the Clifford circuit can be reduced to a
constant, depending on the structure of the hypergraph state. After that, we
decode it using the fast Hadamard-Walsh transform or some approximation method.
The approximation with respect to the $l_2$-norm can be done efficiently by the
number of qubits while keeping sufficient precision. Furthermore, the **sparse**
noise assumption, which frequently holds in current experimental setups,
enables $ l_1$ approximation. Compared with state verification methods, our
method allows us to attain richer information on noise rates and apply various
noise-adapted error correction and mitigation methods. Moreover, it bridges the
connection between the convolution equation's nonlinearity and the Clifford
hierarchy of the hypergraph state inputs. Our results provide a deeper
understanding of the nature of highly entangled systems and drive the interests
of the research venues concerning magic state implementation.


## ACT360 An Efficient 360-Degree Action Detection and Summarization Framework for Mission-Critical Training and Debriefing

>Authors: Aditi Tiwari, Klara Nahrstedt

>2025-03-17

> http://arxiv.org/abs/2503.12852v1

Effective training and debriefing are critical in high-stakes,
mission-critical environments such as disaster response, military simulations,
and industrial safety, where precision and minimizing errors are paramount. The
traditional post-training analysis relies on manually reviewing 2D videos, a
time-consuming process that lacks comprehensive situational awareness. To
address these limitations, we introduce ACT360, a system that leverages
360-degree videos and machine learning for automated action detection and
structured debriefing. ACT360 integrates 360YOWO, an enhanced You Only Watch
Once (YOWO) model with spatial attention and equirectangular-aware convolution
(EAC) to mitigate panoramic video distortions. To enable deployment in
resource-constrained environments, we apply **quantization** and model **pruning**,
reducing the model size by 74% while maintaining robust accuracy (mAP drop of
only 1.5%, from 0.865 to 0.850) and improving inference speed. We validate our
approach on a publicly available dataset of 55 labeled 360-degree videos
covering seven key operational actions, recorded across various real-world
training sessions and environmental conditions. Additionally, ACT360 integrates
360AIE (Action Insight Explorer), a web-based interface for automatic action
detection, retrieval, and textual summarization using large language models
(LLMs), significantly enhancing post-incident analysis efficiency. ACT360
serves as a generalized framework for mission-critical debriefing,
incorporating EAC, spatial attention, summarization, and model optimization.
These innovations apply to any training environment requiring lightweight
action detection and structured post-exercise analysis.


## Edge dependence of the Josephson current in the quantum Hall regime

>Authors: Seong Jang, Geon-Hyoung Park, Kenji Watanabe, Takashi Taniguchi, Gil-Ho Lee

>2025-03-17

> http://arxiv.org/abs/2503.12817v1

The observation of Josephson current in the quantum Hall regime has attracted
considerable attention, revealing the coexistence of two seemingly incompatible
phases: the quantum Hall and superconducting states. However, the mechanism
underlying the Josephson current remains unclear because of the observed h/2e
magnetic interference period and the lack of precisely **quantize**d Hall plateaus.
To address this issue, we investigate the edge dependence of the Josephson
current in graphene Josephson junctions operating in the quantum Hall regime.
By systematically comparing devices with native, etched, edge-free, and
gate-defined edges, we demonstrate that the Josephson current is confined to
the physical edges and is highly sensitive to specific edge configurations. Our
findings provide direct evidence that counter-propagating quantum Hall edge
states mediate Andreev bound states, enabling Josephson coupling. These results
clarify the underlying mechanism of Josephson current in the quantum Hall
regime and offer new strategies for engineering superconducting hybrid devices.


## TransDiff Diffusion-Based Method for Manipulating Transparent Objects Using a Single RGB-D Image

>Authors: Haoxiao Wang, Kaichen Zhou, Binrui Gu, Zhiyuan Feng, Weijie Wang, Peilin Sun, Yicheng Xiao, Jianhua Zhang, Hao Dong

>2025-03-17

> http://arxiv.org/abs/2503.12779v1

Manipulating transparent objects presents significant challenges due to the
complexities introduced by their reflection and refraction properties, which
considerably hinder the accurate estimation of their 3D shapes. To address
these challenges, we propose a single-view RGB-D-based depth completion
framework, TransDiff, that leverages the Denoising Diffusion Probabilistic
Models(DDPM) to achieve material-agnostic object grasping in desktop.
Specifically, we leverage features extracted from RGB images, including
semantic segmentation, edge maps, and normal maps, to condition the depth map
generation process. Our method learns an iterative denoising process that
transforms a random depth distribution into a depth map, guided by initially
refined depth information, ensuring more accurate depth estimation in scenarios
involving transparent objects. Additionally, we propose a novel training method
to better align the noisy depth and RGB image features, which are used as
conditions to refine depth estimation step by step. Finally, we utilized an
improved inference process to accelerate the denoising procedure. Through
comprehensive experimental validation, we demonstrate that our method
significantly outperforms the baselines in both synthetic and real-world
benchmarks with acceptable inference time. The demo of our method can be found
on https://wang-haoxiao.github.io/TransDiff/


## Survival Analysis with Machine Learning for Predicting Li-ion Battery Remaining Useful Life

>Authors: Jingyuan Xue, Longfei Wei, Fang Sheng, Yuxin Gao, Jianfei Zhang

>2025-03-17

> http://arxiv.org/abs/2503.13558v1

The accurate prediction of RUL for lithium-ion batteries is crucial for
enhancing the reliability and longevity of energy storage systems. Traditional
methods for RUL prediction often struggle with issues such as data **sparsity**,
varying battery chemistries, and the inability to capture complex degradation
patterns over time. In this study, we propose a survival analysis-based
framework combined with deep learning models to predict the RUL of lithium-ion
batteries. Specifically, we utilize five advanced models: the Cox-type models
(Cox, CoxPH, and CoxTime) and two machine-learning-based models (DeepHit and
MTLR). These models address the challenges of accurate RUL estimation by
transforming raw time-series battery data into survival data, including key
degradation indicators such as voltage, current, and internal resistance.
Advanced feature extraction techniques enhance the model's robustness in
diverse real-world scenarios, including varying charging conditions and battery
chemistries. Our models are tested using 10-fold cross-validation, ensuring
generalizability and minimizing overfitting. Experimental results show that our
survival-based framework significantly improves RUL prediction accuracy
compared to traditional methods, providing a reliable tool for battery
management and maintenance optimization. This study contributes to the
advancement of predictive maintenance in battery technology, offering valuable
insights for both researchers and industry practitioners aiming to enhance the
operational lifespan of lithium-ion batteries.


## LLM-Mediated Guidance of MARL Systems

>Authors: Philipp D. Siedler, Ian Gemp

>2025-03-16

> http://arxiv.org/abs/2503.13553v1

In complex multi-agent environments, achieving efficient learning and
desirable behaviours is a significant challenge for Multi-Agent Reinforcement
Learning (MARL) systems. This work explores the potential of combining MARL
with Large Language Model (LLM)-mediated interventions to guide agents toward
more desirable behaviours. Specifically, we investigate how LLMs can be used to
interpret and facilitate interventions that shape the learning trajectories of
multiple agents. We experimented with two types of interventions, referred to
as controllers: a Natural Language (NL) Controller and a Rule-Based (RB)
Controller. The NL Controller, which uses an LLM to simulate human-like
interventions, showed a stronger impact than the RB Controller. Our findings
indicate that agents particularly benefit from early interventions, leading to
more efficient training and higher performance. Both intervention types
outperform the baseline without interventions, highlighting the potential of
LLM-mediated guidance to accelerate training and enhance MARL performance in
challenging environments.


## Single-Carrier Waveform Design for Joint Sensing and Communication

>Authors: Ayoub Ammar Boudjelal, Rania Yasmine Bir, Huseyin Arslan

>2025-03-16

> http://arxiv.org/abs/2503.12638v1

The emergence of 6G wireless networks demands solutions that seamlessly
integrate communication and sensing. This letter proposes a novel waveform
design for joint sensing and communication (JSAC) systems, combining
single-carrier interleaved frequency division multiplexing (SC-IFDM), a 5G
communication candidate signal, with frequency modulated continuous wave
(FMCW), widely used for sensing. The proposed waveform leverages the **sparse**
nature of FMCW within SC-IFDM to achieve orthogonal integration in three steps:
SC-IFDM symbols are allocated alongside the **sparse** FMCW, followed by the
SC-IFDM transform into the time domain, and a cyclic prefix (CP) is applied in
which phase shifts are introduced to the FMCW. Additionally, an enhanced
channel estimation method is incorporated to boost system performance.
Simulation results demonstrate the proposed waveform's ability to deliver
high-resolution sensing and superior communication performance, surpassing
traditional multicarrier designs.


## SAUCE Selective Concept Unlearning in Vision-Language Models with Sparse Autoencoders

>Authors: Qing Li, Jiahui Geng, Derui Zhu, Fengyu Cai, Chenyang Lyu, Fakhri Karray

>2025-03-16

> http://arxiv.org/abs/2503.14530v2

Unlearning methods for vision-language models (VLMs) have primarily adapted
techniques from large language models (LLMs), relying on weight updates that
demand extensive annotated forget sets. Moreover, these methods perform
unlearning at a coarse granularity, often leading to excessive forgetting and
reduced model utility. To address this issue, we introduce SAUCE, a novel
method that leverages **sparse** autoencoders (SAEs) for fine-grained and selective
concept unlearning in VLMs. Briefly, SAUCE first trains SAEs to capture
high-dimensional, semantically rich **sparse** features. It then identifies the
features most relevant to the target concept for unlearning. During inference,
it selectively modifies these features to suppress specific concepts while
preserving unrelated information. We evaluate SAUCE on two distinct VLMs,
LLaVA-v1.5-7B and LLaMA-3.2-11B-Vision-Instruct, across two types of tasks:
concrete concept unlearning (objects and sports scenes) and abstract concept
unlearning (emotions, colors, and materials), encompassing a total of 60
concepts. Extensive experiments demonstrate that SAUCE outperforms
state-of-the-art methods by 18.04% in unlearning quality while maintaining
comparable model utility. Furthermore, we investigate SAUCE's robustness
against widely used adversarial attacks, its transferability across models, and
its scalability in handling multiple simultaneous unlearning requests. Our
findings establish SAUCE as an effective and scalable solution for selective
concept unlearning in VLMs.


## BFANet Revisiting 3D Semantic Segmentation with Boundary Feature Analysis

>Authors: Weiguang Zhao, Rui Zhang, Qiufeng Wang, Guangliang Cheng, Kaizhu Huang

>2025-03-16

> http://arxiv.org/abs/2503.12539v1

3D semantic segmentation plays a fundamental and crucial role to understand
3D scenes. While contemporary state-of-the-art techniques predominantly
concentrate on elevating the overall performance of 3D semantic segmentation
based on general metrics (e.g. mIoU, mAcc, and oAcc), they unfortunately leave
the exploration of challenging regions for segmentation mostly neglected. In
this paper, we revisit 3D semantic segmentation through a more granular lens,
shedding light on subtle complexities that are typically overshadowed by
broader performance metrics. Concretely, we have delineated 3D semantic
segmentation errors into four comprehensive categories as well as corresponding
evaluation metrics tailored to each. Building upon this categorical framework,
we introduce an innovative 3D semantic segmentation network called BFANet that
incorporates detailed analysis of semantic boundary features. First, we design
the boundary-semantic module to decouple point cloud features into semantic and
boundary features, and fuse their query queue to enhance semantic features with
attention. Second, we introduce a more concise and accelerated boundary
pseudo-label calculation algorithm, which is 3.9 times faster than the
state-of-the-art, offering compatibility with data augmentation and enabling
efficient computation in training. Extensive experiments on benchmark data
indicate the superiority of our BFANet model, confirming the significance of
emphasizing the four uniquely designed metrics. Code is available at
https://github.com/weiguangzhao/BFANet.


## CAKE Cascading and Adaptive KV Cache Eviction with Layer Preferences

>Authors: Ziran Qin, Yuchen Cao, Mingbao Lin, Wen Hu, Shixuan Fan, Ke Cheng, Weiyao Lin, Jianguo Li

>2025-03-16

> http://arxiv.org/abs/2503.12491v1

Large language models (LLMs) excel at processing long sequences, boosting
demand for key-value (**KV**) caching. While recent efforts to evict **KV** cache have
alleviated the inference burden, they often fail to allocate resources
rationally across layers with different attention patterns. In this paper, we
introduce Cascading and Adaptive **KV** cache Eviction (CAKE), a novel approach
that frames **KV** cache eviction as a "cake-slicing problem." CAKE assesses
layer-specific preferences by considering attention dynamics in both spatial
and temporal dimensions, allocates rational cache size for layers accordingly,
and manages memory constraints in a cascading manner. This approach enables a
global view of cache allocation, adaptively distributing resources across
diverse attention mechanisms while maintaining memory budgets. CAKE also
employs a new eviction indicator that considers the shifting importance of
tokens over time, addressing limitations in existing methods that overlook
temporal dynamics. Comprehensive experiments on LongBench and NeedleBench show
that CAKE maintains model performance with only 3.2% of the **KV** cache and
consistently outperforms current baselines across various models and memory
constraints, particularly in low-memory settings. Additionally, CAKE achieves
over 10x speedup in decoding latency compared to full cache when processing
contexts of 128K tokens with FlashAttention-2. Our code is available at
https://github.com/antgroup/cakekv.


## LazyMAR Accelerating Masked Autoregressive Models via Feature Caching

>Authors: Feihong Yan, Qingyan Wei, Jiayi Tang, Jiajun Li, Yulin Wang, Xuming Hu, Huiqi Li, Linfeng Zhang

>2025-03-16

> http://arxiv.org/abs/2503.12450v1

Masked Autoregressive (MAR) models have emerged as a promising approach in
image generation, expected to surpass traditional autoregressive models in
computational efficiency by leveraging the capability of parallel decoding.
However, their dependence on bidirectional self-attention inherently conflicts
with conventional **KV** caching mechanisms, creating unexpected computational
bottlenecks that undermine their expected efficiency. To address this problem,
this paper studies the caching mechanism for MAR by leveraging two types of
redundancy: Token Redundancy indicates that a large portion of tokens have very
similar representations in the adjacent decoding steps, which allows us to
first cache them in previous steps and then reuse them in the later steps.
Condition Redundancy indicates that the difference between conditional and
unconditional output in classifier-free guidance exhibits very similar values
in adjacent steps. Based on these two redundancies, we propose LazyMAR, which
introduces two caching mechanisms to handle them one by one. LazyMAR is
training-free and plug-and-play for all MAR models. Experimental results
demonstrate that our method achieves 2.83 times **acceleration** with almost no
drop in generation quality. Our codes will be released in
https://github.com/feihongyan1/LazyMAR.


## Enhancing Visual Representation with Textual Semantics Textual Semantics-Powered Prototypes for Heterogeneous Federated Learning

>Authors: Xinghao Wu, Jianwei Niu, Xuefeng Liu, Guogang Zhu, Jiayuan Zhang, Shaojie Tang

>2025-03-16

> http://arxiv.org/abs/2503.13543v1

Federated Prototype Learning (FedPL) has emerged as an effective strategy for
handling data heterogeneity in Federated Learning (FL). In FedPL, clients
collaboratively construct a set of global feature centers (prototypes), and let
local features align with these prototypes to mitigate the effects of data
heterogeneity. The performance of FedPL highly depends on the quality of
prototypes. Existing methods assume that larger inter-class distances among
prototypes yield better performance, and thus design different methods to
increase these distances. However, we observe that while these methods increase
prototype distances to enhance class discrimination, they inevitably disrupt
essential semantic relationships among classes, which are crucial for model
generalization. This raises an important question: how to construct prototypes
that inherently preserve semantic relationships among classes? Directly
learning these relationships from limited and heterogeneous client data can be
problematic in FL. Recently, the success of pre-trained language models (PLMs)
demonstrates their ability to capture semantic relationships from vast textual
corpora. Motivated by this, we propose FedTSP, a novel method that leverages
PLMs to construct semantically enriched prototypes from the textual modality,
enabling more effective collaboration in heterogeneous data settings. We first
use a large language model (LLM) to generate fine-grained textual descriptions
for each class, which are then processed by a PLM on the server to form textual
prototypes. To address the modality gap between client image models and the
PLM, we introduce trainable prompts, allowing prototypes to adapt better to
client tasks. Extensive experiments demonstrate that FedTSP mitigates data
heterogeneity while significantly accelerating convergence.


## When neural implant meets multimodal LLM A dual-loop system for neuromodulation and naturalistic neuralbehavioral research

>Authors: Edward Hong Wang, Cynthia Xin Wen

>2025-03-16

> http://arxiv.org/abs/2503.12334v1

We propose a novel dual-loop system that synergistically combines responsive
neurostimulation (RNS) implants with artificial intelligence-driven wearable
devices for treating post-traumatic stress disorder (PTSD) and enabling
naturalistic brain research. In PTSD Therapy Mode, an implanted closed-loop
neural device monitors amygdala activity and provides on-demand stimulation
upon detecting pathological theta oscillations, while an ensemble of wearables
(smart glasses, smartwatches, smartphones) uses multimodal large language model
(LLM) analysis of sensory data to detect environmental or physiological PTSD
triggers and deliver timely audiovisual interventions. Logged events from both
the neural and wearable loops are analyzed to personalize trigger detection and
progressively transition patients to non-invasive interventions. In
Neuroscience Research Mode, the same platform is adapted for real-world brain
activity capture. Wearable-LLM systems recognize naturalistic events (social
interactions, emotional situations, compulsive behaviors, decision making) and
signal implanted RNS devices (via wireless triggers) to record synchronized
intracranial data during these moments. This approach builds on recent advances
in mobile intracranial EEG recording and closed-loop neuromodulation in humans
(BRAIN Initiative, 2023) (Mobbs et al., 2021). We discuss how our
interdisciplinary system could revolutionize PTSD therapy and cognitive
neuroscience by enabling 24/7 monitoring, context-aware intervention, and rich
data collection outside traditional labs. The vision is a future where
AI-enhanced devices continuously collaborate with the human brain, offering
therapeutic support and deep insights into neural function, with the resulting
real-world context rich neural data, in turn, accelerating the development of
more biologically-grounded and human-centric AI.


## Leveraging Vision Capabilities of Multimodal LLMs for Automated Data Extraction from Plots

>Authors: Maciej P. Polak, Dane Morgan

>2025-03-16

> http://arxiv.org/abs/2503.12326v1

Automated data extraction from research texts has been steadily improving,
with the emergence of large language models (LLMs) accelerating progress even
further. Extracting data from plots in research papers, however, has been such
a complex task that it has predominantly been confined to manual data
extraction. We show that current multimodal large language models, with proper
instructions and engineered workflows, are capable of accurately extracting
data from plots. This capability is inherent to the pretrained models and can
be achieved with a chain-of-thought sequence of zero-shot engineered prompts we
call PlotExtract, without the need to fine-tune. We demonstrate PlotExtract
here and assess its performance on synthetic and published plots. We consider
only plots with two axes in this analysis. For plots identified as extractable,
PlotExtract finds points with over 90% precision (and around 90% recall) and
errors in x and y position of around 5% or lower. These results prove that
multimodal LLMs are a viable path for high-throughput data extraction for plots
and in many circumstances can replace the current manual methods of data
extraction.


## Changing Base Without Losing Pace A GPU-Efficient Alternative to MatMul in DNNs

>Authors: Nir Ailon, Akhiad Bercovich, Omri Weinstein

>2025-03-15

> http://arxiv.org/abs/2503.12211v1

We propose a cheaper alternative bilinear operator to matrix-multiplication
in deep neural networks (DNNs). Unlike many stubborn attempts to accelerate
MatMuls in DNN inference, this operator is supported by capabilities of
existing GPU hardware, most notably NVIDIA TensorCores. To our knowledge, this
is the first GPU-native **acceleration** technique which \emph{does not decrease}
(in fact, increases) the number of trainable parameters of the network,
mitigating the accuracy-loss of compression-based techniques. Hence, this
operator is at the same time more expressive than MatMul, yet requires
substantially \emph{fewer} FLOPs to evaluate. We term this new operator
\emph{Strassen-Tile} (STL).
  The main idea behind STL$(X,W)$ is a \emph{local} change-of-basis (learnable
encoder) on weights and activation \emph{tiles}, after which we perform batched
\emph{elementwise} products between tiles, and a final decoding transformation
(inspired by algebraic pipelines from fast matrix and polynomial
multiplication).
  We compare STL against two benchmarks. The first one is SoTA T2T-ViT on
Imagenet-1K. Here we show that replacing \emph{all} linear layers with STL and
training from scratch, results in factor x2.7 reduction in FLOPs with a 0.5
\emph{accuracy improvement}. Our second speed-accuracy comparison benchmark for
pretrained LLMs is the most practical GPU-**acceleration** technique, \twofour
structured Sparsity. Finetuning TinyLlama \cite{tinyllama24} with STL layers on
the Slim Pajama dataset, achieves similar accuracy to 2:4, with x2.2 FLOP
speedup compared to x1.7 of the latter.
  Finally, we discuss a group-theoretic approach for discovering
\emph{universal} encoders for STL, which could lead to fast \emph{black-box}
**acceleration** via approximate matrix-multiplication (AMM).


## Bridging Textual-Collaborative Gap through Semantic Codes for Sequential Recommendation

>Authors: Enze Liu, Bowen Zheng, Wayne Xin Zhao, Ji-Rong Wen

>2025-03-15

> http://arxiv.org/abs/2503.12183v1

In recent years, substantial research efforts have been devoted to enhancing
sequential recommender systems by integrating abundant side information with
ID-based collaborative information. This study specifically focuses on
leveraging the textual metadata (e.g., titles and brands) associated with
items. While existing methods have achieved notable success by combining text
and ID representations, they often struggle to strike a balance between textual
information embedded in text representations and collaborative information from
sequential patterns of user behavior. In light of this, we propose CoCoRec, a
novel Code-based textual and Collaborative semantic fusion method for
sequential Recommendation. The key idea behind our approach is to bridge the
gap between textual and collaborative information using semantic codes.
Specifically, we generate fine-grained semantic codes from multi-view text
embeddings through vector **quantization** techniques. Subsequently, we develop a
code-guided semantic-fusion module based on the cross-attention mechanism to
flexibly extract and integrate relevant information from text representations.
In order to further enhance the fusion of textual and collaborative semantics,
we introduce an optimization strategy that employs code masking with two
specific objectives: masked code modeling and masked sequence alignment. The
merit of these objectives lies in leveraging mask prediction tasks and
augmented item representations to capture code correlations within individual
items and enhance the sequence modeling of the recommendation backbone.
Extensive experiments conducted on four public datasets demonstrate the
superiority of CoCoRec, showing significant improvements over various
sequential recommendation models. Our code is available at
https://anonymous.4open.science/r/CoCoRec-6E41.


## PLM Efficient Peripheral Language Models Hardware-Co-Designed for Ubiquitous Computing

>Authors: Cheng Deng, Luoyang Sun, Jiwen Jiang, Yongcheng Zeng, Xinjian Wu, Wenxin Zhao, Qingfa Xiao, Jiachuan Wang, Haoyang Li, Lei Chen, Lionel M. Ni, Haifeng Zhang, Jun Wang

>2025-03-15

> http://arxiv.org/abs/2503.12167v2

While scaling laws have been continuously validated in large language models
(LLMs) with increasing model parameters, the inherent tension between the
inference demands of LLMs and the limited resources of edge devices poses a
critical challenge to the development of edge intelligence. Recently, numerous
small language models have emerged, aiming to distill the capabilities of LLMs
into smaller footprints. However, these models often retain the fundamental
architectural principles of their larger counterparts, still imposing
considerable strain on the storage and bandwidth capacities of edge devices. In
this paper, we introduce the PLM, a Peripheral Language Model, developed
through a co-design process that jointly optimizes model architecture and edge
system constraints. The PLM utilizes a Multi-head Latent Attention mechanism
and employs the squared ReLU activation function to encourage **sparsity**, thereby
reducing peak memory footprint during inference. During training, we collect
and reorganize open-source datasets, implement a multi-phase training strategy,
and empirically investigate the Warmup-Stable-Decay-Constant (WSDC) learning
rate scheduler. Additionally, we incorporate Reinforcement Learning from Human
Feedback (RLHF) by adopting the ARIES preference learning approach. Following a
two-phase SFT process, this method yields performance gains of 2% in general
tasks, 9% in the GSM8K task, and 11% in coding tasks. In addition to its novel
architecture, evaluation results demonstrate that PLM outperforms existing
small language models trained on publicly available data while maintaining the
lowest number of activated parameters. Furthermore, deployment across various
edge devices, including consumer-grade GPUs, mobile phones, and Raspberry Pis,
validates PLM's suitability for peripheral applications. The PLM series models
are publicly available at https://github.com/plm-team/PLM.


## SFMNet Sparse Focal Modulation for 3D Object Detection

>Authors: Oren Shrout, Ayellet Tal

>2025-03-15

> http://arxiv.org/abs/2503.12093v1

We propose SFMNet, a novel 3D **sparse** detector that combines the efficiency of
**sparse** convolutions with the ability to model long-range dependencies. While
traditional **sparse** convolution techniques efficiently capture local structures,
they struggle with modeling long-range relationships. However, capturing
long-range dependencies is fundamental for 3D object detection. In contrast,
transformers are designed to capture these long-range dependencies through
attention mechanisms. But, they come with high computational costs, due to
their quadratic query-key-value interactions. Furthermore, directly applying
attention to non-empty voxels is inefficient due to the **sparse** nature of 3D
scenes. Our SFMNet is built on a novel Sparse Focal Modulation (SFM) module,
which integrates short- and long-range contexts with linear complexity by
leveraging a new hierarchical **sparse** convolution design. This approach enables
SFMNet to achieve high detection performance with improved efficiency, making
it well-suited for large-scale LiDAR scenes. We show that our detector achieves
state-of-the-art performance on autonomous driving datasets.


## Diffusion Dynamics Models with Generative State Estimation for Cloth Manipulation

>Authors: Tongxuan Tian, Haoyang Li, Bo Ai, Xiaodi Yuan, Zhiao Huang, Hao Su

>2025-03-15

> http://arxiv.org/abs/2503.11999v1

Manipulating deformable objects like cloth is challenging due to their
complex dynamics, near-infinite degrees of freedom, and frequent
self-occlusions, which complicate state estimation and dynamics modeling. Prior
work has struggled with robust cloth state estimation, while dynamics models,
primarily based on Graph Neural Networks (GNNs), are limited by their locality.
Inspired by recent advances in generative models, we hypothesize that these
expressive models can effectively capture intricate cloth configurations and
deformation patterns from data. Building on this insight, we propose a
diffusion-based generative approach for both perception and dynamics modeling.
Specifically, we formulate state estimation as reconstructing the full cloth
state from **sparse** RGB-D observations conditioned on a canonical cloth mesh and
dynamics modeling as predicting future states given the current state and robot
actions. Leveraging a transformer-based diffusion model, our method achieves
high-fidelity state reconstruction while reducing long-horizon dynamics
prediction errors by an order of magnitude compared to GNN-based approaches.
Integrated with model-predictive control (MPC), our framework successfully
executes cloth folding on a real robotic system, demonstrating the potential of
generative models for manipulation tasks with partial observability and complex
dynamics.


## Fraesormer Learning Adaptive Sparse Transformer for Efficient Food Recognition

>Authors: Shun Zou, Yi Zou, Mingya Zhang, Shipeng Luo, Zhihao Chen, Guangwei Gao

>2025-03-15

> http://arxiv.org/abs/2503.11995v1

In recent years, Transformer has witnessed significant progress in food
recognition. However, most existing approaches still face two critical
challenges in lightweight food recognition: (1) the quadratic complexity and
redundant feature representation from interactions with irrelevant tokens; (2)
static feature recognition and single-scale representation, which overlook the
unstructured, non-fixed nature of food images and the need for multi-scale
features. To address these, we propose an adaptive and efficient **sparse**
Transformer architecture (Fraesormer) with two core designs: Adaptive Top-k
Sparse Partial Attention (ATK-SPA) and Hierarchical Scale-Sensitive Feature
Gating Network (HSSFGN). ATK-SPA uses a learnable Gated Dynamic Top-K Operator
(GDTKO) to retain critical attention scores, filtering low query-key matches
that hinder feature aggregation. It also introduces a partial channel mechanism
to reduce redundancy and promote expert information flow, enabling local-global
collaborative modeling. HSSFGN employs gating mechanism to achieve multi-scale
feature representation, enhancing contextual semantic information. Extensive
experiments show that Fraesormer outperforms state-of-the-art methods. code is
available at https://zs1314.github.io/Fraesormer.


## Key, Value, Compress A Systematic Exploration of KV Cache Compression Techniques

>Authors: Neusha Javidnia, Bita Darvish Rouhani, Farinaz Koushanfar

>2025-03-14

> http://arxiv.org/abs/2503.11816v1

Large language models (LLMs) have demonstrated exceptional capabilities in
generating text, images, and video content. However, as context length grows,
the computational cost of attention increases quadratically with the number of
tokens, presenting significant efficiency challenges. This paper presents an
analysis of various Key-Value (**KV**) cache compression strategies, offering a
comprehensive taxonomy that categorizes these methods by their underlying
principles and implementation techniques. Furthermore, we evaluate their impact
on performance and inference latency, providing critical insights into their
effectiveness. Our findings highlight the trade-offs involved in **KV** cache
compression and its influence on handling long-context scenarios, paving the
way for more efficient LLM implementations.


## Tensor Convolutional Network for Higher-Order Interaction Prediction in Sparse Tensors

>Authors: Jun-Gi Jang, Jingrui He, Andrew Margenot, Hanghang Tong

>2025-03-14

> http://arxiv.org/abs/2503.11786v1

Many real-world data, such as recommendation data and temporal graphs, can be
represented as incomplete **sparse** tensors where most entries are unobserved. For
such **sparse** tensors, identifying the top-k higher-order interactions that are
most likely to occur among unobserved ones is crucial. Tensor factorization
(TF) has gained significant attention in various tensor-based applications,
serving as an effective method for finding these top-k potential interactions.
However, existing TF methods primarily focus on effectively fusing latent
vectors of entities, which limits their expressiveness. Since most entities in
**sparse** tensors have only a few interactions, their latent representations are
often insufficiently trained. In this paper, we propose TCN, an accurate and
compatible tensor convolutional network that integrates seamlessly with
existing TF methods for predicting higher-order interactions. We design a
highly effective encoder to generate expressive latent vectors of entities. To
achieve this, we propose to (1) construct a graph structure derived from a
**sparse** tensor and (2) develop a relation-aware encoder, TCN, that learns latent
representations of entities by leveraging the graph structure. Since TCN
complements traditional TF methods, we seamlessly integrate TCN with existing
TF methods, enhancing the performance of predicting top-k interactions.
Extensive experiments show that TCN integrated with a TF method outperforms
competitors, including TF methods and a hyperedge prediction method. Moreover,
TCN is broadly compatible with various TF methods and GNNs (Graph Neural
Networks), making it a versatile solution.


## ASMA-Tune Unlocking LLMs' Assembly Code Comprehension via Structural-Semantic Instruction Tuning

>Authors: Xinyi Wang, Jiashui Wang, Peng Chen, Jinbo Su, Yanming Liu, Long Liu, Yangdong Wang, Qiyuan Chen, Kai Yun, Chunfu Jia

>2025-03-14

> http://arxiv.org/abs/2503.11617v1

Analysis and comprehension of assembly code are crucial in various
applications, such as reverse engineering. However, the low information density
and lack of explicit syntactic structures in assembly code pose significant
challenges. Pioneering approaches with masked language modeling (MLM)-based
methods have been limited by facilitating natural language interaction. While
recent methods based on decoder-focused large language models (LLMs) have
significantly enhanced semantic representation, they still struggle to capture
the nuanced and **sparse** semantics in assembly code. In this paper, we propose
Assembly Augmented Tuning (ASMA-Tune), an end-to-end structural-semantic
instruction-tuning framework. Our approach synergizes encoder architectures
with decoder-based LLMs through projector modules to enable comprehensive code
understanding. Experiments show that ASMA-Tune outperforms existing benchmarks,
significantly enhancing assembly code comprehension and instruction-following
abilities. Our model and dataset are public at
https://github.com/wxy3596/ASMA-Tune.


## BioMamba Leveraging Spectro-Temporal Embedding in Bidirectional Mamba for Enhanced Biosignal Classification

>Authors: Jian Qian, Teck Lun Goh, Bingyu Xie, Chengyao Zhu, Biao Wan, Yawen Guan, Patrick Yin Chiang

>2025-03-14

> http://arxiv.org/abs/2503.11741v2

Biological signals, such as electroencephalograms (EEGs) and
electrocardiograms (ECGs), play a pivotal role in numerous clinical practices,
such as diagnosing brain and cardiac arrhythmic diseases. Existing methods for
biosignal classification rely on Attention-based frameworks with dense Feed
Forward layers, which lead to inefficient learning, high computational
overhead, and suboptimal performance. In this work, we introduce BioMamba, a
Spectro-Temporal Embedding strategy applied to the Bidirectional Mamba
framework with Sparse Feed Forward layers to enable effective learning of
biosignal sequences. By integrating these three key components, BioMamba
effectively addresses the limitations of existing methods. Extensive
experiments demonstrate that BioMamba significantly outperforms
state-of-the-art methods with marked improvement in classification performance.
The advantages of the proposed BioMamba include (1) Reliability: BioMamba
consistently delivers robust results, confirmed across six evaluation metrics.
(2) Efficiency: We assess both model and training efficiency, the BioMamba
demonstrates computational effectiveness by reducing model size and resource
consumption compared to existing approaches. (3) Generality: With the capacity
to effectively classify a diverse set of tasks, BioMamba demonstrates
adaptability and effectiveness across various domains and applications.


## Similarity-Aware Token Pruning Your VLM but Faster

>Authors: Ahmadreza Jeddi, Negin Baghbanzadeh, Elham Dolatabadi, Babak Taati

>2025-03-14

> http://arxiv.org/abs/2503.11549v1

The computational demands of Vision Transformers (ViTs) and Vision-Language
Models (VLMs) remain a significant challenge due to the quadratic complexity of
self-attention. While token **pruning** offers a promising solution, existing
methods often introduce training overhead or fail to adapt dynamically across
layers. We present SAINT, a training-free token **pruning** framework that
leverages token similarity and a graph-based formulation to dynamically
optimize **pruning** rates and redundancy thresholds. Through systematic analysis,
we identify a universal three-stage token evolution process
(aligner-explorer-aggregator) in transformers, enabling aggressive **pruning** in
early stages without sacrificing critical information. For ViTs, SAINT doubles
the throughput of ViT-H/14 at 224px with only 0.6% accuracy loss on
ImageNet-1K, surpassing the closest competitor by 0.8%. For VLMs, we apply
SAINT in three modes: ViT-only, LLM-only, and hybrid. SAINT reduces LLaVA-13B's
tokens by 75%, achieving latency comparable to LLaVA-7B with less than 1%
performance loss across benchmarks. Our work establishes a unified, practical
framework for efficient inference in ViTs and VLMs.


## Multi-View Node Pruning for Accurate Graph Representation

>Authors: Jiseong Park, Hanjin Kim, Seojin Kim, Jueun Choi

>2025-03-14

> http://arxiv.org/abs/2503.11737v2

Graph pooling, which compresses a whole graph into a smaller coarsened graph,
is an essential component of graph representation learning. To efficiently
compress a given graph, graph pooling methods often drop their nodes with
attention-based scoring with the task loss. However, this often results in
simply removing nodes with lower degrees without consideration of their
feature-level relevance to the given task. To fix this problem, we propose a
Multi-View Pruning(MVP), a graph **pruning** method based on a multi-view framework
and reconstruction loss. Given a graph, MVP first constructs multiple graphs
for different views either by utilizing the predefined modalities or by
randomly partitioning the input features, to consider the importance of each
node in diverse perspectives. Then, it learns the score for each node by
considering both the reconstruction and the task loss. MVP can be incorporated
with any hierarchical pooling framework to score the nodes. We validate MVP on
multiple benchmark datasets by coupling it with two graph pooling methods, and
show that it significantly improves the performance of the base graph pooling
method, outperforming all baselines. Further analysis shows that both the
encoding of multiple views and the consideration of reconstruction loss are the
key to the success of MVP, and that it indeed identifies nodes that are less
important according to domain knowledge.


## Text Compression for Efficient Language Generation

>Authors: David Gu, Peter Belcak, Roger Wattenhofer

>2025-03-14

> http://arxiv.org/abs/2503.11426v1

We challenge the prevailing assumption that LLMs must rely fully on sub-word
tokens for high-quality text generation. To this end, we propose the
"Generative Pretrained Thoughtformer" (GPTHF), a hierarchical transformer
language model capable of text generation by compressing text into sentence
embeddings and employing a sentence attention mechanism. GPTHF retains GPT's
architecture, modifying only token interactions via dynamic **sparse** attention
masks.
  Our experiments show that GPTHF achieves an up to an order of magnitude
improvement in FLOPs efficiency and a threefold increase in runtime speed
compared to equally-sized GPT models in the low-size regime. This is achieved
through a unique generation method that caches and reuses sentence embeddings,
allowing significant portions of the input to bypass large parts of the
network.


## TransiT Transient Transformer for Non-line-of-sight Videography

>Authors: Ruiqian Li, Siyuan Shen, Suan Xia, Ziheng Wang, Xingyue Peng, Chengxuan Song, Yingsheng Zhu, Tao Wu, Shiying Li, Jingyi Yu

>2025-03-14

> http://arxiv.org/abs/2503.11328v1

High quality and high speed videography using Non-Line-of-Sight (NLOS)
imaging benefit autonomous navigation, collision prevention, and post-disaster
search and rescue tasks. Current solutions have to balance between the frame
rate and image quality. High frame rates, for example, can be achieved by
reducing either per-point scanning time or scanning density, but at the cost of
lowering the information density at individual frames. Fast scanning process
further reduces the signal-to-noise ratio and different scanning systems
exhibit different distortion characteristics. In this work, we design and
employ a new Transient Transformer architecture called TransiT to achieve
real-time NLOS recovery under fast scans. TransiT directly compresses the
temporal dimension of input transients to extract features, reducing
computation costs and meeting high frame rate requirements. It further adopts a
feature fusion mechanism as well as employs a spatial-temporal Transformer to
help capture features of NLOS transient videos. Moreover, TransiT applies
transfer learning to bridge the gap between synthetic and real-measured data.
In real experiments, TransiT manages to reconstruct from **sparse** transients of
$16 \times 16$ measured at an exposure time of 0.4 ms per point to NLOS videos
at a $64 \times 64$ resolution at 10 frames per second. We will make our code
and dataset available to the community.


## When Do Transformers Outperform Feedforward and Recurrent Networks? A Statistical Perspective

>Authors: Alireza Mousavi-Hosseini, Clayton Sanford, Denny Wu, Murat A. Erdogdu

>2025-03-14

> http://arxiv.org/abs/2503.11272v1

Theoretical efforts to prove advantages of Transformers in comparison with
classical architectures such as feedforward and recurrent neural networks have
mostly focused on representational power. In this work, we take an alternative
perspective and prove that even with infinite compute, feedforward and
recurrent networks may suffer from larger sample complexity compared to
Transformers, as the latter can adapt to a form of dynamic **sparsity**.
Specifically, we consider a sequence-to-sequence data generating model on
sequences of length $N$, in which the output at each position depends only on
$q$ relevant tokens with $q \ll N$, and the positions of these tokens are
described in the input prompt. We prove that a single-layer Transformer can
learn this model if and only if its number of attention heads is at least $q$,
in which case it achieves a sample complexity almost independent of $N$, while
recurrent networks require $N^{\Omega(1)}$ samples on the same problem. If we
simplify this model, recurrent networks may achieve a complexity almost
independent of $N$, while feedforward networks still require $N$ samples.
Consequently, our proposed **sparse** retrieval model illustrates a natural
hierarchy in sample complexity across these architectures.


## PrivacyScalpel Enhancing LLM Privacy via Interpretable Feature Intervention with Sparse Autoencoders

>Authors: Ahmed Frikha, Muhammad Reza Ar Razi, Krishna Kanth Nakka, Ricardo Mendes, Xue Jiang, Xuebing Zhou

>2025-03-14

> http://arxiv.org/abs/2503.11232v1

Large Language Models (LLMs) have demonstrated remarkable capabilities in
natural language processing but also pose significant privacy risks by
memorizing and leaking Personally Identifiable Information (PII). Existing
mitigation strategies, such as differential privacy and neuron-level
interventions, often degrade model utility or fail to effectively prevent
leakage. To address this challenge, we introduce PrivacyScalpel, a novel
privacy-preserving framework that leverages LLM interpretability techniques to
identify and mitigate PII leakage while maintaining performance. PrivacyScalpel
comprises three key steps: (1) Feature Probing, which identifies layers in the
model that encode PII-rich representations, (2) Sparse Autoencoding, where a
k-Sparse Autoencoder (k-SAE) disentangles and isolates privacy-sensitive
features,
  and (3) Feature-Level Interventions, which employ targeted ablation and
vector steering to suppress PII leakage.
  Our empirical evaluation on Gemma2-2b and Llama2-7b, fine-tuned on the Enron
dataset, shows that PrivacyScalpel significantly reduces email leakage from
5.15\% to as low as 0.0\%, while maintaining over 99.4\% of the original
model's utility. Notably, our method outperforms neuron-level interventions in
privacy-utility trade-offs, demonstrating that acting on **sparse**, monosemantic
features is more effective than manipulating polysemantic neurons. Beyond
improving LLM privacy, our approach offers insights into the mechanisms
underlying PII memorization, contributing to the broader field of model
interpretability and secure AI deployment.


## FastVID Dynamic Density Pruning for Fast Video Large Language Models

>Authors: Leqi Shen, Guoqiang Gong, Tao He, Yifeng Zhang, Pengzhang Liu, Sicheng Zhao, Guiguang Ding

>2025-03-14

> http://arxiv.org/abs/2503.11187v1

Video Large Language Models have shown impressive capabilities in video
comprehension, yet their practical deployment is hindered by substantial
inference costs caused by redundant video tokens. Existing **pruning** techniques
fail to fully exploit the spatiotemporal redundancy inherent in video data. To
bridge this gap, we perform a systematic analysis of video redundancy from two
perspectives: temporal context and visual context. Leveraging this insight, we
propose Dynamic Density Pruning for Fast Video LLMs termed FastVID.
Specifically, FastVID dynamically partitions videos into temporally ordered
segments to preserve temporal structure and applies a density-based token
**pruning** strategy to maintain essential visual information. Our method
significantly reduces computational overhead while maintaining temporal and
visual integrity. Extensive evaluations show that FastVID achieves
state-of-the-art performance across various short- and long-video benchmarks on
leading Video LLMs, including LLaVA-OneVision and LLaVA-Video. Notably, FastVID
effectively prunes 90% of video tokens while retaining 98.0% of
LLaVA-OneVision's original performance. The code is available at
https://github.com/LunarShen/FastVID.


## Learnable Group Transform Enhancing Genotype-to-Phenotype Prediction for Rice Breeding with Small, Structured Datasets

>Authors: Yunxuan Dong, Siyuan Chen, Jisen Zhang

>2025-03-14

> http://arxiv.org/abs/2503.11180v1

Genotype-to-Phenotype (G2P) prediction plays a pivotal role in crop breeding,
enabling the identification of superior genotypes based on genomic data. Rice
(Oryza sativa), one of the most important staple crops, faces challenges in
improving yield and resilience due to the complex genetic architecture of
agronomic traits and the limited sample size in breeding datasets. Current G2P
prediction methods, such as GWAS and linear models, often fail to capture
complex non-linear relationships between genotypes and phenotypes, leading to
suboptimal prediction accuracy. Additionally, population stratification and
overfitting are significant obstacles when models are applied to small datasets
with diverse genetic backgrounds. This study introduces the Learnable Group
Transform (LGT) method, which aims to overcome these challenges by combining
the advantages of traditional linear models with advanced machine learning
techniques. LGT utilizes a group-based transformation of genotype data to
capture spatial relationships and genetic structures across diverse rice
populations, offering flexibility to generalize even with limited data. Through
extensive experiments on the Rice529 dataset, a panel of 529 rice accessions,
LGT demonstrated substantial improvements in prediction accuracy for multiple
agronomic traits, including yield and plant height, compared to
state-of-the-art baselines such as linear models and recent deep learning
approaches. Notably, LGT achieved an R^2 improvement of up to 15\% for yield
prediction, significantly reducing error and demonstrating its ability to
extract meaningful signals from high-dimensional, noisy genomic data. These
results highlight the potential of LGT as a powerful tool for genomic
prediction in rice breeding, offering a promising solution for accelerating the
identification of high-yielding and resilient rice varieties.


## Multi-constraint Graph Partitioning Problems Via Recursive Bipartition Algorithm Based on Subspace Minimization Conjugate Gradient Method

>Authors: Wumwi Sun, Hongwei Liu, Xiaoyu Wang

>2025-03-14

> http://arxiv.org/abs/2503.11168v1

The graph partitioning problem is a well-known NP-hard problem. In this
paper, we formulate a 0-1 quadratic integer programming model for the graph
partitioning problem with vertex weight constraints and fixed vertex
constraints, and propose a recursive bipartition algorithm based on the
subspace minimization conjugate gradient method. To alleviate the difficulty of
solving the model, the constrained problem is transformed into an unconstrained
optimization problem using equilibrium terms, elimination methods, and
trigonometric properties, and solved via an accelerated subspace minimization
conjugate gradient algorithm. Initial feasible partitions are generated using a
hyperplane rounding algorithm, followed by heuristic refinement strategies,
including one-neighborhood and two-interchange adjustments, to iteratively
improve the results. Numerical experiments on knapsack-constrained graph
partitioning and industrial examples demonstrate the effectiveness and
feasibility of the proposed algorithm.


## Towards Extreme Pruning of LLMs with Plug-and-Play Mixed Sparsity

>Authors: Chi Xu, Gefei Zhang, Yantong Zhu, Luca Benini, Guosheng Hu, Yawei Li, Zhihong Zhang

>2025-03-14

> http://arxiv.org/abs/2503.11164v1

N:M structured **pruning** is essential for large language models (LLMs) because
it can remove less important network weights and reduce the memory and
computation requirements. Existing **pruning** methods mainly focus on designing
metrics to measure the importance of network components to guide **pruning**. Apart
from the impact of these metrics, we observe that different layers have
different sensitivities over the network performance. Thus, we propose an
efficient method based on the trace of Fisher Information Matrix (FIM) to
quantitatively measure and verify the different sensitivities across layers.
Based on this, we propose Mixed Sparsity Pruning (MSP) which uses a
**pruning**-oriented evolutionary algorithm (EA) to determine the optimal **sparsity**
levels for different layers. To guarantee fast convergence and achieve
promising performance, we utilize efficient FIM-inspired layer-wise sensitivity
to initialize the population of EA. In addition, our MSP can work as a
plug-and-play module, ready to be integrated into existing **pruning** methods.
Extensive experiments on LLaMA and LLaMA-2 on language modeling and zero-shot
tasks demonstrate our superior performance. In particular, in extreme **pruning**
ratio (e.g. 75%), our method significantly outperforms existing methods in
terms of perplexity (PPL) by orders of magnitude (Figure 1).


## X-EcoMLA Upcycling Pre-Trained Attention into MLA for Efficient and Extreme KV Compression

>Authors: Guihong Li, Mehdi Rezagholizadeh, Mingyu Yang, Vikram Appia, Emad Barsoum

>2025-03-14

> http://arxiv.org/abs/2503.11132v1

Multi-head latent attention (MLA) is designed to optimize **KV** cache memory
through low-rank key-value joint compression. Rather than caching keys and
values separately, MLA stores their compressed latent representations, reducing
memory overhead while maintaining the performance. While MLA improves memory
efficiency without compromising language model accuracy, its major limitation
lies in its integration during the pre-training phase, requiring models to be
trained from scratch. This raises a key question: can we use MLA's benefits
fully or partially in models that have already been pre-trained with different
attention mechanisms? In this paper, we propose X-EcoMLA to deploy post
training distillation to enable the upcycling of Transformer-based attention
into an efficient hybrid (i.e., combination of regular attention and MLA
layers) or full MLA variant through lightweight post-training adaptation,
bypassing the need for extensive pre-training. We demonstrate that leveraging
the dark knowledge of a well-trained model can enhance training accuracy and
enable extreme **KV** cache compression in MLA without compromising model
performance. Our results show that using an 8B teacher model allows us to
compress the **KV** cache size of the Llama3.2-1B-Inst baseline by 6.4x while
preserving 100% of its average score across multiple tasks on the LM Harness
Evaluation benchmark. This is achieved with only 3.6B training tokens and about
70 GPU hours on AMD MI300 GPUs, compared to the 370K GPU hours required for
pre-training the Llama3.2-1B model.


## Don't Forget It! Conditional Sparse Autoencoder Clamping Works for Unlearning

>Authors: Matthew Khoriaty, Andrii Shportko, Gustavo Mercier, Zach Wood-Doughty

>2025-03-14

> http://arxiv.org/abs/2503.11127v1

Recent developments in Large Language Model (LLM) capabilities have brought
great potential but also posed new risks. For example, LLMs with knowledge of
bioweapons, advanced chemistry, or cyberattacks could cause violence if placed
in the wrong hands or during malfunctions. Because of their nature as
near-black boxes, intuitive interpretation of LLM internals remains an open
research question, preventing developers from easily controlling model behavior
and capabilities. The use of Sparse Autoencoders (SAEs) has recently emerged as
a potential method of unraveling representations of concepts in LLMs internals,
and has allowed developers to steer model outputs by directly modifying the
hidden activations. In this paper, we use SAEs to identify unwanted concepts
from the Weapons of Mass Destruction Proxy (WMDP) dataset within gemma-2-2b
internals and use feature steering to reduce the model's ability to answer
harmful questions while retaining its performance on harmless queries. Our
results bring back optimism to the viability of SAE-based explicit knowledge
unlearning techniques.


## Modeling and Optimization for Flexible Cylindrical Arrays-Enabled Wireless Communications

>Authors: Songjie Yang, Jiahe Guo, Zilin He, Boyu Ning, Weidong Mei, Zhongpei Zhang, Chadi Assi, Chau Yuen

>2025-03-14

> http://arxiv.org/abs/2503.11123v1

Flexible-geometry arrays have garnered much attention in wireless
communications, which dynamically adjust wireless channels to improve the
system performance. In this paper, we propose a novel flexible-geometry array
for a $360^\circ$ coverage, named flxible cylindrical array (FCLA), comprised
of multiple flexible circular arrays (FCAs). The elements in each FCA can
revolve around the circle track to change their horizontal positions, and the
FCAs can move along the vertical axis to change the elements' heights.
Considering that horizontal revolving can change the antenna orientation, we
adopt both the omni-directional and the directional antenna patterns. Based on
the regularized zero-forcing (RZF) precoding scheme, we formulate a particular
compressive sensing (CS) problem incorporating joint precoding and antenna
position optimization, and propose two effective methods, namely FCLA-J and
FCLA-A, to solve it. Specifically, the first method involves jointly optimizing
the element's revolving angle, height, and precoding coefficient within a
single CS framework. The second method decouples the CS problem into two
subproblems by utilizing an alternative **sparse** optimization approach for the
revolving angle and height, thereby reducing time complexity. Simulation
results reveal that, when utilizing directional radiation patterns, FCLA-J and
FCLA-A achieve substantial performance improvements of 43.32\% and 25.42\%,
respectively, compared to uniform cylindrical arrays (UCLAs) with RZF
precoding.


## Limits of KV Cache Compression for Tensor Attention based Autoregressive Transformers

>Authors: Yifang Chen, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song, Yu Tian

>2025-03-14

> http://arxiv.org/abs/2503.11108v1

The key-value (**KV**) cache in autoregressive transformers presents a
significant bottleneck during inference, which restricts the context length
capabilities of large language models (LLMs). While previous work analyzes the
fundamental space complexity barriers in standard attention mechanism [Haris
and Onak, 2025], our work generalizes the space complexity barriers result to
tensor attention version. Our theoretical contributions rely on a novel
reduction from communication complexity and deduce the memory lower bound for
tensor-structured attention mechanisms when $d = \Omega(\log n)$. In the low
dimensional regime where $d = o(\log n)$, we analyze the theoretical bounds of
the space complexity as well. Overall, our work provides a theoretical
foundation for us to understand the compression-expressivity tradeoff in tensor
attention mechanisms and offers more perspectives in developing more
memory-efficient transformer architectures.


## Quantifying Interpretability in CLIP Models with Concept Consistency

>Authors: Avinash Madasu, Vasudev Lal, Phillip Howard

>2025-03-14

> http://arxiv.org/abs/2503.11103v1

CLIP is one of the most popular foundational models and is heavily used for
many vision-language tasks. However, little is known about the inner workings
of CLIP. While recent work has proposed decomposition-based interpretability
methods for identifying textual descriptions of attention heads in CLIP, the
implications of conceptual consistency in these text labels on interpretability
and model performance has not been explored. To bridge this gap, we study the
conceptual consistency of text descriptions for attention heads in CLIP-like
models. We conduct extensive experiments on six different models from OpenAI
and OpenCLIP which vary by size, type of pre-training data and patch size. We
propose Concept Consistency Score (CCS), a novel interpretability metric that
measures how consistently individual attention heads in CLIP models align with
specific concepts. To assign concept labels to heads, we use in-context
learning with ChatGPT, guided by a few manually-curated examples, and validate
these labels using an LLM-as-a-judge approach. Our soft-**pruning** experiments
reveal that high CCS heads are critical for preserving model performance, as
**pruning** them leads to a significantly larger performance drop than **pruning**
random or low CCS heads. Notably, we find that high CCS heads capture essential
concepts and play a key role in out-of-domain detection, concept-specific
reasoning, and video-language understanding. These results position CCS as a
powerful interpretability metric for analyzing CLIP-like models.


## DeepSeek Powered Solid Dosage Formulation Design and Development

>Authors: Leqi Lin, Xingyu Zhou, Kaiyuan Yang, Xizhong Chen

>2025-03-14

> http://arxiv.org/abs/2503.11068v1

Pharmaceutical process design and development for generic, innovative, or
personalized drugs have always been a time consuming, costly, rigorous process,
that involves multistage evaluation for better quality control and assurance.
Large language models (LLMs), a type of generative artificial intelligence
system, can augment laboratory research in the pharmaceutical engineering
process by helping scientists to extract knowledge from literature, design
parameters, and collect and interpret experimental data, ultimately
accelerating scientific discovery. LLMs with prompt engineering technologies
change the researcher's thinking protocol from traditional empirical knowledg
to streamlined thinking that connects the performance and structured parameters
together. In this work, we investigate and evaluate how prompt engineering
technologies can enhance the drug design process from different strategies such
as zero shot, few shot, chain of thought, etc. The dissolution profile for
specific drugs is predicted and suggested from the LLMs model. Furthermore, the
fundamental physical properties such as PSD, aspect ratio, and specific surface
area could be inversely designed from the LLMs model. Finally, all the results
are evaluated and validated by real-world cases to prove the reliability of
prompt engineering techniques. This work breaks down any barriers in developing
a systematic framework where LLMs assist in formulation design, process
control, and decision making. Finally, we conclude the work by discussing open
challenges and future research directions in pharmaceutical processes.


## Stochastic resolution of identity to CC2 for large systems Oscillator strength and ground state gradient calculations

>Authors: Chongxiao Zhao, Qi Ou, Chenyang Li, Wenjie Dou

>2025-03-14

> http://arxiv.org/abs/2503.11027v1

An implementation of stochastic resolution of identity (sRI) approximation to
CC2 oscillator strengths as well as ground state analytical gradients is
presented. The essential 4-index electron repulsion integrals (ERIs) are
contracted with a set of stochastic orbitals on the basis of the RI technique
and the orbital energy differences in the denominators are decoupled with the
Laplace transform. These lead to a significant scaling reduction from O(N^5) to
O(N^3) for oscillator strengths and gradients with the size of the basis set,
N. The gradients need a large number of stochastic orbitals with O(N^3), so we
provide an additional O(N^4) version with better accuracy and smaller prefactor
by adopting sRI partially. Such steep computational **acceleration** of nearly two
or one order of magnitude is very attractive for large systems. This work is an
extension to our previous implementations of sRI-CC2 ground and excited state
energies and shows the feasibility of introducing sRI to CC2 properties beyond
energies.


## Exploration of metastable A-site-ordered perovskites (Ca,Ba)FeO3-δ by computationally-guided multi-step synthesis

>Authors: Masaho Onose, Hidefumi Takahashi, Hajime Sagayama, Yuichi Yamasaki, Shintaro Ishiwata

>2025-03-14

> http://arxiv.org/abs/2503.10960v1

Perovskite-type iron oxides with Fe4+ ions have attracted much attention for
their versatile helimagnetic phases. While the introduction of a layered A-site
ordered structure to AFeO3 with Fe4+ ions potentially lead to novel
helimagnetic phases, the synthetic pathway spanning high pressure range is
apparently difficult to elucidate. Here, we explored new A-site ordered
perovskite-type iron oxides (Ca,Ba)FeO3-{\delta} with Fe4+ ions with the
support of first-principles calculations evaluating thermodynamic stability at
selected pressures and chemical compositions. Among the six types of putative
A-site ordered perovskites with and without oxygen vacancy, only two types of
oxygen-deficient perovskites CaBaFe2O6-{\delta} and
Ca(Ba0.9Ca0.1)2Fe3O9-{\delta} ({\delta}~1) were successfully obtained by
high-pressure synthesis, being consistent with the DFT-based convex-hull
calculations. Considering the evaluated stability of the putative perovskites
at selected pressures, we adopted low-temperature topotactic oxidation using
ozone at ambient pressure and obtained the oxidized perovskites
CaBaFe2O6-{\delta} ({\delta}~0.4) and Ca(Ba0.9Ca0.1)2Fe3O9-{\delta}
({\delta}~0.6), potentially showing novel helimagnetic phases. This study
demonstrates that computational visualization of multi-step synthetic pathways
involving high pressure can accelerate the search for new metastable
perovskites with rich magnetic phases.

