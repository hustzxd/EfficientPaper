# 2024-12-06

# Table of Contents
* [Style3D Attention-guided Multi-view Style Transfer for 3D Object Generation](#Style3D-Attention-guided-Multi-view-Style-Transfer-for-3D-Object-Generation)
* [A Stitch in Time Saves Nine Small VLM is a Precise Guidance for Accelerating Large VLMs](#A-Stitch-in-Time-Saves-Nine-Small-VLM-is-a-Precise-Guidance-for-Accelerating-Large-VLMs)
* [FlashAttention on a Napkin A Diagrammatic Approach to Deep Learning IO-Awareness](#FlashAttention-on-a-Napkin-A-Diagrammatic-Approach-to-Deep-Learning-IO-Awareness)
* [GERD Geometric event response data generation](#GERD-Geometric-event-response-data-generation)
* [AIM Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning](#AIM-Adaptive-Inference-of-Multi-Modal-LLMs-via-Token-Merging-and-Pruning)
* [Linq-Embed-Mistral Technical Report](#Linq-Embed-Mistral-Technical-Report)
* [Continual Low-Rank Scaled Dot-product Attention](#Continual-Low-Rank-Scaled-Dot-product-Attention)
* [ClusterKV Manipulating LLM KV Cache in Semantic Space for Recallable Compression](#ClusterKV-Manipulating-LLM-KV-Cache-in-Semantic-Space-for-Recallable-Compression)
* [Unifying KV Cache Compression for Large Language Models with LeanKV](#Unifying-KV-Cache-Compression-for-Large-Language-Models-with-LeanKV)
* [Revolve Optimizing AI Systems by Tracking Response Evolution in Textual Optimization](#Revolve-Optimizing-AI-Systems-by-Tracking-Response-Evolution-in-Textual-Optimization)
* [PEMF-VVTO Point-Enhanced Video Virtual Try-on via Mask-free Paradigm](#PEMF-VVTO-Point-Enhanced-Video-Virtual-Try-on-via-Mask-free-Paradigm)
* [EvRT-DETR The Surprising Effectiveness of DETR-based Detection for Event Cameras](#EvRT-DETR-The-Surprising-Effectiveness-of-DETR-based-Detection-for-Event-Cameras)
* [Effortless Efficiency Low-Cost Pruning of Diffusion Models](#Effortless-Efficiency-Low-Cost-Pruning-of-Diffusion-Models)
* [T-REG Preference Optimization with Token-Level Reward Regularization](#T-REG-Preference-Optimization-with-Token-Level-Reward-Regularization)
* [Ground State Energy Estimation on Current Quantum Hardware Through The Variational Quantum Eigensolver A Comprehensive Study](#Ground-State-Energy-Estimation-on-Current-Quantum-Hardware-Through-The-Variational-Quantum-Eigensolver-A-Comprehensive-Study)
* [Interpretable Company Similarity with Sparse Autoencoders](#Interpretable-Company-Similarity-with-Sparse-Autoencoders)
* [CEGI Measuring the trade-off between efficiency and carbon emissions for SLMs and VLMs](#CEGI-Measuring-the-trade-off-between-efficiency-and-carbon-emissions-for-SLMs-and-VLMs)
* [Hamiltonian Monte Carlo-Based Near-Optimal MIMO Signal Detection](#Hamiltonian-Monte-Carlo-Based-Near-Optimal-MIMO-Signal-Detection)
* [ScImage How Good Are Multimodal Large Language Models at Scientific Text-to-Image Generation?](#ScImage-How-Good-Are-Multimodal-Large-Language-Models-at-Scientific-Text-to-Image-Generation?)
* [CADMR Cross-Attention and Disentangled Learning for Multimodal Recommender Systems](#CADMR-Cross-Attention-and-Disentangled-Learning-for-Multimodal-Recommender-Systems)
* [Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity](#Compressing-KV-Cache-for-Long-Context-LLM-Inference-with-Inter-Layer-Attention-Similarity)
* [Unlocking Tuning-Free Few-Shot Adaptability in Visual Foundation Models by Recycling Pre-Tuned LoRAs](#Unlocking-Tuning-Free-Few-Shot-Adaptability-in-Visual-Foundation-Models-by-Recycling-Pre-Tuned-LoRAs)
* [3D representation in 512-ByteVariational tokenizer is the key for autoregressive 3D generation](#3D-representation-in-512-ByteVariational-tokenizer-is-the-key-for-autoregressive-3D-generation)
* [CLERF Contrastive LEaRning for Full Range Head Pose Estimation](#CLERF-Contrastive-LEaRning-for-Full-Range-Head-Pose-Estimation)
* [Comparing Dynamics, Pinning and Ratchet Effects for Skyrmionium, Skyrmions, and Antiskyrmions](#Comparing-Dynamics,-Pinning-and-Ratchet-Effects-for-Skyrmionium,-Skyrmions,-and-Antiskyrmions)
* [Measuring the expansion history of the Universe with cosmic chronometers](#Measuring-the-expansion-history-of-the-Universe-with-cosmic-chronometers)
* [RandAR Decoder-only Autoregressive Visual Generation in Random Orders](#RandAR-Decoder-only-Autoregressive-Visual-Generation-in-Random-Orders)
* [Switti Designing Scale-Wise Transformers for Text-to-Image Synthesis](#Switti-Designing-Scale-Wise-Transformers-for-Text-to-Image-Synthesis)
* [[CLS] Attention is All You Need for Training-Free Visual Token Pruning Make VLM Inference Faster](#[CLS]-Attention-is-All-You-Need-for-Training-Free-Visual-Token-Pruning-Make-VLM-Inference-Faster)
* [CTRL-D Controllable Dynamic 3D Scene Editing with Personalized 2D Diffusion](#CTRL-D-Controllable-Dynamic-3D-Scene-Editing-with-Personalized-2D-Diffusion)
* [ReHub Linear Complexity Graph Transformers with Adaptive Hub-Spoke Reassignment](#ReHub-Linear-Complexity-Graph-Transformers-with-Adaptive-Hub-Spoke-Reassignment)
* [Structured 3D Latents for Scalable and Versatile 3D Generation](#Structured-3D-Latents-for-Scalable-and-Versatile-3D-Generation)
* [Neutrinos from stochastic acceleration in black hole environments](#Neutrinos-from-stochastic-acceleration-in-black-hole-environments)
* [Early Exit Is a Natural Capability in Transformer-based Models An Empirical Study on Early Exit without Joint Optimization](#Early-Exit-Is-a-Natural-Capability-in-Transformer-based-Models-An-Empirical-Study-on-Early-Exit-without-Joint-Optimization)
* [PLD+ Accelerating LLM inference by leveraging Language Model Artifacts](#PLD+-Accelerating-LLM-inference-by-leveraging-Language-Model-Artifacts)
* [CPA Camera-pose-awareness Diffusion Transformer for Video Generation](#CPA-Camera-pose-awareness-Diffusion-Transformer-for-Video-Generation)
* [Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware Masking](#Efficient-LLM-Inference-using-Dynamic-Input-Pruning-and-Cache-Aware-Masking)
* [Yi-Lightning Technical Report](#Yi-Lightning-Technical-Report)
* [Multiple rebrightenings in the optical afterglow of GRB 210731A evidence for an asymmetric jet](#Multiple-rebrightenings-in-the-optical-afterglow-of-GRB-210731A-evidence-for-an-asymmetric-jet)
* [TinyFusion Diffusion Transformers Learned Shallow](#TinyFusion-Diffusion-Transformers-Learned-Shallow)
* [RILQ Rank-Insensitive LoRA-based Quantization Error Compensation for Boosting 2-bit Large Language Model Accuracy](#RILQ-Rank-Insensitive-LoRA-based-Quantization-Error-Compensation-for-Boosting-2-bit-Large-Language-Model-Accuracy)
* [TAS-TsC A Data-Driven Framework for Estimating Time of Arrival Using Temporal-Attribute-Spatial Tri-space Coordination of Truck Trajectories](#TAS-TsC-A-Data-Driven-Framework-for-Estimating-Time-of-Arrival-Using-Temporal-Attribute-Spatial-Tri-space-Coordination-of-Truck-Trajectories)
* [FreeCodec A disentangled neural speech codec with fewer tokens](#FreeCodec-A-disentangled-neural-speech-codec-with-fewer-tokens)
* [Quantization-Aware Imitation-Learning for Resource-Efficient Robotic Control](#Quantization-Aware-Imitation-Learning-for-Resource-Efficient-Robotic-Control)
* [Token Cropr Faster ViTs for Quite a Few Tasks](#Token-Cropr-Faster-ViTs-for-Quite-a-Few-Tasks)
* [Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion](#Advanced-Video-Inpainting-Using-Optical-Flow-Guided-Efficient-Diffusion)
* [DynSUP Dynamic Gaussian Splatting from An Unposed Image Pair](#DynSUP-Dynamic-Gaussian-Splatting-from-An-Unposed-Image-Pair)
* [DFRot Achieving Outlier-Free and Massive Activation-Free for Rotated LLMs with Refined Rotation](#DFRot-Achieving-Outlier-Free-and-Massive-Activation-Free-for-Rotated-LLMs-with-Refined-Rotation)
* [Pruned Convolutional Attention Network Based Wideband Spectrum Sensing with Sub-Nyquist Sampling](#Pruned-Convolutional-Attention-Network-Based-Wideband-Spectrum-Sensing-with-Sub-Nyquist-Sampling)
* [Accelerating Multimodel Large Language Models by Searching Optimal Vision Token Reduction](#Accelerating-Multimodel-Large-Language-Models-by-Searching-Optimal-Vision-Token-Reduction)
* [A novel algorithm for the decomposition of non-stationary multidimensional and multivariate signals](#A-novel-algorithm-for-the-decomposition-of-non-stationary-multidimensional-and-multivariate-signals)
* [Homeostazis and Sparsity in Transformer](#Homeostazis-and-Sparsity-in-Transformer)
* [Joint Beam Scheduling and Resource Allocation for Flexible RSMA-aided Near-Field Communications](#Joint-Beam-Scheduling-and-Resource-Allocation-for-Flexible-RSMA-aided-Near-Field-Communications)
* [ATP-LLaVA Adaptive Token Pruning for Large Vision Language Models](#ATP-LLaVA-Adaptive-Token-Pruning-for-Large-Vision-Language-Models)
* [CaDA Cross-Problem Routing Solver with Constraint-Aware Dual-Attention](#CaDA-Cross-Problem-Routing-Solver-with-Constraint-Aware-Dual-Attention)
* [Analyzing the Energy and Accuracy of LLMs in Software Development](#Analyzing-the-Energy-and-Accuracy-of-LLMs-in-Software-Development)
* [Sparse Bayesian Factor Models with Mass-Nonlocal Factor Scores](#Sparse-Bayesian-Factor-Models-with-Mass-Nonlocal-Factor-Scores)
* [An AI-Driven Data Mesh Architecture Enhancing Decision-Making in Infrastructure Construction and Public Procurement](#An-AI-Driven-Data-Mesh-Architecture-Enhancing-Decision-Making-in-Infrastructure-Construction-and-Public-Procurement)
* [N端shuRescue Revitalization of the endangered N端shu Language with AI](#N端shuRescue-Revitalization-of-the-endangered-N端shu-Language-with-AI)
* [Scaling Transformers for Low-Bitrate High-Quality Speech Coding](#Scaling-Transformers-for-Low-Bitrate-High-Quality-Speech-Coding)
* [Fast Mutual Information Computation for Large Binary Datasets](#Fast-Mutual-Information-Computation-for-Large-Binary-Datasets)
* [ChineseWebText 2.0 Large-Scale High-quality Chinese Web Text with Multi-dimensional and fine-grained information](#ChineseWebText-2.0-Large-Scale-High-quality-Chinese-Web-Text-with-Multi-dimensional-and-fine-grained-information)
* [CogACT A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation](#CogACT-A-Foundational-Vision-Language-Action-Model-for-Synergizing-Cognition-and-Action-in-Robotic-Manipulation)
* [KV Shifting Attention Enhances Language Modeling](#KV-Shifting-Attention-Enhances-Language-Modeling)
* [Quantized Delta Weight Is Safety Keeper](#Quantized-Delta-Weight-Is-Safety-Keeper)
* [Hierarchical Framework for Retrosynthesis Prediction with Enhanced Reaction Center Localization](#Hierarchical-Framework-for-Retrosynthesis-Prediction-with-Enhanced-Reaction-Center-Localization)
* [A Simple Sparse Matrix Vector Multiplication Approach to Padded Convolution](#A-Simple-Sparse-Matrix-Vector-Multiplication-Approach-to-Padded-Convolution)


## Style3D Attention-guided Multi-view Style Transfer for 3D Object Generation

>Authors: Bingjie Song, Xin Huang, Ruting Xie, Xue Wang, Qing Wang

>2024-12-04

> http://arxiv.org/abs/2412.03571v1

We present Style3D, a novel approach for generating stylized 3D objects from
a content image and a style image. Unlike most previous methods that require
case- or style-specific training, Style3D supports instant 3D object
stylization. Our key insight is that 3D object stylization can be decomposed
into two interconnected processes: multi-view dual-feature alignment and
**sparse**-view spatial reconstruction. We introduce MultiFusion Attention, an
attention-guided technique to achieve multi-view stylization from the
content-style pair. Specifically, the query features from the content image
preserve geometric consistency across multiple views, while the key and value
features from the style image are used to guide the stylistic transfer. This
dual-feature alignment ensures that spatial coherence and stylistic fidelity
are maintained across multi-view images. Finally, a large 3D reconstruction
model is introduced to generate coherent stylized 3D objects. By establishing
an interplay between structural and stylistic features across multiple views,
our approach enables a holistic 3D stylization process. Extensive experiments
demonstrate that Style3D offers a more flexible and scalable solution for
generating style-consistent 3D assets, surpassing existing methods in both
computational efficiency and visual quality.


## A Stitch in Time Saves Nine Small VLM is a Precise Guidance for Accelerating Large VLMs

>Authors: Wangbo Zhao, Yizeng Han, Jiasheng Tang, Zhikai Li, Yibing Song, Kai Wang, Zhangyang Wang, Yang You

>2024-12-04

> http://arxiv.org/abs/2412.03324v2

Vision-language models (VLMs) have shown remarkable success across various
multi-modal tasks, yet large VLMs encounter significant efficiency challenges
due to processing numerous visual tokens. A promising approach to accelerating
large VLM inference is using partial information, such as attention maps from
specific layers, to assess token importance and prune less essential tokens.
However, our study reveals three key insights: (i) Partial attention
information is insufficient for accurately identifying critical visual tokens,
resulting in suboptimal performance, especially at low token retention ratios;
(ii) Global attention information, such as the attention map aggregated across
all layers, more effectively preserves essential tokens and maintains
comparable performance under aggressive **pruning**. However, the attention maps
from all layers requires a full inference pass, which increases computational
load and is therefore impractical in existing methods; and (iii) The global
attention map aggregated from a small VLM closely resembles that of a large
VLM, suggesting an efficient alternative. Based on these findings, we introduce
a \textbf{training-free} method, \underline{\textbf{S}}mall VLM
\underline{\textbf{G}}uidance for accelerating \underline{\textbf{L}}arge VLMs
(\textbf{SGL}). Specifically, we employ the attention map aggregated from a
small VLM to guide visual token **pruning** in a large VLM. Additionally, an early
exiting mechanism is developed to fully use the small VLM's predictions,
dynamically invoking the larger VLM only when necessary, yielding a superior
trade-off between accuracy and computation. Extensive evaluations across 11
benchmarks demonstrate the effectiveness and generalizability of SGL, achieving
up to 91\% **pruning** ratio for visual tokens while retaining competitive
performance.


## FlashAttention on a Napkin A Diagrammatic Approach to Deep Learning IO-Awareness

>Authors: Vincent Abbott, Gioele Zardini

>2024-12-04

> http://arxiv.org/abs/2412.03317v1

Optimizing deep learning algorithms currently requires slow, manual
derivation, potentially leaving much performance untapped. Methods like
FlashAttention have achieved a x6 performance improvement over native PyTorch
by avoiding unnecessary data transfers, but required three iterations over
three years. Automated compiled methods have consistently lagged behind. GPUs
are limited by both transfers to processors and available compute, with
transfer bandwidth having improved at a far slower pace. Already, transfer
bandwidth accounts for 46% of GPU energy costs. This indicates the future of
energy and capital-efficient algorithms relies on improved consideration of
transfer costs (IO-awareness) and a systematic method for deriving optimized
algorithms. In this paper, we present a diagrammatic approach to deep learning
models which, with simple relabelings, derive optimal implementations and
performance models that consider low-level memory. Diagrams generalize down the
GPU hierarchy, providing a universal performance model for comparing hardware
and **quantization** choices. Diagrams generate pseudocode, which reveals the
application of hardware-specific features such as coalesced memory access,
tensor core operations, and overlapped computation. We present attention
algorithms for Ampere, which fits 13 warps per SM (FlashAttention fits 8), and
for Hopper, which has improved overlapping and may achieve 1.32 PFLOPs.


## GERD Geometric event response data generation

>Authors: Jens Egholm Pedersen, Dimitris Korakovounis, J旦rg Conradt

>2024-12-04

> http://arxiv.org/abs/2412.03259v1

Event-based vision sensors are appealing because of their time resolution,
higher dynamic range, and low-power consumption. They also provide data that is
fundamentally different from conventional frame-based cameras: events are
**sparse**, discrete, and require integration in time. Unlike conventional models
grounded in established geometric and physical principles, event-based models
lack comparable foundations. We introduce a method to generate event-based data
under controlled transformations. Specifically, we subject a prototypical
object to transformations that change over time to produce carefully curated
event videos. We hope this work simplifies studies for geometric approaches in
event-based vision. GERD is available at https://github.com/ncskth/gerd


## AIM Adaptive Inference of Multi-Modal LLMs via Token Merging and Pruning

>Authors: Yiwu Zhong, Zhuoming Liu, Yin Li, Liwei Wang

>2024-12-04

> http://arxiv.org/abs/2412.03248v1

Large language models (LLMs) have enabled the creation of multi-modal LLMs
that exhibit strong comprehension of visual data such as images and videos.
However, these models usually rely on extensive visual tokens from visual
encoders, leading to high computational demands, which limits their
applicability in resource-constrained environments and for long-context tasks.
In this work, we propose a training-free adaptive inference method for
multi-modal LLMs that can accommodate a broad range of efficiency requirements
with a minimum performance drop. Our method consists of a) iterative token
merging based on embedding similarity before LLMs, and b) progressive token
**pruning** within LLM layers based on multi-modal importance. With a minimalist
design, our method can be applied to both video and image LLMs. Extensive
experiments on diverse video and image benchmarks demonstrate that, our method
substantially reduces computation load (e.g., a $\textbf{7-fold}$ reduction in
FLOPs) while preserving the performance of video and image LLMs. Further, under
a similar computational cost, our method outperforms the state-of-the-art
methods in long video understanding (e.g., $\textbf{+4.6}$ on MLVU).
Additionally, our in-depth analysis provides insights into token redundancy and
LLM layer behaviors, offering guidance for future research in designing
efficient multi-modal LLMs. Our code will be available at
https://github.com/LaVi-Lab/AIM.


## Linq-Embed-Mistral Technical Report

>Authors: Chanyeol Choi, Junseong Kim, Seolhwa Lee, Jihoon Kwon, Sangmo Gu, Yejin Kim, Minkyung Cho, Jy-yong Sohn

>2024-12-04

> http://arxiv.org/abs/2412.03223v1

This report explores the enhancement of text retrieval performance using
advanced data refinement techniques. We develop
Linq-Embed-Mistral\footnote{\url{https://huggingface.co/Linq-AI-Research/Linq-Embed-Mistral}}
by building on the E5-mistral and Mistral-7B-v0.1 models, focusing on
sophisticated data crafting, data filtering, and negative mining methods, which
are highly tailored to each task, applied to both existing benchmark dataset
and highly tailored synthetic dataset generated via large language models
(LLMs). Linq-Embed-Mistral excels in the MTEB benchmarks (as of May 29, 2024),
achieving an average score of 68.2 across 56 datasets, and ranks 1st among all
models for retrieval tasks on the MTEB leaderboard with a performance score of
60.2. This performance underscores its superior capability in enhancing search
precision and reliability. Our contributions include advanced data refinement
methods that significantly improve model performance on benchmark and synthetic
datasets, techniques for homogeneous task ordering and mixed task fine-tuning
to enhance model generalization and stability, and a streamlined evaluation
process using 4-bit precision and a light retrieval evaluation set, which
accelerates validation without sacrificing accuracy.


## Continual Low-Rank Scaled Dot-product Attention

>Authors: Gin辿s Carreto Pic坦n, Illia Oleksiienko, Lukas Hedegaard, Arian Bakhtiarnia, Alexandros Iosifidis

>2024-12-04

> http://arxiv.org/abs/2412.03214v2

Transformers are widely used for their ability to capture data relations in
sequence processing, with great success for a wide range of static tasks.
However, the computational and memory footprint of their main component, i.e.,
the Scaled Dot-product Attention, is commonly overlooked. This makes their
adoption in applications involving stream data processing with constraints in
response latency, computational and memory resources infeasible. Some works
have proposed methods to lower the computational cost of transformers, i.e.
low-rank approximations, **sparsity** in attention, and efficient formulations for
Continual Inference. In this paper, we introduce a new formulation of the
Scaled Dot-product Attention based on the Nystr\"om approximation that is
suitable for Continual Inference. In experiments on Online Audio Classification
and Online Action Detection tasks, the proposed Continual Scaled Dot-product
Attention can lower the number of operations by up to three orders of magnitude
compared to the original Transformers while retaining the predictive
performance of competing models.


## ClusterKV Manipulating LLM KV Cache in Semantic Space for Recallable Compression

>Authors: Guangda Liu, Chengwei Li, Jieru Zhao, Chenqi Zhang, Minyi Guo

>2024-12-04

> http://arxiv.org/abs/2412.03213v1

Large Language Models (LLMs) have been widely deployed in a variety of
applications, and the context length is rapidly increasing to handle tasks such
as long-document QA and complex logical reasoning. However, long context poses
significant challenges for inference efficiency, including high memory costs of
key-value (**KV**) cache and increased latency due to extensive memory accesses.
Recent works have proposed compressing **KV** cache to approximate computation, but
these methods either evict tokens permanently, never recalling them for later
inference, or recall previous tokens at the granularity of pages divided by
textual positions. Both approaches degrade the model accuracy and output
quality. To achieve efficient and accurate recallable **KV** cache compression, we
introduce Cluster**KV**, which recalls tokens at the granularity of semantic
clusters. We design and implement efficient algorithms and systems for
clustering, selection, indexing and caching. Experiment results show that
Cluster**KV** attains negligible accuracy loss across various tasks with 32k
context lengths, using only a 1k to 2k **KV** cache budget, and achieves up to a
2$\times$ speedup in latency and a 2.5$\times$ improvement in decoding
throughput. Compared to SoTA recallable **KV** compression methods, Cluster**KV**
demonstrates higher model accuracy and output quality, while maintaining or
exceeding inference efficiency.


## Unifying KV Cache Compression for Large Language Models with LeanKV

>Authors: Yanqi Zhang, Yuwei Hu, Runyuan Zhao, John C. S. Lui, Haibo Chen

>2024-12-04

> http://arxiv.org/abs/2412.03131v1

Large language models (LLMs) demonstrate exceptional performance but incur
high serving costs due to substantial memory demands, with the key-value (**KV**)
cache being a primary bottleneck. Existing **KV** cache compression methods,
including **quantization** and **pruning**, struggle with limitations such as uniform
treatment of keys and values and static memory allocation across attention
heads. To address these challenges, we introduce Lean**KV**, a unified **KV** cache
compression framework that enhances LLM serving efficiency without compromising
accuracy through three innovations: (1) Hetero-**KV** **quantization**, which stores
keys at a higher precision than values to reflect their greater impact on
attention computations; (2) per-head dynamic **sparsity**, which allocates memory
based on token importance per head and per request; and (3) unified **KV**
compression, integrating mixed-precision **quantization** and selective **pruning** to
enable a smooth tradeoff between model accuracy and memory efficiency. To
efficiently support these techniques, Lean**KV** introduces systems optimizations
including unified paging and on-GPU parallel memory management. Implemented on
vLLM, Lean**KV** compresses the **KV** cache by $3.0\times$ to $5.0\times$ without
accuracy loss and up to $11.0\times$ with under 5% accuracy loss, enhancing
throughput by $1.9\times$ to $2.5\times$, and up to $6.9\times$.


## Revolve Optimizing AI Systems by Tracking Response Evolution in Textual Optimization

>Authors: Peiyan Zhang, Haibo Jin, Leyang Hu, Xinnuo Li, Liying Kang, Man Luo, Yangqiu Song, Haohan Wang

>2024-12-04

> http://arxiv.org/abs/2412.03092v1

Recent advancements in large language models (LLMs) have significantly
enhanced the ability of LLM-based systems to perform complex tasks through
natural language processing and tool interaction. However, optimizing these
LLM-based systems for specific tasks remains challenging, often requiring
manual interventions like prompt engineering and hyperparameter tuning.
Existing automatic optimization methods, such as textual feedback-based
techniques (e.g., TextGrad), tend to focus on immediate feedback, analogous to
using immediate derivatives in traditional numerical gradient descent. However,
relying solely on such feedback can be limited when the adjustments made in
response to this feedback are either too small or fluctuate irregularly,
potentially slowing down or even stalling the optimization process. To overcome
these challenges, more adaptive methods are needed, especially in situations
where the system's response is evolving slowly or unpredictably. In this paper,
we introduce REVOLVE, an optimization method that tracks how "R"esponses
"EVOLVE" across iterations in LLM systems. By focusing on the evolution of
responses over time, REVOLVE enables more stable and effective optimization by
making thoughtful, progressive adjustments at each step. Experimental results
demonstrate that REVOLVE outperforms competitive baselines, achieving a 7.8%
improvement in prompt optimization, a 20.72% gain in solution refinement, and a
29.17% increase in code optimization. Additionally, REVOLVE converges in fewer
iterations, resulting in significant computational savings. These advantages
highlight its adaptability and efficiency, positioning REVOLVE as a valuable
tool for optimizing LLM-based systems and accelerating the development of
next-generation AI technologies. Code is available at:
https://github.com/Peiyance/REVOLVE.


## PEMF-VVTO Point-Enhanced Video Virtual Try-on via Mask-free Paradigm

>Authors: Tianyu Chang, Xiaohao Chen. Zhichao Wei, Xuanpu Zhang, Qing-Guo Chen, Weihua Luo, Xun Yang

>2024-12-04

> http://arxiv.org/abs/2412.03021v2

Video Virtual Try-on aims to fluently transfer the garment image to a
semantically aligned try-on area in the source person video. Previous methods
leveraged the inpainting mask to remove the original garment in the source
video, thus achieving accurate garment transfer on simple model videos.
However, when these methods are applied to realistic video data with more
complex scene changes and posture movements, the overly large and incoherent
agnostic masks will destroy the essential spatial-temporal information of the
original video, thereby inhibiting the fidelity and coherence of the try-on
video. To alleviate this problem, we propose a novel point-enhanced mask-free
video virtual try-on framework (PEMF-VVTO). Specifically, we first leverage the
pre-trained mask-based try-on model to construct large-scale paired training
data (pseudo-person samples). Training on these mask-free data enables our
model to perceive the original spatial-temporal information while realizing
accurate garment transfer. Then, based on the pre-acquired **sparse** frame-cloth
and frame-frame point alignments, we design the point-enhanced spatial
attention (PSA) and point-enhanced temporal attention (PTA) to further improve
the try-on accuracy and video coherence of the mask-free model. Concretely, PSA
explicitly guides the garment transfer to desirable locations through the
**sparse** semantic alignments of video frames and cloth. PTA exploits the temporal
attention on **sparse** point correspondences to enhance the smoothness of
generated videos. Extensive qualitative and quantitative experiments clearly
illustrate that our PEMF-VVTO can generate more natural and coherent try-on
videos than existing state-of-the-art methods.


## EvRT-DETR The Surprising Effectiveness of DETR-based Detection for Event Cameras

>Authors: Dmitrii Torbunov, Yihui Ren, Animesh Ghose, Odera Dim, Yonggang Cui

>2024-12-03

> http://arxiv.org/abs/2412.02890v1

Event-based cameras (EBCs) have emerged as a bio-inspired alternative to
traditional cameras, offering advantages in power efficiency, temporal
resolution, and high dynamic range. However, the development of image analysis
methods for EBCs is challenging due to the **sparse** and asynchronous nature of
the data. This work addresses the problem of object detection for the EBC
cameras. The current approaches to EBC object detection focus on constructing
complex data representations and rely on specialized architectures. Here, we
demonstrate that the combination of a Real-Time DEtection TRansformer, or
RT-DETR, a state-of-the-art natural image detector, with a simple image-like
representation of the EBC data achieves remarkable performance, surpassing
current state-of-the-art results. Specifically, we show that a properly trained
RT-DETR model on the EBC data achieves performance comparable to the most
advanced EBC object detection methods. Next, we propose a low-rank adaptation
(LoRA)-inspired way to augment the RT-DETR model to handle temporal dynamics of
the data. The designed EvRT-DETR model outperforms the current, most advanced
results on standard benchmark datasets Gen1 (mAP $+2.3$) and Gen4 (mAP $+1.4$)
while only using standard modules from natural image and video analysis. These
results demonstrate that effective EBC object detection can be achieved through
careful adaptation of mainstream object detection architectures without
requiring specialized architectural engineering. The code is available at:
https://github.com/realtime-intelligence/evrt-detr


## Effortless Efficiency Low-Cost Pruning of Diffusion Models

>Authors: Yang Zhang, Er Jin, Yanfei Dong, Ashkan Khakzar, Philip Torr, Johannes Stegmaier, Kenji Kawaguchi

>2024-12-03

> http://arxiv.org/abs/2412.02852v1

Diffusion models have achieved impressive advancements in various vision
tasks. However, these gains often rely on increasing model size, which
escalates computational complexity and memory demands, complicating deployment,
raising inference costs, and causing environmental impact. While some studies
have explored **pruning** techniques to improve the memory efficiency of diffusion
models, most existing methods require extensive retraining to retain the model
performance. Retraining a modern large diffusion model is extremely costly and
resource-intensive, which limits the practicality of these methods. In this
work, we achieve low-cost diffusion **pruning** without retraining by proposing a
model-agnostic structural **pruning** framework for diffusion models that learns a
differentiable mask to sparsify the model. To ensure effective **pruning** that
preserves the quality of the final denoised latent, we design a novel
end-to-end **pruning** objective that spans the entire diffusion process. As
end-to-end **pruning** is memory-intensive, we further propose time step gradient
checkpointing, a technique that significantly reduces memory usage during
optimization, enabling end-to-end **pruning** within a limited memory budget.
Results on state-of-the-art U-Net diffusion models SDXL and diffusion
transformers (FLUX) demonstrate that our method can effectively prune up to 20%
parameters with minimal perceptible performance degradation, and notably,
without the need for model retraining. We also showcase that our method can
still prune on top of time step distilled diffusion models.


## T-REG Preference Optimization with Token-Level Reward Regularization

>Authors: Wenxuan Zhou, Shujian Zhang, Lingxiao Zhao, Tao Meng

>2024-12-03

> http://arxiv.org/abs/2412.02685v1

Reinforcement learning from human feedback (RLHF) has been crucial in
aligning large language models (LLMs) with human values. Traditionally, RLHF
involves generating responses to a query and using a reward model to assign a
reward to the entire response. However, this approach faces challenges due to
its reliance on a single, **sparse** reward, which makes it challenging for the
model to identify which parts of the sequence contribute most significantly to
the final reward. Recent methods have attempted to address this limitation by
introducing token-level rewards. However, these methods often rely on either a
trained credit assignment model or AI annotators, raising concerns about the
quality and reliability of the rewards. In this paper, we propose token-level
reward regularization (T-REG), a novel approach that leverages both
sequence-level and token-level rewards for preference optimization. Harnessing
the self-refinement capabilities of LLMs, our method uses contrastive prompting
to enable LLMs to self-generate token-level rewards. These self-generated
rewards then act as reward regularization, guiding the model to more
effectively distribute sequence-level rewards across tokens. This facilitates
better token-level credit assignment and enhances alignment performance.
Experiments on the instruction following benchmarks, including Alpaca Eval 2
and Arena-Hard, show that our method consistently outperforms baseline methods
by up to 3.8% and 4.4%, respectively. We will release the code and models at
https://github.com/wzhouad/T-REG.


## Ground State Energy Estimation on Current Quantum Hardware Through The Variational Quantum Eigensolver A Comprehensive Study

>Authors: Nacer Eddine Belaloui, Abdellah Tounsi, Rabah Abdelmouheymen Khamadja, Mohamed Messaoud Louamri, Achour Benslama, David E. Bernal Neira, Mohamed Taha Rouabah

>2024-12-03

> http://arxiv.org/abs/2412.02606v1

While numerical simulations are presented in most papers introducing new
methods to enhance the VQE performance, comprehensive, comparative, and applied
studies remain relatively rare. We present a comprehensive, yet concise guide
for the implementation of the VQE for molecular problems on NISQ devices,
specifically applied to estimate the ground state energy of the BeH2 molecule
using hardware-efficient and chemically informed ans\"atze. This work clarifies
several under-documented aspects in the literature, such as the construction of
the electronic Hamiltonian, the transformation of fermionic operators into
qubit operators via second **quantization**, and the mathematical framework's
details for the unitary coupled cluster single and double (UCCSD) ansatz. Our
methodology, implemented using Qiskit 1.2, the latest release as of the date of
this writing, is demonstrated on a noiseless simulator and further tested with
noisy quantum circuits. The resilience of the VQE to quantum noise remains an
open question. This study compares the computational accuracy of ground state
energy estimations for molecules using the VQE across three different current
quantum hardware noise models. Furthermore, our experiment on IBM's 156-qubit
actual quantum computer revealed valuable insights on the real performance of
the VQE on current quantum hardware.


## Interpretable Company Similarity with Sparse Autoencoders

>Authors: Marco Molinari, Vladimir Tregubiak, Victor Shao, Abhimanyu Pandey, Mateusz Mikolajczak, Sebasti達o Kuznetsov Ryder Torres Pereira

>2024-12-03

> http://arxiv.org/abs/2412.02605v1

Determining company similarity is a vital task in finance, underpinning
hedging, risk management, portfolio diversification, and more. Practitioners
often rely on sector and industry classifications to gauge similarity, such as
SIC-codes and GICS-codes, the former being used by the U.S. Securities and
Exchange Commission (SEC), and the latter widely used by the investment
community. Clustering embeddings of company descriptions has been proposed as a
potential technique for determining company similarity, but the lack of
interpretability in token embeddings poses a significant barrier to adoption in
high-stakes contexts. Sparse Autoencoders have shown promise in enhancing the
interpretability of Large Language Models by decomposing LLM activations into
interpretable features. In this paper, we explore the use of SAE features in
measuring company similarity and benchmark them against (1) SIC codes and (2)
Major Group codes. We conclude that SAE features can reproduce and even surpass
sector classifications in quantifying fundamental characteristics of companies,
evaluated by the correlation of monthly returns, a proxy for similarity, and
PnL from cointegration.


## CEGI Measuring the trade-off between efficiency and carbon emissions for SLMs and VLMs

>Authors: Abhas Kumar, Kapil Pathak, Rajesh Kavuru, Prabhakar Srinivasan

>2024-12-03

> http://arxiv.org/abs/2412.02602v1

This paper analyzes the performance of Small Language Models (SLMs) and
Vision Language Models (VLMs) and evaluates the trade-off between model
performance and carbon emissions across 4 essential tasks: Image Captioning,
Visual Question Answering (VQA), Dialogue Summarization and Text-to-SQL
conversion. Various SLMs and VLMs belonging to the Qwen and LLaMA architecture
family are chosen and variants based on model size in terms of the number of
parameters, **quantization** level and fine-tuning parameters are evaluated. The
model variant's performance and carbon emissions are calculated. To quantify
the trade-off between model performance and carbon emissions, we introduce a
novel metric called CEGI (Carbon Efficient Gain Index). This metric represents
the carbon emission per unit percentage gain per million trainable parameters .
This metric provides a normalized measure to compare model's efficiency in
terms of performance improvement relative to their environmental cost. The
experiment's outcome demonstrates that fine-tuning SLMs and VLMs can achieve
performance levels comparable to Large Language Models (LLMs) while producing
significantly less carbon emissions. Our findings suggest that the marginal
gains in accuracy from larger models do not justify the substantial increase in
carbon emissions. Leveraging lower-bit **quantization** levels, the proposed metric
further enhances energy efficiency without compromising performance. This study
highlights balancing high performance and environmental sustainability. It
offers a valuable metric for selecting models suitable for
environmentally-friendly AI development.


## Hamiltonian Monte Carlo-Based Near-Optimal MIMO Signal Detection

>Authors: Junichiro Hagiwara, Toshihiko Nishimura, Takanori Sato, Yasutaka Ogawa, Takeo Ohgane

>2024-12-03

> http://arxiv.org/abs/2412.02391v1

Multiple-input multiple-output (MIMO) technology is essential for the optimal
functioning of next-generation wireless networks; however, enhancing its
signal-detection performance for improved spectral efficiency is challenging.
Here, we propose an approach that transforms the discrete MIMO detection
problem into a continuous problem while leveraging the efficient Hamiltonian
Monte Carlo algorithm. For this continuous framework, we employ a mixture of
t-distributions as the prior distribution. To improve the performance in the
coded case further, we treat the likelihood's temperature parameter as a random
variable and address its optimization. This treatment leads to the adoption of
a horseshoe density for the likelihood. Theoretical analysis and extensive
simulations demonstrate that our method achieves near-optimal detection
performance while maintaining polynomial computational complexity. This MIMO
detection technique can accelerate the development of 6G mobile communication
systems.


## ScImage How Good Are Multimodal Large Language Models at Scientific Text-to-Image Generation?

>Authors: Leixin Zhang, Steffen Eger, Yinjie Cheng, Weihe Zhai, Jonas Belouadi, Christoph Leiter, Simone Paolo Ponzetto, Fahimeh Moafian, Zhixue Zhao

>2024-12-03

> http://arxiv.org/abs/2412.02368v1

Multimodal large language models (LLMs) have demonstrated impressive
capabilities in generating high-quality images from textual instructions.
However, their performance in generating scientific images--a critical
application for accelerating scientific progress--remains underexplored. In
this work, we address this gap by introducing ScImage, a benchmark designed to
evaluate the multimodal capabilities of LLMs in generating scientific images
from textual descriptions. ScImage assesses three key dimensions of
understanding: spatial, numeric, and attribute comprehension, as well as their
combinations, focusing on the relationships between scientific objects (e.g.,
squares, circles). We evaluate five models, GPT-4o, Llama, AutomaTikZ, Dall-E,
and StableDiffusion, using two modes of output generation: code-based outputs
(Python, TikZ) and direct raster image generation. Additionally, we examine
four different input languages: English, German, Farsi, and Chinese. Our
evaluation, conducted with 11 scientists across three criteria (correctness,
relevance, and scientific accuracy), reveals that while GPT-4o produces outputs
of decent quality for simpler prompts involving individual dimensions such as
spatial, numeric, or attribute understanding in isolation, all models face
challenges in this task, especially for more complex prompts.


## CADMR Cross-Attention and Disentangled Learning for Multimodal Recommender Systems

>Authors: Yasser Khalafaoui, Martino Lovisetto, Basarab Matei, Nistor Grozavu

>2024-12-03

> http://arxiv.org/abs/2412.02295v1

The increasing availability and diversity of multimodal data in recommender
systems offer new avenues for enhancing recommendation accuracy and user
satisfaction. However, these systems must contend with high-dimensional, **sparse**
user-item rating matrices, where reconstructing the matrix with only small
subsets of preferred items for each user poses a significant challenge. To
address this, we propose CADMR, a novel autoencoder-based multimodal
recommender system framework. CADMR leverages multi-head cross-attention
mechanisms and Disentangled Learning to effectively integrate and utilize
heterogeneous multimodal data in reconstructing the rating matrix. Our approach
first disentangles modality-specific features while preserving their
interdependence, thereby learning a joint latent representation. The multi-head
cross-attention mechanism is then applied to enhance user-item interaction
representations with respect to the learned multimodal item latent
representations. We evaluate CADMR on three benchmark datasets, demonstrating
significant performance improvements over state-of-the-art methods.


## Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity

>Authors: Da Ma, Lu Chen, Situo Zhang, Yuxun Miao, Su Zhu, Zhi Chen, Hongshen Xu, Hanqi Li, Shuai Fan, Lei Pan, Kai Yu

>2024-12-03

> http://arxiv.org/abs/2412.02252v1

The increasing context window size in Large Language Models (LLMs), such as
the GPT and LLaMA series, has improved their ability to tackle complex,
long-text tasks, but at the cost of inference efficiency, particularly
regarding memory and computational complexity. Existing methods, including
selective token retention and window-based attention, improve efficiency but
risk discarding important tokens needed for future text generation. In this
paper, we propose an approach that enhances LLM efficiency without token loss
by reducing the memory and computational load of less important tokens, rather
than discarding them.We address two challenges: 1) investigating the
distribution of important tokens in the context, discovering recent tokens are
more important than distant tokens in context, and 2) optimizing resources for
distant tokens by sharing attention scores across layers. The experiments show
that our method saves $35\%$ **KV** cache without compromising the performance.


## Unlocking Tuning-Free Few-Shot Adaptability in Visual Foundation Models by Recycling Pre-Tuned LoRAs

>Authors: Zixuan Hu, Yongxian Wei, Li Shen, Chun Yuan, Dacheng Tao

>2024-12-03

> http://arxiv.org/abs/2412.02220v1

Large Language Models (LLMs) such as ChatGPT demonstrate strong few-shot
adaptability without requiring fine-tuning, positioning them ideal for
data-limited and real-time applications. However, this adaptability has not yet
been replicated in current Visual Foundation Models (VFMs), which require
explicit fine-tuning with sufficient tuning data. Besides, the
pretraining-finetuning paradigm has led to the surge of numerous task-specific
modular components, such as Low-Rank Adaptation (LoRA). For the first time, we
explore the potential of reusing diverse pre-tuned LoRAs without accessing
their original training data, to achieve tuning-free few-shot adaptation in
VFMs. Our framework, LoRA Recycle, distills a meta-LoRA from diverse pre-tuned
LoRAs with a meta-learning objective, using surrogate data generated inversely
from pre-tuned LoRAs themselves. The VFM, once equipped with the meta-LoRA, is
empowered to solve new few-shot tasks in a single forward pass, akin to the
in-context learning of LLMs. Additionally, we incorporate a double-efficient
mechanism tailored to our framework, significantly accelerating the
meta-training process while maintaining or even improving performance.
Extensive experiments across various few-shot classification benchmarks across
both in- and cross-domain scenarios demonstrate the superiority of our
framework.


## 3D representation in 512-ByteVariational tokenizer is the key for autoregressive 3D generation

>Authors: Jinzhi Zhang, Feng Xiong, Mu Xu

>2024-12-03

> http://arxiv.org/abs/2412.02202v1

Autoregressive transformers have revolutionized high-fidelity image
generation. One crucial ingredient lies in the tokenizer, which compresses
high-resolution image patches into manageable discrete tokens with a scanning
or hierarchical order suitable for large language models. Extending these
tokenizers to 3D generation, however, presents a significant challenge: unlike
image patches that naturally exhibit spatial sequence and multi-scale
relationships, 3D data lacks an inherent order, making it difficult to compress
into fewer tokens while preserving structural details. To address this, we
introduce the Variational Tokenizer (VAT), which transforms unordered 3D data
into compact latent tokens with an implicit hierarchy, suited for efficient and
high-fidelity coarse-to-fine autoregressive modeling. VAT begins with an
in-context transformer, which compress numerous unordered 3D features into a
reduced token set with minimal information loss. This latent space is then
mapped to a Gaussian distribution for residual **quantization**, with token counts
progressively increasing across scales. In this way, tokens at different scales
naturally establish the interconnections by allocating themselves into
different subspaces within the same Gaussian distribution, facilitating
discrete modeling of token relationships across scales. During the decoding
phase, a high-resolution triplane is utilized to convert these compact latent
tokens into detailed 3D shapes. Extensive experiments demonstrate that VAT
enables scalable and efficient 3D generation, outperforming existing methods in
quality, efficiency, and generalization. Remarkably, VAT achieves up to a 250x
compression, reducing a 1MB mesh to just 3.9KB with a 96% F-score, and can
further compress to 256 int8 tokens, achieving a 2000x reduction while
maintaining a 92% F-score.


## CLERF Contrastive LEaRning for Full Range Head Pose Estimation

>Authors: Ting-Ruen Wei, Haowei Liu, Huei-Chung Hu, Xuyang Wu, Yi Fang, Hsin-Tai Wu

>2024-12-03

> http://arxiv.org/abs/2412.02066v1

We introduce a novel framework for representation learning in head pose
estimation (HPE). Previously such a scheme was difficult due to head pose data
**sparsity**, making triplet sampling infeasible. Recent progress in 3D generative
adversarial networks (3D-aware GAN) has opened the door for easily sampling
triplets (anchor, positive, negative). We perform contrastive learning on
extensively augmented data including geometric transformations and demonstrate
that contrastive learning allows networks to learn genuine features that
contribute to accurate HPE. On the other hand, we observe that existing HPE
works struggle to predict head poses as accurately when test image rotation
matrices are slightly out of the training dataset distribution. Experiments
show that our methodology performs on par with state-of-the-art models on
standard test datasets and outperforms them when images are slightly rotated/
flipped or full range head pose. To the best of our knowledge, we are the first
to deliver a true full range HPE model capable of accurately predicting any
head pose including upside-down pose. Furthermore, we compared with other
existing full-yaw range models and demonstrated superior results.


## Comparing Dynamics, Pinning and Ratchet Effects for Skyrmionium, Skyrmions, and Antiskyrmions

>Authors: J. C. Bellizotti Souza, N. P. Vizarim, C. J. O. Reichhardt, C. Reichhardt, P. A. Venegas

>2024-12-02

> http://arxiv.org/abs/2412.02001v1

We compare the driven dynamics of skyrmions, antiskyrmions, and skyrmionium
interacting with random disorder, circular defects, and asymmetric potentials.
When interacting with a line defect at a constant drive, skyrmions and
antiskyrmions show an **acceleration** effect for motion along the wall and a drop
in velocity when they can cross the barrier. In contrast, skyrmionium travels
at a reduced velocity when moving along a wall, and exhibits an increase in
velocity once it can cross the barrier. For point defects, skyrmionium can be
pinned for a finite fixed period of time, while for skyrmions and
antiskyrmions, the Magnus force creates a deflection from the defect and an
**acceleration** effect. For a given drive, skyrmionium moves twice as fast as
skyrmions; however, skyrmionium is more susceptible to pinning effects than
skyrmions and antiskyrmions. Additionally, there is a critical threshold where
the skyrmionium transforms to a skyrmion that is associated with a drop in the
velocity of the texture. We show that all three textures exhibit diode and
ratchet effects when interacting with an asymmetric substrate, but skyrmions
and antiskyrmions show a stronger ratcheting effect than skyrmionium due to the
Magnus force.


## Measuring the expansion history of the Universe with cosmic chronometers

>Authors: Michele Moresco

>2024-12-02

> http://arxiv.org/abs/2412.01994v1

As revealed by Hubble in 1928, our Universe is expanding. This discovery was
fundamental to widening our horizons and our conception of space, and since
then determining the rate at which our Universe is expanding has become one of
the crucial measurements in cosmology. At the beginning of this century, these
measurements revealed the unexpected behavior that this expansion is
accelerating and allowed us to have a first glimpse of the dark components that
constitute $\sim$95\% of our Universe. Cosmic chronometers represent a novel
technique to obtain a cosmology-independent determination of the expansion of
the Universe, based on the differential age dating of a population of very
massive and passively evolving galaxies. Currently, with this new cosmological
probe it is possible to constrain the Hubble parameter with an accuracy of
around 5\% at $z\sim0.5$ up to 10-20\% at $z\sim2$. In this Chapter, the cosmic
chronometers approach is presented, describing the method and how an optimal
sample can be selected; it is then discussed how the most recent measurements
of the expansion history of the Universe have been obtained with this approach,
as well as the cosmological constraints that can be derived. Particular
attention will be given to the systematics involved in this approach and the
treatment to properly take them into account. We conclude by presenting
forecasts that show how future spectroscopic surveys will significantly boost
the accuracy of this method and open the possibility to a percent determination
of the Hubble constant, making cosmic chronometers a powerful independent tool
to derive information on the expansion history of the Universe.


## RandAR Decoder-only Autoregressive Visual Generation in Random Orders

>Authors: Ziqi Pang, Tianyuan Zhang, Fujun Luan, Yunze Man, Hao Tan, Kai Zhang, William T. Freeman, Yu-Xiong Wang

>2024-12-02

> http://arxiv.org/abs/2412.01827v1

We introduce RandAR, a decoder-only visual autoregressive (AR) model capable
of generating images in arbitrary token orders. Unlike previous decoder-only AR
models that rely on a predefined generation order, RandAR removes this
inductive bias, unlocking new capabilities in decoder-only generation. Our
essential design enables random order by inserting a "position instruction
token" before each image token to be predicted, representing the spatial
location of the next image token. Trained on randomly permuted token sequences
-- a more challenging task than fixed-order generation, RandAR achieves
comparable performance to its conventional raster-order counterpart. More
importantly, decoder-only transformers trained from random orders acquire new
capabilities. For the efficiency bottleneck of AR models, RandAR adopts
parallel decoding with **KV**-Cache at inference time, enjoying 2.5x **acceleration**
without sacrificing generation quality. Additionally, RandAR supports
inpainting, outpainting and resolution extrapolation in a zero-shot manner. We
hope RandAR inspires new directions for decoder-only visual generation models
and broadens their applications across diverse scenarios. Our project page is
at https://rand-ar.github.io/.


## Switti Designing Scale-Wise Transformers for Text-to-Image Synthesis

>Authors: Anton Voronov, Denis Kuznedelev, Mikhail Khoroshikh, Valentin Khrulkov, Dmitry Baranchuk

>2024-12-02

> http://arxiv.org/abs/2412.01819v3

This work presents Switti, a scale-wise transformer for text-to-image
generation. Starting from existing next-scale prediction AR models, we first
explore them for T2I generation and propose architectural modifications to
improve their convergence and overall performance. We then argue that
scale-wise transformers do not require causality and propose a non-causal
counterpart facilitating ~11% faster sampling and lower memory usage while also
achieving slightly better generation quality. Furthermore, we reveal that
classifier-free guidance at high-resolution scales is often unnecessary and can
even degrade performance. By disabling guidance at these scales, we achieve an
additional sampling **acceleration** of ~20% and improve the generation of
fine-grained details. Extensive human preference studies and automated
evaluations show that Switti outperforms existing T2I AR models and competes
with state-of-the-art T2I diffusion models while being up to 7 times faster.


## [CLS] Attention is All You Need for Training-Free Visual Token Pruning Make VLM Inference Faster

>Authors: Qizhe Zhang, Aosong Cheng, Ming Lu, Zhiyong Zhuo, Minqi Wang, Jiajun Cao, Shaobo Guo, Qi She, Shanghang Zhang

>2024-12-02

> http://arxiv.org/abs/2412.01818v1

Large vision-language models (VLMs) often rely on a substantial number of
visual tokens when interacting with large language models (LLMs), which has
proven to be inefficient. Recent efforts have aimed to accelerate VLM inference
by **pruning** visual tokens. Most existing methods assess the importance of visual
tokens based on the text-visual cross-attentions in LLMs. In this study, we
find that the cross-attentions between text and visual tokens in LLMs are
inaccurate. Pruning tokens based on these inaccurate attentions leads to
significant performance degradation, especially at high reduction ratios. To
this end, we introduce FasterVLM, a simple yet effective training-free visual
token **pruning** method that evaluates the importance of visual tokens more
accurately by utilizing attentions between the [CLS] token and image tokens
from the visual encoder. Since FasterVLM eliminates redundant visual tokens
immediately after the visual encoder, ensuring they do not interact with LLMs
and resulting in faster VLM inference. It is worth noting that, benefiting from
the accuracy of [CLS] cross-attentions, FasterVLM can prune 95\% of visual
tokens while maintaining 90\% of the performance of LLaVA-1.5-7B. We apply
FasterVLM to various VLMs, including LLaVA-1.5, LLaVA-NeXT, and Video-LLaVA, to
demonstrate its effectiveness. Experimental results show that our FasterVLM
maintains strong performance across various VLM architectures and reduction
ratios, significantly outperforming existing text-visual attention-based
methods. Our code is available at https://github.com/Theia-4869/FasterVLM.


## CTRL-D Controllable Dynamic 3D Scene Editing with Personalized 2D Diffusion

>Authors: Kai He, Chin-Hsuan Wu, Igor Gilitschenski

>2024-12-02

> http://arxiv.org/abs/2412.01792v1

Recent advances in 3D representations, such as Neural Radiance Fields and 3D
Gaussian Splatting, have greatly improved realistic scene modeling and
novel-view synthesis. However, achieving controllable and consistent editing in
dynamic 3D scenes remains a significant challenge. Previous work is largely
constrained by its editing backbones, resulting in inconsistent edits and
limited controllability. In our work, we introduce a novel framework that first
fine-tunes the InstructPix2Pix model, followed by a two-stage optimization of
the scene based on deformable 3D Gaussians. Our fine-tuning enables the model
to "learn" the editing ability from a single edited reference image,
transforming the complex task of dynamic scene editing into a simple 2D image
editing process. By directly learning editing regions and styles from the
reference, our approach enables consistent and precise local edits without the
need for tracking desired editing regions, effectively addressing key
challenges in dynamic scene editing. Then, our two-stage optimization
progressively edits the trained dynamic scene, using a designed edited image
buffer to accelerate convergence and improve temporal consistency. Compared to
state-of-the-art methods, our approach offers more flexible and controllable
local scene editing, achieving high-quality and consistent results.


## ReHub Linear Complexity Graph Transformers with Adaptive Hub-Spoke Reassignment

>Authors: Tomer Borreda, Daniel Freedman, Or Litany

>2024-12-02

> http://arxiv.org/abs/2412.01519v1

We present ReHub, a novel graph transformer architecture that achieves linear
complexity through an efficient reassignment technique between nodes and
virtual nodes. Graph transformers have become increasingly important in graph
learning for their ability to utilize long-range node communication explicitly,
addressing limitations such as oversmoothing and oversquashing found in
message-passing graph networks. However, their dense attention mechanism scales
quadratically with the number of nodes, limiting their applicability to
large-scale graphs. ReHub draws inspiration from the airline industry's
hub-and-spoke model, where flights are assigned to optimize operational
efficiency. In our approach, graph nodes (spokes) are dynamically reassigned to
a fixed number of virtual nodes (hubs) at each model layer. Recent work, Neural
Atoms (Li et al., 2024), has demonstrated impressive and consistent
improvements over GNN baselines by utilizing such virtual nodes; their findings
suggest that the number of hubs strongly influences performance. However,
increasing the number of hubs typically raises complexity, requiring a
trade-off to maintain linear complexity. Our key insight is that each node only
needs to interact with a small subset of hubs to achieve linear complexity,
even when the total number of hubs is large. To leverage all hubs without
incurring additional computational costs, we propose a simple yet effective
adaptive reassignment technique based on hub-hub similarity scores, eliminating
the need for expensive node-hub computations. Our experiments on LRGB indicate
a consistent improvement in results over the base method, Neural Atoms, while
maintaining a linear complexity. Remarkably, our **sparse** model achieves
performance on par with its non-**sparse** counterpart. Furthermore, ReHub
outperforms competitive baselines and consistently ranks among top performers
across various benchmarks.


## Structured 3D Latents for Scalable and Versatile 3D Generation

>Authors: Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, Jiaolong Yang

>2024-12-02

> http://arxiv.org/abs/2412.01506v1

We introduce a novel 3D generation method for versatile and high-quality 3D
asset creation. The cornerstone is a unified Structured LATent (SLAT)
representation which allows decoding to different output formats, such as
Radiance Fields, 3D Gaussians, and meshes. This is achieved by integrating a
**sparse**ly-populated 3D grid with dense multiview visual features extracted from
a powerful vision foundation model, comprehensively capturing both structural
(geometry) and textural (appearance) information while maintaining flexibility
during decoding. We employ rectified flow transformers tailored for SLAT as our
3D generation models and train models with up to 2 billion parameters on a
large 3D asset dataset of 500K diverse objects. Our model generates
high-quality results with text or image conditions, significantly surpassing
existing methods, including recent ones at similar scales. We showcase flexible
output format selection and local 3D editing capabilities which were not
offered by previous models. Code, model, and data will be released.


## Neutrinos from stochastic acceleration in black hole environments

>Authors: M. Lemoine, F. Rieger

>2024-12-02

> http://arxiv.org/abs/2412.01457v1

Recent results from the IceCube detector and their phenomenological
interpretation suggest that the corona of nearby X-ray luminous Seyfert
galaxies can produce $\sim 1-10\,$TeV neutrinos via photo-hadronic
interactions. We investigate in detail the physics of stochastic **acceleration**
in such environments and examine under which conditions one can explain the
inferred proton spectrum. To do so, we borrow recent findings on particle
**acceleration** in turbulence and pay particular attention to the transport
equation, notably for what concerns transport in momentum space, turbulent
transport outside of the corona and advection through the corona. We first
remark that the spectra obtained are highly sensitive to the value of the
**acceleration** rate, e.g., to the Alfv\'enic velocity. Then we examine three
prototype scenarios, one describing turbulent **acceleration** in the test-particle
picture, one in which particles are pre-accelerated by turbulence and further
energized by shear **acceleration**, and one in which we consider the effect of
particle backreaction on the turbulence (damping), which self-regulates the
**acceleration** process. We show that it is possible to obtain satisfactory fits
to the inferred proton spectrum in all three cases, but stress that in the
first two, the energy content in supra-thermal protons has to be fixed in an
ad-hoc manner to match the inferred spectrum, at an energy density close to
that contained in the turbulence. Interestingly, self-regulated **acceleration** by
turbulence damping naturally brings the suprathermal particle energy content
close to that of the turbulence and allows to reproduce the inferred flux level
without additional fine tuning. We suggest that, given the strong sensitivity
of the maximal proton energy to the **acceleration** rate, any variation of that
quantity in the corona could affect, and in fact set the slope of the
high-energy proton spectrum.


## Early Exit Is a Natural Capability in Transformer-based Models An Empirical Study on Early Exit without Joint Optimization

>Authors: Weiqiao Shan, Long Meng, Tong Zheng, Yingfeng Luo, Bei Li, junxin Wang, Tong Xiao, Jingbo Zhu

>2024-12-02

> http://arxiv.org/abs/2412.01455v1

Large language models (LLMs) exhibit exceptional performance across various
downstream tasks. However, they encounter limitations due to slow inference
speeds stemming from their extensive parameters. The early exit (EE) is an
approach that aims to accelerate auto-regressive decoding. EE generates outputs
from intermediate layers instead of using the whole model, which offers a
promising solution to this challenge. However, additional output layers and
joint optimization used in conventional EE hinder the application of EE in
LLMs.
  In this paper, we explore the possibility of LLMs EE without additional
output layers and joint optimization. Our findings indicate that EE is a
natural capability within transformer-based models. While joint optimization
does not give model EE capability, it must be employed to address challenges by
improving the accuracy of locating the optimal EE layer through gating
functions. Additionally, our study reveals patterns in EE behavior from a
sub-word perspective based on the LLaMA model and the potential possibility for
EE based on sub-layers.


## PLD+ Accelerating LLM inference by leveraging Language Model Artifacts

>Authors: Shwetha Somasundaram, Anirudh Phukan, Apoorv Saxena

>2024-12-02

> http://arxiv.org/abs/2412.01447v1

To reduce the latency associated with autoretrogressive LLM inference,
speculative decoding has emerged as a novel decoding paradigm, where future
tokens are drafted and verified in parallel. However, the practical deployment
of speculative decoding is hindered by its requirements for additional
computational resources and fine-tuning, which limits its out-of-the-box
usability. To address these challenges, we present PLD+, a suite of novel
algorithms developed to accelerate the inference process of LLMs, particularly
for input-guided tasks. These tasks, which include code editing, text editing,
summarization, etc., often feature outputs with substantial overlap with their
inputs-an attribute PLD+ is designed to exploit. PLD+ also leverages the
artifacts (attention and hidden states) generated during inference to
accelerate inference speed. We test our approach on five input-guided tasks and
through extensive experiments we find that PLD+ outperforms all tuning-free
approaches. In the greedy setting, it even outperforms the state-of-the-art
tuning-dependent approach EAGLE on four of the tasks. (by a margin of upto 2.31
in terms of avg. speedup). Our approach is tuning free, does not require any
additional compute and can easily be used for accelerating inference of any
LLM.


## CPA Camera-pose-awareness Diffusion Transformer for Video Generation

>Authors: Yuelei Wang, Jian Zhang, Pengtao Jiang, Hao Zhang, Jinwei Chen, Bo Li

>2024-12-02

> http://arxiv.org/abs/2412.01429v1

Despite the significant advancements made by Diffusion Transformer
(DiT)-based methods in video generation, there remains a notable gap with
controllable camera pose perspectives. Existing works such as OpenSora do NOT
adhere precisely to anticipated trajectories and physical interactions, thereby
limiting the flexibility in downstream applications. To alleviate this issue,
we introduce CPA, a unified camera-pose-awareness text-to-video generation
approach that elaborates the camera movement and integrates the textual,
visual, and spatial conditions. Specifically, we deploy the Sparse Motion
Encoding (SME) module to transform camera pose information into a
spatial-temporal embedding and activate the Temporal Attention Injection (TAI)
module to inject motion patches into each ST-DiT block. Our plug-in
architecture accommodates the original DiT parameters, facilitating diverse
types of camera poses and flexible object movement. Extensive qualitative and
quantitative experiments demonstrate that our method outperforms LDM-based
methods for long video generation while achieving optimal performance in
trajectory consistency and object consistency.


## Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware Masking

>Authors: Marco Federici, Davide Belli, Mart van Baalen, Amir Jalalirad, Andrii Skliar, Bence Major, Markus Nagel, Paul Whatmough

>2024-12-02

> http://arxiv.org/abs/2412.01380v1

While mobile devices provide ever more compute power, improvements in DRAM
bandwidth are much slower. This is unfortunate for large language model (LLM)
token generation, which is heavily memory-bound. Previous work has proposed to
leverage natural dynamic activation **sparsity** in ReLU-activated LLMs to reduce
effective DRAM bandwidth per token. However, more recent LLMs use SwiGLU
instead of ReLU, which result in little inherent **sparsity**. While SwiGLU
activations can be pruned based on magnitude, the resulting **sparsity** patterns
are difficult to predict, rendering previous approaches ineffective. To
circumvent this issue, our work introduces Dynamic Input Pruning (DIP): a
predictor-free dynamic sparsification approach, which preserves accuracy with
minimal fine-tuning. DIP can further use lightweight LoRA adapters to regain
some performance lost during sparsification. Lastly, we describe a novel
cache-aware masking strategy, which considers the cache state and activation
magnitude to further increase cache hit rate, improving LLM token rate on
mobile devices. DIP outperforms other methods in terms of accuracy, memory and
throughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP
achieves a 46% reduction in memory and 40% increase in throughput with $<$ 0.1
loss in perplexity.


## Yi-Lightning Technical Report

>Authors: 01. AI, :, Alan Wake, Albert Wang, Bei Chen, C. X. Lv, Chao Li, Chengen Huang, Chenglin Cai, Chujie Zheng, Daniel Cooper, Ethan Dai, Fan Zhou, Feng Hu, Heng Ji, Howard Qiu, Jiangcheng Zhu, Jun Tian, Katherine Su, Lihuan Zhang, Liying Li, Ming Song, Mou Li, Peng Liu, Qicheng Hu, Shawn Wang, Shijun Zhou, Shiyong Li, Tianhang Zhu, Wen Xie, Xiang He, Xiaobo Chen, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Yanpeng Li, Yongke Zhao, Yongzhen Luo, Yuchi Xu, Yuxuan Sha, Zhaodong Yan, Zhiyuan Liu, Zirui Zhang

>2024-12-02

> http://arxiv.org/abs/2412.01253v3

This technical report presents Yi-Lightning, our latest flagship large
language model (LLM). It achieves exceptional performance, ranking 6th overall
on Chatbot Arena, with particularly strong results (2nd to 4th place) in
specialized categories including Chinese, Math, Coding, and Hard Prompts.
Yi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,
featuring advanced expert segmentation and routing mechanisms coupled with
optimized **KV**-caching techniques. Our development process encompasses
comprehensive pre-training, supervised fine-tuning (SFT), and reinforcement
learning from human feedback (RLHF), where we devise deliberate strategies for
multi-stage training, synthetic data construction, and reward modeling.
Furthermore, we implement RAISE (Responsible AI Safety Engine), a
four-component framework to address safety issues across pre-training,
post-training, and serving phases. Empowered by our scalable super-computing
infrastructure, all these innovations substantially reduce training, deployment
and inference costs while maintaining high-performance standards. With further
evaluations on public academic benchmarks, Yi-Lightning demonstrates
competitive performance against top-tier LLMs, while we observe a notable
disparity between traditional, static benchmark results and real-world, dynamic
human preferences. This observation prompts a critical reassessment of
conventional benchmarks' utility in guiding the development of more intelligent
and powerful AI systems for practical applications. Yi-Lightning is now
available through our developer platform at https://platform.lingyiwanwu.com.


## Multiple rebrightenings in the optical afterglow of GRB 210731A evidence for an asymmetric jet

>Authors: Jin-Da Li, He Gao, Shunke Ai, Wei-Hua Lei

>2024-12-02

> http://arxiv.org/abs/2412.01229v1

The broadband afterglow of Gamma-ray bursts (GRBs) is usually believed to
originate from the synchrotron radiation of electrons accelerated by the
external shock of relativistic jets. Therefore, the jet structure should have a
significant impact on the GRB afterglow features. The latest observations
indicate that the GRB jets may possess intricate structures, such as Gaussian
structure, power-law structure, or jet-cocoon structure. Most recently, an
abnormal afterglow of GRB 210731A has raised extensive attention, whose optical
afterglow exhibites multiple rebrightening phenomena within 4 hours, posing a
serious challenge to the standard afterglow model. Here we intend to interpret
the characteristics of GRB 210731A afterglows within the framework of
non-axisymmetric structured jets, where multiple distinct peaks in the
afterglow light curve are caused by the uneven distribution of energy and
velocity within the jet in the azimuth angle direction. Through Monte Carlo
Markov Chain fitting, we show that a three-component asymmetric structured jet
can well explain the multi-band afterglow data. The energy difference among the
three components is about 1.5 orders of magnitude, with higher-energy
components exhibiting slower speeds. The radiation contribution of each
component has sequentially dominated the light curve of the afterglow,
resulting in multiple peaks, with the highest peak occurring at the latest
time. We suggest that in the future, polarization observations should be
conducted on afterglows with multiple brightening signatures, which will help
to effectively distinguish the structured jet model from other alternative
models, such as energy injection, and ultimately help to determine the true
configuration of jets.


## TinyFusion Diffusion Transformers Learned Shallow

>Authors: Gongfan Fang, Kunjun Li, Xinyin Ma, Xinchao Wang

>2024-12-02

> http://arxiv.org/abs/2412.01199v1

Diffusion Transformers have demonstrated remarkable capabilities in image
generation but often come with excessive parameterization, resulting in
considerable inference overhead in real-world applications. In this work, we
present TinyFusion, a depth **pruning** method designed to remove redundant layers
from diffusion transformers via end-to-end learning. The core principle of our
approach is to create a pruned model with high recoverability, allowing it to
regain strong performance after fine-tuning. To accomplish this, we introduce a
differentiable sampling technique to make **pruning** learnable, paired with a
co-optimized parameter to simulate future fine-tuning. While prior works focus
on minimizing loss or error after **pruning**, our method explicitly models and
optimizes the post-fine-tuning performance of pruned models. Experimental
results indicate that this learnable paradigm offers substantial benefits for
layer **pruning** of diffusion transformers, surpassing existing importance-based
and error-based methods. Additionally, TinyFusion exhibits strong
generalization across diverse architectures, such as DiTs, MARs, and SiTs.
Experiments with DiT-XL show that TinyFusion can craft a shallow diffusion
transformer at less than 7% of the pre-training cost, achieving a 2$\times$
speedup with an FID score of 2.86, outperforming competitors with comparable
efficiency. Code is available at https://github.com/VainF/TinyFusion.


## RILQ Rank-Insensitive LoRA-based Quantization Error Compensation for Boosting 2-bit Large Language Model Accuracy

>Authors: Geonho Lee, Janghwan Lee, Sukjin Hong, Minsoo Kim, Euijai Ahn, Du-Seong Chang, Jungwook Choi

>2024-12-02

> http://arxiv.org/abs/2412.01129v2

Low-rank adaptation (LoRA) has become the dominant method for
parameter-efficient LLM fine-tuning, with LoRA-based **quantization** error
compensation (LQEC) emerging as a powerful tool for recovering accuracy in
compressed LLMs. However, LQEC has underperformed in sub-4-bit scenarios, with
no prior investigation into understanding this limitation. We propose RILQ
(Rank-Insensitive LoRA-based Quantization Error Compensation) to understand
fundamental limitation and boost 2-bit LLM accuracy. Based on rank analysis
revealing model-wise activation discrepancy loss's rank-insensitive nature,
RILQ employs this loss to adjust adapters cooperatively across layers, enabling
robust error compensation with low-rank adapters. Evaluations on LLaMA-2 and
LLaMA-3 demonstrate RILQ's consistent improvements in 2-bit **quantize**d inference
across various state-of-the-art **quantize**rs and enhanced accuracy in
task-specific fine-tuning. RILQ maintains computational efficiency comparable
to existing LoRA methods, enabling adapter-merged weight-**quantize**d LLM
inference with significantly enhanced accuracy, making it a promising approach
for boosting 2-bit LLM performance.


## TAS-TsC A Data-Driven Framework for Estimating Time of Arrival Using Temporal-Attribute-Spatial Tri-space Coordination of Truck Trajectories

>Authors: Mengran Li, Junzhou Chen, Guanying Jiang, Fuliang Li, Ronghui Zhang, Siyuan Gong, Zhihan Lv

>2024-12-02

> http://arxiv.org/abs/2412.01122v1

Accurately estimating time of arrival (ETA) for trucks is crucial for
optimizing transportation efficiency in logistics. GPS trajectory data offers
valuable information for ETA, but challenges arise due to temporal **sparsity**,
variable sequence lengths, and the interdependencies among multiple trucks. To
address these issues, we propose the Temporal-Attribute-Spatial Tri-space
Coordination (TAS-TsC) framework, which leverages three feature
spaces-temporal, attribute, and spatial-to enhance ETA. Our framework consists
of a Temporal Learning Module (TLM) using state space models to capture
temporal dependencies, an Attribute Extraction Module (AEM) that transforms
sequential features into structured attribute embeddings, and a Spatial Fusion
Module (SFM) that models the interactions among multiple trajectories using
graph representation learning.These modules collaboratively learn trajectory
embeddings, which are then used by a Downstream Prediction Module (DPM) to
estimate arrival times. We validate TAS-TsC on real truck trajectory datasets
collected from Shenzhen, China, demonstrating its superior performance compared
to existing methods.


## FreeCodec A disentangled neural speech codec with fewer tokens

>Authors: Youqiang Zheng, Weiping Tu, Yueteng Kang, Jie Chen, Yike Zhang, Li Xiao, Yuhong Yang, Long Ma

>2024-12-02

> http://arxiv.org/abs/2412.01053v1

Neural speech codecs have gained great attention for their outstanding
reconstruction with discrete token representations.
  It is a crucial component in generative tasks such as speech coding and large
language models (LLM).
  However, most works based on residual vector **quantization** perform worse with
fewer tokens due to low coding efficiency for modeling complex coupled
information.
  In this paper, we propose a neural speech codec named FreeCodec which employs
a more effective encoding framework by decomposing intrinsic properties of
speech into different components:
  1) a global vector is extracted as the timbre information,
  2) a prosody encoder with a long stride level is used to model the prosody
information,
  3) the content information is from a content encoder.
  Using different training strategies, FreeCodec achieves state-of-the-art
performance in reconstruction and disentanglement scenarios.
  Results from subjective and objective experiments demonstrate that our
framework outperforms existing methods.


## Quantization-Aware Imitation-Learning for Resource-Efficient Robotic Control

>Authors: Seongmin Park, Hyungmin Kim, Wonseok Jeon, Juyoung Yang, Byeongwook Jeon, Yoonseon Oh, Jungwook Choi

>2024-12-02

> http://arxiv.org/abs/2412.01034v1

Deep neural network (DNN)-based policy models like vision-language-action
(VLA) models are transformative in automating complex decision-making across
applications by interpreting multi-modal data. However, scaling these models
greatly increases computational costs, which presents challenges in fields like
robot manipulation and autonomous driving that require quick, accurate
responses. To address the need for deployment on resource-limited hardware, we
propose a new **quantization** framework for IL-based policy models that fine-tunes
parameters to enhance robustness against **low-bit** precision errors during
training, thereby maintaining efficiency and reliability under constrained
conditions. Our evaluations with representative robot manipulation for 4-bit
weight-**quantization** on a real edge GPU demonstrate that our framework achieves
up to 2.5x speedup and 2.5x energy savings while preserving accuracy. For 4-bit
weight and activation **quantize**d self-driving models, the framework achieves up
to 3.7x speedup and 3.1x energy saving on a low-end GPU. These results
highlight the practical potential of deploying IL-based policy models on
resource-constrained devices.


## Token Cropr Faster ViTs for Quite a Few Tasks

>Authors: Benjamin Bergner, Christoph Lippert, Aravindh Mahendran

>2024-12-01

> http://arxiv.org/abs/2412.00965v1

The adoption of Vision Transformers (ViTs) in resource-constrained
applications necessitates improvements in inference throughput. To this end
several token **pruning** and merging approaches have been proposed that improve
efficiency by successively reducing the number of tokens. However, it remains
an open problem to design a token reduction method that is fast, maintains high
performance, and is applicable to various vision tasks. In this work, we
present a token pruner that uses auxiliary prediction heads that learn to
select tokens end-to-end based on task relevance. These auxiliary heads can be
removed after training, leading to throughput close to that of a random pruner.
We evaluate our method on image classification, semantic segmentation, object
detection, and instance segmentation, and show speedups of 1.5 to 4x with small
drops in performance. As a best case, on the ADE20k semantic segmentation
benchmark, we observe a 2x speedup relative to the no-**pruning** baseline, with a
negligible performance penalty of 0.1 median mIoU across 5 seeds.


## Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion

>Authors: Bohai Gu, Hao Luo, Song Guo, Peiran Dong

>2024-12-01

> http://arxiv.org/abs/2412.00857v1

Recently, diffusion-based methods have achieved great improvements in the
video inpainting task. However, these methods still face many challenges, such
as maintaining temporal consistency and the time-consuming issue. This paper
proposes an advanced video inpainting framework using optical Flow-guided
Efficient Diffusion, called FloED. Specifically, FloED employs a dual-branch
architecture, where a flow branch first restores corrupted flow and a
multi-scale flow adapter provides motion guidance to the main inpainting
branch. Additionally, a training-free latent interpolation method is proposed
to accelerate the multi-step denoising process using flow warping. Further
introducing a flow attention cache mechanism, FLoED efficiently reduces the
computational cost brought by incorporating optical flow. Comprehensive
experiments in both background restoration and object removal tasks demonstrate
that FloED outperforms state-of-the-art methods from the perspective of both
performance and efficiency.


## DynSUP Dynamic Gaussian Splatting from An Unposed Image Pair

>Authors: Weihang Li, Weirong Chen, Shenhan Qian, Jiajie Chen, Daniel Cremers, Haoang Li

>2024-12-01

> http://arxiv.org/abs/2412.00851v1

Recent advances in 3D Gaussian Splatting have shown promising results.
Existing methods typically assume static scenes and/or multiple images with
prior poses. Dynamics, **sparse** views, and unknown poses significantly increase
the problem complexity due to insufficient geometric constraints. To overcome
this challenge, we propose a method that can use only two images without prior
poses to fit Gaussians in dynamic environments. To achieve this, we introduce
two technical contributions. First, we propose an object-level two-view bundle
adjustment. This strategy decomposes dynamic scenes into piece-wise rigid
components, and jointly estimates the camera pose and motions of dynamic
objects. Second, we design an SE(3) field-driven Gaussian training method. It
enables fine-grained motion modeling through learnable per-Gaussian
transformations. Our method leads to high-fidelity novel view synthesis of
dynamic scenes while accurately preserving temporal consistency and object
motion. Experiments on both synthetic and real-world datasets demonstrate that
our method significantly outperforms state-of-the-art approaches designed for
the cases of static environments, multiple images, and/or known poses. Our
project page is available at https://colin-de.github.io/DynSUP/.


## DFRot Achieving Outlier-Free and Massive Activation-Free for Rotated LLMs with Refined Rotation

>Authors: Jingyang Xiang, Sai Qian Zhang

>2024-12-01

> http://arxiv.org/abs/2412.00648v2

Rotating the activation and weight matrices to reduce the influence of
outliers in large language models (LLMs) has recently attracted significant
attention, particularly in the context of model **quantization**. Prior studies
have shown that in low-precision **quantization** scenarios, such as 4-bit weights
and 4-bit activations (W4A4), randomized Hadamard transforms can achieve
significantly higher accuracy than randomized orthogonal transforms. Notably,
the reason behind this phenomena remains unknown. In this paper, we find that
these transformations show substantial improvement in eliminating outliers for
common tokens and achieve similar **quantization** error. The primary reason for
the accuracy difference lies in the fact that randomized Hadamard transforms
can slightly reduce the **quantization** error for tokens with massive activations
while randomized orthogonal transforms increase the **quantization** error. Due to
the extreme rarity of these tokens and their critical impact on model accuracy,
we consider this a long-tail optimization problem, and therefore construct a
simple yet effective method: a weighted loss function. Additionally, we propose
an optimization strategy for the rotation matrix that involves alternating
optimization of **quantization** parameters while employing orthogonal Procrustes
transforms to refine the rotation matrix. This makes the distribution of the
rotated activation values more conducive to **quantization**, especially for tokens
with massive activations. Our method enhances the Rotated LLMs by achieving
dual free, Outlier-Free and Massive Activation-Free, dubbed as DFRot. Extensive
experiments demonstrate the effectiveness and efficiency of DFRot. By tuning
the rotation matrix using just a single sample, DFRot achieves a perplexity
improvement of 0.25 and 0.21 on W4A4**KV**4 and W4A4**KV**16, respectively, for
LLaMA3-8B, a model known for its **quantization** challenges.


## Pruned Convolutional Attention Network Based Wideband Spectrum Sensing with Sub-Nyquist Sampling

>Authors: Peihao Dong, Jibin Jia, Shen Gao, Fuhui Zhou, Qihui Wu

>2024-11-30

> http://arxiv.org/abs/2412.00562v1

Wideband spectrum sensing (WSS) is critical for orchestrating multitudinous
wireless transmissions via spectrum sharing, but may incur excessive costs of
hardware, power and computation due to the high sampling rate. In this article,
a deep learning based WSS framework embedding the multicoset preprocessing is
proposed to enable the low-cost sub-Nyquist sampling. A pruned convolutional
attention WSS network (PCA-WSSNet) is designed to organically integrate the
multicoset preprocessing and the convolutional attention mechanism as well as
to reduce the model complexity remarkably via the selective weight **pruning**
without the performance loss. Furthermore, a transfer learning (TL) strategy
benefiting from the model **pruning** is developed to improve the robustness of
PCA-WSSNet with few adaptation samples of new scenarios. Simulation results
show the performance superiority of PCA-WSSNet over the state of the art.
Compared with direct TL, the pruned TL strategy can simultaneously improve the
prediction accuracy in unseen scenarios, reduce the model size, and accelerate
the model inference.


## Accelerating Multimodel Large Language Models by Searching Optimal Vision Token Reduction

>Authors: Shiyu Zhao, Zhenting Wang, Felix Juefei-Xu, Xide Xia, Miao Liu, Xiaofang Wang, Mingfu Liang, Ning Zhang, Dimitris N. Metaxas, Licheng Yu

>2024-11-30

> http://arxiv.org/abs/2412.00556v1

Prevailing Multimodal Large Language Models (MLLMs) encode the input image(s)
as vision tokens and feed them into the language backbone, similar to how Large
Language Models (LLMs) process the text tokens. However, the number of vision
tokens increases quadratically as the image resolutions, leading to huge
computational costs. In this paper, we consider improving MLLM's efficiency
from two scenarios, (I) Reducing computational cost without degrading the
performance. (II) Improving the performance with given budgets. We start with
our main finding that the ranking of each vision token sorted by attention
scores is similar in each layer except the first layer. Based on it, we assume
that the number of essential top vision tokens does not increase along layers.
Accordingly, for Scenario I, we propose a greedy search algorithm (G-Search) to
find the least number of vision tokens to keep at each layer from the shallow
to the deep. Interestingly, G-Search is able to reach the optimal reduction
strategy based on our assumption. For Scenario II, based on the reduction
strategy from G-Search, we design a parametric sigmoid function (P-Sigmoid) to
guide the reduction at each layer of the MLLM, whose parameters are optimized
by Bayesian Optimization. Extensive experiments demonstrate that our approach
can significantly accelerate those popular MLLMs, e.g. LLaVA, and InternVL2
models, by more than $2 \times$ without performance drops. Our approach also
far outperforms other token reduction methods when budgets are limited,
achieving a better trade-off between efficiency and effectiveness.


## A novel algorithm for the decomposition of non-stationary multidimensional and multivariate signals

>Authors: Roberto Cavassi, Antonio Cicone, Enza Pellegrino, Haomin Zhou

>2024-11-30

> http://arxiv.org/abs/2412.00553v1

The decomposition of a signal is a fundamental tool in many fields of
research, including signal processing, geophysics, astrophysics, engineering,
medicine, and many more. By breaking down complex signals into simpler
oscillatory components we can enhance the understanding and processing of the
data, unveiling hidden information contained in them. Traditional methods, such
as Fourier analysis and wavelet transforms, which are effective in handling
mono-dimensional stationary signals struggle with non-stationary data sets and
they require, this is the case of the wavelet, the selection of predefined
basis functions. In contrast, the Empirical Mode Decomposition (EMD) method and
its variants, such as Iterative Filtering (IF), have emerged as effective
nonlinear approaches, adapting to signals without any need for a priori
assumptions. To accelerate these methods, the Fast Iterative Filtering (FIF)
algorithm was developed, and further extensions, such as Multivariate FIF
(MvFIF) and Multidimensional FIF (FIF2), have been proposed to handle
higher-dimensional data.
  In this work, we introduce the Multidimensional and Multivariate Fast
Iterative Filtering (MdMvFIF) technique, an innovative method that extends FIF
to handle data that vary simultaneously in space and time. This new algorithm
is capable of extracting Intrinsic Mode Functions (IMFs) from complex signals
that vary in both space and time, overcoming limitations found in prior
methods. The potentiality of the proposed method is demonstrated through
applications to artificial and real-life signals, highlighting its versatility
and effectiveness in decomposing multidimensional and multivariate
nonstationary signals. The MdMvFIF method offers a powerful tool for advanced
signal analysis across many scientific and engineering disciplines.


## Homeostazis and Sparsity in Transformer

>Authors: Leonid Kotyuzanskiy, Artem Klimov

>2024-11-30

> http://arxiv.org/abs/2412.00503v1

The transformer architecture has become an integral part of the field of
modern neural networks, playing a crucial role in a variety of tasks, such as
text generation, machine translation, image and audio processing, among others.
There is also an alternative approach to building intelligent systems, proposed
by Jeff Hawkins and inspired by the processes occurring in the neocortex. In
our article we want to combine some of these ideas and to propose the use of
homeostazis mechanisms, such as RFB-kWTA and "Smart" Inhibition, in the
attention mechanism of the transformer and at the output of the transformer
block, as well as conducting an experiment involving the introduction of **sparse**
distributed representations of the transformer at various points. RFB-kWTA
utilizes statistics of layer activations across time to adjust the entire
layer, enhancing the values of rare activations while reducing those of
frequent ones. "Smart" Inhibition also uses activation statistics to sample
**sparsity** masks, with rarer activation times are more likely to be activated.
Our proposed mechanisms significantly outperform the classical transformer
0.2768 BLEU and a model that only makes use of dropout in the attention
mechanism and output of the transformer block 0.3007 BLEU, achieving a score of
0.3062 on the Multi30K dataset.


## Joint Beam Scheduling and Resource Allocation for Flexible RSMA-aided Near-Field Communications

>Authors: Jiasi Zhou, Cong Zhou, Yijie Mao, Chintha Tellambura

>2024-11-30

> http://arxiv.org/abs/2412.00487v1

Supporting immense throughput and ubiquitous connectivity holds paramount
importance for future wireless networks. To this end, this letter focuses on
how the spatial beams configured for legacy near-field (NF) users can be
leveraged to serve extra NF or far-field users while ensuring the rate
requirements of legacy NF users. In particular, a flexible rate splitting
multiple access (RSMA) scheme is proposed to efficiently manage interference,
which carefully selects a subset of legacy users to decode the common stream.
Beam scheduling, power allocation, common rate allocation, and user selection
are jointly optimized to maximize the sum rate of additional users. To solve
the formulated discrete non-convex problem, it is split into three subproblems.
The accelerated bisection searching, quadratic transform, and simulated
annealing approaches are developed to attack them. Simulation results reveal
that the proposed transmit scheme and algorithm achieve significant gains over
three competing benchmarks.


## ATP-LLaVA Adaptive Token Pruning for Large Vision Language Models

>Authors: Xubing Ye, Yukang Gan, Yixiao Ge, Xiao-Ping Zhang, Yansong Tang

>2024-11-30

> http://arxiv.org/abs/2412.00447v1

Large Vision Language Models (LVLMs) have achieved significant success across
multi-modal tasks. However, the computational cost of processing long visual
tokens can be prohibitively expensive on resource-limited devices. Previous
methods have identified redundancy in visual tokens within the Large Language
Model (LLM) decoder layers and have mitigated this by **pruning** tokens using a
pre-defined or fixed ratio, thereby reducing computational overhead.
Nonetheless, we observe that the impact of **pruning** ratio varies across
different LLM layers and instances (image-prompt pairs). Therefore, it is
essential to develop a layer-wise and instance-wise vision token **pruning**
strategy to balance computational cost and model performance effectively. We
propose ATP-LLaVA, a novel approach that adaptively determines
instance-specific token **pruning** ratios for each LLM layer. Specifically, we
introduce an Adaptive Token Pruning (ATP) module, which computes the importance
score and **pruning** threshold based on input instance adaptively. The ATP module
can be seamlessly integrated between any two LLM layers with negligible
computational overhead. Additionally, we develop a Spatial Augmented Pruning
(SAP) strategy that prunes visual tokens with both token redundancy and spatial
modeling perspectives. Our approach reduces the average token count by 75%
while maintaining performance, with only a minimal 1.9% degradation across
seven widely used benchmarks. The project page can be accessed via
https://yxxxb.github.io/ATP-LLaVA-page/.


## CaDA Cross-Problem Routing Solver with Constraint-Aware Dual-Attention

>Authors: Han Li, Fei Liu, Zhi Zheng, Yu Zhang, Zhenkun Wang

>2024-11-30

> http://arxiv.org/abs/2412.00346v1

Vehicle Routing Problems (VRPs) are significant Combinatorial Optimization
(CO) problems holding substantial practical importance. Recently, Neural
Combinatorial Optimization (NCO), which involves training deep learning models
on extensive data to learn vehicle routing heuristics, has emerged as a
promising approach due to its efficiency and the reduced need for manual
algorithm design. However, applying NCO across diverse real-world scenarios
with various constraints necessitates cross-problem capabilities. Current NCO
methods typically employ a unified model lacking a constraint-specific
structure, thereby restricting their cross-problem performance. Current
multi-task methods for VRPs typically employ a constraint-unaware model,
limiting their cross-problem performance. Furthermore, they rely solely on
global connectivity, which fails to focus on key nodes and leads to inefficient
representation learning. This paper introduces a Constraint-Aware
Dual-Attention Model (CaDA), designed to address these limitations. CaDA
incorporates a constraint prompt that efficiently represents different problem
variants. Additionally, it features a dual-attention mechanism with a global
branch for capturing broader graph-wide information and a **sparse** branch that
selectively focuses on the most relevant nodes. We comprehensively evaluate our
model on 16 different VRPs and compare its performance against existing
cross-problem VRP solvers. CaDA achieves state-of-the-art results across all
the VRPs. Our ablation study further confirms that each component of CaDA
contributes positively to its cross-problem learning performance.


## Analyzing the Energy and Accuracy of LLMs in Software Development

>Authors: Negar Alizadeh, Boris Belchev, Nishant Saurabh, Patricia Kelbert, Fernando Castor

>2024-11-30

> http://arxiv.org/abs/2412.00329v1

The use of generative AI-based coding assistants like ChatGPT and Github
Copilot is a reality in contemporary software development. Many of these tools
are provided as remote APIs. Using third-party APIs raises data privacy and
security concerns for client companies, which motivates the use of
locally-deployed language models. In this study, we explore the trade-off
between model accuracy and energy consumption, aiming to provide valuable
insights to help developers make informed decisions when selecting a language
model. We investigate the performance of 18 families of LLMs in typical
software development tasks on two real-world infrastructures, a commodity GPU
and a powerful AI-specific GPU. Given that deploying LLMs locally requires
powerful infrastructure which might not be affordable for everyone, we consider
both full-precision and **quantize**d models. Our findings reveal that employing a
big LLM with a higher energy budget does not always translate to significantly
improved accuracy. Additionally, **quantize**d versions of large models generally
offer better efficiency and accuracy compared to full-precision versions of
medium-sized ones. Apart from that, not a single model is suitable for all
types of software development tasks.


## Sparse Bayesian Factor Models with Mass-Nonlocal Factor Scores

>Authors: Dafne Zorzetto, Yingjie Huang, Roberta De Vito

>2024-11-30

> http://arxiv.org/abs/2412.00304v1

Bayesian factor models are widely used for dimensionality reduction and
pattern discovery in high-dimensional datasets across diverse fields. These
models typically focus on imposing priors on factor loading to induce **sparsity**
and improve interpretability. However, factor score, which plays a critical
role in individual-level associations with factors, has received less attention
and is assumed to have standard multivariate normal distribution. This
oversimplification fails to capture the heterogeneity observed in real-world
applications. We propose the Sparse Bayesian Factor Model with Mass-Nonlocal
Factor Scores (BFMAN), a novel framework that addresses these limitations by
introducing a mass-nonlocal prior for factor scores. This prior provides a more
flexible posterior distribution that captures individual heterogeneity while
assigning positive probability to zero value. The zeros entries in the score
matrix, characterize the **sparsity**, offering a robust and novel approach for
determining the optimal number of factors. Model parameters are estimated using
a fast and efficient Gibbs sampler. Extensive simulations demonstrate that
BFMAN outperforms standard Bayesian **sparse** factor models in factor recovery,
**sparsity** detection, and score estimation. We apply BFMAN to the Hispanic
Community Health Study/Study of Latinos and identify dietary patterns and their
associations with cardiovascular outcomes, showcasing the model's ability to
uncover meaningful insights in diet.


## An AI-Driven Data Mesh Architecture Enhancing Decision-Making in Infrastructure Construction and Public Procurement

>Authors: Saurabh Mishra, Mahendra Shinde, Aniket Yadav, Bilal Ayyub, Anand Rao

>2024-11-29

> http://arxiv.org/abs/2412.00224v1

Infrastructure construction, often dubbed an "industry of industries," is
closely linked with government spending and public procurement, offering
significant opportunities for improved efficiency and productivity through
better transparency and information access. By leveraging these opportunities,
we can achieve notable gains in productivity, cost savings, and broader
economic benefits. Our approach introduces an integrated software ecosystem
utilizing Data Mesh and Service Mesh architectures. This system includes the
largest training dataset for infrastructure and procurement, encompassing over
100 billion tokens, scientific publications, activities, and risk data, all
structured by a systematic AI framework. Supported by a Knowledge Graph linked
to domain-specific multi-agent tasks and Q&A capabilities, our platform
standardizes and ingests diverse data sources, transforming them into
structured knowledge. Leveraging large language models (LLMs) and automation,
our system revolutionizes data structuring and knowledge creation, aiding
decision-making in early-stage project planning, detailed research, market
trend analysis, and qualitative assessments. Its web-scalable architecture
delivers domain-curated information, enabling AI agents to facilitate reasoning
and manage uncertainties, while preparing for future expansions with
specialized agents targeting particular challenges. This integration of AI with
domain expertise not only boosts efficiency and decision-making in construction
and infrastructure but also establishes a framework for enhancing government
efficiency and accelerating the transition of traditional industries to digital
workflows. This work is poised to significantly influence AI-driven initiatives
in this sector and guide best practices in AI Operations.


## N端shuRescue Revitalization of the endangered N端shu Language with AI

>Authors: Ivory Yang, Weicheng Ma, Soroush Vosoughi

>2024-11-29

> http://arxiv.org/abs/2412.00218v2

The preservation and revitalization of endangered and extinct languages is a
meaningful endeavor, conserving cultural heritage while enriching fields like
linguistics and anthropology. However, these languages are typically
low-resource, making their reconstruction labor-intensive and costly. This
challenge is exemplified by N\"ushu, a rare script historically used by Yao
women in China for self-expression within a patriarchal society. To address
this challenge, we introduce N\"ushuRescue, an AI-driven framework designed to
train large language models (LLMs) on endangered languages with minimal data.
N\"ushuRescue automates evaluation and expands target corpora to accelerate
linguistic revitalization. As a foundational component, we developed NCGold, a
500-sentence N\"ushu-Chinese parallel corpus, the first publicly available
dataset of its kind. Leveraging GPT-4-Turbo, with no prior exposure to N\"ushu
and only 35 short examples from NCGold, N\"ushuRescue achieved 48.69\%
translation accuracy on 50 withheld sentences and generated NCSilver, a set of
98 newly translated modern Chinese sentences of varying lengths. A sample of
both NCGold and NCSilver is included in the Supplementary Materials.
Additionally, we developed FastText-based and Seq2Seq models to further support
research on N\"ushu. N\"ushuRescue provides a versatile and scalable tool for
the revitalization of endangered languages, minimizing the need for extensive
human input.


## Scaling Transformers for Low-Bitrate High-Quality Speech Coding

>Authors: Julian D Parker, Anton Smirnov, Jordi Pons, CJ Carr, Zack Zukowski, Zach Evans, Xubo Liu

>2024-11-29

> http://arxiv.org/abs/2411.19842v1

The tokenization of speech with neural audio codec models is a vital part of
modern AI pipelines for the generation or understanding of speech, alone or in
a multimodal context. Traditionally such tokenization models have concentrated
on low parameter-count architectures using only components with strong
inductive biases. In this work we show that by scaling a transformer
architecture with large parameter count to this problem, and applying a
flexible Finite Scalar Quantization (FSQ) based bottleneck, it is possible to
reach state-of-the-art speech quality at extremely low bit-rates of $400$ or
$700$ bits-per-second. The trained models strongly out-perform existing
baselines in both objective and subjective tests.


## Fast Mutual Information Computation for Large Binary Datasets

>Authors: Andre O. Falcao

>2024-11-29

> http://arxiv.org/abs/2411.19702v1

Mutual Information (MI) is a powerful statistical measure that quantifies
shared information between random variables, particularly valuable in
high-dimensional data analysis across fields like genomics, natural language
processing, and network science. However, computing MI becomes computationally
prohibitive for large datasets where it is typically required a pairwise
computational approach where each column is compared to others. This work
introduces a matrix-based algorithm that accelerates MI computation by
leveraging vectorized operations and optimized matrix calculations. By
transforming traditional pairwise computational approaches into bulk matrix
operations, the proposed method enables efficient MI calculation across all
variable pairs. Experimental results demonstrate significant performance
improvements, with computation times reduced up to 50,000 times in the largest
dataset using optimized implementations, particularly when utilizing hardware
optimized frameworks. The approach promises to expand MI's applicability in
data-driven research by overcoming previous computational limitations.


## ChineseWebText 2.0 Large-Scale High-quality Chinese Web Text with Multi-dimensional and fine-grained information

>Authors: Wanyue Zhang, Ziyong Li, Wen Yang, Chunlin Leng, Yinan Bai, Qianlong Du, Chengqing Zong, Jiajun Zhang

>2024-11-29

> http://arxiv.org/abs/2411.19668v1

During the development of large language models (LLMs), pre-training data
play a critical role in shaping LLMs' capabilities. In recent years several
large-scale and high-quality pre-training datasets have been released to
accelerate the research of LLMs, including ChineseWebText1.0, C4, Pile,
WanJuan, MAPCC and others. However, as LLMs continue to evolve, focus has
increasingly shifted to domain-specific capabilities and safety concerns,
making those previous coarse-grained texts insufficient for meeting training
requirements. Furthermore, fine-grained information, such as quality, domain
and toxicity, is becoming increasingly important in building powerful and
reliable LLMs for various scenarios. To address these challenges, in this paper
we propose a new tool-chain called MDFG-tool for constructing large-scale and
high-quality Chinese datasets with multi-dimensional and fine-grained
information. First, we employ manually crafted rules to discard explicit noisy
texts from raw contents. Second, the quality evaluation model, domain
classifier, and toxicity evaluation model are well-designed to assess the
remaining cleaned data respectively. Finally, we integrate these three types of
fine-grained information for each text. With this approach, we release the
largest, high-quality and fine-grained Chinese text ChineseWebText2.0, which
consists of 3.8TB and each text is associated with a quality score, domain
labels, a toxicity label and a toxicity score, facilitating the LLM researchers
to select data based on various types of fine-grained information. The data,
codes and the tool-chain are available on this website
https://github.com/CASIA-LM/ChineseWebText-2.0


## CogACT A Foundational Vision-Language-Action Model for Synergizing Cognition and Action in Robotic Manipulation

>Authors: Qixiu Li, Yaobo Liang, Zeyu Wang, Lin Luo, Xi Chen, Mozheng Liao, Fangyun Wei, Yu Deng, Sicheng Xu, Yizhong Zhang, Xiaofan Wang, Bei Liu, Jianlong Fu, Jianmin Bao, Dong Chen, Yuanchun Shi, Jiaolong Yang, Baining Guo

>2024-11-29

> http://arxiv.org/abs/2411.19650v1

The advancement of large Vision-Language-Action (VLA) models has
significantly improved robotic manipulation in terms of language-guided task
execution and generalization to unseen scenarios. While existing VLAs adapted
from pretrained large Vision-Language-Models (VLM) have demonstrated promising
generalizability, their task performance is still unsatisfactory as indicated
by the low tasks success rates in different environments. In this paper, we
present a new advanced VLA architecture derived from VLM. Unlike previous works
that directly repurpose VLM for action prediction by simple action
**quantization**, we propose a omponentized VLA architecture that has a specialized
action module conditioned on VLM output. We systematically study the design of
the action module and demonstrates the strong performance enhancement with
diffusion action transformers for action sequence modeling, as well as their
favorable scaling behaviors. We also conduct comprehensive experiments and
ablation studies to evaluate the efficacy of our models with varied designs.
The evaluation on 5 robot embodiments in simulation and real work shows that
our model not only significantly surpasses existing VLAs in task performance
and but also exhibits remarkable adaptation to new robots and generalization to
unseen objects and backgrounds. It exceeds the average success rates of OpenVLA
which has similar model size (7B) with ours by over 35% in simulated evaluation
and 55% in real robot experiments. It also outperforms the large RT-2-X model
(55B) by 18% absolute success rates in simulation. Code and models can be found
on our project page (https://cogact.github.io/).


## KV Shifting Attention Enhances Language Modeling

>Authors: Mingyu Xu, Wei Cheng, Bingning Wang, Weipeng Chen

>2024-11-29

> http://arxiv.org/abs/2411.19574v2

The current large language models are mainly based on decode-only structure
transformers, which have great in-context learning (ICL) capabilities. It is
generally believed that the important foundation of its ICL capability is the
induction heads mechanism, which requires at least two layers attention. In
order to more efficiently implement the ability of the model's induction, we
revisit the induction heads mechanism and proposed a **KV** shifting attention. We
theoretically prove that the **KV** shifting attention reducing the model's
requirements for the depth and width of the induction heads mechanism. Our
experimental results demonstrate that **KV** shifting attention is beneficial to
learning induction heads and language modeling, which lead to better
performance or faster convergence from toy models to the pre-training models
with more than 10 B parameters.


## Quantized Delta Weight Is Safety Keeper

>Authors: Yule Liu, Zhen Sun, Xinlei He, Xinyi Huang

>2024-11-29

> http://arxiv.org/abs/2411.19530v1

Recent advancements in fine-tuning proprietary language models enable
customized applications across various domains but also introduce two major
challenges: high resource demands and security risks. Regarding resource
demands, recent work proposes novel partial compression, such as BitDelta, to
**quantize** the delta weights between the fine-tuned model and base model.
Regarding the security risks, user-defined fine-tuning can introduce security
vulnerabilities, such as alignment issues, backdoor attacks, and
hallucinations. However, most of the current efforts in security assessment
focus on the full-precision or full-compression models, it is not
well-discussed how the partial compression methods affect security concerns. To
bridge this gap, we evaluate the robustness of delta-weight **quantization**
against these security threats. In this paper, we uncover a "free lunch"
phenomenon: partial compression can enhance model security against
fine-tuning-based attacks with bearable utility loss. Using Llama-2-7b-chat as
a case study, we show that, with under 10% utility degradation, the partial
compression mitigates alignment-breaking risks by up to 66.17%, harmful
backdoor vulnerabilities by 64.46%, and targeted output manipulation risks by
up to 90.53%. We further apply LogitLens to visualize internal state
transformations during forward passes, suggesting mechanisms for both security
failure and recovery in standard versus compressed fine-tuning. This work
offers new insights into selecting effective delta compression methods for
secure, resource-efficient multi-tenant services.


## Hierarchical Framework for Retrosynthesis Prediction with Enhanced Reaction Center Localization

>Authors: Seongeun Yun, Won Bo Lee

>2024-11-29

> http://arxiv.org/abs/2411.19503v1

Retrosynthesis is essential for designing synthetic pathways for complex
molecules and can be revolutionized by AI to automate and accelerate chemical
synthesis planning for drug discovery and materials science. Here, we propose a
hierarchical framework for retrosynthesis prediction that systematically
integrates reaction center identification, action prediction, and termination
decision into a unified pipeline. Leveraging a molecular encoder pretrained
with contrastive learning, the model captures both atom and bond level
representations, enabling accurate identification of reaction centers and
prediction of chemical actions. The framework addresses the scarcity of
multiple reaction center data through augmentation strategies, enhancing the
ability of the model to generalize to diverse reaction scenarios. The proposed
approach achieves competitive performance across benchmark datasets, with
notably high topk accuracy and exceptional reaction center identification
capabilities, demonstrating its robustness in handling complex transformations.
These advancements position the framework as a promising tool for future
applications in material design and drug discovery.


## A Simple Sparse Matrix Vector Multiplication Approach to Padded Convolution

>Authors: Zan Chaudhry

>2024-11-29

> http://arxiv.org/abs/2411.19419v1

We introduce an algorithm for efficiently representing convolution with
zero-padding and stride as a **sparse** transformation matrix, applied to a
vectorized input through **sparse** matrix-vector multiplication (SpMV). We provide
a theoretical contribution with an explicit expression for the number of
non-zero multiplications in convolutions with stride and padding, offering
insight into the potential for leveraging **sparsity** in convolution operations. A
proof-of-concept implementation is presented in Python, demonstrating the
performance of our method on both CPU and GPU architectures. This work
contributes to the broader exploration of **sparse** matrix techniques in
convolutional algorithms, with a particular focus on leveraging matrix
multiplications for parallelization. Our findings lay the groundwork for future
advancements in exploiting **sparsity** to improve the efficiency of convolution
operations in fields such as machine learning and signal processing.

