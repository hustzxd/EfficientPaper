# 2024-11-15

# Table of Contents
* [Flow reconstruction in time-varying geometries using graph neural networks](#Flow-reconstruction-in-time-varying-geometries-using-graph-neural-networks)
* [Towards Secure Intelligent O-RAN Architecture Vulnerabilities, Threats and Promising Technical Solutions using LLMs](#Towards-Secure-Intelligent-O-RAN-Architecture-Vulnerabilities,-Threats-and-Promising-Technical-Solutions-using-LLMs)
* [Hopfield-Fenchel-Young Networks A Unified Framework for Associative Memory Retrieval](#Hopfield-Fenchel-Young-Networks-A-Unified-Framework-for-Associative-Memory-Retrieval)
* [Advanced Nonlinear SCMA Codebook Design Based on Lattice Constellations](#Advanced-Nonlinear-SCMA-Codebook-Design-Based-on-Lattice-Constellations)
* [R3HF Reward Redistribution for Enhancing Reinforcement Learning from Human Feedback](#R3HF-Reward-Redistribution-for-Enhancing-Reinforcement-Learning-from-Human-Feedback)
* [NVCiM-PT An NVCiM-assisted Prompt Tuning Framework for Edge LLMs](#NVCiM-PT-An-NVCiM-assisted-Prompt-Tuning-Framework-for-Edge-LLMs)
* [Towards Low-bit Communication for Tensor Parallel LLM Inference](#Towards-Low-bit-Communication-for-Tensor-Parallel-LLM-Inference)
* [A Note on the Relativistic Transformation Properties of Quantum Stochastic Calculus](#A-Note-on-the-Relativistic-Transformation-Properties-of-Quantum-Stochastic-Calculus)
* [Rendering-Oriented 3D Point Cloud Attribute Compression using Sparse Tensor-based Transformer](#Rendering-Oriented-3D-Point-Cloud-Attribute-Compression-using-Sparse-Tensor-based-Transformer)
* [CDXFormer Boosting Remote Sensing Change Detection with Extended Long Short-Term Memory](#CDXFormer-Boosting-Remote-Sensing-Change-Detection-with-Extended-Long-Short-Term-Memory)
* [Duality in a 3D Field-Theoretic Model](#Duality-in-a-3D-Field-Theoretic-Model)
* [Horticultural Temporal Fruit Monitoring via 3D Instance Segmentation and Re-Identification using Point Clouds](#Horticultural-Temporal-Fruit-Monitoring-via-3D-Instance-Segmentation-and-Re-Identification-using-Point-Clouds)
* [ASER Activation Smoothing and Error Reconstruction for Large Language Model Quantization](#ASER-Activation-Smoothing-and-Error-Reconstruction-for-Large-Language-Model-Quantization)
* [Navigation with QPHIL Quantizing Planner for Hierarchical Implicit Q-Learning](#Navigation-with-QPHIL-Quantizing-Planner-for-Hierarchical-Implicit-Q-Learning)
* [OWLed Outlier-weighed Layerwise Pruning for Efficient Autonomous Driving Framework](#OWLed-Outlier-weighed-Layerwise-Pruning-for-Efficient-Autonomous-Driving-Framework)
* [Breaking the Low-Rank Dilemma of Linear Attention](#Breaking-the-Low-Rank-Dilemma-of-Linear-Attention)
* [Direct Preference Optimization Using Sparse Feature-Level Constraints](#Direct-Preference-Optimization-Using-Sparse-Feature-Level-Constraints)
* [Quasinormal modes of Plebański-Demiański black hole in the near-Nariai regime](#Quasinormal-modes-of-Plebański-Demiański-black-hole-in-the-near-Nariai-regime)
* [ADMM for Structured Fractional Minimization](#ADMM-for-Structured-Fractional-Minimization)
* [All-in-one Weather-degraded Image Restoration via Adaptive Degradation-aware Self-prompting Model](#All-in-one-Weather-degraded-Image-Restoration-via-Adaptive-Degradation-aware-Self-prompting-Model)


## Flow reconstruction in time-varying geometries using graph neural networks

>Authors: Bogdan A. Danciu, Vito A. Pagone, Benjamin Böhm, Marius Schmidt, Christos E. Frouzakis

>2024-11-13

> http://arxiv.org/abs/2411.08764v1

The paper presents a Graph Attention Convolutional Network (GACN) for flow
reconstruction from very **sparse** data in time-varying geometries. The model
incorporates a feature propagation algorithm as a preprocessing step to handle
extremely **sparse** inputs, leveraging information from neighboring nodes to
initialize missing features. In addition, a binary indicator is introduced as a
validity mask to distinguish between the original and propagated data points,
enabling more effective learning from **sparse** inputs. Trained on a unique data
set of Direct Numerical Simulations (DNS) of a motored engine at a technically
relevant operating condition, the GACN shows robust performance across
different resolutions and domain sizes and can effectively handle unstructured
data and variable input sizes. The model is tested on previously unseen DNS
data as well as on an experimental data set from Particle Image Velocimetry
(PIV) measurements that were not considered during training. A comparative
analysis shows that the GACN consistently outperforms both a conventional
Convolutional Neural Network (CNN) and cubic interpolation methods on the DNS
and PIV test sets by achieving lower reconstruction errors and better capturing
fine-scale turbulent structures. In particular, the GACN effectively
reconstructs flow fields from domains up to 14 times larger than those observed
during training, with the performance advantage increasing for larger domains.


## Towards Secure Intelligent O-RAN Architecture Vulnerabilities, Threats and Promising Technical Solutions using LLMs

>Authors: Mojdeh Karbalaee Motalleb, Chafika Benzaid, Tarik Taleb, Marcos Katz, Vahid Shah-Mansouri, JaeSeung Song

>2024-11-13

> http://arxiv.org/abs/2411.08640v1

The evolution of wireless communication systems will be fundamentally
impacted by an open radio access network (O-RAN), a new concept defining an
intelligent architecture with enhanced flexibility, openness, and the ability
to slice services more efficiently. For all its promises, and like any
technological advancement, O-RAN is not without risks that need to be carefully
assessed and properly addressed to accelerate its wide adoption in future
mobile networks. In this paper, we present an in-depth security analysis of the
O-RAN architecture, discussing the potential threats that may arise in the
different O-RAN architecture layers and their impact on the Confidentiality,
Integrity, and Availability (CIA) triad. We also promote the potential of zero
trust, Moving Target Defense (MTD), blockchain, and large language models(LLM)
technologies in fortifying O-RAN's security posture. Furthermore, we
numerically demonstrate the effectiveness of MTD in empowering robust deep
reinforcement learning methods for dynamic network slice admission control in
the O-RAN architecture. Moreover, we examine the effect of explainable AI (XAI)
based on LLMs in securing the system.


## Hopfield-Fenchel-Young Networks A Unified Framework for Associative Memory Retrieval

>Authors: Saul Santos, Vlad Niculae, Daniel McNamee, André F. T. Martins

>2024-11-13

> http://arxiv.org/abs/2411.08590v1

Associative memory models, such as Hopfield networks and their modern
variants, have garnered renewed interest due to advancements in memory capacity
and connections with self-attention in transformers. In this work, we introduce
a unified framework-Hopfield-Fenchel-Young networks-which generalizes these
models to a broader family of energy functions. Our energies are formulated as
the difference between two Fenchel-Young losses: one, parameterized by a
generalized entropy, defines the Hopfield scoring mechanism, while the other
applies a post-transformation to the Hopfield output. By utilizing Tsallis and
norm entropies, we derive end-to-end differentiable update rules that enable
**sparse** transformations, uncovering new connections between loss margins,
**sparsity**, and exact retrieval of single memory patterns. We further extend this
framework to structured Hopfield networks using the SparseMAP transformation,
allowing the retrieval of pattern associations rather than a single pattern.
Our framework unifies and extends traditional and modern Hopfield networks and
provides an energy minimization perspective for widely used
post-transformations like $\ell_2$-normalization and layer normalization-all
through suitable choices of Fenchel-Young losses and by using convex analysis
as a building block. Finally, we validate our Hopfield-Fenchel-Young networks
on diverse memory recall tasks, including free and sequential recall.
Experiments on simulated data, image retrieval, multiple instance learning, and
text rationalization demonstrate the effectiveness of our approach.


## Advanced Nonlinear SCMA Codebook Design Based on Lattice Constellations

>Authors: Qu Luo, Jing Zhu, Gaojie Chen, Pei Xiao, Rahim Tafazolli

>2024-11-13

> http://arxiv.org/abs/2411.08493v1

The design of efficient **sparse** codebooks in **sparse** code multiple access
(SCMA) system have attracted tremendous research attention in the past few
years. This paper proposes a novel nonlinear SCMA (NL-SCMA) that can subsume
the conventional SCMA system which is referred to as linear SCMA, as special
cases for downlink channels. This innovative approach allows a direct mapping
of users' messages to a superimposed codeword for transmission, eliminating the
need of a codebook for each user. This mapping is referred to as nonlinear
mapping (codebook) in this paper.
  Hence, the primary objective is to design the nonlinear mapping, rather than
the linear codebook for each user. We leverage the Lattice constellation to
design the superimposed constellation due to its advantages such as the minimum
Euclidean distance (MED), constellation volume, design flexibility and shape
gain. Then, by analyzing the error patterns of the Lattice-designed
superimposed codewords with the aid of the pair-wise error probability, it is
found that the MED of the proposed nonlinear codebook is lower bounded by the
``single error pattern''. To this end, an error pattern-inspired codebook
design is proposed, which can achieve large MEDs of the nonlinear codebooks.
Numerical results show that the proposed codebooks can achieve lower error rate
performance over both Gaussian and Rayleigh fading channels than
the-state-of-the-art linear codebooks.


## R3HF Reward Redistribution for Enhancing Reinforcement Learning from Human Feedback

>Authors: Jiahui Li, Tai-wei Chang, Fengda Zhang, Kun Kuang, Long Chen

>2024-11-13

> http://arxiv.org/abs/2411.08302v1

Reinforcement learning from human feedback (RLHF) provides a paradigm for
aligning large language models (LLMs) with human preferences. This involves the
initial training of a reward model based on pairwise human feedback. The reward
model is subsequently utilized in reinforcement learning to assess the scores
of each generated sentence as a whole, further guiding the optimization of
LLMs. However, current approaches have a significant shortcoming: \emph{They
allocate a single, **sparse**, and delayed reward to an entire sequence of output}.
This may overlook some significant individual contributions of each token
towards the desired outcome. To overcome this limitation, our paper proposes a
novel reward redistribution method called R3HF, which facilitates a more
fine-grained, token-level reward allocation. Specifically, our method treats
the reward prediction task of the reward model as a regression problem. As a
result, the redistributed rewards are computed by evaluating the specific
contribution of each token to the reward model's output. This detailed approach
improves the model's understanding of language nuances, leading to more precise
enhancements in its performance. Our method is crafted to integrate seamlessly
with most current techniques while incurring minimal computational costs.
Through comprehensive experiments across diverse datasets and tasks, we have
verified the effectiveness and superiority of our approach.


## NVCiM-PT An NVCiM-assisted Prompt Tuning Framework for Edge LLMs

>Authors: Ruiyang Qin, Pengyu Ren, Zheyu Yan, Liu Liu, Dancheng Liu, Amir Nassereldine, Jinjun Xiong, Kai Ni, Sharon Hu, Yiyu Shi

>2024-11-12

> http://arxiv.org/abs/2411.08244v1

Large Language Models (LLMs) deployed on edge devices, known as edge LLMs,
need to continuously fine-tune their model parameters from user-generated data
under limited resource constraints. However, most existing learning methods are
not applicable for edge LLMs because of their reliance on high resources and
low learning capacity. Prompt tuning (PT) has recently emerged as an effective
fine-tuning method for edge LLMs by only modifying a small portion of LLM
parameters, but it suffers from user domain shifts, resulting in repetitive
training and losing resource efficiency. Conventional techniques to address
domain shift issues often involve complex neural networks and sophisticated
training, which are incompatible for PT for edge LLMs. Therefore, an open
research question is how to address domain shift issues for edge LLMs with
limited resources. In this paper, we propose a prompt tuning framework for edge
LLMs, exploiting the benefits offered by non-volatile computing-in-memory
(NVCiM) architectures. We introduce a novel NVCiM-assisted PT framework, where
we narrow down the core operations to matrix-matrix multiplication, which can
then be accelerated by performing in-situ computation on NVCiM. To the best of
our knowledge, this is the first work employing NVCiM to improve the edge LLM
PT performance.


## Towards Low-bit Communication for Tensor Parallel LLM Inference

>Authors: Harry Dong, Tyler Johnson, Minsik Cho, Emad Soroush

>2024-11-12

> http://arxiv.org/abs/2411.07942v1

Tensor parallelism provides an effective way to increase server large
language model (LLM) inference efficiency despite adding an additional
communication cost. However, as server LLMs continue to scale in size, they
will need to be distributed across more devices, magnifying the communication
cost. One way to approach this problem is with **quantization**, but current
methods for LLMs tend to avoid quantizing the features that tensor parallelism
needs to communicate. Taking advantage of consistent outliers in communicated
features, we introduce a **quantization** method that reduces communicated values
on average from 16 bits to 4.2 bits while preserving nearly all of the original
performance. For instance, our method maintains around 98.0% and 99.5% of Gemma
2 27B's and Llama 2 13B's original performance, respectively, averaged across
all tasks we evaluated on.


## A Note on the Relativistic Transformation Properties of Quantum Stochastic Calculus

>Authors: J. E. Gough

>2024-11-12

> http://arxiv.org/abs/2411.07915v1

We give a simple argument to derive the transformation of quantum stochastic
calculus formalism between inertial observers, and derive the quantum open
system dynamics for a system moving in a vacuum (more generally coherent)
quantum field under the usual Markov approximation. We argue that for uniformly
accelerated open systems, however, the formalism must breakdown as we move from
a Fock representation of the quantum noise to a unitarily inequivalent non-Fock
representation - in particular, the latter is a thermal representation at the
Unruh temperature. The unitary inequivalence ultimately being a consequence of
the underlying flat noise spectrum approximation for the fundamental quantum
stochastic processes.


## Rendering-Oriented 3D Point Cloud Attribute Compression using Sparse Tensor-based Transformer

>Authors: Xiao Huo, Junhui Ho, Shuai Wan, Fuzheng Yang

>2024-11-12

> http://arxiv.org/abs/2411.07899v1

The evolution of 3D visualization techniques has fundamentally transformed
how we interact with digital content. At the forefront of this change is point
cloud technology, offering an immersive experience that surpasses traditional
2D representations. However, the massive data size of point clouds presents
significant challenges in data compression. Current methods for lossy point
cloud attribute compression (PCAC) generally focus on reconstructing the
original point clouds with minimal error. However, for point cloud
visualization scenarios, the reconstructed point clouds with distortion still
need to undergo a complex rendering process, which affects the final
user-perceived quality. In this paper, we propose an end-to-end deep learning
framework that seamlessly integrates PCAC with differentiable rendering,
denoted as rendering-oriented PCAC (RO-PCAC), directly targeting the quality of
rendered multiview images for viewing. In a differentiable manner, the impact
of the rendering process on the reconstructed point clouds is taken into
account. Moreover, we characterize point clouds as **sparse** tensors and propose a
**sparse** tensor-based transformer, called SP-Trans. By aligning with the local
density of the point cloud and utilizing an enhanced local attention mechanism,
SP-Trans captures the intricate relationships within the point cloud, further
improving feature analysis and synthesis within the framework. Extensive
experiments demonstrate that the proposed RO-PCAC achieves state-of-the-art
compression performance, compared to existing reconstruction-oriented methods,
including traditional, learning-based, and hybrid methods.


## CDXFormer Boosting Remote Sensing Change Detection with Extended Long Short-Term Memory

>Authors: Zhenkai Wu, Xiaowen Ma, Rongrong Lian, Zhentao Lin, Wei Zhang

>2024-11-12

> http://arxiv.org/abs/2411.07863v1

In complex scenes and varied conditions, effectively integrating
spatial-temporal context is crucial for accurately identifying changes.
However, current RS-CD methods lack a balanced consideration of performance and
efficiency. CNNs lack global context, Transformers have quadratic computational
complexity, and Mambas are restricted by CUDA **acceleration**. In this paper, we
propose CDXFormer, with a core component that is a powerful XLSTM-based feature
enhancement layer, integrating the advantages of linear computational
complexity, global context perception, and strong interpret-ability.
Specifically, we introduce a scale-specific Feature Enhancer layer,
incorporating a Cross-Temporal Global Perceptron customized for
semantic-accurate deep features, and a Cross-Temporal Spatial Refiner
customized for detail-rich shallow features. Additionally, we propose a
Cross-Scale Interactive Fusion module to progressively interact global change
representations with spatial responses. Extensive experimental results
demonstrate that CDXFormer achieves state-of-the-art performance across three
benchmark datasets, offering a compelling balance between efficiency and
accuracy. Code is available at https://github.com/xwmaxwma/rschange.


## Duality in a 3D Field-Theoretic Model

>Authors: R. Kumar, R. P. Malik

>2024-11-12

> http://arxiv.org/abs/2411.07849v1

We demonstrate the duality symmetry between the Abelian 1-form and 2-form
basic gauge fields in the context of a three (2 + 1)-dimensional (3D) combined
system of the field-theoretic model for the free Abelian 1-from and 2-form
gauge theories within the framework of Becchi-Rouet-Stora-Tyutin (BRST)
formalism where the classical gauge-fixed Lagrangian density of this theory is
generalized to its quantum counterpart as the BRST and co-BRST invariant
Lagrangian density. We show clearly the existence of the off-shell nilpotent
(co-)BRST symmetry transformations and establish their intimate connection
through a set of underlying discrete duality symmetry transformations in our
BRST-**quantize**d theory. We briefly mention a bosonic symmetry transformation
which is generated from the anticommutator of the above (co-)BRST symmetry
transformations. We lay emphasis on the algebraic structures of the existing
continuous as well as the discrete symmetry transformations in our theory
(where they are treated as operators).


## Horticultural Temporal Fruit Monitoring via 3D Instance Segmentation and Re-Identification using Point Clouds

>Authors: Daniel Fusaro, Federico Magistri, Jens Behley, Alberto Pretto, Cyrill Stachniss

>2024-11-12

> http://arxiv.org/abs/2411.07799v1

Robotic fruit monitoring is a key step toward automated agricultural
production systems. Robots can significantly enhance plant and temporal fruit
monitoring by providing precise, high-throughput assessments that overcome the
limitations of traditional manual methods. Fruit monitoring is a challenging
task due to the significant variation in size, shape, orientation, and
occlusion of fruits. Also, fruits may be harvested or newly grown between
recording sessions. Most methods are 2D image-based and they lack the 3D
structure, depth, and spatial information, which represent key aspects of fruit
monitoring. 3D colored point clouds, instead, can offer this information but
they introduce challenges such as their **sparsity** and irregularity. In this
paper, we present a novel approach for temporal fruit monitoring that addresses
point clouds collected in a greenhouse over time. Our method segments fruits
using a learning-based instance segmentation approach directly on the point
cloud. Each segmented fruit is processed by a 3D **sparse** convolutional neural
network to extract descriptors, which are used in an attention-based matching
network to associate fruits with their instances from previous data
collections. Experimental results on a real dataset of strawberries demonstrate
that our approach outperforms other methods for fruits re-identification over
time, allowing for precise temporal fruit monitoring in real and complex
scenarios.


## ASER Activation Smoothing and Error Reconstruction for Large Language Model Quantization

>Authors: Weibo Zhao, Yubin Shi, Xinyu Lyu, Wanchen Sui, Shen Li, Yong Li

>2024-11-12

> http://arxiv.org/abs/2411.07762v1

Quantization stands as a pivotal technique for large language model (LLM)
serving, yet it poses significant challenges particularly in achieving
effective **low-bit** **quantization**. The limited numerical mapping makes the
**quantize**d model produce a non-trivial error, bringing out intolerable
performance degration. This paper is anchored in the basic idea of model
compression objectives, and delves into the layer-wise error distribution of
LLMs during post-training **quantization**. Subsequently, we introduce ASER, an
algorithm consisting of (1) Error Reconstruction: low-rank compensation for
**quantization** error with LoRA-style matrices constructed by whitening SVD; (2)
Activation Smoothing: outlier extraction to gain smooth activation and better
error compensation. ASER is capable of quantizing typical LLMs to **low-bit** ones,
particularly preserving accuracy even in W4A8 per-channel setup. Experimental
results show that ASER is competitive among the state-of-the-art **quantization**
algorithms, showing potential to activation **quantization**, with minor overhead.


## Navigation with QPHIL Quantizing Planner for Hierarchical Implicit Q-Learning

>Authors: Alexi Canesse, Mathieu Petitbois, Ludovic Denoyer, Sylvain Lamprier, Rémy Portelas

>2024-11-12

> http://arxiv.org/abs/2411.07760v1

Offline Reinforcement Learning (RL) has emerged as a powerful alternative to
imitation learning for behavior modeling in various domains, particularly in
complex navigation tasks. An existing challenge with Offline RL is the
signal-to-noise ratio, i.e. how to mitigate incorrect policy updates due to
errors in value estimates. Towards this, multiple works have demonstrated the
advantage of hierarchical offline RL methods, which decouples high-level path
planning from low-level path following. In this work, we present a novel
hierarchical transformer-based approach leveraging a learned **quantize**r of the
space. This **quantization** enables the training of a simpler zone-conditioned
low-level policy and simplifies planning, which is reduced to discrete
autoregressive prediction. Among other benefits, zone-level reasoning in
planning enables explicit trajectory stitching rather than implicit stitching
based on noisy value function estimates. By combining this transformer-based
planner with recent advancements in offline RL, our proposed approach achieves
state-of-the-art results in complex long-distance navigation environments.


## OWLed Outlier-weighed Layerwise Pruning for Efficient Autonomous Driving Framework

>Authors: Jiaxi Li, Lu Yin, Xilu Wang

>2024-11-12

> http://arxiv.org/abs/2411.07711v1

The integration of Large Language Models (LLMs) into autonomous driving
systems offers promising enhancements in environmental understanding and
decision-making. However, the substantial computational demands of deploying
LLMs locally on vehicles render this approach unfeasible for real-world
automotive applications. To address this challenge, we introduce OWLed, the
Outlier-Weighed Layerwise Pruning for Efficient Autonomous Driving Framework
that leverages outlier-weighted layerwise **sparsity** for model compression. Our
method assigns non-uniform **sparsity** ratios to different layers based on the
distribution of outlier features, significantly reducing the model size without
the need for fine-tuning. To ensure the compressed model adapts well to
autonomous driving tasks, we incorporate driving environment data into both the
calibration and **pruning** processes. Our empirical studies reveal that the
encoder component is more sensitive to **pruning** than the LLM, highlighting its
critical role in the system. Experimental results demonstrate that OWLed
outperforms existing methods in perception, action prediction, and language
understanding while substantially lowering computational requirements. These
findings underscore the potential of combining advanced **pruning** techniques with
LLMs to develop efficient and robust autonomous driving systems capable of
handling complex scenarios. Code will be made publicly available.


## Breaking the Low-Rank Dilemma of Linear Attention

>Authors: Qihang Fan, Huaibo Huang, Ran He

>2024-11-12

> http://arxiv.org/abs/2411.07635v2

The Softmax attention mechanism in Transformer models is notoriously
computationally expensive, particularly due to its quadratic complexity, posing
significant challenges in vision applications. In contrast, linear attention
provides a far more efficient solution by reducing the complexity to linear
levels. However, compared to Softmax attention, linear attention often
experiences significant performance degradation. Our experiments indicate that
this performance drop is due to the low-rank nature of linear attention's
feature map, which hinders its ability to adequately model complex spatial
information. In this paper, to break the low-rank dilemma of linear attention,
we conduct rank analysis from two perspectives: the **KV** buffer and the output
features. Consequently, we introduce Rank-Augmented Linear Attention (RALA),
which rivals the performance of Softmax attention while maintaining linear
complexity and high efficiency. Based on RALA, we construct the Rank-Augmented
Vision Linear Transformer (RAVLT). Extensive experiments demonstrate that RAVLT
achieves excellent performance across various vision tasks. Specifically,
without using any additional labels, data, or supervision during training,
RAVLT achieves an 84.4% Top-1 accuracy on ImageNet-1k with only 26M parameters
and 4.6G FLOPs. This result significantly surpasses previous linear attention
mechanisms, fully illustrating the potential of RALA. Code will be available at
https://github.com/qhfan/RALA.


## Direct Preference Optimization Using Sparse Feature-Level Constraints

>Authors: Qingyu Yin, Chak Tou Leong, Hongbo Zhang, Minjun Zhu, Hanqi Yan, Qiang Zhang, Yulan He, Wenjie Li, Jun Wang, Yue Zhang, Linyi Yang

>2024-11-12

> http://arxiv.org/abs/2411.07618v1

The alignment of large language models (LLMs) with human preferences remains
a key challenge. While post-training techniques like Reinforcement Learning
from Human Feedback (RLHF) and Direct Preference Optimization (DPO) have
achieved notable success, they often introduce computational inefficiencies and
training instability. In this paper, we propose Feature-level constrained
Preference Optimization (FPO), a novel method designed to simplify the
alignment process while ensuring stability. FPO leverages pre-trained Sparse
Autoencoders (SAEs) and introduces feature-level constraints, allowing for
efficient, **sparsity**-enforced alignment. Our approach enjoys efficiency by using
**sparse** features activated in a well-trained **sparse** autoencoder and the quality
of sequential KL divergence by using the feature-level offline reference.
Experimental results on benchmark datasets demonstrate that FPO achieves a
5.08% absolute improvement in win rate with much lower computational cost
compared to state-of-the-art baselines, making it a promising solution for
efficient and controllable LLM alignments.


## Quasinormal modes of Plebański-Demiański black hole in the near-Nariai regime

>Authors: Hyewon Han, Bogeun Gwak

>2024-11-12

> http://arxiv.org/abs/2411.07568v1

We investigates the massless scalar perturbations of the
Pleba\'nski-Demia\'nski black hole considering the general case that admits all
nonzero parameters. This case is the most generic black hole spacetime in
general relativity, characterized by mass, spin, **acceleration**, electric and
magnetic charges, NUT parameter, and cosmological constant. Employing conformal
transformations, we can separate the massless scalar field equation and reduce
the effective potential in the radial perturbation equation into the
P\"oschl--Teller potential in the near-Nariai limit where the event and
cosmo-**acceleration** horizons are close. This allows us to obtain an exact
analytical solution of the quasinormal frequency, implying that the decay rate
of the field is **quantize**d depending only on the surface gravity of the black
hole.


## ADMM for Structured Fractional Minimization

>Authors: Ganzhao Yuan

>2024-11-12

> http://arxiv.org/abs/2411.07496v1

We consider a class of structured fractional minimization problems, where the
numerator includes a differentiable function, a simple nonconvex nonsmooth
function, a concave nonsmooth function, and a convex nonsmooth function
composed with a linear operator, while the denominator is a continuous function
that is either weakly convex or has a weakly convex square root. These problems
are widespread and span numerous essential applications in machine learning and
data science. Existing methods are mainly based on subgradient methods and
smoothing proximal gradient methods, which may suffer from slow convergence and
numerical stability issues. In this paper, we introduce {\sf FADMM}, the first
Alternating Direction Method of Multipliers tailored for this class of
problems. {\sf FADMM} decouples the original problem into linearized proximal
subproblems, featuring two variants: one using Dinkelbach's parametric method
({\sf FADMM-D}) and the other using the quadratic transform method ({\sf
FADMM-Q}). By introducing a novel Lyapunov function, we establish that {\sf
FADMM} converges to $\epsilon$-approximate critical points of the problem
within an oracle complexity of $\mathcal{O}(1/\epsilon^{3})$. Our experiments
on synthetic and real-world data for **sparse** Fisher discriminant analysis,
robust Sharpe ratio minimization, and robust **sparse** recovery demonstrate the
effectiveness of our approach.
  Keywords: Fractional Minimization, Nonconvex Optimization, Proximal
Linearized ADMM, Nonsmooth Optimization, Convergence Analysis


## All-in-one Weather-degraded Image Restoration via Adaptive Degradation-aware Self-prompting Model

>Authors: Yuanbo Wen, Tao Gao, Ziqi Li, Jing Zhang, Kaihao Zhang, Ting Chen

>2024-11-12

> http://arxiv.org/abs/2411.07445v1

Existing approaches for all-in-one weather-degraded image restoration suffer
from inefficiencies in leveraging degradation-aware priors, resulting in
sub-optimal performance in adapting to different weather conditions. To this
end, we develop an adaptive degradation-aware self-prompting model (ADSM) for
all-in-one weather-degraded image restoration. Specifically, our model employs
the contrastive language-image pre-training model (CLIP) to facilitate the
training of our proposed latent prompt generators (LPGs), which represent three
types of latent prompts to characterize the degradation type, degradation
property and image caption. Moreover, we integrate the acquired
degradation-aware prompts into the time embedding of diffusion model to improve
degradation perception. Meanwhile, we employ the latent caption prompt to guide
the reverse sampling process using the cross-attention mechanism, thereby
guiding the accurate image reconstruction. Furthermore, to accelerate the
reverse sampling procedure of diffusion model and address the limitations of
frequency perception, we introduce a wavelet-oriented noise estimating network
(WNE-Net). Extensive experiments conducted on eight publicly available datasets
demonstrate the effectiveness of our proposed approach in both task-specific
and all-in-one applications.

