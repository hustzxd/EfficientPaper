<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>Weekly Paper - Efficient Paper</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Weekly Paper";
        var mkdocs_page_input_path = "weekly_paper/latest.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> Efficient Paper
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Paper List</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../cls_year/">By Year</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cls_keyword/">By Keyword</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cls_publication/">By Publication</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cls_institution/">By Institution</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cls_author/">By Author</a>
                  </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="#">Weekly Paper</a>
    <ul class="current">
    </ul>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">Efficient Paper</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Weekly Paper</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="2025-08-12">2025-08-12</h1>
<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#BlindGuard-Safeguarding-LLM-based-Multi-Agent-Systems-under-Unknown-Attacks">BlindGuard Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks</a></li>
<li><a href="#TeamMedAgents-Enhancing-Medical-Decision-Making-of-LLMs-Through-Structured-Teamwork">TeamMedAgents Enhancing Medical Decision-Making of LLMs Through Structured Teamwork</a></li>
<li><a href="#ChatGPT-on-the-Road-Leveraging-Large-Language-Model-Powered-In-vehicle-Conversational-Agents-for-Safer-and-More-Enjoyable-Driving-Experience">ChatGPT on the Road Leveraging Large Language Model-Powered In-vehicle Conversational Agents for Safer and More Enjoyable Driving Experience</a></li>
<li><a href="#Interpreting-Fedspeak-with-Confidence-A-LLM-Based-Uncertainty-Aware-Framework-Guided-by-Monetary-Policy-Transmission-Paths">Interpreting Fedspeak with Confidence A LLM-Based Uncertainty-Aware Framework Guided by Monetary Policy Transmission Paths</a></li>
<li><a href="#EvoCoT-Overcoming-the-Exploration-Bottleneck-in-Reinforcement-Learning">EvoCoT Overcoming the Exploration Bottleneck in Reinforcement Learning</a></li>
<li><a href="#Grove-MoE-Towards-Efficient-and-Superior-MoE-LLMs-with-Adjugate-Experts">Grove MoE Towards Efficient and Superior MoE LLMs with Adjugate Experts</a></li>
<li><a href="#GLiClass-Generalist-Lightweight-Model-for-Sequence-Classification-Tasks">GLiClass Generalist Lightweight Model for Sequence Classification Tasks</a></li>
<li><a href="#HGMF-A-Hierarchical-Gaussian-Mixture-Framework-for-Scalable-Tool-Invocation-within-the-Model-Context-Protocol">HGMF A Hierarchical Gaussian Mixture Framework for Scalable Tool Invocation within the Model Context Protocol</a></li>
<li><a href="#Grounding-Natural-Language-for-Multi-agent-Decision-Making-with-Multi-agentic-LLMs">Grounding Natural Language for Multi-agent Decision-Making with Multi-agentic LLMs</a></li>
<li><a href="#LET-US-Long-Event-Text-Understanding-of-Scenes">LET-US Long Event-Text Understanding of Scenes</a></li>
<li><a href="#Efficient-Edge-LLMs-Deployment-via-HessianAware-Quantization-and-CPU-GPU-Collaborative">Efficient Edge LLMs Deployment via HessianAware Quantization and CPU GPU Collaborative</a></li>
<li><a href="#BEVANet-Bilateral-Efficient-Visual-Attention-Network-for-Real-Time-Semantic-Segmentation">BEVANet Bilateral Efficient Visual Attention Network for Real-Time Semantic Segmentation</a></li>
<li><a href="#LP-Spec-Leveraging-LPDDR-PIM-for-Efficient-LLM-Mobile-Speculative-Inference-with-Architecture-Dataflow-Co-Optimization">LP-Spec Leveraging LPDDR PIM for Efficient LLM Mobile Speculative Inference with Architecture-Dataflow Co-Optimization</a></li>
<li><a href="#DySK-Attn-A-Framework-for-Efficient,-Real-Time-Knowledge-Updating-in-Large-Language-Models-via-Dynamic-Sparse-Knowledge-Attention">DySK-Attn A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention</a></li>
<li><a href="#How-Effectively-Can-Large-Language-Models-Connect-SNP-Variants-and-ECG-Phenotypes-for-Cardiovascular-Risk-Prediction?">How Effectively Can Large Language Models Connect SNP Variants and ECG Phenotypes for Cardiovascular Risk Prediction?</a></li>
<li><a href="#From-Nodes-to-Narratives-Explaining-Graph-Neural-Networks-with-LLMs-and-Graph-Context">From Nodes to Narratives Explaining Graph Neural Networks with LLMs and Graph Context</a></li>
<li><a href="#Narrative-Memory-in-Machines-Multi-Agent-Arc-Extraction-in-Serialized-TV">Narrative Memory in Machines Multi-Agent Arc Extraction in Serialized TV</a></li>
<li><a href="#SSD-Offloading-for-LLM-Mixture-of-Experts-Weights-Considered-Harmful-in-Energy-Efficiency">SSD Offloading for LLM Mixture-of-Experts Weights Considered Harmful in Energy Efficiency</a></li>
<li><a href="#Fed-MobiLLM-Efficient-Federated-LLM-Fine-Tuning-over-Heterogeneous-Mobile-Devices-via-Server-Assisted-Side-Tuning">Fed MobiLLM Efficient Federated LLM Fine-Tuning over Heterogeneous Mobile Devices via Server Assisted Side-Tuning</a></li>
<li><a href="#SlimInfer-Accelerating-Long-Context-LLM-Inference-via-Dynamic-Token-Pruning">SlimInfer Accelerating Long-Context LLM Inference via Dynamic Token Pruning</a></li>
<li><a href="#Aligning-Effective-Tokens-with-Video-Anomaly-in-Large-Language-Models">Aligning Effective Tokens with Video Anomaly in Large Language Models</a></li>
<li><a href="#M2IO-R1-An-Efficient-RL-Enhanced-Reasoning-Framework-for-Multimodal-Retrieval-Augmented-Multimodal-Generation">M2IO-R1 An Efficient RL-Enhanced Reasoning Framework for Multimodal Retrieval Augmented Multimodal Generation</a></li>
<li><a href="#Matrix-Driven-Instant-Review-Confident-Detection-and-Reconstruction-of-LLM-Plagiarism-on-PC">Matrix-Driven Instant Review Confident Detection and Reconstruction of LLM Plagiarism on PC</a></li>
<li><a href="#KV-Cache-Compression-for-Inference-Efficiency-in-LLMs-A-Review">KV Cache Compression for Inference Efficiency in LLMs A Review</a></li>
<li><a href="#MA-CBP-A-Criminal-Behavior-Prediction-Framework-Based-on-Multi-Agent-Asynchronous-Collaboration">MA-CBP A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration</a></li>
<li><a href="#Comparing-Knowledge-Injection-Methods-for-LLMs-in-a-Low-Resource-Regime">Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime</a></li>
<li><a href="#Pragmatics-beyond-humans-meaning,-communication,-and-LLMs">Pragmatics beyond humans meaning, communication, and LLMs</a></li>
<li><a href="#LLM-Serving-Optimization-with-Variable-Prefill-and-Decode-Lengths">LLM Serving Optimization with Variable Prefill and Decode Lengths</a></li>
<li><a href="#You-Don't-Need-Pre-built-Graphs-for-RAG-Retrieval-Augmented-Generation-with-Adaptive-Reasoning-Structures">You Don't Need Pre-built Graphs for RAG Retrieval Augmented Generation with Adaptive Reasoning Structures</a></li>
<li><a href="#ConlangCrafter-Constructing-Languages-with-a-Multi-Hop-LLM-Pipeline">ConlangCrafter Constructing Languages with a Multi-Hop LLM Pipeline</a></li>
<li><a href="#Prosocial-Behavior-Detection-in-Player-Game-Chat-From-Aligning-Human-AI-Definitions-to-Efficient-Annotation-at-Scale">Prosocial Behavior Detection in Player Game Chat From Aligning Human-AI Definitions to Efficient Annotation at Scale</a></li>
</ul>
<h2 id="blindguard-safeguarding-llm-based-multi-agent-systems-under-unknown-attacks">BlindGuard Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks</h2>
<blockquote>
<p>Authors: Rui Miao, Yixin Liu, Yili Wang, Xu Shen, Yue Tan, Yiwei Dai, Shirui Pan, Xin Wang</p>
<p>2025-08-11</p>
<p>http://arxiv.org/abs/2508.08127v1</p>
</blockquote>
<p>The security of <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />-based multi-agent systems (MAS) is critically threatened
by propagation vulnerability, where malicious agents can distort collective
decision-making through inter-agent message interactions. While existing
supervised defense methods demonstrate promising performance, they may be
impractical in real-world scenarios due to their heavy reliance on labeled
malicious agents to train a supervised malicious detection model. To enable
practical and generalizable MAS defenses, in this paper, we propose BlindGuard,
an unsupervised defense method that learns without requiring any
attack-specific labels or prior knowledge of malicious behaviors. To this end,
we establish a hierarchical agent encoder to capture individual, neighborhood,
and global interaction patterns of each agent, providing a comprehensive
understanding for malicious agent detection. Meanwhile, we design a
corruption-guided detector that consists of directional noise injection and
contrastive learning, allowing effective detection model training solely on
normal agent behaviors. Extensive experiments show that BlindGuard effectively
detects diverse attack types (i.e., prompt injection, memory poisoning, and
tool attack) across MAS with various <img alt="key" src="https://img.shields.io/badge/communication-F08080" /> patterns while maintaining
superior generalizability compared to supervised baselines. The code is
available at: https://github.com/MR9812/BlindGuard.</p>
<h2 id="teammedagents-enhancing-medical-decision-making-of-llms-through-structured-teamwork">TeamMedAgents Enhancing Medical Decision-Making of LLMs Through Structured Teamwork</h2>
<blockquote>
<p>Authors: Pranav Pushkar Mishra, Mohammad Arvan, Mohan Zalake</p>
<p>2025-08-11</p>
<p>http://arxiv.org/abs/2508.08115v1</p>
</blockquote>
<p>We present TeamMedAgents, a novel multi-agent approach that systematically
integrates evidence-based teamwork components from human-human collaboration
into medical decision-making with large language models (<img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s). Our approach
validates an organizational psychology teamwork model from human collaboration
to computational multi-agent medical systems by operationalizing six core
teamwork components derived from Salas et al.'s "Big Five" model: team
leadership, mutual performance monitoring, team orientation, shared mental
models, closed-loop <img alt="key" src="https://img.shields.io/badge/communication-F08080" />, and mutual trust. We implement and evaluate
these components as modular, configurable mechanisms within an adaptive
collaboration architecture while assessing the effect of the number of agents
involved based on the task's requirements and domain. Systematic evaluation of
computational implementations of teamwork behaviors across eight medical
benchmarks (MedQA, MedMCQA, MMLU-Pro Medical, PubMedQA, DDXPlus, MedBullets,
Path-VQA, and PMC-VQA) demonstrates consistent improvements across 7 out of 8
evaluated datasets. Controlled ablation studies conducted on 50 questions per
configuration across 3 independent runs provide mechanistic insights into
individual component contributions, revealing optimal teamwork configurations
that vary by reasoning task complexity and domain-specific requirements. Our
ablation analyses reveal dataset-specific optimal teamwork configurations,
indicating that different medical reasoning modalities benefit from distinct
collaborative patterns. TeamMedAgents represents an advancement in
collaborative AI by providing a systematic translation of established teamwork
theories from human collaboration into agentic collaboration, establishing a
foundation for evidence-based multi-agent system design in critical
decision-making domains.</p>
<h2 id="chatgpt-on-the-road-leveraging-large-language-model-powered-in-vehicle-conversational-agents-for-safer-and-more-enjoyable-driving-experience">ChatGPT on the Road Leveraging Large Language Model-Powered In-vehicle Conversational Agents for Safer and More Enjoyable Driving Experience</h2>
<blockquote>
<p>Authors: Yeana Lee Bond, Mungyeong Choe, Baker Kasim Hasan, Arsh Siddiqui, Myounghoon Jeon</p>
<p>2025-08-11</p>
<p>http://arxiv.org/abs/2508.08101v1</p>
</blockquote>
<p>Studies on in-vehicle conversational agents have traditionally relied on
pre-scripted prompts or limited voice commands, constraining natural
driver-agent interaction. To resolve this issue, the present study explored the
potential of a ChatGPT-based in-vehicle agent capable of carrying continuous,
multi-turn dialogues. Forty drivers participated in our experiment using a
motion-based driving simulator, comparing three conditions (No agent,
Pre-scripted agent, and ChatGPT-based agent) as a within-subjects variable.
Results showed that the ChatGPT-based agent condition led to more stable
driving performance across multiple metrics. Participants demonstrated lower
variability in longitudinal <img alt="key" src="https://img.shields.io/badge/acceleration-F08080" />, lateral <img alt="key" src="https://img.shields.io/badge/acceleration-F08080" />, and lane
deviation compared to the other two conditions. In subjective evaluations, the
ChatGPT-based agent also received significantly higher ratings in competence,
animacy, affective trust, and preference compared to the Pre-scripted agent.
Our thematic analysis of driver-agent conversations revealed diverse
interaction patterns in topics, including driving assistance/questions,
entertainment requests, and anthropomorphic interactions. Our results highlight
the potential of <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />-powered in-vehicle conversational agents to enhance
driving safety and user experience through natural, context-rich interactions.</p>
<h2 id="interpreting-fedspeak-with-confidence-a-llm-based-uncertainty-aware-framework-guided-by-monetary-policy-transmission-paths">Interpreting Fedspeak with Confidence A LLM-Based Uncertainty-Aware Framework Guided by Monetary Policy Transmission Paths</h2>
<blockquote>
<p>Authors: Rui Yao, Qi Chai, Jinhai Yao, Siyuan Li, Junhao Chen, Qi Zhang, Hao Wang</p>
<p>2025-08-11</p>
<p>http://arxiv.org/abs/2508.08001v1</p>
</blockquote>
<p>"Fedspeak", the stylized and often nuanced language used by the U.S. Federal
Reserve, encodes implicit policy signals and strategic stances. The Federal
Open Market Committee strategically employs Fedspeak as a <img alt="key" src="https://img.shields.io/badge/communication-F08080" /> tool to
shape market expectations and influence both domestic and global economic
conditions. As such, automatically parsing and interpreting Fedspeak presents a
high-impact challenge, with significant implications for financial forecasting,
algorithmic trading, and data-driven policy analysis. In this paper, we propose
an <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />-based, uncertainty-aware framework for deciphering Fedspeak and
classifying its underlying monetary policy stance. Technically, to enrich the
semantic and contextual representation of Fedspeak texts, we incorporate
domain-specific reasoning grounded in the monetary policy transmission
mechanism. We further introduce a dynamic uncertainty decoding module to assess
the confidence of model predictions, thereby enhancing both classification
accuracy and model reliability. Experimental results demonstrate that our
framework achieves state-of-the-art performance on the policy stance analysis
task. Moreover, statistical analysis reveals a significant positive correlation
between perceptual uncertainty and model error rates, validating the
effectiveness of perceptual uncertainty as a diagnostic signal.</p>
<h2 id="evocot-overcoming-the-exploration-bottleneck-in-reinforcement-learning">EvoCoT Overcoming the Exploration Bottleneck in Reinforcement Learning</h2>
<blockquote>
<p>Authors: Huanyu Liu, Jia Li, Chang Yu, Taozhi Chen, Yihong Dong, Lecheng Wang, Hu XiaoLong, Ge Li</p>
<p>2025-08-11</p>
<p>http://arxiv.org/abs/2508.07809v1</p>
</blockquote>
<p>Reinforcement learning with verifiable reward (RLVR) has become a promising
paradigm for post-training large language models (<img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s) to improve their
reasoning capability. However, when the rollout accuracy is low on hard
problems, the reward becomes <img alt="key" src="https://img.shields.io/badge/sparse-F08080" />, limiting learning efficiency and causing
exploration bottlenecks. Existing approaches either rely on stronger <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s for
distillation or filter out difficult problems, which limits scalability or
restricts reasoning improvement through exploration.
  We propose EvoCoT, a self-evolving curriculum learning framework based on
two-stage chain-of-thought (CoT) reasoning optimization. EvoCoT constrains the
exploration space by self-generating and verifying CoT trajectories, then
gradually shortens them to expand the space in a controlled way. This enables
<img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s to stably learn from initially unsolved hard problems under <img alt="key" src="https://img.shields.io/badge/sparse-F08080" />
rewards. We apply EvoCoT to multiple <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /> families, including Qwen, DeepSeek,
and Llama. Experiments show that EvoCoT enables <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s to solve previously
unsolved problems, improves reasoning capability without external CoT
supervision, and is compatible with various RL fine-tuning methods. We release
the source code to support future research.</p>
<h2 id="grove-moe-towards-efficient-and-superior-moe-llms-with-adjugate-experts">Grove MoE Towards Efficient and Superior MoE LLMs with Adjugate Experts</h2>
<blockquote>
<p>Authors: Haoyuan Wu, Haoxing Chen, Xiaodong Chen, Zhanchao Zhou, Tieyuan Chen, Yihong Zhuang, Guoshan Lu, Zenan Huang, Junbo Zhao, Lin Liu, Zhenzhong Lan, Bei Yu, Jianguo Li</p>
<p>2025-08-11</p>
<p>http://arxiv.org/abs/2508.07785v1</p>
</blockquote>
<p>The Mixture of Experts (MoE) architecture is a cornerstone of modern
state-of-the-art (SOTA) large language models (<img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s). MoE models facilitate
scalability by enabling <img alt="key" src="https://img.shields.io/badge/sparse-F08080" /> parameter activation. However, traditional MoE
architecture uses homogeneous experts of a uniform size, activating a fixed
number of parameters irrespective of input complexity and thus limiting
computational efficiency. To overcome this limitation, we introduce Grove MoE,
a novel architecture incorporating experts of varying sizes, inspired by the
heterogeneous big.LITTLE CPU architecture. This architecture features novel
adjugate experts with a dynamic activation mechanism, enabling model capacity
expansion while maintaining manageable computational overhead. Building on this
architecture, we present GroveMoE-Base and GroveMoE-Inst, 33B-parameter <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s
developed by applying an upcycling strategy to the Qwen3-30B-A3B-Base model
during mid-training and post-training. GroveMoE models dynamically activate
3.14-3.28B parameters based on token complexity and achieve performance
comparable to SOTA open-source models of similar or even larger size.</p>
<h2 id="gliclass-generalist-lightweight-model-for-sequence-classification-tasks">GLiClass Generalist Lightweight Model for Sequence Classification Tasks</h2>
<blockquote>
<p>Authors: Ihor Stepanov, Mykhailo Shtopko, Dmytro Vodianytskyi, Oleksandr Lukashov, Alexander Yavorskyi, Mykyta Yaroshenko</p>
<p>2025-08-11</p>
<p>http://arxiv.org/abs/2508.07662v1</p>
</blockquote>
<p>Classification is one of the most widespread tasks in AI applications,
serving often as the first step in filtering, sorting, and categorizing data.
Since modern AI systems must handle large volumes of input data and early
pipeline stages can propagate errors downstream, achieving high efficiency and
accuracy is critical. Moreover, classification requirements can change
dynamically based on user needs, necessitating models with strong zero-shot
capabilities. While generative <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s have become mainstream for zero-shot
classification due to their versatility, they suffer from inconsistent
instruction following and computational inefficiency. Cross-encoders, commonly
used as rerankers in RAG pipelines, face a different bottleneck: they must
process text-label pairs sequentially, significantly reducing efficiency with
large label sets. Embedding-based approaches offer good efficiency but struggle
with complex scenarios involving logical and semantic constraints. We propose
GLiClass, a novel method that adapts the GLiNER architecture for sequence
classification tasks. Our approach achieves strong accuracy and efficiency
comparable to embedding-based methods, while maintaining the flexibility needed
for zero-shot and few-shot learning scenarios. Additionally, we adapted
proximal policy optimization (PPO) for multi-label text classification,
enabling training classifiers in data-<img alt="key" src="https://img.shields.io/badge/sparse-F08080" /> conditions or from human feedback.</p>
<h2 id="hgmf-a-hierarchical-gaussian-mixture-framework-for-scalable-tool-invocation-within-the-model-context-protocol">HGMF A Hierarchical Gaussian Mixture Framework for Scalable Tool Invocation within the Model Context Protocol</h2>
<blockquote>
<p>Authors: Wenpeng Xing, Zhipeng Chen, Changting Lin, Meng Han</p>
<p>2025-08-11</p>
<p>http://arxiv.org/abs/2508.07602v1</p>
</blockquote>
<p>Invoking external tools enables Large Language Models (<img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s) to perform
complex, real-world tasks, yet selecting the correct tool from large,
hierarchically-structured libraries remains a significant challenge. The
limited context windows of <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s and noise from irrelevant options often lead to
low selection accuracy and high computational costs. To address this, we
propose the Hierarchical Gaussian Mixture Framework (HGMF), a probabilistic
<img alt="key" src="https://img.shields.io/badge/pruning-F08080" /> method for scalable tool invocation. HGMF first maps the user query and
all tool descriptions into a unified semantic space. The framework then
operates in two stages: it clusters servers using a Gaussian Mixture Model
(GMM) and filters them based on the query's likelihood. Subsequently, it
applies the same GMM-based clustering and filtering to the tools associated
with the selected servers. This hierarchical process produces a compact,
high-relevance candidate set, simplifying the final selection task for the <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />.
Experiments on a public dataset show that HGMF significantly improves tool
selection accuracy while reducing inference latency, confirming the framework's
scalability and effectiveness for large-scale tool libraries.</p>
<h2 id="grounding-natural-language-for-multi-agent-decision-making-with-multi-agentic-llms">Grounding Natural Language for Multi-agent Decision-Making with Multi-agentic LLMs</h2>
<blockquote>
<p>Authors: Dom Huh, Prasant Mohapatra</p>
<p>2025-08-10</p>
<p>http://arxiv.org/abs/2508.07466v1</p>
</blockquote>
<p>Language is a ubiquitous tool that is foundational to reasoning and
collaboration, ranging from everyday interactions to sophisticated
problem-solving tasks. The establishment of a common language can serve as a
powerful asset in ensuring clear <img alt="key" src="https://img.shields.io/badge/communication-F08080" /> and understanding amongst
agents, facilitating desired coordination and strategies. In this work, we
extend the capabilities of large language models (<img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s) by integrating them
with advancements in multi-agent decision-making algorithms. We propose a
systematic framework for the design of multi-agentic large language models
(<img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s), focusing on key integration practices. These include advanced prompt
engineering techniques, the development of effective memory architectures,
multi-modal information processing, and alignment strategies through
fine-tuning algorithms. We evaluate these design choices through extensive
ablation studies on classic game settings with significant underlying social
dilemmas and game-theoretic considerations.</p>
<h2 id="let-us-long-event-text-understanding-of-scenes">LET-US Long Event-Text Understanding of Scenes</h2>
<blockquote>
<p>Authors: Rui Chen, Xingyu Chen, Shaoan Wang, Shihan Kong, Junzhi Yu</p>
<p>2025-08-10</p>
<p>http://arxiv.org/abs/2508.07401v1</p>
</blockquote>
<p>Event cameras output event streams as <img alt="key" src="https://img.shields.io/badge/sparse-F08080" />, asynchronous data with
microsecond-level temporal resolution, enabling visual perception with low
latency and a high dynamic range. While existing Multimodal Large Language
Models (M<img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s) have achieved significant success in understanding and analyzing
RGB video content, they either fail to interpret event streams effectively or
remain constrained to very short sequences. In this paper, we introduce LET-US,
a framework for long event-stream--text comprehension that employs an adaptive
compression mechanism to reduce the volume of input events while preserving
critical visual details. LET-US thus establishes a new frontier in cross-modal
inferential understanding over extended event sequences. To bridge the
substantial modality gap between event streams and textual representations, we
adopt a two-stage optimization paradigm that progressively equips our model
with the capacity to interpret event-based scenes. To handle the voluminous
temporal information inherent in long event streams, we leverage text-guided
cross-modal queries for feature reduction, augmented by hierarchical clustering
and similarity computation to distill the most representative event features.
Moreover, we curate and construct a large-scale event-text aligned dataset to
train our model, achieving tighter alignment of event features within the <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />
embedding space. We also develop a comprehensive benchmark covering a diverse
set of tasks -- reasoning, captioning, classification, temporal localization
and moment retrieval. Experimental results demonstrate that LET-US outperforms
prior state-of-the-art M<img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s in both descriptive accuracy and semantic
comprehension on long-duration event streams. All datasets, codes, and models
will be publicly available.</p>
<h2 id="efficient-edge-llms-deployment-via-hessianaware-quantization-and-cpu-gpu-collaborative">Efficient Edge LLMs Deployment via HessianAware Quantization and CPU GPU Collaborative</h2>
<blockquote>
<p>Authors: Tuo Zhang, Ning Li, Xin Yuan, Wenchao Xu, Quan Chen, Song Guo, Haijun Zhang</p>
<p>2025-08-10</p>
<p>http://arxiv.org/abs/2508.07329v1</p>
</blockquote>
<p>With the breakthrough progress of large language models (<img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s) in natural
language processing and multimodal tasks, efficiently deploying them on
resource-constrained edge devices has become a critical challenge. The Mixture
of Experts (MoE) architecture enhances model capacity through <img alt="key" src="https://img.shields.io/badge/sparse-F08080" />
activation, but faces two major difficulties in practical deployment: (1) The
presence of numerous outliers in activation distributions leads to severe
degradation in quantization accuracy for both activations and weights,
significantly impairing inference performance; (2) Under limited memory,
efficient offloading and collaborative inference of expert modules struggle to
balance latency and throughput. To address these issues, this paper proposes an
efficient MoE edge deployment scheme based on Hessian-Aware Quantization (HAQ)
and CPU-GPU collaborative inference. First, by introducing smoothed Hessian
matrix quantization, we achieve joint 8-bit quantization of activations and
weights, which significantly alleviates the accuracy loss caused by outliers
while ensuring efficient implementation on mainstream hardware. Second, we
design an expert-level collaborative offloading and inference mechanism, which,
combined with expert activation path statistics, enables efficient deployment
and scheduling of expert modules between CPU and GPU, greatly reducing memory
footprint and inference latency. Extensive experiments validate the
effectiveness of our method on mainstream large models such as the OPT series
and Mixtral 8*7B: on datasets like Wikitext2 and C4, the inference accuracy of
the low-bit quantized model approaches that of the full-precision model, while
GPU memory usage is reduced by about 60%, and inference latency is
significantly improved.</p>
<h2 id="bevanet-bilateral-efficient-visual-attention-network-for-real-time-semantic-segmentation">BEVANet Bilateral Efficient Visual Attention Network for Real-Time Semantic Segmentation</h2>
<blockquote>
<p>Authors: Ping-Mao Huang, I-Tien Chao, Ping-Chia Huang, Jia-Wei Liao, Yung-Yu Chuang</p>
<p>2025-08-10</p>
<p>http://arxiv.org/abs/2508.07300v1</p>
</blockquote>
<p>Real-time semantic segmentation presents the dual challenge of designing
efficient architectures that capture large receptive fields for semantic
understanding while also refining detailed contours. Vision <img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" />s model
long-range dependencies effectively but incur high computational cost. To
address these challenges, we introduce the Large Kernel Attention (LKA)
mechanism. Our proposed Bilateral Efficient Visual Attention Network (BEVANet)
expands the receptive field to capture contextual information and extracts
visual and structural features using Sparse Decomposed Large Separable Kernel
Attentions (SDLSKA). The Comprehensive Kernel Selection (CKS) mechanism
dynamically adapts the receptive field to further enhance performance.
Furthermore, the Deep Large Kernel Pyramid Pooling Module (DLKPPM) enriches
contextual features by synergistically combining dilated convolutions and large
kernel attention. The bilateral architecture facilitates frequent branch
<img alt="key" src="https://img.shields.io/badge/communication-F08080" />, and the Boundary Guided Adaptive Fusion (BGAF) module enhances
boundary delineation by integrating spatial and semantic features under
boundary guidance. BEVANet achieves real-time segmentation at 33 FPS, yielding
79.3% mIoU without pretraining and 81.0% mIoU on Cityscapes after ImageNet
pretraining, demonstrating state-of-the-art performance. The code and model is
available at https://github.com/maomao0819/BEVANet.</p>
<h2 id="lp-spec-leveraging-lpddr-pim-for-efficient-llm-mobile-speculative-inference-with-architecture-dataflow-co-optimization">LP-Spec Leveraging LPDDR PIM for Efficient LLM Mobile Speculative Inference with Architecture-Dataflow Co-Optimization</h2>
<blockquote>
<p>Authors: Siyuan He, Zhantong Zhu, Yandong He, Tianyu Jia</p>
<p>2025-08-10</p>
<p>http://arxiv.org/abs/2508.07227v1</p>
</blockquote>
<p><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /> inference on mobile devices faces extraneous challenges due to limited
memory bandwidth and computational resources. To address these issues,
speculative inference and processing-in-memory (PIM) techniques have been
explored at the algorithmic and hardware levels. However, speculative inference
results in more compute-intensive GEMM operations, creating new design
trade-offs for existing GEMV-accelerated PIM architectures. Furthermore, there
exists a significant amount of redundant draft tokens in tree-based speculative
inference, necessitating efficient token management schemes to minimize energy
consumption. In this work, we present LP-Spec, an architecture-dataflow
co-design leveraging hybrid LPDDR5 performance-enhanced PIM architecture with
draft token <img alt="key" src="https://img.shields.io/badge/pruning-F08080" /> and dynamic workload scheduling to accelerate <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />
speculative inference. A near-data memory controller is proposed to enable data
reallocation between DRAM and PIM banks. Furthermore, a data allocation unit
based on the hardware-aware draft token pruner is developed to minimize energy
consumption and fully exploit parallel execution opportunities. Compared to
end-to-end <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /> inference on other mobile solutions such as mobile NPUs or
GEMV-accelerated PIMs, our LP-Spec achieves 13.21x, 7.56x, and 99.87x
improvements in performance, energy efficiency, and energy-delay-product (EDP).
Compared with prior AttAcc PIM and RTX 3090 GPU, LP-Spec can obtain 12.83x and
415.31x EDP reduction benefits.</p>
<h2 id="dysk-attn-a-framework-for-efficient-real-time-knowledge-updating-in-large-language-models-via-dynamic-sparse-knowledge-attention">DySK-Attn A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention</h2>
<blockquote>
<p>Authors: Kabir Khan, Priya Sharma, Arjun Mehta, Neha Gupta, Ravi Narayanan</p>
<p>2025-08-10</p>
<p>http://arxiv.org/abs/2508.07185v1</p>
</blockquote>
<p>Large Language Models (<img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s) suffer from a critical limitation: their
knowledge is static and quickly becomes outdated. Retraining these massive
models is computationally prohibitive, while existing knowledge editing
techniques can be slow and may introduce unforeseen side effects. To address
this, we propose DySK-Attn, a novel framework that enables <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s to efficiently
integrate real-time knowledge from a dynamic external source. Our approach
synergizes an <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /> with a dynamic Knowledge Graph (KG) that can be updated
instantaneously. The core of our framework is a <img alt="key" src="https://img.shields.io/badge/sparse-F08080" /> knowledge attention
mechanism, which allows the <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /> to perform a coarse-to-fine grained search,
efficiently identifying and focusing on a small, highly relevant subset of
facts from the vast KG. This mechanism avoids the high computational cost of
dense attention over the entire knowledge base and mitigates noise from
irrelevant information. We demonstrate through extensive experiments on
time-sensitive question-answering tasks that DySK-Attn significantly
outperforms strong baselines, including standard Retrieval-Augmented Generation
(RAG) and model editing techniques, in both factual accuracy for updated
knowledge and computational efficiency. Our framework offers a scalable and
effective solution for building <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s that can stay current with the
ever-changing world.</p>
<h2 id="how-effectively-can-large-language-models-connect-snp-variants-and-ecg-phenotypes-for-cardiovascular-risk-prediction">How Effectively Can Large Language Models Connect SNP Variants and ECG Phenotypes for Cardiovascular Risk Prediction?</h2>
<blockquote>
<p>Authors: Niranjana Arun Menon, Iqra Farooq, Yulong Li, Sara Ahmed, Yutong Xie, Muhammad Awais, Imran Razzak</p>
<p>2025-08-10</p>
<p>http://arxiv.org/abs/2508.07127v1</p>
</blockquote>
<p>Cardiovascular disease (CVD) prediction remains a tremendous challenge due to
its multifactorial etiology and global burden of morbidity and mortality.
Despite the growing availability of genomic and electrophysiological data,
extracting biologically meaningful insights from such high-dimensional, noisy,
and <img alt="key" src="https://img.shields.io/badge/sparse-F08080" />ly annotated datasets remains a non-trivial task. Recently, <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s has
been applied effectively to predict structural variations in biological
sequences. In this work, we explore the potential of fine-tuned <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s to predict
cardiac diseases and SNPs potentially leading to CVD risk using genetic markers
derived from high-throughput genomic profiling. We investigate the effect of
genetic patterns associated with cardiac conditions and evaluate how <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s can
learn latent biological relationships from structured and semi-structured
genomic data obtained by mapping genetic aspects that are inherited from the
family tree. By framing the problem as a Chain of Thought (CoT) reasoning task,
the models are prompted to generate disease labels and articulate informed
clinical deductions across diverse patient profiles and phenotypes. The
findings highlight the promise of <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s in contributing to early detection, risk
assessment, and ultimately, the advancement of personalized medicine in cardiac
care.</p>
<h2 id="from-nodes-to-narratives-explaining-graph-neural-networks-with-llms-and-graph-context">From Nodes to Narratives Explaining Graph Neural Networks with LLMs and Graph Context</h2>
<blockquote>
<p>Authors: Peyman Baghershahi, Gregoire Fournier, Pranav Nyati, Sourav Medya</p>
<p>2025-08-09</p>
<p>http://arxiv.org/abs/2508.07117v1</p>
</blockquote>
<p>Graph Neural Networks (GNNs) have emerged as powerful tools for learning over
structured data, including text-attributed graphs, which are common in domains
such as citation networks, social platforms, and knowledge graphs. GNNs are not
inherently interpretable and thus, many explanation methods have been proposed.
However, existing explanation methods often struggle to generate interpretable,
fine-grained rationales, especially when node attributes include rich natural
language. In this work, we introduce LOGIC, a lightweight, post-hoc framework
that uses large language models (<img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s) to generate faithful and interpretable
explanations for GNN predictions. LOGIC projects GNN node embeddings into the
<img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /> embedding space and constructs hybrid prompts that interleave soft prompts
with textual inputs from the graph structure. This enables the <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /> to reason
about GNN internal representations and produce natural language explanations
along with concise explanation subgraphs. Our experiments across four
real-world TAG datasets demonstrate that LOGIC achieves a favorable trade-off
between fidelity and <img alt="key" src="https://img.shields.io/badge/sparsity-F08080" />, while significantly improving human-centric
metrics such as insightfulness. LOGIC sets a new direction for <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />-based
explainability in graph learning by aligning GNN internals with human
reasoning.</p>
<h2 id="narrative-memory-in-machines-multi-agent-arc-extraction-in-serialized-tv">Narrative Memory in Machines Multi-Agent Arc Extraction in Serialized TV</h2>
<blockquote>
<p>Authors: Roberto Balestri, Guglielmo Pescatore</p>
<p>2025-08-09</p>
<p>http://arxiv.org/abs/2508.07010v1</p>
</blockquote>
<p>Serialized television narratives present significant analytical challenges
due to their complex, temporally distributed storylines that necessitate
sophisticated information management. This paper introduces a multi-agent
system (MAS) designed to extract and analyze narrative arcs by implementing
principles of computational memory architectures. The system conceptualizes
narrative understanding through analogues of human memory: Large Language
Models (<img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s) provide a form of semantic memory for general narrative patterns,
while a vector database stores specific arc progressions as episodic memories.
A multi-agent workflow simulates working memory processes to integrate these
information types. Tested on the first season of Grey's Anatomy (ABC 2005-),
the MAS identifies three arc types: Anthology (self-contained), Soap
(relationship-focused), and Genre-Specific. These arcs and their episodic
developments are stored in a vector database, facilitating structured analysis
and semantic comparison. To bridge automation with critical interpretation, a
graphical interface enables human oversight and refinement of the system's
narrative memory. While demonstrating strong performance in identifying
Anthology Arcs and character entities, the system's reliance on textual
paratexts (episode summaries) revealed limitations in discerning <img alt="key" src="https://img.shields.io/badge/overlap-F08080" />ping
arcs and opaque dynamics, underscoring the challenges in computational memory
consolidation versus human holistic understanding. This memory-centric approach
highlights the potential of combining AI-driven memory processing with human
expertise. Beyond television, it offers promise for serialized written formats
where narrative is entirely text-based. Future work will focus on integrating
multimodal inputs to enrich episodic memory, refining memory integration
mechanisms within the MAS, and expanding testing across diverse genres.</p>
<h2 id="ssd-offloading-for-llm-mixture-of-experts-weights-considered-harmful-in-energy-efficiency">SSD Offloading for LLM Mixture-of-Experts Weights Considered Harmful in Energy Efficiency</h2>
<blockquote>
<p>Authors: Kwanhee Kyung, Sungmin Yun, Jung Ho Ahn</p>
<p>2025-08-09</p>
<p>http://arxiv.org/abs/2508.06978v1</p>
</blockquote>
<p>Large Language Models (<img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s) applying Mixture-of-Experts (MoE) scale to
trillions of parameters but require vast memory, motivating a line of research
to offload expert weights from fast-but-small DRAM (HBM) to denser Flash SSDs.
While SSDs provide cost-effective capacity, their read energy per bit is
substantially higher than that of DRAM. This paper quantitatively analyzes the
energy implications of offloading MoE expert weights to SSDs during the
critical decode stage of <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /> inference. Our analysis, comparing SSD, CPU memory
(DDR), and HBM storage scenarios for models like DeepSeek-R1, reveals that
offloading MoE weights to current SSDs drastically increases
per-token-generation energy consumption (e.g., by up to ~12x compared to the
HBM baseline), dominating the total inference energy budget. Although
techniques like prefetching effectively hide access latency, they cannot
mitigate this fundamental energy penalty. We further explore future
technological scaling, finding that the inherent <img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /> of MoE models could
potentially make SSDs energy-viable if Flash read energy improves
significantly, roughly by an order of magnitude.</p>
<h2 id="fed-mobillm-efficient-federated-llm-fine-tuning-over-heterogeneous-mobile-devices-via-server-assisted-side-tuning">Fed MobiLLM Efficient Federated LLM Fine-Tuning over Heterogeneous Mobile Devices via Server Assisted Side-Tuning</h2>
<blockquote>
<p>Authors: Xingke Yang, Liang Li, Sicong Li, Liwei Guan, Hao Wang, Xiaoqi Qi, Jiang Liu, Xin Fu, Miao Pan</p>
<p>2025-08-09</p>
<p>http://arxiv.org/abs/2508.06765v1</p>
</blockquote>
<p>Collaboratively fine-tuning (FT) large language models (<img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s) over
heterogeneous mobile devices fosters immense potential applications of
personalized intelligence. However, such a vision faces critical system
challenges. Conventional federated <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /> FT approaches place prohibitive
computational and memory burdens on mobile hardware, and their synchronous
model aggregation protocols stall for slower devices. In this paper, we propose
Fed Mobi<img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />, a novel design to facilitate efficient federated <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /> FT across
mobile devices with diverse computing/<img alt="key" src="https://img.shields.io/badge/communication-F08080" /> speeds and local model
architectures. In particular, Fed Mobi<img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /> implements a pioneering
server-assisted federated side-tuning paradigm. Briefly, mobile devices perform
lightweight forward propagation computations on local data using their frozen
pre-scaled backbone <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s, and then upload selected intermediate activations.
The server trains a shared side-network independently, eliminating client-side
backpropagation and enabling asynchronous updates. To bridge model
heterogeneity across different devices, we introduce an adaptive layer-wise
feature alignment method, which ensures consistent representations for
collaboratively tuning a shared side network. Extensive experimental results
demonstrate that Fed Mobi<img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /> can maintain robust fine-tuning performance while
achieving extremely low on-device memory, with at least 95.2% reduction in
computation overhead, 93.2% reduction in <img alt="key" src="https://img.shields.io/badge/communication-F08080" /> costs and 5.1x faster
convergence compared to existing methods, validating its efficacy for practical
<img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /> adaptation over heterogeneous mobile devices.</p>
<h2 id="sliminfer-accelerating-long-context-llm-inference-via-dynamic-token-pruning">SlimInfer Accelerating Long-Context LLM Inference via Dynamic Token Pruning</h2>
<blockquote>
<p>Authors: Lingkun Long, Rubing Yang, Yushi Huang, Desheng Hui, Ao Zhou, Jianlei Yang</p>
<p>2025-08-08</p>
<p>http://arxiv.org/abs/2508.06447v1</p>
</blockquote>
<p>Long-context inference for Large Language Models (<img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s) is heavily limited by
high computational demands. While several existing methods optimize attention
computation, they still process the full set of hidden states at each layer,
limiting overall efficiency. In this work, we propose SlimInfer, an innovative
framework that aims to accelerate inference by directly <img alt="key" src="https://img.shields.io/badge/pruning-F08080" /> less critical
prompt tokens during the forward pass. Our key insight is an information
diffusion phenomenon: As information from critical tokens propagates through
layers, it becomes distributed across the entire sequence. This diffusion
process suggests that <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s can maintain their semantic integrity when excessive
tokens, even including these critical ones, are pruned in hidden states.
Motivated by this, SlimInfer introduces a dynamic fine-grained <img alt="key" src="https://img.shields.io/badge/pruning-F08080" />
mechanism that accurately removes redundant tokens of hidden state at
intermediate layers. This layer-wise <img alt="key" src="https://img.shields.io/badge/pruning-F08080" /> naturally enables an asynchronous
<img alt="key" src="https://img.shields.io/badge/KV-F08080" /> cache manager that prefetches required token blocks without complex
predictors, reducing both memory usage and I/O costs. Extensive experiments
show that SlimInfer can achieve up to $\mathbf{2.53\times}$ time-to-first-token
(TTFT) speedup and $\mathbf{1.88\times}$ end-to-end latency reduction for
LLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on
LongBench. Our code will be released upon acceptance.</p>
<h2 id="aligning-effective-tokens-with-video-anomaly-in-large-language-models">Aligning Effective Tokens with Video Anomaly in Large Language Models</h2>
<blockquote>
<p>Authors: Yingxian Chen, Jiahui Liu, Ruifan Di, Yanwei Li, Chirui Chang, Shizhen Zhao, Wilton W. T. Fok, Xiaojuan Qi, Yik-Chung Wu</p>
<p>2025-08-08</p>
<p>http://arxiv.org/abs/2508.06350v1</p>
</blockquote>
<p>Understanding abnormal events in videos is a vital and challenging task that
has garnered significant attention in a wide range of applications. Although
current video understanding Multi-modal Large Language Models (M<img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s) are
capable of analyzing general videos, they often struggle to handle anomalies
due to the spatial and temporal <img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /> of abnormal events, where the
redundant information always leads to suboptimal outcomes. To address these
challenges, exploiting the representation and generalization capabilities of
Vison Language Models (VLMs) and Large Language Models (<img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s), we propose
VA-GPT, a novel M<img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /> designed for summarizing and localizing abnormal events in
various videos. Our approach efficiently aligns effective tokens between visual
encoders and <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s through two key proposed modules: Spatial Effective Token
Selection (SETS) and Temporal Effective Token Generation (TETG). These modules
enable our model to effectively capture and analyze both spatial and temporal
information associated with abnormal events, resulting in more accurate
responses and interactions. Furthermore, we construct an instruction-following
dataset specifically for fine-tuning video-anomaly-aware M<img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s, and introduce a
cross-domain evaluation benchmark based on XD-Violence dataset. Our proposed
method outperforms existing state-of-the-art methods on various benchmarks.</p>
<h2 id="m2io-r1-an-efficient-rl-enhanced-reasoning-framework-for-multimodal-retrieval-augmented-multimodal-generation">M2IO-R1 An Efficient RL-Enhanced Reasoning Framework for Multimodal Retrieval Augmented Multimodal Generation</h2>
<blockquote>
<p>Authors: Zhiyou Xiao, Qinhan Yu, Binghui Li, Geng Chen, Chong Chen, Wentao Zhang</p>
<p>2025-08-08</p>
<p>http://arxiv.org/abs/2508.06328v1</p>
</blockquote>
<p>Current research on Multimodal Retrieval-Augmented Generation (MRAG) enables
diverse multimodal inputs but remains limited to single-modality outputs,
restricting expressive capacity and practical utility. In contrast, real-world
applications often demand both multimodal inputs and multimodal outputs for
effective <img alt="key" src="https://img.shields.io/badge/communication-F08080" /> and grounded reasoning. Motivated by the recent success
of Reinforcement Learning (RL) in complex reasoning tasks for Large Language
Models (<img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s), we adopt RL as a principled and effective paradigm to address
the multi-step, outcome-driven challenges inherent in multimodal output
generation. Here, we introduce M2IO-R1, a novel framework for Multimodal
Retrieval-Augmented Multimodal Generation (MRAMG) that supports both multimodal
inputs and outputs. Central to our framework is an RL-based inserter,
Inserter-R1-3B, trained with Group Relative Policy Optimization to guide image
selection and placement in a controllable and semantically aligned manner.
Empirical results show that our lightweight 3B inserter achieves strong
reasoning capabilities with significantly reduced latency, outperforming
baselines in both quality and efficiency.</p>
<h2 id="matrix-driven-instant-review-confident-detection-and-reconstruction-of-llm-plagiarism-on-pc">Matrix-Driven Instant Review Confident Detection and Reconstruction of LLM Plagiarism on PC</h2>
<blockquote>
<p>Authors: Ruichong Zhang</p>
<p>2025-08-08</p>
<p>http://arxiv.org/abs/2508.06309v1</p>
</blockquote>
<p>In recent years, concerns about intellectual property (IP) in large language
models (<img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s) have grown significantly. Plagiarizing other <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s (through direct
weight copying, upcycling, <img alt="key" src="https://img.shields.io/badge/pruning-F08080" />, or continual pretraining) and claiming
authorship without properly attributing to the original license, is a serious
misconduct that can lead to significant financial and reputational harm to the
original developers. However, existing methods for detecting <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /> plagiarism
fall short in key areas. They fail to accurately reconstruct weight
correspondences, lack the ability to compute statistical significance measures
such as $p$-values, and may mistakenly flag models trained on similar data as
being related. To address these limitations, we propose Matrix-Driven Instant
Review (MDIR), a novel method that leverages matrix analysis and Large
Deviation Theory. MDIR achieves accurate reconstruction of weight
relationships, provides rigorous $p$-value estimation, and focuses exclusively
on weight similarity without requiring full model inference. Experimental
results demonstrate that MDIR reliably detects plagiarism even after extensive
transformations, such as random permutations and continual pretraining with
trillions of tokens. Moreover, all detections can be performed on a single PC
within an hour, making MDIR both efficient and accessible.</p>
<h2 id="kv-cache-compression-for-inference-efficiency-in-llms-a-review">KV Cache Compression for Inference Efficiency in LLMs A Review</h2>
<blockquote>
<p>Authors: Yanyu Liu, Jingying Fu, Sixiang Liu, Yitian Zou, You Fu, Jiehan Zhou, Shouhua Zhang</p>
<p>2025-08-08</p>
<p>http://arxiv.org/abs/2508.06297v1</p>
</blockquote>
<p>Withtherapid advancement of large language models (<img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s), the context length
for inference has been continuously increasing, leading to an exponential
growth in the demand for Key-Value (<img alt="key" src="https://img.shields.io/badge/KV-F08080" />) caching. This has resulted in a
significant memory bottleneck, limiting the inference efficiency and
scalability of the models. Therefore, optimizing the <img alt="key" src="https://img.shields.io/badge/KV-F08080" /> cache during inference
is crucial for enhancing performance and efficiency. This review systematically
examines current <img alt="key" src="https://img.shields.io/badge/KV-F08080" /> cache optimization techniques, including compression
strategies such as selective token strategies, quantization, and attention
compression. We evaluate the effectiveness, trade-offs, and application
scenarios of these methods, providing a comprehensive analysis of their impact
on memory usage and inference speed. We focus on identifying the limitations
and challenges of existing methods, such as compatibility issues with different
models and tasks. Additionally, this review highlights future research
directions, including hybrid optimization techniques, adaptive dynamic
strategies, and software-hardware co-design. These approaches aim to improve
inference efficiency and promote the practical application of large language
models.</p>
<h2 id="ma-cbp-a-criminal-behavior-prediction-framework-based-on-multi-agent-asynchronous-collaboration">MA-CBP A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration</h2>
<blockquote>
<p>Authors: Cheng Liu, Daou Zhang, Tingxu Liu, Yuhan Wang, Jinyang Chen, Yuexuan Li, Xinying Xiao, Chenbo Xin, Ziru Wang, Weichao Wu</p>
<p>2025-08-08</p>
<p>http://arxiv.org/abs/2508.06189v1</p>
</blockquote>
<p>With the <img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /> of urbanization, criminal behavior in public scenes
poses an increasingly serious threat to social security. Traditional anomaly
detection methods based on feature recognition struggle to capture high-level
behavioral semantics from historical information, while generative approaches
based on Large Language Models (<img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s) often fail to meet real-time
requirements. To address these challenges, we propose MA-CBP, a criminal
behavior prediction framework based on multi-agent asynchronous collaboration.
This framework transforms real-time video streams into frame-level semantic
descriptions, constructs causally consistent historical summaries, and fuses
adjacent image frames to perform joint reasoning over long- and short-term
contexts. The resulting behavioral decisions include key elements such as event
subjects, locations, and causes, enabling early warning of potential criminal
activity. In addition, we construct a high-quality criminal behavior dataset
that provides multi-scale language supervision, including frame-level,
summary-level, and event-level semantic annotations. Experimental results
demonstrate that our method achieves superior performance on multiple datasets
and offers a promising solution for risk warning in urban public safety
scenarios.</p>
<h2 id="comparing-knowledge-injection-methods-for-llms-in-a-low-resource-regime">Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime</h2>
<blockquote>
<p>Authors: Hugo Abonizio, Thales Almeida, Roberto Lotufo, Rodrigo Nogueira</p>
<p>2025-08-08</p>
<p>http://arxiv.org/abs/2508.06178v1</p>
</blockquote>
<p>Large language models (<img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s) often require vast amounts of text to
effectively acquire new knowledge. While continuing pre-training on large
corpora or employing retrieval-augmented generation (RAG) has proven
successful, updating an <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /> with only a few thousand or million tokens remains
challenging. In this work, we investigate the task of injecting small,
unstructured information into <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s and its relation to the catastrophic
forgetting phenomenon. We use a dataset of recent news -- ensuring no <img alt="key" src="https://img.shields.io/badge/overlap-F08080" />
with the model's pre-training data -- to evaluate the knowledge acquisition by
probing the model with question-answer pairs related the learned information.
Starting from a continued pre-training baseline, we explored different
augmentation algorithms to generate synthetic data to improve the knowledge
acquisition capabilities. Our experiments show that simply continuing
pre-training on limited data yields modest improvements, whereas exposing the
model to diverse textual variations significantly improves the learning of new
facts -- particularly with methods that induce greater variability through
diverse prompting. Furthermore, we shed light on the forgetting phenomenon in
small-data regimes, illustrating the delicate balance between learning new
content and retaining existing capabilities. We also confirm the sensitivity of
RAG-based approaches for knowledge injection, which often lead to greater
degradation on control datasets compared to parametric methods. Finally, we
demonstrate that models can generate effective synthetic training data
themselves, suggesting a pathway toward self-improving model updates. All code
and generated data used in our experiments are publicly available, providing a
resource for studying efficient knowledge injection in <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s with limited data
at https://github.com/hugoabonizio/knowledge-injection-methods.</p>
<h2 id="pragmatics-beyond-humans-meaning-communication-and-llms">Pragmatics beyond humans meaning, communication, and LLMs</h2>
<blockquote>
<p>Authors: Vt Gvodiak</p>
<p>2025-08-08</p>
<p>http://arxiv.org/abs/2508.06167v1</p>
</blockquote>
<p>The paper reconceptualizes pragmatics not as a subordinate, third dimension
of meaning, but as a dynamic interface through which language operates as a
socially embedded tool for action. With the emergence of large language models
(<img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s) in communicative contexts, this understanding needs to be further
refined and methodologically reconsidered. The first section challenges the
traditional semiotic trichotomy, arguing that connectionist <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /> architectures
destabilize established hierarchies of meaning, and proposes the Human-Machine
Communication (HMC) framework as a more suitable alternative. The second
section examines the tension between human-centred pragmatic theories and the
machine-centred nature of <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s. While traditional, Gricean-inspired pragmatics
continue to dominate, it relies on human-specific assumptions ill-suited to
predictive systems like <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s. Probabilistic pragmatics, particularly the
Rational Speech Act framework, offers a more compatible teleology by focusing
on optimization rather than truth-evaluation. The third section addresses the
issue of substitutionalism in three forms - generalizing, linguistic, and
communicative - highlighting the anthropomorphic biases that distort <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />
evaluation and obscure the role of human communicative subjects. Finally, the
paper introduces the concept of context frustration to describe the paradox of
increased contextual input paired with a collapse in contextual understanding,
emphasizing how users are compelled to co-construct pragmatic conditions both
for the model and themselves. These arguments suggest that pragmatic theory may
need to be adjusted or expanded to better account for <img alt="key" src="https://img.shields.io/badge/communication-F08080" /> involving
generative AI.</p>
<h2 id="llm-serving-optimization-with-variable-prefill-and-decode-lengths">LLM Serving Optimization with Variable Prefill and Decode Lengths</h2>
<blockquote>
<p>Authors: Meixuan Wang, Yinyu Ye, Zijie Zhou</p>
<p>2025-08-08</p>
<p>http://arxiv.org/abs/2508.06133v1</p>
</blockquote>
<p>We study the problem of serving <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /> (Large Language Model) requests where
each request has heterogeneous prefill and decode lengths. In <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /> serving, the
prefill length corresponds to the input prompt length, which determines the
initial memory usage in the <img alt="key" src="https://img.shields.io/badge/KV-F08080" /> cache. The decode length refers to the number of
output tokens generated sequentially, with each additional token increasing the
<img alt="key" src="https://img.shields.io/badge/KV-F08080" /> cache memory usage by one unit. Given a set of n requests, our goal is to
schedule and process them to minimize the total completion time. We show that
this problem is NP-hard due to the interplay of batching, placement
constraints, precedence relationships, and linearly increasing memory usage. We
then analyze commonly used scheduling strategies in practice, such as
First-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their
competitive ratios scale up sublinearly with the memory limit-a significant
drawback in real-world settings where memory demand is large. To address this,
we propose a novel algorithm based on a new selection metric that efficiently
forms batches over time. We prove that this algorithm achieves a constant
competitive ratio. Finally, we develop and evaluate a few algorithm variants
inspired by this approach, including dynamic programming variants, local search
methods, and an LP-based scheduler, demonstrating through comprehensive
simulations that they outperform standard baselines while maintaining
computational efficiency.</p>
<h2 id="you-dont-need-pre-built-graphs-for-rag-retrieval-augmented-generation-with-adaptive-reasoning-structures">You Don't Need Pre-built Graphs for RAG Retrieval Augmented Generation with Adaptive Reasoning Structures</h2>
<blockquote>
<p>Authors: Shengyuan Chen, Chuang Zhou, Zheng Yuan, Qinggang Zhang, Zeyang Cui, Hao Chen, Yilin Xiao, Jiannong Cao, Xiao Huang</p>
<p>2025-08-08</p>
<p>http://arxiv.org/abs/2508.06105v1</p>
</blockquote>
<p>Large language models (<img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s) often suffer from hallucination, generating
factually incorrect statements when handling questions beyond their knowledge
and perception. Retrieval-augmented generation (RAG) addresses this by
retrieving query-relevant contexts from knowledge bases to support <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />
reasoning. Recent advances leverage pre-constructed graphs to capture the
relational connections among distributed documents, showing remarkable
performance in complex tasks. However, existing Graph-based RAG (GraphRAG)
methods rely on a costly process to transform the corpus into a graph,
introducing overwhelming token cost and update latency. Moreover, real-world
queries vary in type and complexity, requiring different logic structures for
accurate reasoning. The pre-built graph may not align with these required
structures, resulting in ineffective knowledge retrieval. To this end, we
propose a \textbf{\underline{Logic}}-aware
\textbf{\underline{R}}etrieval-\textbf{\underline{A}}ugmented
\textbf{\underline{G}}eneration framework (\textbf{LogicRAG}) that dynamically
extracts reasoning structures at inference time to guide adaptive retrieval
without any pre-built graph. LogicRAG begins by decomposing the input query
into a set of subproblems and constructing a directed acyclic graph (DAG) to
model the logical dependencies among them. To support coherent multi-step
reasoning, LogicRAG then linearizes the graph using topological sort, so that
subproblems can be addressed in a logically consistent order. Besides, LogicRAG
applies graph <img alt="key" src="https://img.shields.io/badge/pruning-F08080" /> to reduce redundant retrieval and uses context <img alt="key" src="https://img.shields.io/badge/pruning-F08080" /> to
filter irrelevant context, significantly reducing the overall token cost.
Extensive experiments demonstrate that LogicRAG achieves both superior
performance and efficiency compared to state-of-the-art baselines.</p>
<h2 id="conlangcrafter-constructing-languages-with-a-multi-hop-llm-pipeline">ConlangCrafter Constructing Languages with a Multi-Hop LLM Pipeline</h2>
<blockquote>
<p>Authors: Morris Alper, Moran Yanuka, Raja Giryes, Gaper Begu</p>
<p>2025-08-08</p>
<p>http://arxiv.org/abs/2508.06094v1</p>
</blockquote>
<p>Constructed languages (conlangs) such as Esperanto and Quenya have played
diverse roles in art, philosophy, and international <img alt="key" src="https://img.shields.io/badge/communication-F08080" />. Meanwhile,
large-scale foundation models have revolutionized creative generation in text,
images, and beyond. In this work, we leverage modern <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s as computational
creativity aids for end-to-end conlang creation. We introduce ConlangCrafter, a
multi-hop pipeline that decomposes language design into modular stages --
phonology, morphology, syntax, lexicon generation, and translation. At each
stage, our method leverages <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />s' meta-linguistic reasoning capabilities,
injecting randomness to encourage diversity and leveraging self-refinement
feedback to encourage consistency in the emerging language description. We
evaluate ConlangCrafter on metrics measuring coherence and typological
diversity, demonstrating its ability to produce coherent and varied conlangs
without human linguistic expertise.</p>
<h2 id="prosocial-behavior-detection-in-player-game-chat-from-aligning-human-ai-definitions-to-efficient-annotation-at-scale">Prosocial Behavior Detection in Player Game Chat From Aligning Human-AI Definitions to Efficient Annotation at Scale</h2>
<blockquote>
<p>Authors: Rafal Kocielnik, Min Kim, Penphob, Boonyarungsrit, Fereshteh Soltani, Deshawn Sambrano, Animashree Anandkumar, R. Michael Alvarez</p>
<p>2025-08-08</p>
<p>http://arxiv.org/abs/2508.05938v1</p>
</blockquote>
<p>Detecting prosociality in text--<img alt="key" src="https://img.shields.io/badge/communication-F08080" /> intended to affirm, support, or
improve others' behavior--is a novel and increasingly important challenge for
trust and safety systems. Unlike toxic content detection, prosociality lacks
well-established definitions and labeled data, requiring new approaches to both
annotation and deployment. We present a practical, three-stage pipeline that
enables scalable, high-precision prosocial content classification while
minimizing human labeling effort and inference costs. First, we identify the
best <img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" />-based labeling strategy using a small seed set of human-labeled
examples. We then introduce a human-AI refinement loop, where annotators review
high-disagreement cases between GPT-4 and humans to iteratively clarify and
expand the task definition-a critical step for emerging annotation tasks like
prosociality. This process results in improved label quality and definition
alignment. Finally, we synthesize 10k high-quality labels using GPT-4 and train
a two-stage inference system: a lightweight classifier handles high-confidence
predictions, while only $\sim$35\% of ambiguous instances are escalated to
GPT-4o. This architecture reduces inference costs by $\sim$70% while achieving
high precision ($\sim$0.90). Our pipeline demonstrates how targeted human-AI
interaction, careful task formulation, and deployment-aware architecture design
can unlock scalable solutions for novel responsible AI tasks.</p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../../cls_author/" class="btn btn-neutral float-left" title="By Author"><span class="icon icon-circle-arrow-left"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../../cls_author/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
