<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../img/favicon.ico" />
    <title>Weekly Paper - Efficient Paper</title>
    <link rel="stylesheet" href="../../css/theme.css" />
    <link rel="stylesheet" href="../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.css" rel="stylesheet" />
        <link href="../../css/extra.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Weekly Paper";
        var mkdocs_page_input_path = "weekly_paper/latest.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
 <link href="../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }</style> <script src="../../assets/javascripts/glightbox.min.js"></script></head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../.." class="icon icon-home"> Efficient Paper
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../..">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Paper List</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../cls_year/">By Year</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cls_keyword/">By Keyword</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cls_publication/">By Publication</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cls_institution/">By Institution</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../cls_author/">By Author</a>
                  </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="#">Weekly Paper</a>
    <ul class="current">
    </ul>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../..">Efficient Paper</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../.." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Weekly Paper</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="2025-08-15">2025-08-15</h1>
<h1 id="table-of-contents">Table of Contents</h1>
<ul>
<li><a href="#Generalizable-Federated-Learning-using-Client-Adaptive-Focal-Modulation">Generalizable Federated Learning using Client Adaptive Focal Modulation</a></li>
<li><a href="#Video-BLADE-Block-Sparse-Attention-Meets-Step-Distillation-for-Efficient-Video-Generation">Video-BLADE Block-Sparse Attention Meets Step Distillation for Efficient Video Generation</a></li>
<li><a href="#Thinking-Inside-the-Mask-In-Place-Prompting-in-Diffusion-LLMs">Thinking Inside the Mask In-Place Prompting in Diffusion LLMs</a></li>
<li><a href="#Continuous-Bangla-Sign-Language-Translation-Mitigating-the-Expense-of-Gloss-Annotation-with-the-Assistance-of-Graph">Continuous Bangla Sign Language Translation Mitigating the Expense of Gloss Annotation with the Assistance of Graph</a></li>
<li><a href="#SemPT-Semantic-Prompt-Tuning-for-Vision-Language-Models">SemPT Semantic Prompt Tuning for Vision-Language Models</a></li>
<li><a href="#Efficient-Methods-for-Accurate-Sparse-Trajectory-Recovery-and-Map-Matching">Efficient Methods for Accurate Sparse Trajectory Recovery and Map Matching</a></li>
<li><a href="#Computational-Economics-in-Large-Language-Models-Exploring-Model-Behavior-and-Incentive-Design-under-Resource-Constraints">Computational Economics in Large Language Models Exploring Model Behavior and Incentive Design under Resource Constraints</a></li>
<li><a href="#Layer-Wise-Perturbations-via-Sparse-Autoencoders-for-Adversarial-Text-Generation">Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation</a></li>
<li><a href="#XQuant-Breaking-the-Memory-Wall-for-LLM-Inference-with-KV-Cache-Rematerialization">XQuant Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization</a></li>
<li><a href="#eMamba-Efficient-Acceleration-Framework-for-Mamba-Models-in-Edge-Computing">eMamba Efficient Acceleration Framework for Mamba Models in Edge Computing</a></li>
<li><a href="#What-to-Ask-Next?-Probing-the-Imaginative-Reasoning-of-LLMs-with-TurtleSoup-Puzzles">What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles</a></li>
<li><a href="#DiffAxE-Diffusion-driven-Hardware-Accelerator-Generation-and-Design-Space-Exploration">DiffAxE Diffusion-driven Hardware Accelerator Generation and Design Space Exploration</a></li>
<li><a href="#Pruning-and-Malicious-Injection-A-Retraining-Free-Backdoor-Attack-on-Transformer-Models">Pruning and Malicious Injection A Retraining-Free Backdoor Attack on Transformer Models</a></li>
<li><a href="#Personalized-Real-time-Jargon-Support-for-Online-Meetings">Personalized Real-time Jargon Support for Online Meetings</a></li>
<li><a href="#Agentic-AI-Frameworks-Architectures,-Protocols,-and-Design-Challenges">Agentic AI Frameworks Architectures, Protocols, and Design Challenges</a></li>
<li><a href="#From-Intent-to-Execution-Multimodal-Chain-of-Thought-Reinforcement-Learning-for-Precise-CAD-Code-Generation">From Intent to Execution Multimodal Chain-of-Thought Reinforcement Learning for Precise CAD Code Generation</a></li>
<li><a href="#Language-of-Persuasion-and-Misrepresentation-in-Business-Communication-A-Textual-Detection-Approach">Language of Persuasion and Misrepresentation in Business Communication A Textual Detection Approach</a></li>
<li><a href="#Speed-Always-Wins-A-Survey-on-Efficient-Architectures-for-Large-Language-Models">Speed Always Wins A Survey on Efficient Architectures for Large Language Models</a></li>
<li><a href="#MoIIE-Mixture-of-Intra--and-Inter-Modality-Experts-for-Large-Vision-Language-Models">MoIIE Mixture of Intra- and Inter-Modality Experts for Large Vision Language Models</a></li>
<li><a href="#MEML-GRPO-Heterogeneous-Multi-Expert-Mutual-Learning-for-RLVR-Advancement">MEML-GRPO Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement</a></li>
<li><a href="#HierMoE-Accelerating-MoE-Training-with-Hierarchical-Token-Deduplication-and-Expert-Swap">HierMoE Accelerating MoE Training with Hierarchical Token Deduplication and Expert Swap</a></li>
<li><a href="#NeuronTune-Fine-Grained-Neuron-Modulation-for-Balanced-Safety-Utility-Alignment-in-LLMs">NeuronTune Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs</a></li>
<li><a href="#EGGS-PTP-An-Expander-Graph-Guided-Structured-Post-training-Pruning-Method-for-Large-Language-Models">EGGS-PTP An Expander-Graph Guided Structured Post-training Pruning Method for Large Language Models</a></li>
<li><a href="#Gen-AFFECT-Generation-of-Avatar-Fine-grained-Facial-Expressions-with-Consistent-identiTy">Gen-AFFECT Generation of Avatar Fine-grained Facial Expressions with Consistent identiTy</a></li>
<li><a href="#Shadow-in-the-Cache-Unveiling-and-Mitigating-Privacy-Risks-of-KV-cache-in-LLM-Inference">Shadow in the Cache Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference</a></li>
<li><a href="#Synaptic-Pruning-A-Biological-Inspiration-for-Deep-Learning-Regularization">Synaptic Pruning A Biological Inspiration for Deep Learning Regularization</a></li>
<li><a href="#READER-Retrieval-Assisted-Drafter-for-Efficient-LLM-Inference">READER Retrieval-Assisted Drafter for Efficient LLM Inference</a></li>
<li><a href="#FetFIDS-A-Feature-Embedding-Attention-based-Federated-Network-Intrusion-Detection-Algorithm">FetFIDS A Feature Embedding Attention based Federated Network Intrusion Detection Algorithm</a></li>
<li><a href="#Retrospective-Sparse-Attention-for-Efficient-Long-Context-Generation">Retrospective Sparse Attention for Efficient Long-Context Generation</a></li>
<li><a href="#NEFMind-Parameter-Efficient-Fine-Tuning-of-Open-Source-LLMs-for-Telecom-APIs-Automation">NEFMind Parameter-Efficient Fine-Tuning of Open-Source LLMs for Telecom APIs Automation</a></li>
<li><a href="#ColorGPT-Leveraging-Large-Language-Models-for-Multimodal-Color-Recommendation">ColorGPT Leveraging Large Language Models for Multimodal Color Recommendation</a></li>
<li><a href="#ASPD-Unlocking-Adaptive-Serial-Parallel-Decoding-by-Exploring-Intrinsic-Parallelism-in-LLMs">ASPD Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic Parallelism in LLMs</a></li>
<li><a href="#DiffPose-Animal-A-Language-Conditioned-Diffusion-Framework-for-Animal-Pose-Estimation">DiffPose-Animal A Language-Conditioned Diffusion Framework for Animal Pose Estimation</a></li>
<li><a href="#Interpretable-Reward-Model-via-Sparse-Autoencoder">Interpretable Reward Model via Sparse Autoencoder</a></li>
<li><a href="#A-Survey-on-Parallel-Text-Generation-From-Parallel-Decoding-to-Diffusion-Language-Models">A Survey on Parallel Text Generation From Parallel Decoding to Diffusion Language Models</a></li>
<li><a href="#Prompt-and-Check-Using-Large-Language-Models-to-Evaluate-Communication-Protocol-Compliance-in-Simulation-Based-Training">Prompt-and-Check Using Large Language Models to Evaluate Communication Protocol Compliance in Simulation-Based Training</a></li>
<li><a href="#Classifier-Language-Models-Unifying-Sparse-Finetuning-and-Adaptive-Tokenization-for-Specialized-Classification-Tasks">Classifier Language Models Unifying Sparse Finetuning and Adaptive Tokenization for Specialized Classification Tasks</a></li>
<li><a href="#AgriGPT-a-Large-Language-Model-Ecosystem-for-Agriculture">AgriGPT a Large Language Model Ecosystem for Agriculture</a></li>
<li><a href="#QoE-Aware-Service-Provision-for-Mobile-AR-Rendering-An-Agent-Driven-Approach">QoE-Aware Service Provision for Mobile AR Rendering An Agent-Driven Approach</a></li>
<li><a href="#Agentic-Graph-Neural-Networks-for-Wireless-Communications-and-Networking-Towards-Edge-General-Intelligence-A-Survey">Agentic Graph Neural Networks for Wireless Communications and Networking Towards Edge General Intelligence A Survey</a></li>
<li><a href="#Securing-Agentic-AI-Threat-Modeling-and-Risk-Analysis-for-Network-Monitoring-Agentic-AI-System">Securing Agentic AI Threat Modeling and Risk Analysis for Network Monitoring Agentic AI System</a></li>
<li><a href="#Using-LLMs-to-Capture-Users'-Temporal-Context-for-Recommendation">Using LLMs to Capture Users' Temporal Context for Recommendation</a></li>
<li><a href="#When-the-Domain-Expert-Has-No-Time-and-the-LLM-Developer-Has-No-Clinical-Expertise-Real-World-Lessons-from-LLM-Co-Design-in-a-Safety-Net-Hospital">When the Domain Expert Has No Time and the LLM Developer Has No Clinical Expertise Real-World Lessons from LLM Co-Design in a Safety-Net Hospital</a></li>
<li><a href="#Architecting-Long-Context-LLM-Acceleration-with-Packing-Prefetch-Scheduler-and-Ultra-Large-Capacity-On-Chip-Memories">Architecting Long-Context LLM Acceleration with Packing-Prefetch Scheduler and Ultra-Large Capacity On-Chip Memories</a></li>
<li><a href="#Selective-KV-Cache-Sharing-to-Mitigate-Timing-Side-Channels-in-LLM-Inference">Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM Inference</a></li>
<li><a href="#BlindGuard-Safeguarding-LLM-based-Multi-Agent-Systems-under-Unknown-Attacks">BlindGuard Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks</a></li>
<li><a href="#TeamMedAgents-Enhancing-Medical-Decision-Making-of-LLMs-Through-Structured-Teamwork">TeamMedAgents Enhancing Medical Decision-Making of LLMs Through Structured Teamwork</a></li>
<li><a href="#ChatGPT-on-the-Road-Leveraging-Large-Language-Model-Powered-In-vehicle-Conversational-Agents-for-Safer-and-More-Enjoyable-Driving-Experience">ChatGPT on the Road Leveraging Large Language Model-Powered In-vehicle Conversational Agents for Safer and More Enjoyable Driving Experience</a></li>
<li><a href="#Interpreting-Fedspeak-with-Confidence-A-LLM-Based-Uncertainty-Aware-Framework-Guided-by-Monetary-Policy-Transmission-Paths">Interpreting Fedspeak with Confidence A LLM-Based Uncertainty-Aware Framework Guided by Monetary Policy Transmission Paths</a></li>
<li><a href="#EvoCoT-Overcoming-the-Exploration-Bottleneck-in-Reinforcement-Learning">EvoCoT Overcoming the Exploration Bottleneck in Reinforcement Learning</a></li>
<li><a href="#Grove-MoE-Towards-Efficient-and-Superior-MoE-LLMs-with-Adjugate-Experts">Grove MoE Towards Efficient and Superior MoE LLMs with Adjugate Experts</a></li>
<li><a href="#GLiClass-Generalist-Lightweight-Model-for-Sequence-Classification-Tasks">GLiClass Generalist Lightweight Model for Sequence Classification Tasks</a></li>
<li><a href="#HGMF-A-Hierarchical-Gaussian-Mixture-Framework-for-Scalable-Tool-Invocation-within-the-Model-Context-Protocol">HGMF A Hierarchical Gaussian Mixture Framework for Scalable Tool Invocation within the Model Context Protocol</a></li>
<li><a href="#Grounding-Natural-Language-for-Multi-agent-Decision-Making-with-Multi-agentic-LLMs">Grounding Natural Language for Multi-agent Decision-Making with Multi-agentic LLMs</a></li>
<li><a href="#LET-US-Long-Event-Text-Understanding-of-Scenes">LET-US Long Event-Text Understanding of Scenes</a></li>
<li><a href="#Efficient-Edge-LLMs-Deployment-via-HessianAware-Quantization-and-CPU-GPU-Collaborative">Efficient Edge LLMs Deployment via HessianAware Quantization and CPU GPU Collaborative</a></li>
<li><a href="#BEVANet-Bilateral-Efficient-Visual-Attention-Network-for-Real-Time-Semantic-Segmentation">BEVANet Bilateral Efficient Visual Attention Network for Real-Time Semantic Segmentation</a></li>
<li><a href="#LP-Spec-Leveraging-LPDDR-PIM-for-Efficient-LLM-Mobile-Speculative-Inference-with-Architecture-Dataflow-Co-Optimization">LP-Spec Leveraging LPDDR PIM for Efficient LLM Mobile Speculative Inference with Architecture-Dataflow Co-Optimization</a></li>
<li><a href="#DySK-Attn-A-Framework-for-Efficient,-Real-Time-Knowledge-Updating-in-Large-Language-Models-via-Dynamic-Sparse-Knowledge-Attention">DySK-Attn A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention</a></li>
<li><a href="#How-Effectively-Can-Large-Language-Models-Connect-SNP-Variants-and-ECG-Phenotypes-for-Cardiovascular-Risk-Prediction?">How Effectively Can Large Language Models Connect SNP Variants and ECG Phenotypes for Cardiovascular Risk Prediction?</a></li>
<li><a href="#From-Nodes-to-Narratives-Explaining-Graph-Neural-Networks-with-LLMs-and-Graph-Context">From Nodes to Narratives Explaining Graph Neural Networks with LLMs and Graph Context</a></li>
<li><a href="#Narrative-Memory-in-Machines-Multi-Agent-Arc-Extraction-in-Serialized-TV">Narrative Memory in Machines Multi-Agent Arc Extraction in Serialized TV</a></li>
<li><a href="#SSD-Offloading-for-LLM-Mixture-of-Experts-Weights-Considered-Harmful-in-Energy-Efficiency">SSD Offloading for LLM Mixture-of-Experts Weights Considered Harmful in Energy Efficiency</a></li>
<li><a href="#Fed-MobiLLM-Efficient-Federated-LLM-Fine-Tuning-over-Heterogeneous-Mobile-Devices-via-Server-Assisted-Side-Tuning">Fed MobiLLM Efficient Federated LLM Fine-Tuning over Heterogeneous Mobile Devices via Server Assisted Side-Tuning</a></li>
<li><a href="#SlimInfer-Accelerating-Long-Context-LLM-Inference-via-Dynamic-Token-Pruning">SlimInfer Accelerating Long-Context LLM Inference via Dynamic Token Pruning</a></li>
<li><a href="#Aligning-Effective-Tokens-with-Video-Anomaly-in-Large-Language-Models">Aligning Effective Tokens with Video Anomaly in Large Language Models</a></li>
<li><a href="#M2IO-R1-An-Efficient-RL-Enhanced-Reasoning-Framework-for-Multimodal-Retrieval-Augmented-Multimodal-Generation">M2IO-R1 An Efficient RL-Enhanced Reasoning Framework for Multimodal Retrieval Augmented Multimodal Generation</a></li>
<li><a href="#Matrix-Driven-Instant-Review-Confident-Detection-and-Reconstruction-of-LLM-Plagiarism-on-PC">Matrix-Driven Instant Review Confident Detection and Reconstruction of LLM Plagiarism on PC</a></li>
<li><a href="#KV-Cache-Compression-for-Inference-Efficiency-in-LLMs-A-Review">KV Cache Compression for Inference Efficiency in LLMs A Review</a></li>
<li><a href="#MA-CBP-A-Criminal-Behavior-Prediction-Framework-Based-on-Multi-Agent-Asynchronous-Collaboration">MA-CBP A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration</a></li>
<li><a href="#Meta-Learning-for-Speeding-Up-Large-Model-Inference-in-Decentralized-Environments">Meta-Learning for Speeding Up Large Model Inference in Decentralized Environments</a></li>
<li><a href="#Comparing-Knowledge-Injection-Methods-for-LLMs-in-a-Low-Resource-Regime">Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime</a></li>
<li><a href="#Pragmatics-beyond-humans-meaning,-communication,-and-LLMs">Pragmatics beyond humans meaning, communication, and LLMs</a></li>
<li><a href="#LLM-Serving-Optimization-with-Variable-Prefill-and-Decode-Lengths">LLM Serving Optimization with Variable Prefill and Decode Lengths</a></li>
<li><a href="#You-Don't-Need-Pre-built-Graphs-for-RAG-Retrieval-Augmented-Generation-with-Adaptive-Reasoning-Structures">You Don't Need Pre-built Graphs for RAG Retrieval Augmented Generation with Adaptive Reasoning Structures</a></li>
<li><a href="#ConlangCrafter-Constructing-Languages-with-a-Multi-Hop-LLM-Pipeline">ConlangCrafter Constructing Languages with a Multi-Hop LLM Pipeline</a></li>
<li><a href="#Diffusion-LLMs-Can-Do-Faster-Than-AR-Inference-via-Discrete-Diffusion-Forcing">Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion Forcing</a></li>
<li><a href="#Prosocial-Behavior-Detection-in-Player-Game-Chat-From-Aligning-Human-AI-Definitions-to-Efficient-Annotation-at-Scale">Prosocial Behavior Detection in Player Game Chat From Aligning Human-AI Definitions to Efficient Annotation at Scale</a></li>
</ul>
<h2 id="generalizable-federated-learning-using-client-adaptive-focal-modulation">Generalizable Federated Learning using Client Adaptive Focal Modulation</h2>
<blockquote>
<p>Authors: Tajamul Ashraf, Iqra Altaf Gillani</p>
<p>2025-08-14</p>
<p><a href="http://arxiv.org/abs/2508.10840v1">http://arxiv.org/abs/2508.10840v1</a></p>
</blockquote>
<p>Federated learning (FL) has proven essential for privacy-preserving,
collaborative training across distributed clients. Our prior work, TransFed,
introduced a robust <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>-based FL framework that leverages a
learn-to-adapt hypernetwork to generate personalized focal modulation layers
per client, outperforming traditional methods in non-IID and cross-domain
settings. In this extended version, we propose AdaptFED, where we deepen the
investigation of focal modulation in generalizable FL by incorporating: (1) a
refined adaptation strategy that integrates task-aware client embeddings to
personalize modulation dynamics further, (2) enhanced theoretical bounds on
adaptation performance, and (3) broader empirical validation across additional
modalities, including time-series and multilingual data. We also introduce an
efficient variant of TransFed that reduces server-client <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> overhead
via low-rank hypernetwork conditioning, enabling scalable deployment in
resource-constrained environments. Extensive experiments on eight diverse
datasets reaffirm the superiority of our method over state-of-the-art
baselines, particularly in source-free and cross-task federated setups. Our
findings not only extend the capabilities of focal modulation in FL but also
pave the way for more adaptive, scalable, and generalizable <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>-based
federated systems. The code is available at
http://github.com/Tajamul21/TransFed</p>
<h2 id="video-blade-block-sparse-attention-meets-step-distillation-for-efficient-video-generation">Video-BLADE Block-Sparse Attention Meets Step Distillation for Efficient Video Generation</h2>
<blockquote>
<p>Authors: Youping Gu, Xiaolong Li, Yuhao Hu, Bohan Zhuang</p>
<p>2025-08-14</p>
<p><a href="http://arxiv.org/abs/2508.10774v1">http://arxiv.org/abs/2508.10774v1</a></p>
</blockquote>
<p>Diffusion <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>s currently lead the field in high-quality video
generation, but their slow iterative denoising process and prohibitive
quadratic attention costs for long sequences create significant inference
bottlenecks. While both step distillation and <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> attention mechanisms have
shown promise as independent <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> strategies, effectively combining
these approaches presents critical challenges -- training-free integration
yields suboptimal results, while separately training <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> attention after
step distillation requires prohibitively expensive high-quality video data. To
overcome these limitations, we propose BLADE, an innovative data-free joint
training framework that introduces: (1) an Adaptive Block-Sparse Attention
(ASA) mechanism for dynamically generating content-aware <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a> masks to
focus computation on salient spatiotemporal features, and (2) a <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a>-aware
step distillation paradigm built upon Trajectory Distribution Matching (TDM)
that directly incorporates <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a> into the distillation process rather than
treating it as a separate compression step, with fast convergence. We validate
BLADE on text-to-video models like CogVideoX-5B and Wan2.1-1.3B. Our framework
demonstrates remarkable efficiency gains across different scales. On
Wan2.1-1.3B, BLADE achieves a 14.10x end-to-end inference <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> over a
50-step baseline. Moreover, on models such as CogVideoX-5B with short video
sequence lengths, our framework delivers a robust 8.89x speedup. Crucially, the
<a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> is accompanied by a consistent quality improvement. On the
VBench-2.0 benchmark, BLADE boosts the score of CogVideoX-5B to 0.569 (from
0.534) and Wan2.1-1.3B to 0.570 (from 0.563), results that are further
corroborated by superior ratings in human evaluations. Our code and model
weights are publicly available at: http://ziplab.co/BLADE-Homepage/.</p>
<h2 id="thinking-inside-the-mask-in-place-prompting-in-diffusion-llms">Thinking Inside the Mask In-Place Prompting in Diffusion LLMs</h2>
<blockquote>
<p>Authors: Xiangqi Jin, Yuxuan Wang, Yifeng Gao, Zichen Wen, Biqing Qi, Dongrui Liu, Linfeng Zhang</p>
<p>2025-08-14</p>
<p><a href="http://arxiv.org/abs/2508.10736v1">http://arxiv.org/abs/2508.10736v1</a></p>
</blockquote>
<p>Despite large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) have achieved remarkable success, their
prefix-only prompting paradigm and sequential generation process offer limited
flexibility for bidirectional information. Diffusion large language models
(d<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) present new opportunities through their bidirectional attention
mechanisms and iterative refinement processes, enabling more flexible in-place
prompting strategies. We introduce ICE (In-Place Chain-of-Thought Prompting
with Early Exit), a novel framework that transforms prefix-only prompting into
in-place prompting specifically designed for d<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s. ICE integrates in-place
prompts directly within masked token positions during iterative refinement and
employs a confidence-aware early exit mechanism to significantly reduce
computational overhead. Extensive experiments demonstrate ICE's effectiveness,
achieving up to 17.29% accuracy improvement with 4.12<script type="math/tex">\times</script> speedup on GSM8K,
and up to 276.67<script type="math/tex">\times</script>
<a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> on MMLU while maintaining competitive
performance.</p>
<h2 id="continuous-bangla-sign-language-translation-mitigating-the-expense-of-gloss-annotation-with-the-assistance-of-graph">Continuous Bangla Sign Language Translation Mitigating the Expense of Gloss Annotation with the Assistance of Graph</h2>
<blockquote>
<p>Authors: Safaeid Hossain Arib, Rabeya Akter, Sejuti Rahman</p>
<p>2025-08-14</p>
<p><a href="http://arxiv.org/abs/2508.10687v1">http://arxiv.org/abs/2508.10687v1</a></p>
</blockquote>
<p>Millions of individuals worldwide are affected by deafness and hearing
impairment. Sign language serves as a sophisticated means of <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> for
the deaf and hard of hearing. However, in societies that prioritize spoken
languages, sign language often faces underestimation, leading to <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>
barriers and social exclusion. The Continuous Bangla Sign Language Translation
project aims to address this gap by enhancing translation methods. While recent
approaches leverage <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> architecture for state-of-the-art results, our
method integrates graph-based methods with the <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> architecture. This
fusion, combining <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> and STGCN-LSTM architectures, proves more
effective in gloss-free translation. Our contributions include architectural
fusion, exploring various fusion strategies, and achieving a new
state-of-the-art performance on diverse sign language datasets, namely
RWTH-PHOENIX-2014T, CSL-Daily, How2Sign, and BornilDB v1.0. Our approach
demonstrates superior performance compared to current translation outcomes
across all datasets, showcasing notable improvements of BLEU-4 scores of 4.01,
2.07, and 0.5, surpassing those of GASLT, GASLT and slt_how2sign in
RWTH-PHOENIX-2014T, CSL-Daily, and How2Sign, respectively. Also, we introduce
benchmarking on the BornilDB v1.0 dataset for the first time. Our method sets a
benchmark for future research, emphasizing the importance of gloss-free
translation to improve <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> accessibility for the deaf and hard of
hearing.</p>
<h2 id="sempt-semantic-prompt-tuning-for-vision-language-models">SemPT Semantic Prompt Tuning for Vision-Language Models</h2>
<blockquote>
<p>Authors: Xiao Shi, Yangjun Ou, Zhenzhong Chen</p>
<p>2025-08-14</p>
<p><a href="http://arxiv.org/abs/2508.10645v1">http://arxiv.org/abs/2508.10645v1</a></p>
</blockquote>
<p>Visual transfer learning for unseen categories presents an active research
topic yet a challenging task, due to the inherent conflict between preserving
category-specific representations and acquiring transferable knowledge.
Vision-Language Models (VLMs) pre-trained on large amounts of image-text pairs
offer a promising solution. However, existing prompt tuning methods rely on
<a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> category labels or disparate <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-generated descriptions, which fragment
knowledge representation and hinder transferability. To address this
limitation, we introduce Semantic Prompt Tuning (SemPT), a novel framework that
tackles the generalization challenge by leveraging shared attribute-level
knowledge across categories. Specifically, SemPT adopts a two-step prompting
strategy to guide <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> in extracting shared visual attributes and generating
attribute-level descriptions, capturing transferable semantic cues beyond
labels while ensuring coherent structure. Then, visually guided weighting is
applied to the embeddings of attribute-level descriptions to reduce noise from
irrelevant attributes and enhance the text embeddings. Additionally, image
embeddings are jointly aligned with both label and attribute-enhanced text
embeddings, balancing discrimination for seen categories and transferability to
unseen ones. Considering the availability of category exposure, our inference
dynamically selects between standard label embeddings for seen categories and
attribute-enhanced embeddings for unseen ones to ensure effective adaptation.
Extensive experiments on 15 benchmark datasets demonstrate that SemPT achieves
state-of-the-art performance across various settings, including base-to-novel
generalization, cross-dataset transfer, cross-domain transfer, and few-shot
learning.</p>
<h2 id="efficient-methods-for-accurate-sparse-trajectory-recovery-and-map-matching">Efficient Methods for Accurate Sparse Trajectory Recovery and Map Matching</h2>
<blockquote>
<p>Authors: Wei Tian, Jieming Shi, Man Lung Yiu</p>
<p>2025-08-14</p>
<p><a href="http://arxiv.org/abs/2508.10460v1">http://arxiv.org/abs/2508.10460v1</a></p>
</blockquote>
<p>Real-world trajectories are often <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> with low-sampling rates (i.e., long
intervals between consecutive GPS points) and misaligned with road networks,
yet many applications demand high-quality data for optimal performance. To
improve data quality with <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> trajectories as input, we systematically study
two related research problems: trajectory recovery on road network, which aims
to infer missing points to recover high-sampling trajectories, and map
matching, which aims to map GPS points to road segments to determine underlying
routes. In this paper, we present efficient methods TRMMA and MMA for accurate
trajectory recovery and map matching, respectively, where MMA serves as the
first step of TRMMA. In MMA, we carefully formulate a classification task to
map a GPS point from <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> trajectories to a road segment over a small
candidate segment set, rather than the entire road network. We develop
techniques in MMA to generate effective embeddings that capture the patterns of
GPS data, directional information, and road segments, to accurately align
<a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> trajectories to routes. For trajectory recovery, TRMMA focuses on the
segments in the route returned by MMA to infer missing points with position
ratios on road segments, producing high-sampling trajectories efficiently by
avoiding evaluation of all road segments. Specifically, in TRMMA, we design a
dual-<a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> encoding process to cohesively capture latent patterns in
trajectories and routes, and an effective decoding technique to sequentially
predict the position ratios and road segments of missing points. We conduct
extensive experiments to compare TRMMA and MMA with numerous existing methods
for trajectory recovery and map matching, respectively, on 4 large real-world
datasets. TRMMA and MMA consistently achieve the best result quality, often by
a significant margin.</p>
<h2 id="computational-economics-in-large-language-models-exploring-model-behavior-and-incentive-design-under-resource-constraints">Computational Economics in Large Language Models Exploring Model Behavior and Incentive Design under Resource Constraints</h2>
<blockquote>
<p>Authors: Sandeep Reddy, Kabir Khan, Rohit Patil, Ananya Chakraborty, Faizan A. Khan, Swati Kulkarni, Arjun Verma, Neha Singh</p>
<p>2025-08-14</p>
<p><a href="http://arxiv.org/abs/2508.10426v1">http://arxiv.org/abs/2508.10426v1</a></p>
</blockquote>
<p>Large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) are limited by substantial computational cost.
We introduce a "computational economics" framework that treats an <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> as an
internal economy of resource-constrained agents (attention heads and neuron
blocks) that must allocate scarce computation to maximize task utility. First,
we show empirically that when computation is scarce, standard <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s reallocate
attention toward high-value tokens while preserving accuracy. Building on this
observation, we propose an incentive-driven training paradigm that augments the
task loss with a differentiable computation cost term, encouraging <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> and
efficient activations. On GLUE (MNLI, STS-B, CoLA) and WikiText-103, the method
yields a family of models that trace a Pareto frontier and consistently
dominate post-hoc <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a>; for a similar accuracy we obtain roughly a forty
percent reduction in FLOPS and lower latency, together with more interpretable
attention patterns. These results indicate that economic principles offer a
principled route to designing efficient, adaptive, and more transparent <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s
under strict resource constraints.</p>
<h2 id="layer-wise-perturbations-via-sparse-autoencoders-for-adversarial-text-generation">Layer-Wise Perturbations via Sparse Autoencoders for Adversarial Text Generation</h2>
<blockquote>
<p>Authors: Huizhen Shu, Xuying Li, Qirui Wang, Yuji Kosuga, Mengqiu Tian, Zhuo Li</p>
<p>2025-08-14</p>
<p><a href="http://arxiv.org/abs/2508.10404v1">http://arxiv.org/abs/2508.10404v1</a></p>
</blockquote>
<p>With the rapid proliferation of Natural Language Processing (NLP), especially
Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s), generating adversarial examples to jailbreak <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s
remains a key challenge for understanding model vulnerabilities and improving
robustness. In this context, we propose a new black-box attack method that
leverages the interpretability of large models. We introduce the Sparse Feature
Perturbation Framework (SFPF), a novel approach for adversarial text generation
that utilizes <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> autoencoders to identify and manipulate critical features
in text. After using the SAE model to reconstruct hidden layer representations,
we perform feature clustering on the successfully attacked texts to identify
features with higher activations. These highly activated features are then
perturbed to generate new adversarial texts. This selective perturbation
preserves the malicious intent while amplifying safety signals, thereby
increasing their potential to evade existing defenses. Our method enables a new
red-teaming strategy that balances adversarial effectiveness with safety
alignment. Experimental results demonstrate that adversarial texts generated by
SFPF can bypass state-of-the-art defense mechanisms, revealing persistent
vulnerabilities in current NLP systems.However, the method's effectiveness
varies across prompts and layers, and its generalizability to other
architectures and larger models remains to be validated.</p>
<h2 id="xquant-breaking-the-memory-wall-for-llm-inference-with-kv-cache-rematerialization">XQuant Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization</h2>
<blockquote>
<p>Authors: Aditya Tomar, Coleman Hooper, Minjae Lee, Haocheng Xi, Rishabh Tiwari, Wonjun Kang, Luca Manolache, Michael W. Mahoney, Kurt Keutzer, Amir Gholami</p>
<p>2025-08-14</p>
<p><a href="http://arxiv.org/abs/2508.10395v1">http://arxiv.org/abs/2508.10395v1</a></p>
</blockquote>
<p>Although <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> inference has emerged as a critical workload for many downstream
applications, efficiently inferring <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s is challenging due to the substantial
memory footprint and bandwidth requirements. In parallel, compute capabilities
have steadily outpaced both memory capacity and bandwidth over the last few
decades, a trend that remains evident in modern GPU hardware and exacerbates
the challenge of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> inference. As such, new algorithms are emerging that trade
increased computation for reduced memory operations. To that end, we present
XQuant, which takes advantage of this trend, enabling an order-of-magnitude
reduction in memory consumption through low-bit quantization with substantial
accuracy benefits relative to state-of-the-art <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> cache quantization methods.
We accomplish this by quantizing and caching the layer input activations X,
instead of using standard <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> caching, and then rematerializing the Keys and
Values on-the-fly during inference. This results in an immediate 2<script type="math/tex">\times</script>
memory savings compared to <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> caching. By applying XQuant, we achieve up to
<script type="math/tex">\sim 7.7\times</script> memory savings with <script type="math/tex"><0.1</script> perplexity degradation compared to
the FP16 baseline. Furthermore, our approach leverages the fact that X values
are similar across layers. Building on this observation, we introduce
XQuant-CL, which exploits the cross-layer similarity in the X embeddings for
extreme compression. Across different models, XQuant-CL attains up to
10<script type="math/tex">\times</script> memory savings relative to the FP16 baseline with only 0.01
perplexity degradation, and 12.5<script type="math/tex">\times</script> memory savings with only <script type="math/tex">0.1</script>
perplexity degradation. XQuant exploits the rapidly increasing compute
capabilities of hardware platforms to eliminate the memory bottleneck, while
surpassing state-of-the-art <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> cache quantization methods and achieving
near-FP16 accuracy across a wide range of models.</p>
<h2 id="emamba-efficient-acceleration-framework-for-mamba-models-in-edge-computing">eMamba Efficient Acceleration Framework for Mamba Models in Edge Computing</h2>
<blockquote>
<p>Authors: Jiyong Kim, Jaeho Lee, Jiahao Lin, Alish Kanani, Miao Sun, Umit Y. Ogras, Jaehyun Park</p>
<p>2025-08-14</p>
<p><a href="http://arxiv.org/abs/2508.10370v1">http://arxiv.org/abs/2508.10370v1</a></p>
</blockquote>
<p>State Space Model (SSM)-based machine learning architectures have recently
gained significant attention for processing sequential data. Mamba, a recent
sequence-to-sequence SSM, offers competitive accuracy with superior
computational efficiency compared to state-of-the-art <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> models. While
this advantage makes Mamba particularly promising for resource-constrained edge
devices, no hardware <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> frameworks are currently optimized for
deploying it in such environments. This paper presents eMamba, a comprehensive
end-to-end hardware <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> framework explicitly designed for deploying
Mamba models on edge platforms. eMamba maximizes computational efficiency by
replacing complex normalization layers with lightweight hardware-aware
alternatives and approximating expensive operations, such as SiLU activation
and exponentiation, considering the target applications. Then, it performs an
approximation-aware neural architecture search (NAS) to tune the learnable
parameters used during approximation. Evaluations with Fashion-MNIST, CIFAR-10,
and MARS, an open-source human pose estimation dataset, show eMamba achieves
comparable accuracy to state-of-the-art techniques using 1.63-19.9<script type="math/tex">\times</script>
fewer parameters. In addition, it generalizes well to large-scale natural
language tasks, demonstrating stable perplexity across varying sequence lengths
on the WikiText2 dataset. We also quantize and implement the entire eMamba
pipeline on an AMD ZCU102 FPGA and ASIC using GlobalFoundries (GF) 22 nm
technology. Experimental results show 4.95-5.62<script type="math/tex">\times</script> lower latency and
2.22-9.95<script type="math/tex">\times</script> higher throughput, with 4.77<script type="math/tex">\times</script> smaller area,
9.84<script type="math/tex">\times</script> lower power, and 48.6<script type="math/tex">\times</script> lower energy consumption than
baseline solutions while maintaining competitive accuracy.</p>
<h2 id="what-to-ask-next-probing-the-imaginative-reasoning-of-llms-with-turtlesoup-puzzles">What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles</h2>
<blockquote>
<p>Authors: Mengtao Zhou, Sifan Wu, Huan Zhang, Qi Sima, Bang Liu</p>
<p>2025-08-14</p>
<p><a href="http://arxiv.org/abs/2508.10358v1">http://arxiv.org/abs/2508.10358v1</a></p>
</blockquote>
<p>We investigate the capacity of Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) for imaginative
reasoning--the proactive construction, testing, and revision of hypotheses in
information-<a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> environments. Existing benchmarks, often static or focused
on social deduction, fail to capture the dynamic, exploratory nature of this
reasoning process. To address this gap, we introduce a comprehensive research
framework based on the classic "Turtle Soup" game, integrating a benchmark, an
agent, and an evaluation protocol. We present TurtleSoup-Bench, the first
large-scale, bilingual, interactive benchmark for imaginative reasoning,
comprising 800 turtle soup puzzles sourced from both the Internet and expert
authors. We also propose Mosaic-Agent, a novel agent designed to assess <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s'
performance in this setting. To evaluate reasoning quality, we develop a
multi-dimensional protocol measuring logical consistency, detail completion,
and conclusion alignment. Experiments with leading <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s reveal clear capability
limits, common failure patterns, and a significant performance gap compared to
humans. Our work offers new insights into <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s' imaginative reasoning and
establishes a foundation for future research on exploratory agent behavior.</p>
<h2 id="diffaxe-diffusion-driven-hardware-accelerator-generation-and-design-space-exploration">DiffAxE Diffusion-driven Hardware Accelerator Generation and Design Space Exploration</h2>
<blockquote>
<p>Authors: Arkapravo Ghosh, Abhishek Moitra, Abhiroop Bhattacharjee, Ruokai Yin, Priyadarshini Panda</p>
<p>2025-08-14</p>
<p><a href="http://arxiv.org/abs/2508.10303v1">http://arxiv.org/abs/2508.10303v1</a></p>
</blockquote>
<p>Design space exploration (DSE) is critical for developing optimized hardware
architectures, especially for AI workloads such as deep neural networks (DNNs)
and large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s), which require specialized <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a>. As
model complexity grows, accelerator design spaces have expanded to O(10^17),
becoming highly irregular, non-convex, and exhibiting many-to-one mappings from
design configurations to performance metrics. This complexity renders direct
inverse derivation infeasible and necessitates heuristic or sampling-based
optimization. Conventional methods - including Bayesian optimization, gradient
descent, reinforcement learning, and genetic algorithms - depend on iterative
sampling, resulting in long runtimes and sensitivity to initialization. Deep
learning-based approaches have reframed DSE as classification using
recommendation models, but remain limited to small-scale (O(10^3)), less
complex design spaces. To overcome these constraints, we propose a generative
approach that models hardware design as 1-D image synthesis conditioned on
target performance, enabling efficient learning of non-differentiable,
non-bijective hardware-performance mappings. Our framework achieves 0.86% lower
generation error than Bayesian optimization with a 17000x speedup, and
outperforms GANDSE with 30% lower error at only 1.83x slower search. We further
extend the method to a structured DSE setting, attaining 9.8% lower
energy-delay product (EDP) and 6% higher performance, with up to 145.6x and
1312x faster search compared to existing optimization methods on O(10^17)
design spaces. For <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> inference, our method achieves 3.37x and 7.75x lower EDP
on a 32nm ASIC and Xilinx Ultrascale+ VPU13 FPGA, respectively, compared to the
state-of-the-art DOSA framework.</p>
<h2 id="pruning-and-malicious-injection-a-retraining-free-backdoor-attack-on-transformer-models">Pruning and Malicious Injection A Retraining-Free Backdoor Attack on Transformer Models</h2>
<blockquote>
<p>Authors: Taibiao Zhao, Mingxuan Sun, Hao Wang, Xiaobing Chen, Xiangwei Zhou</p>
<p>2025-08-14</p>
<p><a href="http://arxiv.org/abs/2508.10243v1">http://arxiv.org/abs/2508.10243v1</a></p>
</blockquote>
<p>Transformer models have demonstrated exceptional performance and have become
indispensable in computer vision (CV) and natural language processing (NLP)
tasks. However, recent studies reveal that <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>s are susceptible to
backdoor attacks. Prior backdoor attack methods typically rely on retraining
with clean data or altering the model architecture, both of which can be
resource-intensive and intrusive. In this paper, we propose Head-wise Pruning
and Malicious Injection (HPMI), a novel retraining-free backdoor attack on
<a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>s that does not alter the model's architecture. Our approach
requires only a small subset of the original data and basic knowledge of the
model architecture, eliminating the need for retraining the target <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>.
Technically, HPMI works by <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> the least important head and injecting a
pre-trained malicious head to establish the backdoor. We provide a rigorous
theoretical justification demonstrating that the implanted backdoor resists
detection and removal by state-of-the-art defense techniques, under reasonable
assumptions. Experimental evaluations across multiple datasets further validate
the effectiveness of HPMI, showing that it 1) incurs negligible clean accuracy
loss, 2) achieves at least 99.55% attack success rate, and 3) bypasses four
advanced defense mechanisms. Additionally, relative to state-of-the-art
retraining-dependent attacks, HPMI achieves greater concealment and robustness
against diverse defense strategies, while maintaining minimal impact on clean
accuracy.</p>
<h2 id="personalized-real-time-jargon-support-for-online-meetings">Personalized Real-time Jargon Support for Online Meetings</h2>
<blockquote>
<p>Authors: Yifan Song, Wing Yee Au, Hon Yung Wong, Brian P. Bailey, Tal August</p>
<p>2025-08-13</p>
<p><a href="http://arxiv.org/abs/2508.10239v1">http://arxiv.org/abs/2508.10239v1</a></p>
</blockquote>
<p>Effective interdisciplinary <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> is frequently hindered by
domain-specific jargon. To explore the jargon barriers in-depth, we conducted a
formative diary study with 16 professionals, revealing critical limitations in
current jargon-management strategies during workplace meetings. Based on these
insights, we designed ParseJargon, an interactive <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-powered system providing
real-time personalized jargon identification and explanations tailored to
users' individual backgrounds. A controlled experiment comparing ParseJargon
against baseline (no support) and general-purpose (non-personalized) conditions
demonstrated that personalized jargon support significantly enhanced
participants' comprehension, engagement, and appreciation of colleagues' work,
whereas general-purpose support negatively affected engagement. A follow-up
field study validated ParseJargon's usability and practical value in real-time
meetings, highlighting both opportunities and limitations for real-world
deployment. Our findings contribute insights into designing personalized jargon
support tools, with implications for broader interdisciplinary and educational
applications.</p>
<h2 id="agentic-ai-frameworks-architectures-protocols-and-design-challenges">Agentic AI Frameworks Architectures, Protocols, and Design Challenges</h2>
<blockquote>
<p>Authors: Hana Derouiche, Zaki Brahmi, Haithem Mazeni</p>
<p>2025-08-13</p>
<p><a href="http://arxiv.org/abs/2508.10146v1">http://arxiv.org/abs/2508.10146v1</a></p>
</blockquote>
<p>The emergence of Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) has ushered in a transformative
paradigm in artificial intelligence, Agentic AI, where intelligent agents
exhibit goal-directed autonomy, contextual reasoning, and dynamic multi-agent
coordination. This paper provides a systematic review and comparative analysis
of leading Agentic AI frameworks, including CrewAI, LangGraph, AutoGen,
Semantic Kernel, Agno, Google ADK, and MetaGPT, evaluating their architectural
principles, <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> mechanisms, memory management, safety guardrails, and
alignment with service-oriented computing paradigms. Furthermore, we identify
key limitations, emerging trends, and open challenges in the field. To address
the issue of agent <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>, we conduct an in-depth analysis of protocols
such as the Contract Net Protocol (CNP), Agent-to-Agent (A2A), Agent Network
Protocol (ANP), and Agora. Our findings not only establish a foundational
taxonomy for Agentic AI systems but also propose future research directions to
enhance scalability, robustness, and interoperability. This work serves as a
comprehensive reference for researchers and practitioners working to advance
the next generation of autonomous AI systems.</p>
<h2 id="from-intent-to-execution-multimodal-chain-of-thought-reinforcement-learning-for-precise-cad-code-generation">From Intent to Execution Multimodal Chain-of-Thought Reinforcement Learning for Precise CAD Code Generation</h2>
<blockquote>
<p>Authors: Ke Niu, Haiyang Yu, Zhuofan Chen, Mengyang Zhao, Teng Fu, Bin Li, Xiangyang Xue</p>
<p>2025-08-13</p>
<p><a href="http://arxiv.org/abs/2508.10118v1">http://arxiv.org/abs/2508.10118v1</a></p>
</blockquote>
<p>Computer-Aided Design (CAD) plays a vital role in engineering and
manufacturing, yet current CAD workflows require extensive domain expertise and
manual modeling effort. Recent advances in large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) have
made it possible to generate code from natural language, opening new
opportunities for automating parametric 3D modeling. However, directly
translating human design intent into executable CAD code remains highly
challenging, due to the need for logical reasoning, syntactic correctness, and
numerical precision. In this work, we propose CAD-RL, a multimodal
Chain-of-Thought (CoT) guided reinforcement learning post training framework
for CAD modeling code generation. Our method combines CoT-based Cold Start with
goal-driven reinforcement learning post training using three task-specific
rewards: executability reward, geometric accuracy reward, and external
evaluation reward. To ensure stable policy learning under <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> and
high-variance reward conditions, we introduce three targeted optimization
strategies: Trust Region Stretch for improved exploration, Precision Token Loss
for enhanced dimensions parameter accuracy, and Overlong Filtering to reduce
noisy supervision. To support training and benchmarking, we release ExeCAD, a
noval dataset comprising 16,540 real-world CAD examples with paired natural
language and structured design language descriptions, executable CADQuery
scripts, and rendered 3D models. Experiments demonstrate that CAD-RL achieves
significant improvements in reasoning quality, output precision, and code
executability over existing VLMs.</p>
<h2 id="language-of-persuasion-and-misrepresentation-in-business-communication-a-textual-detection-approach">Language of Persuasion and Misrepresentation in Business Communication A Textual Detection Approach</h2>
<blockquote>
<p>Authors: Sayem Hossen, Monalisa Moon Joti, Md. Golam Rashed</p>
<p>2025-08-13</p>
<p><a href="http://arxiv.org/abs/2508.09935v1">http://arxiv.org/abs/2508.09935v1</a></p>
</blockquote>
<p>Business <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> digitisation has reorganised the process of persuasive
discourse, which
  allows not only greater transparency but also advanced deception. This
inquiry synthesises classical
  rhetoric and <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> psychology with linguistic theory and empirical
studies in the financial
  reporting, sustainability discourse, and digital marketing to explain how
deceptive language can be
  systematically detected using persuasive lexicon. In controlled settings,
detection accuracies of greater
  than 99% were achieved by using computational textual analysis as well as
personalised <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>
  models. However, reproducing this performance in multilingual settings is
also problematic and,
  to a large extent, this is because it is not easy to find sufficient data,
and because few multilingual
  text-processing infrastructures are in place. This evidence shows that there
has been an increasing
  gap between the theoretical representations of <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> and those
empirically approximated,
  and therefore, there is a need to have strong automatic text-identification
systems where AI-based
  discourse is becoming more realistic in communicating with humans.</p>
<h2 id="speed-always-wins-a-survey-on-efficient-architectures-for-large-language-models">Speed Always Wins A Survey on Efficient Architectures for Large Language Models</h2>
<blockquote>
<p>Authors: Weigao Sun, Jiaxi Hu, Yucheng Zhou, Jusen Du, Disen Lan, Kexin Wang, Tong Zhu, Xiaoye Qu, Yu Zhang, Xiaoyu Mo, Daizong Liu, Yuxuan Liang, Wenliang Chen, Guoqi Li, Yu Cheng</p>
<p>2025-08-13</p>
<p><a href="http://arxiv.org/abs/2508.09834v1">http://arxiv.org/abs/2508.09834v1</a></p>
</blockquote>
<p>Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) have delivered impressive results in language
understanding, generation, reasoning, and pushes the ability boundary of
multimodal models. Transformer models, as the foundation of modern <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s, offer
a strong baseline with excellent scaling properties. However, the traditional
<a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> architecture requires substantial computations and poses
significant obstacles for large-scale training and practical deployment. In
this survey, we offer a systematic examination of innovative <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> architectures
that address the inherent limitations of <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>s and boost the efficiency.
Starting from language modeling, this survey covers the background and
technical details of linear and <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> sequence modeling methods, efficient
full attention variants, <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> mixture-of-experts, hybrid model architectures
incorporating the above techniques, and emerging diffusion <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s. Additionally,
we discuss applications of these techniques to other modalities and consider
their wider implications for developing scalable, resource-aware foundation
models. By grouping recent studies into the above category, this survey
presents a blueprint of modern efficient <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> architectures, and we hope this
could help motivate future research toward more efficient, versatile AI
systems.</p>
<h2 id="moiie-mixture-of-intra-and-inter-modality-experts-for-large-vision-language-models">MoIIE Mixture of Intra- and Inter-Modality Experts for Large Vision Language Models</h2>
<blockquote>
<p>Authors: Dianyi Wang, Siyuan Wang, Zejun Li, Yikun Wang, Yitong Li, Duyu Tang, Xiaoyu Shen, Xuanjing Huang, Zhongyu Wei</p>
<p>2025-08-13</p>
<p><a href="http://arxiv.org/abs/2508.09779v1">http://arxiv.org/abs/2508.09779v1</a></p>
</blockquote>
<p>Large Vision-Language Models (LVLMs) have demonstrated remarkable performance
across multi-modal tasks by scaling model size and training data. However,
these dense LVLMs incur significant computational costs and motivate the
exploration of <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> Mixture of Experts (MoE) architectures. While MoE improve
parameter efficiency, effectively applying MoE to simultaneously model
modality-specific features and cross-modal associations in LVLMs remains
challenging. In this work, we propose to incorporate Mixture of Intra- and
Inter-Modality Experts (MoIIE) to LVLMs. For each token, expert routing is
guided by its modality, directing tokens to their respective intra-modality
experts as well as a shared pool of inter-modality experts, enabling the model
to jointly learn rich intra-modal features and cross-modal interactions. We
further introduce an effective and straightforward two-stage training strategy,
which facilitates the direct activation of both MoE and multi-modal
capabilities. Extensive experiments across different data scales and <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>
backbone demonstrate the effectiveness, efficiency and generality of our
approach. Notably, our MoIIE models with 5.5B and 11.3B activated parameters
match or even surpass the performance of existing advanced open-source MoE-<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s
based multi-modal models that involve more activated parameters. The code is
available at https://github.com/AlenjandroWang/MoIIE.</p>
<h2 id="meml-grpo-heterogeneous-multi-expert-mutual-learning-for-rlvr-advancement">MEML-GRPO Heterogeneous Multi-Expert Mutual Learning for RLVR Advancement</h2>
<blockquote>
<p>Authors: Weitao Jia, Jinghui Lu, Haiyang Yu, Siqi Wang, Guozhi Tang, An-Lan Wang, Weijie Yin, Dingkang Yang, Yuxiang Nie, Bin Shan, Hao Feng, Irene Li, Kun Yang, Han Wang, Jingqun Tang, Teng Fu, Changhong Jin, Chao Feng, Xiaohui Lv, Can Huang</p>
<p>2025-08-13</p>
<p><a href="http://arxiv.org/abs/2508.09670v1">http://arxiv.org/abs/2508.09670v1</a></p>
</blockquote>
<p>Recent advances demonstrate that reinforcement learning with verifiable
rewards (RLVR) significantly enhances the reasoning capabilities of large
language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s). However, standard RLVR faces challenges with reward
<a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a>, where zero rewards from consistently incorrect candidate answers
provide no learning signal, particularly in challenging tasks. To address this,
we propose Multi-Expert Mutual Learning GRPO (MEML-GRPO), an innovative
framework that utilizes diverse expert prompts as system prompts to generate a
broader range of responses, substantially increasing the likelihood of
identifying correct solutions. Additionally, we introduce an inter-expert
mutual learning mechanism that facilitates knowledge sharing and transfer among
experts, further boosting the model's performance through RLVR. Extensive
experiments across multiple reasoning benchmarks show that MEML-GRPO delivers
significant improvements, achieving an average performance gain of 4.89% with
Qwen and 11.33% with Llama, effectively overcoming the core limitations of
traditional RLVR methods.</p>
<h2 id="hiermoe-accelerating-moe-training-with-hierarchical-token-deduplication-and-expert-swap">HierMoE Accelerating MoE Training with Hierarchical Token Deduplication and Expert Swap</h2>
<blockquote>
<p>Authors: Wenxiang Lin, Xinglin Pan, Lin Zhang, Shaohuai Shi, Xuan Wang, Xiaowen Chu</p>
<p>2025-08-13</p>
<p><a href="http://arxiv.org/abs/2508.09591v1">http://arxiv.org/abs/2508.09591v1</a></p>
</blockquote>
<p>The <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a>ly activated mixture-of-experts (MoE) <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> has become a
common architecture for large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) due to its <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a>, which
requires fewer computational demands while easily scaling the model size. In
MoE models, each MoE layer requires to dynamically choose tokens to activate
particular experts for computation while the activated experts may not be
located in the same device or GPU as the token. However, this leads to
substantial <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> and load imbalances across all GPUs, which obstructs
the scalability of distributed systems within a GPU cluster. To this end, we
introduce HierMoE to accelerate the training of MoE models by two
topology-aware techniques: 1) token deduplication to reduce the <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>
traffic, and 2) expert swap to balance the workloads among all GPUs. To enable
the above two proposed approaches to be more general, we build theoretical
models aimed at achieving the best token duplication and expert swap strategy
under different model configurations and hardware environments. We implement
our prototype HierMoE system atop Megatron-LM and conduct experiments on a
32-GPU cluster with DeepSeek-V3 and Qwen3-30B-A3B models. Experimental results
show that our HierMoE achieves <script type="math/tex">1.55\times</script> to <script type="math/tex">3.32\times</script> faster
<a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> and delivers <script type="math/tex">1.18\times</script> to <script type="math/tex">1.27\times</script> faster end-to-end
training compared to state-of-the-art MoE training systems, Tutel-2DH,
SmartMoE, and Megatron-LM.</p>
<h2 id="neurontune-fine-grained-neuron-modulation-for-balanced-safety-utility-alignment-in-llms">NeuronTune Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs</h2>
<blockquote>
<p>Authors: Birong Pan, Mayi Xu, Qiankun Pi, Jianhao Chen, Yuanyuan Zhu, Ming Zhong, Tieyun Qian</p>
<p>2025-08-13</p>
<p><a href="http://arxiv.org/abs/2508.09473v1">http://arxiv.org/abs/2508.09473v1</a></p>
</blockquote>
<p>Ensuring robust safety alignment while preserving utility is critical for the
reliable deployment of Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s). However, current
techniques fundamentally suffer from intertwined deficiencies: insufficient
robustness against malicious attacks, frequent refusal of benign queries,
degradation in generated text quality and general task performance--the former
two reflecting deficits in robust safety and the latter constituting utility
impairment. We trace these limitations to the coarse-grained layer-wise
interventions in existing methods. To resolve this, we propose NeuronTune, a
fine-grained framework that dynamically modulates <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> neurons to achieve
simultaneous safety-utility optimization. Our approach first identifies
safety-critical and utility-preserving neurons across all layers via
attribution, then employs meta-learning to adaptively amplify safety-neuron
activations and suppress utility-neuron activations. Crucially, NeuronTune
enables tunable adjustment of intervention scope via neuron-count thresholds,
supporting flexible adaptation to security-critical or utility-priority
scenarios. Extensive experimental results demonstrate that our method
significantly outperforms existing state-of-the-art technologies, achieving
superior model safety while maintaining excellent utility.</p>
<h2 id="eggs-ptp-an-expander-graph-guided-structured-post-training-pruning-method-for-large-language-models">EGGS-PTP An Expander-Graph Guided Structured Post-training Pruning Method for Large Language Models</h2>
<blockquote>
<p>Authors: Omar Bazarbachi, Zijun Sun, Yanning Shen</p>
<p>2025-08-13</p>
<p><a href="http://arxiv.org/abs/2508.09471v1">http://arxiv.org/abs/2508.09471v1</a></p>
</blockquote>
<p>As Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) become more widely adopted and scale up in
size, the computational and memory challenges involved in deploying these
massive foundation models have grown increasingly severe. This underscores the
urgent need to develop more efficient model variants. Faced with this
challenge, the present work introduces EGGS-PTP: an Expander-Graph Guided
Structured Post-training Pruning method. The proposed approach leverages graph
theory to guide the design of N:M structured <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a>, effectively reducing
model size and computational demands. By incorporating concepts from expander
graphs, EGGS-PTP ensures information flow within the pruned network, preserving
essential model functionality. Extensive numerical experiments demonstrate that
EGGS-PTP not only achieves significant <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> and memory savings due to
structured <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a> but also outperforms existing structured <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> techniques
in terms of accuracy across various <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s.</p>
<h2 id="gen-affect-generation-of-avatar-fine-grained-facial-expressions-with-consistent-identity">Gen-AFFECT Generation of Avatar Fine-grained Facial Expressions with Consistent identiTy</h2>
<blockquote>
<p>Authors: Hao Yu, Rupayan Mallick, Margrit Betke, Sarah Adel Bargal</p>
<p>2025-08-13</p>
<p><a href="http://arxiv.org/abs/2508.09461v1">http://arxiv.org/abs/2508.09461v1</a></p>
</blockquote>
<p>Different forms of customized 2D avatars are widely used in gaming
applications, virtual <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>, education, and content creation. However,
existing approaches often fail to capture fine-grained facial expressions and
struggle to preserve identity across different expressions. We propose
GEN-AFFECT, a novel framework for personalized avatar generation that generates
expressive and identity-consistent avatars with a diverse set of facial
expressions. Our framework proposes conditioning a multimodal diffusion
<a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> on an extracted identity-expression representation. This enables
identity preservation and representation of a wide range of facial expressions.
GEN-AFFECT additionally employs consistent attention at inference for
information sharing across the set of generated expressions, enabling the
generation process to maintain identity consistency over the array of generated
fine-grained expressions. GEN-AFFECT demonstrates superior performance compared
to previous state-of-the-art methods on the basis of the accuracy of the
generated expressions, the preservation of the identity and the consistency of
the target identity across an array of fine-grained facial expressions.</p>
<h2 id="shadow-in-the-cache-unveiling-and-mitigating-privacy-risks-of-kv-cache-in-llm-inference">Shadow in the Cache Unveiling and Mitigating Privacy Risks of KV-cache in LLM Inference</h2>
<blockquote>
<p>Authors: Zhifan Luo, Shuo Shao, Su Zhang, Lijing Zhou, Yuke Hu, Chenxu Zhao, Zhihao Liu, Zhan Qin</p>
<p>2025-08-13</p>
<p><a href="http://arxiv.org/abs/2508.09442v1">http://arxiv.org/abs/2508.09442v1</a></p>
</blockquote>
<p>The Key-Value (<a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>) cache, which stores intermediate attention computations
(Key and Value pairs) to avoid redundant calculations, is a fundamental
mechanism for accelerating Large Language Model (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>) inference. However, this
efficiency optimization introduces significant yet underexplored privacy risks.
This paper provides the first comprehensive analysis of these vulnerabilities,
demonstrating that an attacker can reconstruct sensitive user inputs directly
from the <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>-cache. We design and implement three distinct attack vectors: a
direct Inversion Attack, a more broadly applicable and potent Collision Attack,
and a semantic-based Injection Attack. These methods demonstrate the
practicality and severity of <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>-cache privacy leakage issues. To mitigate this,
we propose <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>-Cloak, a novel, lightweight, and efficient defense mechanism.
<a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>-Cloak uses a reversible matrix-based obfuscation scheme, combined with
operator fusion, to secure the <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>-cache. Our extensive experiments show that
<a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>-Cloak effectively thwarts all proposed attacks, reducing reconstruction
quality to random noise. Crucially, it achieves this robust security with
virtually no degradation in model accuracy and minimal performance overhead,
offering a practical solution for trustworthy <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> deployment.</p>
<h2 id="synaptic-pruning-a-biological-inspiration-for-deep-learning-regularization">Synaptic Pruning A Biological Inspiration for Deep Learning Regularization</h2>
<blockquote>
<p>Authors: Gideon Vos, Liza van Eijk, Zoltan Sarnyai, Mostafa Rahimi Azghadi</p>
<p>2025-08-12</p>
<p><a href="http://arxiv.org/abs/2508.09330v1">http://arxiv.org/abs/2508.09330v1</a></p>
</blockquote>
<p>Synaptic <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> in biological brains removes weak connections to improve
efficiency. In contrast, dropout regularization in artificial neural networks
randomly deactivates neurons without considering activity-dependent <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a>. We
propose a magnitude-based synaptic <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> method that better reflects biology
by progressively removing low-importance connections during training.
Integrated directly into the training loop as a dropout replacement, our
approach computes weight importance from absolute magnitudes across layers and
applies a cubic schedule to gradually increase global <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a>. At fixed
intervals, <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> masks permanently remove low-importance weights while
maintaining gradient flow for active ones, eliminating the need for separate
<a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> and fine-tuning phases. Experiments on multiple time series forecasting
models including RNN, LSTM, and Patch Time Series Transformer across four
datasets show consistent gains. Our method ranked best overall, with
statistically significant improvements confirmed by Friedman tests (p &lt; 0.01).
In financial forecasting, it reduced Mean Absolute Error by up to 20% over
models with no or standard dropout, and up to 52% in select <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> models.
This dynamic <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> mechanism advances regularization by coupling weight
elimination with progressive sparsification, offering easy integration into
diverse architectures. Its strong performance, especially in financial time
series forecasting, highlights its potential as a practical alternative to
conventional dropout techniques.</p>
<h2 id="reader-retrieval-assisted-drafter-for-efficient-llm-inference">READER Retrieval-Assisted Drafter for Efficient LLM Inference</h2>
<blockquote>
<p>Authors: Maxim Divilkovskiy, Vitaly Malygin, Sergey Zlobin, Sultan Isali, Vasily Kalugin, Stanislav Ilyushin, Nuriza Aitassova, Yi Fei, Zeng Weidi</p>
<p>2025-08-12</p>
<p><a href="http://arxiv.org/abs/2508.09072v1">http://arxiv.org/abs/2508.09072v1</a></p>
</blockquote>
<p>Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) generate tokens autoregressively, with each
token depending on the preceding context. This sequential nature makes the
inference process inherently difficult to accelerate, posing a significant
challenge for efficient deployment. In recent years, various methods have been
proposed to address this issue, with the most effective approaches often
involving the training of additional draft models. In this paper, we introduce
READER (Retrieval-Assisted Drafter for Efficient <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> Inference), a novel
lossless speculative decoding method that enhances model-based approaches by
leveraging self-repetitions in the text. Our algorithm expands the speculative
decoding tree using tokens obtained through statistical search. This work
focuses on large batch sizes (&gt;= 8), an underexplored yet important area for
industrial applications. We also analyze the key-value (<a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>) cache size during
speculative decoding and propose an optimization to improve performance for
large batches. As a result, READER outperforms existing speculative decoding
methods. Notably, READER requires no additional training and can reuse
pre-trained speculator models, increasing the speedup by over 40\%. Our method
demonstrates particularly strong performance on search-based tasks, such as
retrieval-augmented generation, where we achieve more than 10x speedup.</p>
<h2 id="fetfids-a-feature-embedding-attention-based-federated-network-intrusion-detection-algorithm">FetFIDS A Feature Embedding Attention based Federated Network Intrusion Detection Algorithm</h2>
<blockquote>
<p>Authors: Shreya Ghosh, Abu Shafin Mohammad Mahdee Jameel, Aly El Gamal</p>
<p>2025-08-12</p>
<p><a href="http://arxiv.org/abs/2508.09056v1">http://arxiv.org/abs/2508.09056v1</a></p>
</blockquote>
<p>Intrusion Detection Systems (IDS) have an increasingly important role in
preventing exploitation of network vulnerabilities by malicious actors. Recent
deep learning based developments have resulted in significant improvements in
the performance of IDS systems. In this paper, we present FetFIDS, where we
explore the employment of feature embedding instead of positional embedding to
improve intrusion detection performance of a <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a> based deep learning
system. Our model is developed with the aim of deployments in edge learning
scenarios, where federated learning over multiple <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> rounds can
ensure both privacy and localized performance improvements. FetFIDS outperforms
multiple state-of-the-art intrusion detection systems in a federated
environment and demonstrates a high degree of suitability to federated
learning. The code for this work can be found at
https://github.com/ghosh64/fetfids.</p>
<h2 id="retrospective-sparse-attention-for-efficient-long-context-generation">Retrospective Sparse Attention for Efficient Long-Context Generation</h2>
<blockquote>
<p>Authors: Seonghwan Choi, Beomseok Kang, Dongwon Jo, Jae-Joon Kim</p>
<p>2025-08-12</p>
<p><a href="http://arxiv.org/abs/2508.09001v1">http://arxiv.org/abs/2508.09001v1</a></p>
</blockquote>
<p>Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) are increasingly deployed in long-context tasks
such as reasoning, code generation, and multi-turn dialogue. However, inference
over extended contexts is bottlenecked by the Key-Value (<a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>) cache, whose
memory footprint grows linearly with sequence length and dominates latency at
each decoding step. While recent <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> cache compression methods identify and load
important tokens, they focus predominantly on input contexts and fail to
address the cumulative attention errors that arise during long decoding. In
this paper, we introduce RetroAttention, a novel <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> cache update technique that
retrospectively revises past attention outputs using newly arrived <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> entries
from subsequent decoding steps. By maintaining a lightweight output cache,
RetroAttention enables past queries to efficiently access more relevant
context, while incurring minimal latency overhead. This breaks the
fixed-attention-output paradigm and allows continual correction of prior
approximations. Extensive experiments on long-generation benchmarks show that
RetroAttention consistently outperforms state-of-the-art (SOTA) <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> compression
methods, increasing effective <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> exposure by up to 1.6<script type="math/tex">\times</script> and accuracy by
up to 21.9\%.</p>
<h2 id="nefmind-parameter-efficient-fine-tuning-of-open-source-llms-for-telecom-apis-automation">NEFMind Parameter-Efficient Fine-Tuning of Open-Source LLMs for Telecom APIs Automation</h2>
<blockquote>
<p>Authors: Zainab Khan, Ahmed Hussain, Mukesh Thakur, Arto Hellas, Panos Papadimitratos</p>
<p>2025-08-12</p>
<p><a href="http://arxiv.org/abs/2508.09240v1">http://arxiv.org/abs/2508.09240v1</a></p>
</blockquote>
<p>The use of Service-Based Architecture in modern tele<a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>s has
exponentially increased Network Functions (NFs) and Application Programming
Interfaces (APIs), creating substantial operational complexities in service
discovery and management. We introduce \textit{NEFMind}, a framework leveraging
parameter-efficient fine-tuning of open-source Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) to
address these challenges. It integrates three core components: synthetic
dataset generation from Network Exposure Function (NEF) API specifications,
model optimization through Quantized-Low-Rank Adaptation, and performance
evaluation via GPT-4 Ref Score and BertScore metrics. Targeting 5G
Service-Based Architecture APIs, our approach achieves 85% reduction in
<a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> overhead compared to manual discovery methods. Experimental
validation using the open-source Phi-2 model demonstrates exceptional API call
identification performance at 98-100% accuracy. The fine-tuned Phi-2 model
delivers performance comparable to significantly larger models like GPT-4 while
maintaining computational efficiency for tele<a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>s infrastructure
deployment. These findings validate domain-specific, parameter-efficient <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>
strategies for managing complex API ecosystems in next-generation
tele<a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>s networks.</p>
<h2 id="colorgpt-leveraging-large-language-models-for-multimodal-color-recommendation">ColorGPT Leveraging Large Language Models for Multimodal Color Recommendation</h2>
<blockquote>
<p>Authors: Ding Xia, Naoto Inoue, Qianru Qiu, Kotaro Kikuchi</p>
<p>2025-08-12</p>
<p><a href="http://arxiv.org/abs/2508.08987v1">http://arxiv.org/abs/2508.08987v1</a></p>
</blockquote>
<p>Colors play a crucial role in the design of vector graphic documents by
enhancing visual appeal, facilitating <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>, improving usability, and
ensuring accessibility. In this context, color recommendation involves
suggesting appropriate colors to complete or refine a design when one or more
colors are missing or require alteration. Traditional methods often struggled
with these challenges due to the complex nature of color design and the limited
data availability. In this study, we explored the use of pretrained Large
Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) and their commonsense reasoning capabilities for color
recommendation, raising the question: Can pretrained <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s serve as superior
designers for color recommendation tasks? To investigate this, we developed a
robust, rigorously validated pipeline, ColorGPT, that was built by
systematically testing multiple color representations and applying effective
prompt engineering techniques. Our approach primarily targeted color palette
completion by recommending colors based on a set of given colors and
accompanying context. Moreover, our method can be extended to full palette
generation, producing an entire color palette corresponding to a provided
textual description. Experimental results demonstrated that our <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-based
pipeline outperformed existing methods in terms of color suggestion accuracy
and the distribution of colors in the color palette completion task. For the
full palette generation task, our approach also yielded improvements in color
diversity and similarity compared to current techniques.</p>
<h2 id="aspd-unlocking-adaptive-serial-parallel-decoding-by-exploring-intrinsic-parallelism-in-llms">ASPD Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic Parallelism in LLMs</h2>
<blockquote>
<p>Authors: Keyu Chen, Zhifeng Shen, Daohai Yu, Haoqian Wu, Wei Wen, Jianfeng He, Ruizhi Qiao, Xing Sun</p>
<p>2025-08-12</p>
<p><a href="http://arxiv.org/abs/2508.08895v2">http://arxiv.org/abs/2508.08895v2</a></p>
</blockquote>
<p>The increasing scale and complexity of large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) pose
significant inference latency challenges, primarily due to their autoregressive
decoding paradigm characterized by the sequential nature of next-token
prediction. By re-examining the outputs of autoregressive models, we observed
that some segments exhibit parallelizable structures, which we term intrinsic
parallelism. Decoding each parallelizable branch simultaneously (i.e. parallel
decoding) can significantly improve the overall inference speed of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s. In
this paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which
addresses two core challenges: automated construction of parallelizable data
and efficient parallel decoding mechanism. More specifically, we introduce a
non-invasive pipeline that automatically extracts and validates parallelizable
structures from the responses of autoregressive models. To empower efficient
adaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which
enables seamless transitions between serial and parallel decoding modes while
maintaining a reusable <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> cache, maximizing computational efficiency. Extensive
evaluations across General Tasks, Retrieval-Augmented Generation, Mathematical
Reasoning, demonstrate that ASPD achieves unprecedented performance in both
effectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up
to 3.19x speedup (1.85x on average) while maintaining response quality within
1% difference compared to autoregressive models, realizing significant
<a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> without compromising generation quality. Our framework sets a
groundbreaking benchmark for efficient <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> parallel inference, paving the way
for its deployment in latency-sensitive applications such as AI-powered
customer service bots and answer retrieval engines.</p>
<h2 id="diffpose-animal-a-language-conditioned-diffusion-framework-for-animal-pose-estimation">DiffPose-Animal A Language-Conditioned Diffusion Framework for Animal Pose Estimation</h2>
<blockquote>
<p>Authors: Tianyu Xiong, Dayi Tan, Wei Tian</p>
<p>2025-08-12</p>
<p><a href="http://arxiv.org/abs/2508.08783v1">http://arxiv.org/abs/2508.08783v1</a></p>
</blockquote>
<p>Animal pose estimation is a fundamental task in computer vision, with growing
importance in ecological monitoring, behavioral analysis, and intelligent
livestock management. Compared to human pose estimation, animal pose estimation
is more challenging due to high interspecies morphological diversity, complex
body structures, and limited annotated data. In this work, we introduce
DiffPose-Animal, a novel diffusion-based framework for top-down animal pose
estimation. Unlike traditional heatmap regression methods, DiffPose-Animal
reformulates pose estimation as a denoising process under the generative
framework of diffusion models. To enhance semantic guidance during keypoint
generation, we leverage large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) to extract both global
anatomical priors and local keypoint-wise semantics based on species-specific
prompts. These textual priors are encoded and fused with image features via
cross-attention modules to provide biologically meaningful constraints
throughout the denoising process. Additionally, a diffusion-based keypoint
decoder is designed to progressively refine pose predictions, improving
robustness to occlusion and annotation <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a>. Extensive experiments on
public animal pose datasets demonstrate the effectiveness and generalization
capability of our method, especially under challenging scenarios with diverse
species, cluttered backgrounds, and incomplete keypoints.</p>
<h2 id="interpretable-reward-model-via-sparse-autoencoder">Interpretable Reward Model via Sparse Autoencoder</h2>
<blockquote>
<p>Authors: Shuyi Zhang, Wei Shi, Sihang Li, Jiayi Liao, Tao Liang, Hengxing Cai, Xiang Wang</p>
<p>2025-08-12</p>
<p><a href="http://arxiv.org/abs/2508.08746v2">http://arxiv.org/abs/2508.08746v2</a></p>
</blockquote>
<p>Large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) have been widely deployed across numerous
fields. Reinforcement Learning from Human Feedback (RLHF) leverages reward
models (RMs) as proxies for human preferences to align <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> behaviors with human
values, making the accuracy, reliability, and interpretability of RMs critical
for effective alignment. However, traditional RMs lack interpretability, offer
limited insight into the reasoning behind reward assignments, and are
inflexible toward user preference shifts. While recent multidimensional RMs aim
for improved interpretability, they often fail to provide feature-level
attribution and require costly annotations. To overcome these limitations, we
introduce the Sparse Autoencoder-enhanced Reward Model (SARM), a novel
architecture that integrates a pretrained Sparse Autoencoder (SAE) into a
reward model. SARM maps the hidden activations of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-based RM into an
interpretable, <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a>, and monosemantic feature space, from which a scalar head
aggregates feature activations to produce transparent and conceptually
meaningful reward scores. Empirical evaluations demonstrate that SARM
facilitates direct feature-level attribution of reward assignments, allows
dynamic adjustment to preference shifts, and achieves superior alignment
performance compared to conventional reward models. Our code is available at
https://github.com/schrieffer-z/sarm.</p>
<h2 id="a-survey-on-parallel-text-generation-from-parallel-decoding-to-diffusion-language-models">A Survey on Parallel Text Generation From Parallel Decoding to Diffusion Language Models</h2>
<blockquote>
<p>Authors: Lingzhe Zhang, Liancheng Fang, Chiming Duan, Minghua He, Leyi Pan, Pei Xiao, Shiyu Huang, Yunpeng Zhai, Xuming Hu, Philip S. Yu, Aiwei Liu</p>
<p>2025-08-12</p>
<p><a href="http://arxiv.org/abs/2508.08712v2">http://arxiv.org/abs/2508.08712v2</a></p>
</blockquote>
<p>As text generation has become a core capability of modern Large Language
Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s), it underpins a wide range of downstream applications. However,
most existing <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s rely on autoregressive (AR) generation, producing one token
at a time based on previously generated context-resulting in limited generation
speed due to the inherently sequential nature of the process. To address this
challenge, an increasing number of researchers have begun exploring parallel
text generation-a broad class of techniques aimed at breaking the
token-by-token generation bottleneck and improving inference efficiency.
Despite growing interest, there remains a lack of comprehensive analysis on
what specific techniques constitute parallel text generation and how they
improve inference performance. To bridge this gap, we present a systematic
survey of parallel text generation methods. We categorize existing approaches
into AR-based and Non-AR-based paradigms, and provide a detailed examination of
the core techniques within each category. Following this taxonomy, we assess
their theoretical trade-offs in terms of speed, quality, and efficiency, and
examine their potential for combination and comparison with alternative
<a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> strategies. Finally, based on our findings, we highlight recent
advancements, identify open challenges, and outline promising directions for
future research in parallel text generation. We have also created a GitHub
repository for indexing relevant papers and open resources available at
https://github.com/zhanglingzhe0820/Awesome-Parallel-Text-Generation.</p>
<h2 id="prompt-and-check-using-large-language-models-to-evaluate-communication-protocol-compliance-in-simulation-based-training">Prompt-and-Check Using Large Language Models to Evaluate Communication Protocol Compliance in Simulation-Based Training</h2>
<blockquote>
<p>Authors: Vishakha Lall, Yisi Liu</p>
<p>2025-08-12</p>
<p><a href="http://arxiv.org/abs/2508.08652v1">http://arxiv.org/abs/2508.08652v1</a></p>
</blockquote>
<p>Accurate evaluation of procedural <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> compliance is essential in
simulation-based training, particularly in safety-critical domains where
adherence to compliance checklists reflects operational competence. This paper
explores a lightweight, deployable approach using prompt-based inference with
open-source large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) that can run efficiently on
consumer-grade GPUs. We present Prompt-and-Check, a method that uses
context-rich prompts to evaluate whether each checklist item in a protocol has
been fulfilled, solely based on transcribed verbal exchanges. We perform a case
study in the maritime domain with participants performing an identical
simulation task, and experiment with models such as LLama 2 7B, LLaMA 3 8B and
Mistral 7B, running locally on an RTX 4070 GPU. For each checklist item, a
prompt incorporating relevant transcript excerpts is fed into the model, which
outputs a compliance judgment. We assess model outputs against expert-annotated
ground truth using classification accuracy and agreement scores. Our findings
demonstrate that prompting enables effective context-aware reasoning without
task-specific training. This study highlights the practical utility of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s in
augmenting debriefing, performance feedback, and automated assessment in
training environments.</p>
<h2 id="classifier-language-models-unifying-sparse-finetuning-and-adaptive-tokenization-for-specialized-classification-tasks">Classifier Language Models Unifying Sparse Finetuning and Adaptive Tokenization for Specialized Classification Tasks</h2>
<blockquote>
<p>Authors: Adit Krishnan, Chu Wang, Chris Kong</p>
<p>2025-08-12</p>
<p><a href="http://arxiv.org/abs/2508.08635v1">http://arxiv.org/abs/2508.08635v1</a></p>
</blockquote>
<p>Semantic text classification requires the understanding of the contextual
significance of specific tokens rather than surface-level patterns or keywords
(as in rule-based or statistical text classification), making large language
models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) well-suited for this task. However, semantic classification
applications in industry, like customer intent detection or semantic role
labeling, tend to be highly specialized. They require annotation by domain
experts in contrast to general-purpose corpora for pretraining. Further, they
typically require high inference throughputs which limits the model size from
latency and cost perspectives. Thus, for a range of specialized classification
tasks, the preferred solution is to develop customized classifiers by
finetuning smaller language models (e.g., mini-encoders, small language
models).
  In this work, we develop a token-driven <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> finetuning strategy to adapt
small language models to specialized classification tasks. We identify and
finetune a small sensitive subset of model parameters by leveraging
task-specific token constructs in the finetuning dataset, while leaving most of
the pretrained weights unchanged. Unlike adapter approaches such as low rank
adaptation (LoRA), we do not introduce additional parameters to the model. Our
approach identifies highly relevant semantic tokens (case study in the
Appendix) and outperforms end-to-end finetuning, LoRA, layer selection, and
prefix tuning on five diverse semantic classification tasks. We achieve greater
stability and half the training costs vs. end-to-end finetuning.</p>
<h2 id="agrigpt-a-large-language-model-ecosystem-for-agriculture">AgriGPT a Large Language Model Ecosystem for Agriculture</h2>
<blockquote>
<p>Authors: Bo Yang, Yu Zhang, Lanfei Feng, Yunkui Chen, Jianyu Zhang, Xiao Xu, Nueraili Aierken, Yurui Li, Yuxuan Chen, Guijun Yang, Yong He, Runhe Huang, Shijian Li</p>
<p>2025-08-12</p>
<p><a href="http://arxiv.org/abs/2508.08632v1">http://arxiv.org/abs/2508.08632v1</a></p>
</blockquote>
<p>Despite the rapid progress of Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s), their application
in agriculture remains limited due to the lack of domain-specific models,
curated datasets, and robust evaluation frameworks. To address these
challenges, we propose AgriGPT, a domain-specialized <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> ecosystem for
agricultural usage. At its core, we design a multi-agent scalable data engine
that systematically compiles credible data sources into Agri-342K, a
high-quality, standardized question-answer (QA) dataset. Trained on this
dataset, AgriGPT supports a broad range of agricultural stakeholders, from
practitioners to policy-makers. To enhance factual grounding, we employ
Tri-RAG, a three-channel Retrieval-Augmented Generation framework combining
dense retrieval, <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> retrieval, and multi-hop knowledge graph reasoning,
thereby improving the <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>'s reasoning reliability. For comprehensive
evaluation, we introduce AgriBench-13K, a benchmark suite comprising 13 tasks
with varying types and complexities. Experiments demonstrate that AgriGPT
significantly outperforms general-purpose <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s on both domain adaptation and
reasoning. Beyond the model itself, AgriGPT represents a modular and extensible
<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> ecosystem for agriculture, comprising structured data construction,
retrieval-enhanced generation, and domain-specific evaluation. This work
provides a generalizable framework for developing scientific and
industry-specialized <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s. All models, datasets, and code will be released to
empower agricultural communities, especially in underserved regions, and to
promote open, impactful research.</p>
<h2 id="qoe-aware-service-provision-for-mobile-ar-rendering-an-agent-driven-approach">QoE-Aware Service Provision for Mobile AR Rendering An Agent-Driven Approach</h2>
<blockquote>
<p>Authors: Conghao Zhou, Lulu Sun, Xiucheng Wang, Peng Yang, Feng Lyu, Sihan Lu, Xuemin Shen</p>
<p>2025-08-12</p>
<p><a href="http://arxiv.org/abs/2508.08627v1">http://arxiv.org/abs/2508.08627v1</a></p>
</blockquote>
<p>Mobile augmented reality (MAR) is envisioned as a key immersive application
in 6G, enabling virtual content rendering aligned with the physical environment
through device pose estimation. In this paper, we propose a novel agent-driven
<a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> service provisioning approach for edge-assisted MAR, aiming to
reduce <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> overhead between MAR devices and the edge server while
ensuring the quality of experience (QoE). First, to address the inaccessibility
of MAR application-specific information to the network controller, we establish
a digital agent powered by large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) on behalf of the MAR
service provider, bridging the data and function gap between the MAR service
and network domains. Second, to cope with the user-dependent and dynamic nature
of data traffic patterns for individual devices, we develop a user-level QoE
modeling method that captures the relationship between <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> resource
demands and perceived user QoE, enabling personalized, agent-driven
<a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> resource management. Trace-driven simulation results demonstrate
that the proposed approach outperforms conventional <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-based QoE-aware service
provisioning methods in both user-level QoE modeling accuracy and <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>
resource efficiency.</p>
<h2 id="agentic-graph-neural-networks-for-wireless-communications-and-networking-towards-edge-general-intelligence-a-survey">Agentic Graph Neural Networks for Wireless Communications and Networking Towards Edge General Intelligence A Survey</h2>
<blockquote>
<p>Authors: Yang Lu, Shengli Zhang, Chang Liu, Ruichen Zhang, Bo Ai, Dusit Niyato, Wei Ni, Xianbin Wang, Abbas Jamalipour</p>
<p>2025-08-12</p>
<p><a href="http://arxiv.org/abs/2508.08620v1">http://arxiv.org/abs/2508.08620v1</a></p>
</blockquote>
<p>The rapid advancement of <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> technologies has driven the evolution
of <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> networks towards both high-dimensional resource utilization
and multifunctional integration. This evolving complexity poses significant
challenges in designing <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> networks to satisfy the growing
quality-of-service and time sensitivity of mobile applications in dynamic
environments. Graph neural networks (GNNs) have emerged as fundamental deep
learning (DL) models for complex <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> networks. GNNs not only augment
the extraction of features over network topologies but also enhance scalability
and facilitate distributed computation. However, most existing GNNs follow a
traditional passive learning framework, which may fail to meet the needs of
increasingly diverse wireless systems. This survey proposes the employment of
agentic artificial intelligence (AI) to organize and integrate GNNs, enabling
scenario- and task-aware implementation towards edge general intelligence. To
comprehend the full capability of GNNs, we holistically review recent
applications of GNNs in wireless <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>s and networking. Specifically,
we focus on the alignment between graph representations and network topologies,
and between neural architectures and wireless tasks. We first provide an
overview of GNNs based on prominent neural architectures, followed by the
concept of agentic GNNs. Then, we summarize and compare GNN applications for
conventional systems and emerging technologies, including physical, MAC, and
network layer designs, integrated sensing and <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> (ISAC),
reconfigurable intelligent surface (RIS) and cell-free network architecture. We
further propose a large language model (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>) framework as an intelligent
question-answering agent, leveraging this survey as a local knowledge base to
enable GNN-related responses tailored to wireless <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> research.</p>
<h2 id="securing-agentic-ai-threat-modeling-and-risk-analysis-for-network-monitoring-agentic-ai-system">Securing Agentic AI Threat Modeling and Risk Analysis for Network Monitoring Agentic AI System</h2>
<blockquote>
<p>Authors: Pallavi Zambare, Venkata Nikhil Thanikella, Ying Liu</p>
<p>2025-08-12</p>
<p><a href="http://arxiv.org/abs/2508.10043v1">http://arxiv.org/abs/2508.10043v1</a></p>
</blockquote>
<p>When combining Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) with autonomous agents, used in
network monitoring and decision-making systems, this will create serious
security issues. In this research, the MAESTRO framework consisting of the
seven layers threat modeling architecture in the system was used to expose,
evaluate, and eliminate vulnerabilities of agentic AI. The prototype agent
system was constructed and implemented, using Python, LangChain, and telemetry
in WebSockets, and deployed with inference, memory, parameter tuning, and
anomaly detection modules. Two practical threat cases were confirmed as
follows: (i) resource denial of service by traffic replay denial-of-service,
and (ii) memory poisoning by tampering with the historical log file maintained
by the agent. These situations resulted in measurable levels of performance
degradation, i.e. telemetry updates were delayed, and computational loads were
increased, as a result of poor system adaptations. It was suggested to use a
multilayered defense-in-depth approach with memory isolation, validation of
planners and anomaly response systems in real-time. These findings verify that
MAESTRO is viable in operational threat mapping, prospective risk scoring, and
the basis of the resilient system design. The authors bring attention to the
importance of the enforcement of memory integrity, paying attention to the
adaptation logic monitoring, and cross-layer <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> protection that
guarantee the agentic AI reliability in adversarial settings.</p>
<h2 id="using-llms-to-capture-users-temporal-context-for-recommendation">Using LLMs to Capture Users' Temporal Context for Recommendation</h2>
<blockquote>
<p>Authors: Milad Sabouri, Masoud Mansoury, Kun Lin, Bamshad Mobasher</p>
<p>2025-08-11</p>
<p><a href="http://arxiv.org/abs/2508.08512v1">http://arxiv.org/abs/2508.08512v1</a></p>
</blockquote>
<p>Effective recommender systems demand dynamic user understanding, especially
in complex, evolving environments. Traditional user profiling often fails to
capture the nuanced, temporal contextual factors of user preferences, such as
transient short-term interests and enduring long-term tastes. This paper
presents an assessment of Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) for generating
semantically rich, time-aware user profiles. We do not propose a novel
end-to-end recommendation architecture; instead, the core contribution is a
systematic investigation into the degree of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> effectiveness in capturing the
dynamics of user context by disentangling short-term and long-term preferences.
This approach, framing temporal preferences as dynamic user contexts for
recommendations, adaptively fuses these distinct contextual components into
comprehensive user embeddings. The evaluation across Movies&amp;TV and Video Games
domains suggests that while <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-generated profiles offer semantic depth and
temporal structure, their effectiveness for context-aware recommendations is
notably contingent on the richness of user interaction histories. Significant
gains are observed in dense domains (e.g., Movies&amp;TV), whereas improvements are
less pronounced in <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> environments (e.g., Video Games). This work
highlights <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s' nuanced potential in enhancing user profiling for adaptive,
context-aware recommendations, emphasizing the critical role of dataset
characteristics for practical applicability.</p>
<h2 id="when-the-domain-expert-has-no-time-and-the-llm-developer-has-no-clinical-expertise-real-world-lessons-from-llm-co-design-in-a-safety-net-hospital">When the Domain Expert Has No Time and the LLM Developer Has No Clinical Expertise Real-World Lessons from LLM Co-Design in a Safety-Net Hospital</h2>
<blockquote>
<p>Authors: Avni Kothari, Patrick Vossler, Jean Digitale, Mohammad Forouzannia, Elise Rosenberg, Michele Lee, Jennee Bryant, Melanie Molina, James Marks, Lucas Zier, Jean Feng</p>
<p>2025-08-11</p>
<p><a href="http://arxiv.org/abs/2508.08504v1">http://arxiv.org/abs/2508.08504v1</a></p>
</blockquote>
<p>Large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) have the potential to address social and
behavioral determinants of health by transforming labor intensive workflows in
resource-constrained settings. Creating <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-based applications that serve the
needs of underserved communities requires a deep understanding of their local
context, but it is often the case that neither <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s nor their developers
possess this local expertise, and the experts in these communities often face
severe time/resource constraints. This creates a disconnect: how can one engage
in meaningful co-design of an <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-based application for an under-resourced
community when the <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> channel between the <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> developer and domain
expert is constrained? We explored this question through a real-world case
study, in which our data science team sought to partner with social workers at
a safety net hospital to build an <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> application that summarizes patients'
social needs. Whereas prior works focus on the challenge of prompt tuning, we
found that the most critical challenge in this setting is the careful and
precise specification of \what information to surface to providers so that the
<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> application is accurate, comprehensive, and verifiable. Here we present a
novel co-design framework for settings with limited access to domain experts,
in which the summary generation task is first decomposed into
individually-optimizable attributes and then each attribute is efficiently
refined and validated through a multi-tier cascading approach.</p>
<h2 id="architecting-long-context-llm-acceleration-with-packing-prefetch-scheduler-and-ultra-large-capacity-on-chip-memories">Architecting Long-Context LLM Acceleration with Packing-Prefetch Scheduler and Ultra-Large Capacity On-Chip Memories</h2>
<blockquote>
<p>Authors: Ming-Yen Lee, Faaiq Waqar, Hanchen Yang, Muhammed Ahosan Ul Karim, Harsono Simka, Shimeng Yu</p>
<p>2025-08-11</p>
<p><a href="http://arxiv.org/abs/2508.08457v1">http://arxiv.org/abs/2508.08457v1</a></p>
</blockquote>
<p>Long-context Large Language Model (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>) inference faces increasing compute
bottlenecks as attention calculations scale with context length, primarily due
to the growing <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>-cache transfer overhead that saturates High Bandwidth Memory
(HBM). While prefetching techniques mitigate cache misses by fetching <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> data
in advance, their spatial and temporal benefits present new opportunities to
exploit. This work proposes a packing-prefetch scheduling architecture with
monolithic 3D (M3D) back-end-of-line (BEOL) compatible embedded memories with
ultra-large on-chip capacity to accelerate long-context <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> inference. Our
optimizations demonstrate 8.06x decode speedup and 1.83x overall latency
reduction on Llama3.1-8B using TPUv6e-like hardware with additional 512MB BEOL
memories over the serial execution. Evaluations of multi-request workloads on
TPU-like architectures show 1.7x-2.4x throughput improvement and 1.5x-2.4x HBM
bandwidth reduction compared to packing-only methods on Llama3.1-8B and
Llama3.1-70B models. With the co-design of packing, prefetching, and BEOL
memories, our approach alleviates HBM constraints and enables efficient
long-context <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> inference.</p>
<h2 id="selective-kv-cache-sharing-to-mitigate-timing-side-channels-in-llm-inference">Selective KV-Cache Sharing to Mitigate Timing Side-Channels in LLM Inference</h2>
<blockquote>
<p>Authors: Kexin Chu, Zecheng Lin, Dawei Xiang, Zixu Shen, Jianchang Su, Cheng Chu, Yiwei Yang, Wenhui Zhang, Wenfei Wu, Wei Zhang</p>
<p>2025-08-11</p>
<p><a href="http://arxiv.org/abs/2508.08438v1">http://arxiv.org/abs/2508.08438v1</a></p>
</blockquote>
<p>Global <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>-cache sharing has emerged as a key optimization for accelerating
large language model (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>) inference. However, it exposes a new class of timing
side-channel attacks, enabling adversaries to infer sensitive user inputs via
shared cache entries. Existing defenses, such as per-user isolation, eliminate
leakage but degrade performance by up to 38.9% in time-to-first-token (TTFT),
making them impractical for high-throughput deployment. To address this gap, we
introduce Safe<a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> (Secure and Flexible <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> Cache Sharing), a privacy-aware
<a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>-cache management framework that selectively shares non-sensitive entries
while confining sensitive content to private caches. Safe<a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> comprises three
components: (i) a hybrid, multi-tier detection pipeline that integrates
rule-based pattern matching, a general-purpose privacy detector, and
context-aware validation; (ii) a unified radix-tree index that manages public
and private entries across heterogeneous memory tiers (HBM, DRAM, SSD); and
(iii) entropy-based access monitoring to detect and mitigate residual
information leakage. Our evaluation shows that Safe<a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> mitigates 94% - 97% of
timing-based side-channel attacks. Compared to per-user isolation method,
Safe<a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> improves TTFT by up to 40.58% and throughput by up to 2.66X across
diverse <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s and workloads. Safe<a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> reduces cache-induced TTFT overhead from
50.41% to 11.74% on Qwen3-235B. By combining fine-grained privacy control with
high cache reuse efficiency, Safe<a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> reclaims the performance advantages of
global sharing while providing robust runtime privacy guarantees for <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>
inference.</p>
<h2 id="blindguard-safeguarding-llm-based-multi-agent-systems-under-unknown-attacks">BlindGuard Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks</h2>
<blockquote>
<p>Authors: Rui Miao, Yixin Liu, Yili Wang, Xu Shen, Yue Tan, Yiwei Dai, Shirui Pan, Xin Wang</p>
<p>2025-08-11</p>
<p><a href="http://arxiv.org/abs/2508.08127v1">http://arxiv.org/abs/2508.08127v1</a></p>
</blockquote>
<p>The security of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-based multi-agent systems (MAS) is critically threatened
by propagation vulnerability, where malicious agents can distort collective
decision-making through inter-agent message interactions. While existing
supervised defense methods demonstrate promising performance, they may be
impractical in real-world scenarios due to their heavy reliance on labeled
malicious agents to train a supervised malicious detection model. To enable
practical and generalizable MAS defenses, in this paper, we propose BlindGuard,
an unsupervised defense method that learns without requiring any
attack-specific labels or prior knowledge of malicious behaviors. To this end,
we establish a hierarchical agent encoder to capture individual, neighborhood,
and global interaction patterns of each agent, providing a comprehensive
understanding for malicious agent detection. Meanwhile, we design a
corruption-guided detector that consists of directional noise injection and
contrastive learning, allowing effective detection model training solely on
normal agent behaviors. Extensive experiments show that BlindGuard effectively
detects diverse attack types (i.e., prompt injection, memory poisoning, and
tool attack) across MAS with various <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> patterns while maintaining
superior generalizability compared to supervised baselines. The code is
available at: https://github.com/MR9812/BlindGuard.</p>
<h2 id="teammedagents-enhancing-medical-decision-making-of-llms-through-structured-teamwork">TeamMedAgents Enhancing Medical Decision-Making of LLMs Through Structured Teamwork</h2>
<blockquote>
<p>Authors: Pranav Pushkar Mishra, Mohammad Arvan, Mohan Zalake</p>
<p>2025-08-11</p>
<p><a href="http://arxiv.org/abs/2508.08115v1">http://arxiv.org/abs/2508.08115v1</a></p>
</blockquote>
<p>We present TeamMedAgents, a novel multi-agent approach that systematically
integrates evidence-based teamwork components from human-human collaboration
into medical decision-making with large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s). Our approach
validates an organizational psychology teamwork model from human collaboration
to computational multi-agent medical systems by operationalizing six core
teamwork components derived from Salas et al.'s "Big Five" model: team
leadership, mutual performance monitoring, team orientation, shared mental
models, closed-loop <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>, and mutual trust. We implement and evaluate
these components as modular, configurable mechanisms within an adaptive
collaboration architecture while assessing the effect of the number of agents
involved based on the task's requirements and domain. Systematic evaluation of
computational implementations of teamwork behaviors across eight medical
benchmarks (MedQA, MedMCQA, MMLU-Pro Medical, PubMedQA, DDXPlus, MedBullets,
Path-VQA, and PMC-VQA) demonstrates consistent improvements across 7 out of 8
evaluated datasets. Controlled ablation studies conducted on 50 questions per
configuration across 3 independent runs provide mechanistic insights into
individual component contributions, revealing optimal teamwork configurations
that vary by reasoning task complexity and domain-specific requirements. Our
ablation analyses reveal dataset-specific optimal teamwork configurations,
indicating that different medical reasoning modalities benefit from distinct
collaborative patterns. TeamMedAgents represents an advancement in
collaborative AI by providing a systematic translation of established teamwork
theories from human collaboration into agentic collaboration, establishing a
foundation for evidence-based multi-agent system design in critical
decision-making domains.</p>
<h2 id="chatgpt-on-the-road-leveraging-large-language-model-powered-in-vehicle-conversational-agents-for-safer-and-more-enjoyable-driving-experience">ChatGPT on the Road Leveraging Large Language Model-Powered In-vehicle Conversational Agents for Safer and More Enjoyable Driving Experience</h2>
<blockquote>
<p>Authors: Yeana Lee Bond, Mungyeong Choe, Baker Kasim Hasan, Arsh Siddiqui, Myounghoon Jeon</p>
<p>2025-08-11</p>
<p><a href="http://arxiv.org/abs/2508.08101v1">http://arxiv.org/abs/2508.08101v1</a></p>
</blockquote>
<p>Studies on in-vehicle conversational agents have traditionally relied on
pre-scripted prompts or limited voice commands, constraining natural
driver-agent interaction. To resolve this issue, the present study explored the
potential of a ChatGPT-based in-vehicle agent capable of carrying continuous,
multi-turn dialogues. Forty drivers participated in our experiment using a
motion-based driving simulator, comparing three conditions (No agent,
Pre-scripted agent, and ChatGPT-based agent) as a within-subjects variable.
Results showed that the ChatGPT-based agent condition led to more stable
driving performance across multiple metrics. Participants demonstrated lower
variability in longitudinal <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a>, lateral <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a>, and lane
deviation compared to the other two conditions. In subjective evaluations, the
ChatGPT-based agent also received significantly higher ratings in competence,
animacy, affective trust, and preference compared to the Pre-scripted agent.
Our thematic analysis of driver-agent conversations revealed diverse
interaction patterns in topics, including driving assistance/questions,
entertainment requests, and anthropomorphic interactions. Our results highlight
the potential of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-powered in-vehicle conversational agents to enhance
driving safety and user experience through natural, context-rich interactions.</p>
<h2 id="interpreting-fedspeak-with-confidence-a-llm-based-uncertainty-aware-framework-guided-by-monetary-policy-transmission-paths">Interpreting Fedspeak with Confidence A LLM-Based Uncertainty-Aware Framework Guided by Monetary Policy Transmission Paths</h2>
<blockquote>
<p>Authors: Rui Yao, Qi Chai, Jinhai Yao, Siyuan Li, Junhao Chen, Qi Zhang, Hao Wang</p>
<p>2025-08-11</p>
<p><a href="http://arxiv.org/abs/2508.08001v2">http://arxiv.org/abs/2508.08001v2</a></p>
</blockquote>
<p>"Fedspeak", the stylized and often nuanced language used by the U.S. Federal
Reserve, encodes implicit policy signals and strategic stances. The Federal
Open Market Committee strategically employs Fedspeak as a <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> tool to
shape market expectations and influence both domestic and global economic
conditions. As such, automatically parsing and interpreting Fedspeak presents a
high-impact challenge, with significant implications for financial forecasting,
algorithmic trading, and data-driven policy analysis. In this paper, we propose
an <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-based, uncertainty-aware framework for deciphering Fedspeak and
classifying its underlying monetary policy stance. Technically, to enrich the
semantic and contextual representation of Fedspeak texts, we incorporate
domain-specific reasoning grounded in the monetary policy transmission
mechanism. We further introduce a dynamic uncertainty decoding module to assess
the confidence of model predictions, thereby enhancing both classification
accuracy and model reliability. Experimental results demonstrate that our
framework achieves state-of-the-art performance on the policy stance analysis
task. Moreover, statistical analysis reveals a significant positive correlation
between perceptual uncertainty and model error rates, validating the
effectiveness of perceptual uncertainty as a diagnostic signal.</p>
<h2 id="evocot-overcoming-the-exploration-bottleneck-in-reinforcement-learning">EvoCoT Overcoming the Exploration Bottleneck in Reinforcement Learning</h2>
<blockquote>
<p>Authors: Huanyu Liu, Jia Li, Chang Yu, Taozhi Chen, Yihong Dong, Lecheng Wang, Hu XiaoLong, Ge Li</p>
<p>2025-08-11</p>
<p><a href="http://arxiv.org/abs/2508.07809v1">http://arxiv.org/abs/2508.07809v1</a></p>
</blockquote>
<p>Reinforcement learning with verifiable reward (RLVR) has become a promising
paradigm for post-training large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) to improve their
reasoning capability. However, when the rollout accuracy is low on hard
problems, the reward becomes <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a>, limiting learning efficiency and causing
exploration bottlenecks. Existing approaches either rely on stronger <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s for
distillation or filter out difficult problems, which limits scalability or
restricts reasoning improvement through exploration.
  We propose EvoCoT, a self-evolving curriculum learning framework based on
two-stage chain-of-thought (CoT) reasoning optimization. EvoCoT constrains the
exploration space by self-generating and verifying CoT trajectories, then
gradually shortens them to expand the space in a controlled way. This enables
<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s to stably learn from initially unsolved hard problems under <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a>
rewards. We apply EvoCoT to multiple <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> families, including Qwen, DeepSeek,
and Llama. Experiments show that EvoCoT enables <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s to solve previously
unsolved problems, improves reasoning capability without external CoT
supervision, and is compatible with various RL fine-tuning methods. We release
the source code to support future research.</p>
<h2 id="grove-moe-towards-efficient-and-superior-moe-llms-with-adjugate-experts">Grove MoE Towards Efficient and Superior MoE LLMs with Adjugate Experts</h2>
<blockquote>
<p>Authors: Haoyuan Wu, Haoxing Chen, Xiaodong Chen, Zhanchao Zhou, Tieyuan Chen, Yihong Zhuang, Guoshan Lu, Zenan Huang, Junbo Zhao, Lin Liu, Zhenzhong Lan, Bei Yu, Jianguo Li</p>
<p>2025-08-11</p>
<p><a href="http://arxiv.org/abs/2508.07785v1">http://arxiv.org/abs/2508.07785v1</a></p>
</blockquote>
<p>The Mixture of Experts (MoE) architecture is a cornerstone of modern
state-of-the-art (SOTA) large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s). MoE models facilitate
scalability by enabling <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> parameter activation. However, traditional MoE
architecture uses homogeneous experts of a uniform size, activating a fixed
number of parameters irrespective of input complexity and thus limiting
computational efficiency. To overcome this limitation, we introduce Grove MoE,
a novel architecture incorporating experts of varying sizes, inspired by the
heterogeneous big.LITTLE CPU architecture. This architecture features novel
adjugate experts with a dynamic activation mechanism, enabling model capacity
expansion while maintaining manageable computational overhead. Building on this
architecture, we present GroveMoE-Base and GroveMoE-Inst, 33B-parameter <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s
developed by applying an upcycling strategy to the Qwen3-30B-A3B-Base model
during mid-training and post-training. GroveMoE models dynamically activate
3.14-3.28B parameters based on token complexity and achieve performance
comparable to SOTA open-source models of similar or even larger size.</p>
<h2 id="gliclass-generalist-lightweight-model-for-sequence-classification-tasks">GLiClass Generalist Lightweight Model for Sequence Classification Tasks</h2>
<blockquote>
<p>Authors: Ihor Stepanov, Mykhailo Shtopko, Dmytro Vodianytskyi, Oleksandr Lukashov, Alexander Yavorskyi, Mykyta Yaroshenko</p>
<p>2025-08-11</p>
<p><a href="http://arxiv.org/abs/2508.07662v1">http://arxiv.org/abs/2508.07662v1</a></p>
</blockquote>
<p>Classification is one of the most widespread tasks in AI applications,
serving often as the first step in filtering, sorting, and categorizing data.
Since modern AI systems must handle large volumes of input data and early
pipeline stages can propagate errors downstream, achieving high efficiency and
accuracy is critical. Moreover, classification requirements can change
dynamically based on user needs, necessitating models with strong zero-shot
capabilities. While generative <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s have become mainstream for zero-shot
classification due to their versatility, they suffer from inconsistent
instruction following and computational inefficiency. Cross-encoders, commonly
used as rerankers in RAG pipelines, face a different bottleneck: they must
process text-label pairs sequentially, significantly reducing efficiency with
large label sets. Embedding-based approaches offer good efficiency but struggle
with complex scenarios involving logical and semantic constraints. We propose
GLiClass, a novel method that adapts the GLiNER architecture for sequence
classification tasks. Our approach achieves strong accuracy and efficiency
comparable to embedding-based methods, while maintaining the flexibility needed
for zero-shot and few-shot learning scenarios. Additionally, we adapted
proximal policy optimization (PPO) for multi-label text classification,
enabling training classifiers in data-<a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> conditions or from human feedback.</p>
<h2 id="hgmf-a-hierarchical-gaussian-mixture-framework-for-scalable-tool-invocation-within-the-model-context-protocol">HGMF A Hierarchical Gaussian Mixture Framework for Scalable Tool Invocation within the Model Context Protocol</h2>
<blockquote>
<p>Authors: Wenpeng Xing, Zhipeng Chen, Changting Lin, Meng Han</p>
<p>2025-08-11</p>
<p><a href="http://arxiv.org/abs/2508.07602v1">http://arxiv.org/abs/2508.07602v1</a></p>
</blockquote>
<p>Invoking external tools enables Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) to perform
complex, real-world tasks, yet selecting the correct tool from large,
hierarchically-structured libraries remains a significant challenge. The
limited context windows of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s and noise from irrelevant options often lead to
low selection accuracy and high computational costs. To address this, we
propose the Hierarchical Gaussian Mixture Framework (HGMF), a probabilistic
<a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> method for scalable tool invocation. HGMF first maps the user query and
all tool descriptions into a unified semantic space. The framework then
operates in two stages: it clusters servers using a Gaussian Mixture Model
(GMM) and filters them based on the query's likelihood. Subsequently, it
applies the same GMM-based clustering and filtering to the tools associated
with the selected servers. This hierarchical process produces a compact,
high-relevance candidate set, simplifying the final selection task for the <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>.
Experiments on a public dataset show that HGMF significantly improves tool
selection accuracy while reducing inference latency, confirming the framework's
scalability and effectiveness for large-scale tool libraries.</p>
<h2 id="grounding-natural-language-for-multi-agent-decision-making-with-multi-agentic-llms">Grounding Natural Language for Multi-agent Decision-Making with Multi-agentic LLMs</h2>
<blockquote>
<p>Authors: Dom Huh, Prasant Mohapatra</p>
<p>2025-08-10</p>
<p><a href="http://arxiv.org/abs/2508.07466v1">http://arxiv.org/abs/2508.07466v1</a></p>
</blockquote>
<p>Language is a ubiquitous tool that is foundational to reasoning and
collaboration, ranging from everyday interactions to sophisticated
problem-solving tasks. The establishment of a common language can serve as a
powerful asset in ensuring clear <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> and understanding amongst
agents, facilitating desired coordination and strategies. In this work, we
extend the capabilities of large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) by integrating them
with advancements in multi-agent decision-making algorithms. We propose a
systematic framework for the design of multi-agentic large language models
(<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s), focusing on key integration practices. These include advanced prompt
engineering techniques, the development of effective memory architectures,
multi-modal information processing, and alignment strategies through
fine-tuning algorithms. We evaluate these design choices through extensive
ablation studies on classic game settings with significant underlying social
dilemmas and game-theoretic considerations.</p>
<h2 id="let-us-long-event-text-understanding-of-scenes">LET-US Long Event-Text Understanding of Scenes</h2>
<blockquote>
<p>Authors: Rui Chen, Xingyu Chen, Shaoan Wang, Shihan Kong, Junzhi Yu</p>
<p>2025-08-10</p>
<p><a href="http://arxiv.org/abs/2508.07401v1">http://arxiv.org/abs/2508.07401v1</a></p>
</blockquote>
<p>Event cameras output event streams as <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a>, asynchronous data with
microsecond-level temporal resolution, enabling visual perception with low
latency and a high dynamic range. While existing Multimodal Large Language
Models (M<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) have achieved significant success in understanding and analyzing
RGB video content, they either fail to interpret event streams effectively or
remain constrained to very short sequences. In this paper, we introduce LET-US,
a framework for long event-stream--text comprehension that employs an adaptive
compression mechanism to reduce the volume of input events while preserving
critical visual details. LET-US thus establishes a new frontier in cross-modal
inferential understanding over extended event sequences. To bridge the
substantial modality gap between event streams and textual representations, we
adopt a two-stage optimization paradigm that progressively equips our model
with the capacity to interpret event-based scenes. To handle the voluminous
temporal information inherent in long event streams, we leverage text-guided
cross-modal queries for feature reduction, augmented by hierarchical clustering
and similarity computation to distill the most representative event features.
Moreover, we curate and construct a large-scale event-text aligned dataset to
train our model, achieving tighter alignment of event features within the <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>
embedding space. We also develop a comprehensive benchmark covering a diverse
set of tasks -- reasoning, captioning, classification, temporal localization
and moment retrieval. Experimental results demonstrate that LET-US outperforms
prior state-of-the-art M<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s in both descriptive accuracy and semantic
comprehension on long-duration event streams. All datasets, codes, and models
will be publicly available.</p>
<h2 id="efficient-edge-llms-deployment-via-hessianaware-quantization-and-cpu-gpu-collaborative">Efficient Edge LLMs Deployment via HessianAware Quantization and CPU GPU Collaborative</h2>
<blockquote>
<p>Authors: Tuo Zhang, Ning Li, Xin Yuan, Wenchao Xu, Quan Chen, Song Guo, Haijun Zhang</p>
<p>2025-08-10</p>
<p><a href="http://arxiv.org/abs/2508.07329v1">http://arxiv.org/abs/2508.07329v1</a></p>
</blockquote>
<p>With the breakthrough progress of large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) in natural
language processing and multimodal tasks, efficiently deploying them on
resource-constrained edge devices has become a critical challenge. The Mixture
of Experts (MoE) architecture enhances model capacity through <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a>
activation, but faces two major difficulties in practical deployment: (1) The
presence of numerous outliers in activation distributions leads to severe
degradation in quantization accuracy for both activations and weights,
significantly impairing inference performance; (2) Under limited memory,
efficient offloading and collaborative inference of expert modules struggle to
balance latency and throughput. To address these issues, this paper proposes an
efficient MoE edge deployment scheme based on Hessian-Aware Quantization (HAQ)
and CPU-GPU collaborative inference. First, by introducing smoothed Hessian
matrix quantization, we achieve joint 8-bit quantization of activations and
weights, which significantly alleviates the accuracy loss caused by outliers
while ensuring efficient implementation on mainstream hardware. Second, we
design an expert-level collaborative offloading and inference mechanism, which,
combined with expert activation path statistics, enables efficient deployment
and scheduling of expert modules between CPU and GPU, greatly reducing memory
footprint and inference latency. Extensive experiments validate the
effectiveness of our method on mainstream large models such as the OPT series
and Mixtral 8*7B: on datasets like Wikitext2 and C4, the inference accuracy of
the low-bit quantized model approaches that of the full-precision model, while
GPU memory usage is reduced by about 60%, and inference latency is
significantly improved.</p>
<h2 id="bevanet-bilateral-efficient-visual-attention-network-for-real-time-semantic-segmentation">BEVANet Bilateral Efficient Visual Attention Network for Real-Time Semantic Segmentation</h2>
<blockquote>
<p>Authors: Ping-Mao Huang, I-Tien Chao, Ping-Chia Huang, Jia-Wei Liao, Yung-Yu Chuang</p>
<p>2025-08-10</p>
<p><a href="http://arxiv.org/abs/2508.07300v1">http://arxiv.org/abs/2508.07300v1</a></p>
</blockquote>
<p>Real-time semantic segmentation presents the dual challenge of designing
efficient architectures that capture large receptive fields for semantic
understanding while also refining detailed contours. Vision <a class="glightbox" href="https://img.shields.io/badge/transformer-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/transformer-FF8C00" /></a>s model
long-range dependencies effectively but incur high computational cost. To
address these challenges, we introduce the Large Kernel Attention (LKA)
mechanism. Our proposed Bilateral Efficient Visual Attention Network (BEVANet)
expands the receptive field to capture contextual information and extracts
visual and structural features using Sparse Decomposed Large Separable Kernel
Attentions (SDLSKA). The Comprehensive Kernel Selection (CKS) mechanism
dynamically adapts the receptive field to further enhance performance.
Furthermore, the Deep Large Kernel Pyramid Pooling Module (DLKPPM) enriches
contextual features by synergistically combining dilated convolutions and large
kernel attention. The bilateral architecture facilitates frequent branch
<a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>, and the Boundary Guided Adaptive Fusion (BGAF) module enhances
boundary delineation by integrating spatial and semantic features under
boundary guidance. BEVANet achieves real-time segmentation at 33 FPS, yielding
79.3% mIoU without pretraining and 81.0% mIoU on Cityscapes after ImageNet
pretraining, demonstrating state-of-the-art performance. The code and model is
available at https://github.com/maomao0819/BEVANet.</p>
<h2 id="lp-spec-leveraging-lpddr-pim-for-efficient-llm-mobile-speculative-inference-with-architecture-dataflow-co-optimization">LP-Spec Leveraging LPDDR PIM for Efficient LLM Mobile Speculative Inference with Architecture-Dataflow Co-Optimization</h2>
<blockquote>
<p>Authors: Siyuan He, Zhantong Zhu, Yandong He, Tianyu Jia</p>
<p>2025-08-10</p>
<p><a href="http://arxiv.org/abs/2508.07227v1">http://arxiv.org/abs/2508.07227v1</a></p>
</blockquote>
<p><a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> inference on mobile devices faces extraneous challenges due to limited
memory bandwidth and computational resources. To address these issues,
speculative inference and processing-in-memory (PIM) techniques have been
explored at the algorithmic and hardware levels. However, speculative inference
results in more compute-intensive GEMM operations, creating new design
trade-offs for existing GEMV-accelerated PIM architectures. Furthermore, there
exists a significant amount of redundant draft tokens in tree-based speculative
inference, necessitating efficient token management schemes to minimize energy
consumption. In this work, we present LP-Spec, an architecture-dataflow
co-design leveraging hybrid LPDDR5 performance-enhanced PIM architecture with
draft token <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> and dynamic workload scheduling to accelerate <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>
speculative inference. A near-data memory controller is proposed to enable data
reallocation between DRAM and PIM banks. Furthermore, a data allocation unit
based on the hardware-aware draft token pruner is developed to minimize energy
consumption and fully exploit parallel execution opportunities. Compared to
end-to-end <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> inference on other mobile solutions such as mobile NPUs or
GEMV-accelerated PIMs, our LP-Spec achieves 13.21x, 7.56x, and 99.87x
improvements in performance, energy efficiency, and energy-delay-product (EDP).
Compared with prior AttAcc PIM and RTX 3090 GPU, LP-Spec can obtain 12.83x and
415.31x EDP reduction benefits.</p>
<h2 id="dysk-attn-a-framework-for-efficient-real-time-knowledge-updating-in-large-language-models-via-dynamic-sparse-knowledge-attention">DySK-Attn A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention</h2>
<blockquote>
<p>Authors: Kabir Khan, Priya Sharma, Arjun Mehta, Neha Gupta, Ravi Narayanan</p>
<p>2025-08-10</p>
<p><a href="http://arxiv.org/abs/2508.07185v1">http://arxiv.org/abs/2508.07185v1</a></p>
</blockquote>
<p>Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) suffer from a critical limitation: their
knowledge is static and quickly becomes outdated. Retraining these massive
models is computationally prohibitive, while existing knowledge editing
techniques can be slow and may introduce unforeseen side effects. To address
this, we propose DySK-Attn, a novel framework that enables <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s to efficiently
integrate real-time knowledge from a dynamic external source. Our approach
synergizes an <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> with a dynamic Knowledge Graph (KG) that can be updated
instantaneously. The core of our framework is a <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a> knowledge attention
mechanism, which allows the <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> to perform a coarse-to-fine grained search,
efficiently identifying and focusing on a small, highly relevant subset of
facts from the vast KG. This mechanism avoids the high computational cost of
dense attention over the entire knowledge base and mitigates noise from
irrelevant information. We demonstrate through extensive experiments on
time-sensitive question-answering tasks that DySK-Attn significantly
outperforms strong baselines, including standard Retrieval-Augmented Generation
(RAG) and model editing techniques, in both factual accuracy for updated
knowledge and computational efficiency. Our framework offers a scalable and
effective solution for building <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s that can stay current with the
ever-changing world.</p>
<h2 id="how-effectively-can-large-language-models-connect-snp-variants-and-ecg-phenotypes-for-cardiovascular-risk-prediction">How Effectively Can Large Language Models Connect SNP Variants and ECG Phenotypes for Cardiovascular Risk Prediction?</h2>
<blockquote>
<p>Authors: Niranjana Arun Menon, Iqra Farooq, Yulong Li, Sara Ahmed, Yutong Xie, Muhammad Awais, Imran Razzak</p>
<p>2025-08-10</p>
<p><a href="http://arxiv.org/abs/2508.07127v1">http://arxiv.org/abs/2508.07127v1</a></p>
</blockquote>
<p>Cardiovascular disease (CVD) prediction remains a tremendous challenge due to
its multifactorial etiology and global burden of morbidity and mortality.
Despite the growing availability of genomic and electrophysiological data,
extracting biologically meaningful insights from such high-dimensional, noisy,
and <a class="glightbox" href="https://img.shields.io/badge/sparse-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparse-F08080" /></a>ly annotated datasets remains a non-trivial task. Recently, <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s has
been applied effectively to predict structural variations in biological
sequences. In this work, we explore the potential of fine-tuned <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s to predict
cardiac diseases and SNPs potentially leading to CVD risk using genetic markers
derived from high-throughput genomic profiling. We investigate the effect of
genetic patterns associated with cardiac conditions and evaluate how <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s can
learn latent biological relationships from structured and semi-structured
genomic data obtained by mapping genetic aspects that are inherited from the
family tree. By framing the problem as a Chain of Thought (CoT) reasoning task,
the models are prompted to generate disease labels and articulate informed
clinical deductions across diverse patient profiles and phenotypes. The
findings highlight the promise of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s in contributing to early detection, risk
assessment, and ultimately, the advancement of personalized medicine in cardiac
care.</p>
<h2 id="from-nodes-to-narratives-explaining-graph-neural-networks-with-llms-and-graph-context">From Nodes to Narratives Explaining Graph Neural Networks with LLMs and Graph Context</h2>
<blockquote>
<p>Authors: Peyman Baghershahi, Gregoire Fournier, Pranav Nyati, Sourav Medya</p>
<p>2025-08-09</p>
<p><a href="http://arxiv.org/abs/2508.07117v1">http://arxiv.org/abs/2508.07117v1</a></p>
</blockquote>
<p>Graph Neural Networks (GNNs) have emerged as powerful tools for learning over
structured data, including text-attributed graphs, which are common in domains
such as citation networks, social platforms, and knowledge graphs. GNNs are not
inherently interpretable and thus, many explanation methods have been proposed.
However, existing explanation methods often struggle to generate interpretable,
fine-grained rationales, especially when node attributes include rich natural
language. In this work, we introduce LOGIC, a lightweight, post-hoc framework
that uses large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) to generate faithful and interpretable
explanations for GNN predictions. LOGIC projects GNN node embeddings into the
<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> embedding space and constructs hybrid prompts that interleave soft prompts
with textual inputs from the graph structure. This enables the <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> to reason
about GNN internal representations and produce natural language explanations
along with concise explanation subgraphs. Our experiments across four
real-world TAG datasets demonstrate that LOGIC achieves a favorable trade-off
between fidelity and <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a>, while significantly improving human-centric
metrics such as insightfulness. LOGIC sets a new direction for <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-based
explainability in graph learning by aligning GNN internals with human
reasoning.</p>
<h2 id="narrative-memory-in-machines-multi-agent-arc-extraction-in-serialized-tv">Narrative Memory in Machines Multi-Agent Arc Extraction in Serialized TV</h2>
<blockquote>
<p>Authors: Roberto Balestri, Guglielmo Pescatore</p>
<p>2025-08-09</p>
<p><a href="http://arxiv.org/abs/2508.07010v1">http://arxiv.org/abs/2508.07010v1</a></p>
</blockquote>
<p>Serialized television narratives present significant analytical challenges
due to their complex, temporally distributed storylines that necessitate
sophisticated information management. This paper introduces a multi-agent
system (MAS) designed to extract and analyze narrative arcs by implementing
principles of computational memory architectures. The system conceptualizes
narrative understanding through analogues of human memory: Large Language
Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) provide a form of semantic memory for general narrative patterns,
while a vector database stores specific arc progressions as episodic memories.
A multi-agent workflow simulates working memory processes to integrate these
information types. Tested on the first season of Grey's Anatomy (ABC 2005-),
the MAS identifies three arc types: Anthology (self-contained), Soap
(relationship-focused), and Genre-Specific. These arcs and their episodic
developments are stored in a vector database, facilitating structured analysis
and semantic comparison. To bridge automation with critical interpretation, a
graphical interface enables human oversight and refinement of the system's
narrative memory. While demonstrating strong performance in identifying
Anthology Arcs and character entities, the system's reliance on textual
paratexts (episode summaries) revealed limitations in discerning <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a>ping
arcs and opaque dynamics, underscoring the challenges in computational memory
consolidation versus human holistic understanding. This memory-centric approach
highlights the potential of combining AI-driven memory processing with human
expertise. Beyond television, it offers promise for serialized written formats
where narrative is entirely text-based. Future work will focus on integrating
multimodal inputs to enrich episodic memory, refining memory integration
mechanisms within the MAS, and expanding testing across diverse genres.</p>
<h2 id="ssd-offloading-for-llm-mixture-of-experts-weights-considered-harmful-in-energy-efficiency">SSD Offloading for LLM Mixture-of-Experts Weights Considered Harmful in Energy Efficiency</h2>
<blockquote>
<p>Authors: Kwanhee Kyung, Sungmin Yun, Jung Ho Ahn</p>
<p>2025-08-09</p>
<p><a href="http://arxiv.org/abs/2508.06978v1">http://arxiv.org/abs/2508.06978v1</a></p>
</blockquote>
<p>Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) applying Mixture-of-Experts (MoE) scale to
trillions of parameters but require vast memory, motivating a line of research
to offload expert weights from fast-but-small DRAM (HBM) to denser Flash SSDs.
While SSDs provide cost-effective capacity, their read energy per bit is
substantially higher than that of DRAM. This paper quantitatively analyzes the
energy implications of offloading MoE expert weights to SSDs during the
critical decode stage of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> inference. Our analysis, comparing SSD, CPU memory
(DDR), and HBM storage scenarios for models like DeepSeek-R1, reveals that
offloading MoE weights to current SSDs drastically increases
per-token-generation energy consumption (e.g., by up to ~12x compared to the
HBM baseline), dominating the total inference energy budget. Although
techniques like prefetching effectively hide access latency, they cannot
mitigate this fundamental energy penalty. We further explore future
technological scaling, finding that the inherent <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a> of MoE models could
potentially make SSDs energy-viable if Flash read energy improves
significantly, roughly by an order of magnitude.</p>
<h2 id="fed-mobillm-efficient-federated-llm-fine-tuning-over-heterogeneous-mobile-devices-via-server-assisted-side-tuning">Fed MobiLLM Efficient Federated LLM Fine-Tuning over Heterogeneous Mobile Devices via Server Assisted Side-Tuning</h2>
<blockquote>
<p>Authors: Xingke Yang, Liang Li, Sicong Li, Liwei Guan, Hao Wang, Xiaoqi Qi, Jiang Liu, Xin Fu, Miao Pan</p>
<p>2025-08-09</p>
<p><a href="http://arxiv.org/abs/2508.06765v1">http://arxiv.org/abs/2508.06765v1</a></p>
</blockquote>
<p>Collaboratively fine-tuning (FT) large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) over
heterogeneous mobile devices fosters immense potential applications of
personalized intelligence. However, such a vision faces critical system
challenges. Conventional federated <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> FT approaches place prohibitive
computational and memory burdens on mobile hardware, and their synchronous
model aggregation protocols stall for slower devices. In this paper, we propose
Fed Mobi<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>, a novel design to facilitate efficient federated <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> FT across
mobile devices with diverse computing/<a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> speeds and local model
architectures. In particular, Fed Mobi<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> implements a pioneering
server-assisted federated side-tuning paradigm. Briefly, mobile devices perform
lightweight forward propagation computations on local data using their frozen
pre-scaled backbone <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s, and then upload selected intermediate activations.
The server trains a shared side-network independently, eliminating client-side
backpropagation and enabling asynchronous updates. To bridge model
heterogeneity across different devices, we introduce an adaptive layer-wise
feature alignment method, which ensures consistent representations for
collaboratively tuning a shared side network. Extensive experimental results
demonstrate that Fed Mobi<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> can maintain robust fine-tuning performance while
achieving extremely low on-device memory, with at least 95.2% reduction in
computation overhead, 93.2% reduction in <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> costs and 5.1x faster
convergence compared to existing methods, validating its efficacy for practical
<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> adaptation over heterogeneous mobile devices.</p>
<h2 id="sliminfer-accelerating-long-context-llm-inference-via-dynamic-token-pruning">SlimInfer Accelerating Long-Context LLM Inference via Dynamic Token Pruning</h2>
<blockquote>
<p>Authors: Lingkun Long, Rubing Yang, Yushi Huang, Desheng Hui, Ao Zhou, Jianlei Yang</p>
<p>2025-08-08</p>
<p><a href="http://arxiv.org/abs/2508.06447v1">http://arxiv.org/abs/2508.06447v1</a></p>
</blockquote>
<p>Long-context inference for Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) is heavily limited by
high computational demands. While several existing methods optimize attention
computation, they still process the full set of hidden states at each layer,
limiting overall efficiency. In this work, we propose SlimInfer, an innovative
framework that aims to accelerate inference by directly <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> less critical
prompt tokens during the forward pass. Our key insight is an information
diffusion phenomenon: As information from critical tokens propagates through
layers, it becomes distributed across the entire sequence. This diffusion
process suggests that <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s can maintain their semantic integrity when excessive
tokens, even including these critical ones, are pruned in hidden states.
Motivated by this, SlimInfer introduces a dynamic fine-grained <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a>
mechanism that accurately removes redundant tokens of hidden state at
intermediate layers. This layer-wise <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> naturally enables an asynchronous
<a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> cache manager that prefetches required token blocks without complex
predictors, reducing both memory usage and I/O costs. Extensive experiments
show that SlimInfer can achieve up to <script type="math/tex">\mathbf{2.53\times}</script> time-to-first-token
(TTFT) speedup and <script type="math/tex">\mathbf{1.88\times}</script> end-to-end latency reduction for
LLaMA3.1-8B-Instruct on a single RTX 4090, without sacrificing performance on
LongBench. Our code will be released upon acceptance.</p>
<h2 id="aligning-effective-tokens-with-video-anomaly-in-large-language-models">Aligning Effective Tokens with Video Anomaly in Large Language Models</h2>
<blockquote>
<p>Authors: Yingxian Chen, Jiahui Liu, Ruifan Di, Yanwei Li, Chirui Chang, Shizhen Zhao, Wilton W. T. Fok, Xiaojuan Qi, Yik-Chung Wu</p>
<p>2025-08-08</p>
<p><a href="http://arxiv.org/abs/2508.06350v1">http://arxiv.org/abs/2508.06350v1</a></p>
</blockquote>
<p>Understanding abnormal events in videos is a vital and challenging task that
has garnered significant attention in a wide range of applications. Although
current video understanding Multi-modal Large Language Models (M<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) are
capable of analyzing general videos, they often struggle to handle anomalies
due to the spatial and temporal <a class="glightbox" href="https://img.shields.io/badge/sparsity-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/sparsity-F08080" /></a> of abnormal events, where the
redundant information always leads to suboptimal outcomes. To address these
challenges, exploiting the representation and generalization capabilities of
Vison Language Models (VLMs) and Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s), we propose
VA-GPT, a novel M<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> designed for summarizing and localizing abnormal events in
various videos. Our approach efficiently aligns effective tokens between visual
encoders and <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s through two key proposed modules: Spatial Effective Token
Selection (SETS) and Temporal Effective Token Generation (TETG). These modules
enable our model to effectively capture and analyze both spatial and temporal
information associated with abnormal events, resulting in more accurate
responses and interactions. Furthermore, we construct an instruction-following
dataset specifically for fine-tuning video-anomaly-aware M<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s, and introduce a
cross-domain evaluation benchmark based on XD-Violence dataset. Our proposed
method outperforms existing state-of-the-art methods on various benchmarks.</p>
<h2 id="m2io-r1-an-efficient-rl-enhanced-reasoning-framework-for-multimodal-retrieval-augmented-multimodal-generation">M2IO-R1 An Efficient RL-Enhanced Reasoning Framework for Multimodal Retrieval Augmented Multimodal Generation</h2>
<blockquote>
<p>Authors: Zhiyou Xiao, Qinhan Yu, Binghui Li, Geng Chen, Chong Chen, Wentao Zhang</p>
<p>2025-08-08</p>
<p><a href="http://arxiv.org/abs/2508.06328v1">http://arxiv.org/abs/2508.06328v1</a></p>
</blockquote>
<p>Current research on Multimodal Retrieval-Augmented Generation (MRAG) enables
diverse multimodal inputs but remains limited to single-modality outputs,
restricting expressive capacity and practical utility. In contrast, real-world
applications often demand both multimodal inputs and multimodal outputs for
effective <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> and grounded reasoning. Motivated by the recent success
of Reinforcement Learning (RL) in complex reasoning tasks for Large Language
Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s), we adopt RL as a principled and effective paradigm to address
the multi-step, outcome-driven challenges inherent in multimodal output
generation. Here, we introduce M2IO-R1, a novel framework for Multimodal
Retrieval-Augmented Multimodal Generation (MRAMG) that supports both multimodal
inputs and outputs. Central to our framework is an RL-based inserter,
Inserter-R1-3B, trained with Group Relative Policy Optimization to guide image
selection and placement in a controllable and semantically aligned manner.
Empirical results show that our lightweight 3B inserter achieves strong
reasoning capabilities with significantly reduced latency, outperforming
baselines in both quality and efficiency.</p>
<h2 id="matrix-driven-instant-review-confident-detection-and-reconstruction-of-llm-plagiarism-on-pc">Matrix-Driven Instant Review Confident Detection and Reconstruction of LLM Plagiarism on PC</h2>
<blockquote>
<p>Authors: Ruichong Zhang</p>
<p>2025-08-08</p>
<p><a href="http://arxiv.org/abs/2508.06309v1">http://arxiv.org/abs/2508.06309v1</a></p>
</blockquote>
<p>In recent years, concerns about intellectual property (IP) in large language
models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) have grown significantly. Plagiarizing other <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s (through direct
weight copying, upcycling, <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a>, or continual pretraining) and claiming
authorship without properly attributing to the original license, is a serious
misconduct that can lead to significant financial and reputational harm to the
original developers. However, existing methods for detecting <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> plagiarism
fall short in key areas. They fail to accurately reconstruct weight
correspondences, lack the ability to compute statistical significance measures
such as <script type="math/tex">p</script>-values, and may mistakenly flag models trained on similar data as
being related. To address these limitations, we propose Matrix-Driven Instant
Review (MDIR), a novel method that leverages matrix analysis and Large
Deviation Theory. MDIR achieves accurate reconstruction of weight
relationships, provides rigorous <script type="math/tex">p</script>-value estimation, and focuses exclusively
on weight similarity without requiring full model inference. Experimental
results demonstrate that MDIR reliably detects plagiarism even after extensive
transformations, such as random permutations and continual pretraining with
trillions of tokens. Moreover, all detections can be performed on a single PC
within an hour, making MDIR both efficient and accessible.</p>
<h2 id="kv-cache-compression-for-inference-efficiency-in-llms-a-review">KV Cache Compression for Inference Efficiency in LLMs A Review</h2>
<blockquote>
<p>Authors: Yanyu Liu, Jingying Fu, Sixiang Liu, Yitian Zou, You Fu, Jiehan Zhou, Shouhua Zhang</p>
<p>2025-08-08</p>
<p><a href="http://arxiv.org/abs/2508.06297v1">http://arxiv.org/abs/2508.06297v1</a></p>
</blockquote>
<p>Withtherapid advancement of large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s), the context length
for inference has been continuously increasing, leading to an exponential
growth in the demand for Key-Value (<a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a>) caching. This has resulted in a
significant memory bottleneck, limiting the inference efficiency and
scalability of the models. Therefore, optimizing the <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> cache during inference
is crucial for enhancing performance and efficiency. This review systematically
examines current <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> cache optimization techniques, including compression
strategies such as selective token strategies, quantization, and attention
compression. We evaluate the effectiveness, trade-offs, and application
scenarios of these methods, providing a comprehensive analysis of their impact
on memory usage and inference speed. We focus on identifying the limitations
and challenges of existing methods, such as compatibility issues with different
models and tasks. Additionally, this review highlights future research
directions, including hybrid optimization techniques, adaptive dynamic
strategies, and software-hardware co-design. These approaches aim to improve
inference efficiency and promote the practical application of large language
models.</p>
<h2 id="ma-cbp-a-criminal-behavior-prediction-framework-based-on-multi-agent-asynchronous-collaboration">MA-CBP A Criminal Behavior Prediction Framework Based on Multi-Agent Asynchronous Collaboration</h2>
<blockquote>
<p>Authors: Cheng Liu, Daou Zhang, Tingxu Liu, Yuhan Wang, Jinyang Chen, Yuexuan Li, Xinying Xiao, Chenbo Xin, Ziru Wang, Weichao Wu</p>
<p>2025-08-08</p>
<p><a href="http://arxiv.org/abs/2508.06189v1">http://arxiv.org/abs/2508.06189v1</a></p>
</blockquote>
<p>With the <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> of urbanization, criminal behavior in public scenes
poses an increasingly serious threat to social security. Traditional anomaly
detection methods based on feature recognition struggle to capture high-level
behavioral semantics from historical information, while generative approaches
based on Large Language Models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) often fail to meet real-time
requirements. To address these challenges, we propose MA-CBP, a criminal
behavior prediction framework based on multi-agent asynchronous collaboration.
This framework transforms real-time video streams into frame-level semantic
descriptions, constructs causally consistent historical summaries, and fuses
adjacent image frames to perform joint reasoning over long- and short-term
contexts. The resulting behavioral decisions include key elements such as event
subjects, locations, and causes, enabling early warning of potential criminal
activity. In addition, we construct a high-quality criminal behavior dataset
that provides multi-scale language supervision, including frame-level,
summary-level, and event-level semantic annotations. Experimental results
demonstrate that our method achieves superior performance on multiple datasets
and offers a promising solution for risk warning in urban public safety
scenarios.</p>
<h2 id="meta-learning-for-speeding-up-large-model-inference-in-decentralized-environments">Meta-Learning for Speeding Up Large Model Inference in Decentralized Environments</h2>
<blockquote>
<p>Authors: Yipeng Du, Zihao Wang, Ahmad Farhan, Claudio Angione, Harry Yang, Fielding Johnston, James P. Buban, Patrick Colangelo, Yue Zhao, Yuzhe Yang</p>
<p>2025-08-08</p>
<p><a href="http://arxiv.org/abs/2508.09194v1">http://arxiv.org/abs/2508.09194v1</a></p>
</blockquote>
<p>The deployment of large-scale models, such as large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s),
incurs substantial costs due to their computational demands. To mitigate these
costs and address challenges related to scalability and data security, there is
a growing shift towards decentralized systems for model deployment, where
choosing efficient inference <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> schemes become crucial to manage
computational resources effectively and enhance system responsiveness. In this
work, we address the challenge of selecting optimal <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> methods in
decentralized systems by introducing a meta-learning-based framework. This
framework automates the selection process by learning from historical
performance data of various <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> techniques across different tasks.
Unlike traditional methods that rely on random selection or expert intuition,
our approach systematically identifies the best <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> strategies based
on the specific characteristics of each task. We demonstrate that our
meta-learning framework not only streamlines the decision-making process but
also consistently outperforms conventional methods in terms of efficiency and
performance. Our results highlight the potential of inference <a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> in
decentralized AI systems, offering a path towards more democratic and
economically feasible artificial intelligence solutions.</p>
<h2 id="comparing-knowledge-injection-methods-for-llms-in-a-low-resource-regime">Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime</h2>
<blockquote>
<p>Authors: Hugo Abonizio, Thales Almeida, Roberto Lotufo, Rodrigo Nogueira</p>
<p>2025-08-08</p>
<p><a href="http://arxiv.org/abs/2508.06178v1">http://arxiv.org/abs/2508.06178v1</a></p>
</blockquote>
<p>Large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) often require vast amounts of text to
effectively acquire new knowledge. While continuing pre-training on large
corpora or employing retrieval-augmented generation (RAG) has proven
successful, updating an <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> with only a few thousand or million tokens remains
challenging. In this work, we investigate the task of injecting small,
unstructured information into <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s and its relation to the catastrophic
forgetting phenomenon. We use a dataset of recent news -- ensuring no <a class="glightbox" href="https://img.shields.io/badge/overlap-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/overlap-F08080" /></a>
with the model's pre-training data -- to evaluate the knowledge acquisition by
probing the model with question-answer pairs related the learned information.
Starting from a continued pre-training baseline, we explored different
augmentation algorithms to generate synthetic data to improve the knowledge
acquisition capabilities. Our experiments show that simply continuing
pre-training on limited data yields modest improvements, whereas exposing the
model to diverse textual variations significantly improves the learning of new
facts -- particularly with methods that induce greater variability through
diverse prompting. Furthermore, we shed light on the forgetting phenomenon in
small-data regimes, illustrating the delicate balance between learning new
content and retaining existing capabilities. We also confirm the sensitivity of
RAG-based approaches for knowledge injection, which often lead to greater
degradation on control datasets compared to parametric methods. Finally, we
demonstrate that models can generate effective synthetic training data
themselves, suggesting a pathway toward self-improving model updates. All code
and generated data used in our experiments are publicly available, providing a
resource for studying efficient knowledge injection in <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s with limited data
at https://github.com/hugoabonizio/knowledge-injection-methods.</p>
<h2 id="pragmatics-beyond-humans-meaning-communication-and-llms">Pragmatics beyond humans meaning, communication, and LLMs</h2>
<blockquote>
<p>Authors: Vt Gvodiak</p>
<p>2025-08-08</p>
<p><a href="http://arxiv.org/abs/2508.06167v1">http://arxiv.org/abs/2508.06167v1</a></p>
</blockquote>
<p>The paper reconceptualizes pragmatics not as a subordinate, third dimension
of meaning, but as a dynamic interface through which language operates as a
socially embedded tool for action. With the emergence of large language models
(<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) in communicative contexts, this understanding needs to be further
refined and methodologically reconsidered. The first section challenges the
traditional semiotic trichotomy, arguing that connectionist <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> architectures
destabilize established hierarchies of meaning, and proposes the Human-Machine
Communication (HMC) framework as a more suitable alternative. The second
section examines the tension between human-centred pragmatic theories and the
machine-centred nature of <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s. While traditional, Gricean-inspired pragmatics
continue to dominate, it relies on human-specific assumptions ill-suited to
predictive systems like <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s. Probabilistic pragmatics, particularly the
Rational Speech Act framework, offers a more compatible teleology by focusing
on optimization rather than truth-evaluation. The third section addresses the
issue of substitutionalism in three forms - generalizing, linguistic, and
communicative - highlighting the anthropomorphic biases that distort <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>
evaluation and obscure the role of human communicative subjects. Finally, the
paper introduces the concept of context frustration to describe the paradox of
increased contextual input paired with a collapse in contextual understanding,
emphasizing how users are compelled to co-construct pragmatic conditions both
for the model and themselves. These arguments suggest that pragmatic theory may
need to be adjusted or expanded to better account for <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> involving
generative AI.</p>
<h2 id="llm-serving-optimization-with-variable-prefill-and-decode-lengths">LLM Serving Optimization with Variable Prefill and Decode Lengths</h2>
<blockquote>
<p>Authors: Meixuan Wang, Yinyu Ye, Zijie Zhou</p>
<p>2025-08-08</p>
<p><a href="http://arxiv.org/abs/2508.06133v1">http://arxiv.org/abs/2508.06133v1</a></p>
</blockquote>
<p>We study the problem of serving <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> (Large Language Model) requests where
each request has heterogeneous prefill and decode lengths. In <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a> serving, the
prefill length corresponds to the input prompt length, which determines the
initial memory usage in the <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> cache. The decode length refers to the number of
output tokens generated sequentially, with each additional token increasing the
<a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> cache memory usage by one unit. Given a set of n requests, our goal is to
schedule and process them to minimize the total completion time. We show that
this problem is NP-hard due to the interplay of batching, placement
constraints, precedence relationships, and linearly increasing memory usage. We
then analyze commonly used scheduling strategies in practice, such as
First-Come-First-Serve (FCFS) and Shortest-First (SF), and prove that their
competitive ratios scale up sublinearly with the memory limit-a significant
drawback in real-world settings where memory demand is large. To address this,
we propose a novel algorithm based on a new selection metric that efficiently
forms batches over time. We prove that this algorithm achieves a constant
competitive ratio. Finally, we develop and evaluate a few algorithm variants
inspired by this approach, including dynamic programming variants, local search
methods, and an LP-based scheduler, demonstrating through comprehensive
simulations that they outperform standard baselines while maintaining
computational efficiency.</p>
<h2 id="you-dont-need-pre-built-graphs-for-rag-retrieval-augmented-generation-with-adaptive-reasoning-structures">You Don't Need Pre-built Graphs for RAG Retrieval Augmented Generation with Adaptive Reasoning Structures</h2>
<blockquote>
<p>Authors: Shengyuan Chen, Chuang Zhou, Zheng Yuan, Qinggang Zhang, Zeyang Cui, Hao Chen, Yilin Xiao, Jiannong Cao, Xiao Huang</p>
<p>2025-08-08</p>
<p><a href="http://arxiv.org/abs/2508.06105v1">http://arxiv.org/abs/2508.06105v1</a></p>
</blockquote>
<p>Large language models (<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) often suffer from hallucination, generating
factually incorrect statements when handling questions beyond their knowledge
and perception. Retrieval-augmented generation (RAG) addresses this by
retrieving query-relevant contexts from knowledge bases to support <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>
reasoning. Recent advances leverage pre-constructed graphs to capture the
relational connections among distributed documents, showing remarkable
performance in complex tasks. However, existing Graph-based RAG (GraphRAG)
methods rely on a costly process to transform the corpus into a graph,
introducing overwhelming token cost and update latency. Moreover, real-world
queries vary in type and complexity, requiring different logic structures for
accurate reasoning. The pre-built graph may not align with these required
structures, resulting in ineffective knowledge retrieval. To this end, we
propose a \textbf{\underline{Logic}}-aware
\textbf{\underline{R}}etrieval-\textbf{\underline{A}}ugmented
\textbf{\underline{G}}eneration framework (\textbf{LogicRAG}) that dynamically
extracts reasoning structures at inference time to guide adaptive retrieval
without any pre-built graph. LogicRAG begins by decomposing the input query
into a set of subproblems and constructing a directed acyclic graph (DAG) to
model the logical dependencies among them. To support coherent multi-step
reasoning, LogicRAG then linearizes the graph using topological sort, so that
subproblems can be addressed in a logically consistent order. Besides, LogicRAG
applies graph <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> to reduce redundant retrieval and uses context <a class="glightbox" href="https://img.shields.io/badge/pruning-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/pruning-F08080" /></a> to
filter irrelevant context, significantly reducing the overall token cost.
Extensive experiments demonstrate that LogicRAG achieves both superior
performance and efficiency compared to state-of-the-art baselines.</p>
<h2 id="conlangcrafter-constructing-languages-with-a-multi-hop-llm-pipeline">ConlangCrafter Constructing Languages with a Multi-Hop LLM Pipeline</h2>
<blockquote>
<p>Authors: Morris Alper, Moran Yanuka, Raja Giryes, Gaper Begu</p>
<p>2025-08-08</p>
<p><a href="http://arxiv.org/abs/2508.06094v1">http://arxiv.org/abs/2508.06094v1</a></p>
</blockquote>
<p>Constructed languages (conlangs) such as Esperanto and Quenya have played
diverse roles in art, philosophy, and international <a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a>. Meanwhile,
large-scale foundation models have revolutionized creative generation in text,
images, and beyond. In this work, we leverage modern <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s as computational
creativity aids for end-to-end conlang creation. We introduce ConlangCrafter, a
multi-hop pipeline that decomposes language design into modular stages --
phonology, morphology, syntax, lexicon generation, and translation. At each
stage, our method leverages <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s' meta-linguistic reasoning capabilities,
injecting randomness to encourage diversity and leveraging self-refinement
feedback to encourage consistency in the emerging language description. We
evaluate ConlangCrafter on metrics measuring coherence and typological
diversity, demonstrating its ability to produce coherent and varied conlangs
without human linguistic expertise.</p>
<h2 id="diffusion-llms-can-do-faster-than-ar-inference-via-discrete-diffusion-forcing">Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion Forcing</h2>
<blockquote>
<p>Authors: Xu Wang, Chenkai Xu, Yijie Jin, Jiachun Jin, Hao Zhang, Zhijie Deng</p>
<p>2025-08-08</p>
<p><a href="http://arxiv.org/abs/2508.09192v1">http://arxiv.org/abs/2508.09192v1</a></p>
</blockquote>
<p>Diffusion Large Language Models (d<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s) have emerged as a promising
alternative to autoregressive (AR) <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s for text generation, with the potential
to decode multiple tokens in a single iteration. However, none of the existing
open-source d<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s have achieved superior inference speed over AR <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s of
similar size. This paper breaks this barrier based on a simple and effective
strategy named discrete diffusion forcing (D2F). D2F equips d<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s with two key
capabilities: (1) block-wise autoregressive generation to enable <a class="glightbox" href="https://img.shields.io/badge/KV-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/KV-F08080" /></a> cache
utilization; (2) prediction of following tokens without requiring completion of
prior blocks for inter-block parallel decoding. In this way, the vanilla d<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s
are refurbished into an AR-diffusion hybrid paradigm for efficient inference.
D2F can be implemented with an asymmetric distillation process based on
pre-trained d<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s. We further propose a pipelined parallel decoding algorithm,
which enables a trade-off between efficiency and efficacy. Empirically, D2F
d<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s achieve more than <script type="math/tex">\mathbf{2.5\times}</script> inference speed than LLaMA3 and
Qwen2.5 on GSM8K. Compared to vanilla d<a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>s like LLaDA and Dream, the
<a class="glightbox" href="https://img.shields.io/badge/acceleration-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/acceleration-F08080" /></a> can be more than <script type="math/tex">\mathbf{50\times}</script> while maintaining comparable
output quality. The code is available at
https://github.com/zhijie-group/Discrete-Diffusion-Forcing.</p>
<h2 id="prosocial-behavior-detection-in-player-game-chat-from-aligning-human-ai-definitions-to-efficient-annotation-at-scale">Prosocial Behavior Detection in Player Game Chat From Aligning Human-AI Definitions to Efficient Annotation at Scale</h2>
<blockquote>
<p>Authors: Rafal Kocielnik, Min Kim, Penphob, Boonyarungsrit, Fereshteh Soltani, Deshawn Sambrano, Animashree Anandkumar, R. Michael Alvarez</p>
<p>2025-08-08</p>
<p><a href="http://arxiv.org/abs/2508.05938v1">http://arxiv.org/abs/2508.05938v1</a></p>
</blockquote>
<p>Detecting prosociality in text--<a class="glightbox" href="https://img.shields.io/badge/communication-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/communication-F08080" /></a> intended to affirm, support, or
improve others' behavior--is a novel and increasingly important challenge for
trust and safety systems. Unlike toxic content detection, prosociality lacks
well-established definitions and labeled data, requiring new approaches to both
annotation and deployment. We present a practical, three-stage pipeline that
enables scalable, high-precision prosocial content classification while
minimizing human labeling effort and inference costs. First, we identify the
best <a class="glightbox" href="https://img.shields.io/badge/LLM-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="key" src="https://img.shields.io/badge/LLM-FF8C00" /></a>-based labeling strategy using a small seed set of human-labeled
examples. We then introduce a human-AI refinement loop, where annotators review
high-disagreement cases between GPT-4 and humans to iteratively clarify and
expand the task definition-a critical step for emerging annotation tasks like
prosociality. This process results in improved label quality and definition
alignment. Finally, we synthesize 10k high-quality labels using GPT-4 and train
a two-stage inference system: a lightweight classifier handles high-confidence
predictions, while only <script type="math/tex">\sim</script>35\% of ambiguous instances are escalated to
GPT-4o. This architecture reduces inference costs by <script type="math/tex">\sim</script>70% while achieving
high precision (<script type="math/tex">\sim</script>0.90). Our pipeline demonstrates how targeted human-AI
interaction, careful task formulation, and deployment-aware architecture design
can unlock scalable solutions for novel responsible AI tasks.</p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../../cls_author/" class="btn btn-neutral float-left" title="By Author"><span class="icon icon-circle-arrow-left"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../../cls_author/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
  </span>
</div>
    <script src="../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../..";</script>
    <script src="../../js/theme_extra.js"></script>
    <script src="../../js/theme.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-clike.min.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
      <script src="../../js/prism-prototxt.js"></script>
      <script src="../../js/preview.js"></script>
      <script src="../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
</script></body>
</html>
