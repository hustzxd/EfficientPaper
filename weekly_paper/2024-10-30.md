# 2024-10-30

# Table of Contents
* [Effective Guidance for Model Attention with Simple Yes-no Annotations](#Effective-Guidance-for-Model-Attention-with-Simple-Yes-no-Annotations)
* [The Impact of Inference Acceleration Strategies on Bias of LLMs](#The-Impact-of-Inference-Acceleration-Strategies-on-Bias-of-LLMs)
* [HRPVT High-Resolution Pyramid Vision Transformer for medium and small-scale human pose estimation](#HRPVT-High-Resolution-Pyramid-Vision-Transformer-for-medium-and-small-scale-human-pose-estimation)
* [Reliable and Compact Graph Fine-tuning via GraphSparse Prompting](#Reliable-and-Compact-Graph-Fine-tuning-via-GraphSparse-Prompting)
* [MotionGPT-2 A General-Purpose Motion-Language Model for Motion Generation and Understanding](#MotionGPT-2-A-General-Purpose-Motion-Language-Model-for-Motion-Generation-and-Understanding)
* [Quarter-quantized thermal Hall effect with parity anomaly](#Quarter-quantized-thermal-Hall-effect-with-parity-anomaly)
* [Multiple-beam Interference Spectroscopy Instrument Analysis and Spectrum Reconstruction](#Multiple-beam-Interference-Spectroscopy-Instrument-Analysis-and-Spectrum-Reconstruction)
* [Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups](#Efficient-Training-of-Sparse-Autoencoders-for-Large-Language-Models-via-Layer-Groups)
* [ShadowKV KV Cache in Shadows for High-Throughput Long-Context LLM Inference](#ShadowKV-KV-Cache-in-Shadows-for-High-Throughput-Long-Context-LLM-Inference)
* [Constrained Transformer-Based Porous Media Generation to Spatial Distribution of Rock Properties](#Constrained-Transformer-Based-Porous-Media-Generation-to-Spatial-Distribution-of-Rock-Properties)
* [Arithmetic Without Algorithms Language Models Solve Math With a Bag of Heuristics](#Arithmetic-Without-Algorithms-Language-Models-Solve-Math-With-a-Bag-of-Heuristics)
* [EoRA Training-free Compensation for Compressed LLM with Eigenspace Low-Rank Approximation](#EoRA-Training-free-Compensation-for-Compressed-LLM-with-Eigenspace-Low-Rank-Approximation)
* [Reconstructing dynamics from sparse observations with no training on target system](#Reconstructing-dynamics-from-sparse-observations-with-no-training-on-target-system)
* [EMOCPD Efficient Attention-based Models for Computational Protein Design Using Amino Acid Microenvironment](#EMOCPD-Efficient-Attention-based-Models-for-Computational-Protein-Design-Using-Amino-Acid-Microenvironment)
* [SparseTem Boosting the Efficiency of CNN-Based Video Encoders by Exploiting Temporal Continuity](#SparseTem-Boosting-the-Efficiency-of-CNN-Based-Video-Encoders-by-Exploiting-Temporal-Continuity)
* [Statistical Inference in High-dimensional Poisson Regression with Applications to Mediation Analysis](#Statistical-Inference-in-High-dimensional-Poisson-Regression-with-Applications-to-Mediation-Analysis)
* [Unsupervised Panoptic Interpretation of Latent Spaces in GANs Using Space-Filling Vector Quantization](#Unsupervised-Panoptic-Interpretation-of-Latent-Spaces-in-GANs-Using-Space-Filling-Vector-Quantization)
* [ARLON Boosting Diffusion Transformers with Autoregressive Models for Long Video Generation](#ARLON-Boosting-Diffusion-Transformers-with-Autoregressive-Models-for-Long-Video-Generation)
* [Uncovering Capabilities of Model Pruning in Graph Contrastive Learning](#Uncovering-Capabilities-of-Model-Pruning-in-Graph-Contrastive-Learning)
* [Accelerating Direct Preference Optimization with Prefix Sharing](#Accelerating-Direct-Preference-Optimization-with-Prefix-Sharing)
* [Sparse Decomposition of Graph Neural Networks](#Sparse-Decomposition-of-Graph-Neural-Networks)
* [GeoLLaVA Efficient Fine-Tuned Vision-Language Models for Temporal Change Detection in Remote Sensing](#GeoLLaVA-Efficient-Fine-Tuned-Vision-Language-Models-for-Temporal-Change-Detection-in-Remote-Sensing)
* [Learning ID-free Item Representation with Token Crossing for Multimodal Recommendation](#Learning-ID-free-Item-Representation-with-Token-Crossing-for-Multimodal-Recommendation)
* [Ripple Accelerating LLM Inference on Smartphones with Correlation-Aware Neuron Management](#Ripple-Accelerating-LLM-Inference-on-Smartphones-with-Correlation-Aware-Neuron-Management)
* [Read-ME Refactorizing LLMs as Router-Decoupled Mixture of Experts with System Co-Design](#Read-ME-Refactorizing-LLMs-as-Router-Decoupled-Mixture-of-Experts-with-System-Co-Design)
* [TesseraQ Ultra Low-Bit LLM Post-Training Quantization with Block Reconstruction](#TesseraQ-Ultra-Low-Bit-LLM-Post-Training-Quantization-with-Block-Reconstruction)
* [PixelGaussian Generalizable 3D Gaussian Reconstruction from Arbitrary Views](#PixelGaussian-Generalizable-3D-Gaussian-Reconstruction-from-Arbitrary-Views)
* [Large Spatial Model End-to-end Unposed Images to Semantic 3D](#Large-Spatial-Model-End-to-end-Unposed-Images-to-Semantic-3D)
* [Dynamic Vocabulary Pruning in Early-Exit LLMs](#Dynamic-Vocabulary-Pruning-in-Early-Exit-LLMs)
* [Dynamic 3D Gaussian Tracking for Graph-Based Neural Dynamics Modeling](#Dynamic-3D-Gaussian-Tracking-for-Graph-Based-Neural-Dynamics-Modeling)
* [Hypersurface deformations](#Hypersurface-deformations)
* [Sliding DFT-based Signal Recovery for Modulo ADC with 1-bit Folding Information](#Sliding-DFT-based-Signal-Recovery-for-Modulo-ADC-with-1-bit-Folding-Information)
* [PESFormer Boosting Macro- and Micro-expression Spotting with Direct Timestamp Encoding](#PESFormer-Boosting-Macro--and-Micro-expression-Spotting-with-Direct-Timestamp-Encoding)
* [The Nature of Mathematical Modeling and Probabilistic Optimization Engineering in Generative AI](#The-Nature-of-Mathematical-Modeling-and-Probabilistic-Optimization-Engineering-in-Generative-AI)
* [Causal Order Discovery based on Monotonic SCMs](#Causal-Order-Discovery-based-on-Monotonic-SCMs)
* [CoreInfer Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation](#CoreInfer-Accelerating-Large-Language-Model-Inference-with-Semantics-Inspired-Adaptive-Sparse-Activation)
* [LEGO Language Model Building Blocks](#LEGO-Language-Model-Building-Blocks)
* [Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration](#Leveraging-Skills-from-Unlabeled-Prior-Data-for-Efficient-Online-Exploration)
* [ExpertFlow Optimized Expert Activation and Token Allocation for Efficient Mixture-of-Experts Inference](#ExpertFlow-Optimized-Expert-Activation-and-Token-Allocation-for-Efficient-Mixture-of-Experts-Inference)
* [Quantizable Ghost-ridden theories using Kinetic Positivity Constraints](#Quantizable-Ghost-ridden-theories-using-Kinetic-Positivity-Constraints)
* [Identifiable Representation and Model Learning for Latent Dynamic Systems](#Identifiable-Representation-and-Model-Learning-for-Latent-Dynamic-Systems)
* [Att2CPC Attention-Guided Lossy Attribute Compression of Point Clouds](#Att2CPC-Attention-Guided-Lossy-Attribute-Compression-of-Point-Clouds)
* [Anomaly Resilient Temporal QoS Prediction using Hypergraph Convoluted Transformer Network](#Anomaly-Resilient-Temporal-QoS-Prediction-using-Hypergraph-Convoluted-Transformer-Network)
* [Escaping the Forest Sparse Interpretable Neural Networks for Tabular Data](#Escaping-the-Forest-Sparse-Interpretable-Neural-Networks-for-Tabular-Data)
* [Beware of Calibration Data for Pruning Large Language Models](#Beware-of-Calibration-Data-for-Pruning-Large-Language-Models)
* [PETAH Parameter Efficient Task Adaptation for Hybrid Transformers in a resource-limited Context](#PETAH-Parameter-Efficient-Task-Adaptation-for-Hybrid-Transformers-in-a-resource-limited-Context)
* [AutoRNet Automatically Optimizing Heuristics for Robust Network Design via Large Language Models](#AutoRNet-Automatically-Optimizing-Heuristics-for-Robust-Network-Design-via-Large-Language-Models)
* [Process Supervision-Guided Policy Optimization for Code Generation](#Process-Supervision-Guided-Policy-Optimization-for-Code-Generation)
* [Adaptive Wireless Image Semantic Transmission Design, Simulation, and Prototype Validation](#Adaptive-Wireless-Image-Semantic-Transmission-Design,-Simulation,-and-Prototype-Validation)
* [Congestion Forecast for Trains with Railroad-Graph-based Semi-Supervised Learning using Sparse Passenger Reports](#Congestion-Forecast-for-Trains-with-Railroad-Graph-based-Semi-Supervised-Learning-using-Sparse-Passenger-Reports)
* [PtychoFormer A Transformer-based Model for Ptychographic Phase Retrieval](#PtychoFormer-A-Transformer-based-Model-for-Ptychographic-Phase-Retrieval)
* [LVSM A Large View Synthesis Model with Minimal 3D Inductive Bias](#LVSM-A-Large-View-Synthesis-Model-with-Minimal-3D-Inductive-Bias)
* [Can General-Purpose Large Language Models Generalize to English-Thai Machine Translation ?](#Can-General-Purpose-Large-Language-Models-Generalize-to-English-Thai-Machine-Translation-?)
* [Math Neurosurgery Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes](#Math-Neurosurgery-Isolating-Language-Models'-Math-Reasoning-Abilities-Using-Only-Forward-Passes)
* [Pyramid Vector Quantization for LLMs](#Pyramid-Vector-Quantization-for-LLMs)
* [Semantic-guided Search for Efficient Program Repair with Large Language Models](#Semantic-guided-Search-for-Efficient-Program-Repair-with-Large-Language-Models)
* [Bayesian High-dimensional Linear Regression with Sparse Projection-posterior](#Bayesian-High-dimensional-Linear-Regression-with-Sparse-Projection-posterior)
* [Does your LLM truly unlearn? An embarrassingly simple approach to recover unlearned knowledge](#Does-your-LLM-truly-unlearn?-An-embarrassingly-simple-approach-to-recover-unlearned-knowledge)
* [Improving Neuron-level Interpretability with White-box Language Models](#Improving-Neuron-level-Interpretability-with-White-box-Language-Models)
* [MagicPIG LSH Sampling for Efficient LLM Generation](#MagicPIG-LSH-Sampling-for-Efficient-LLM-Generation)
* [PODTILE Facilitating Podcast Episode Browsing with Auto-generated Chapters](#PODTILE-Facilitating-Podcast-Episode-Browsing-with-Auto-generated-Chapters)
* [Beyond 24 exploring VNM sparsity for efficient transformer inference on GPUs](#Beyond-24-exploring-VNM-sparsity-for-efficient-transformer-inference-on-GPUs)
* [Continuous Speech Synthesis using per-token Latent Diffusion](#Continuous-Speech-Synthesis-using-per-token-Latent-Diffusion)
* [Steering Knowledge Selection Behaviours in LLMs via SAE-Based Representation Engineering](#Steering-Knowledge-Selection-Behaviours-in-LLMs-via-SAE-Based-Representation-Engineering)
* [Residual vector quantization for KV cache compression in large language model](#Residual-vector-quantization-for-KV-cache-compression-in-large-language-model)
* [A quantum anchor for higher Koszul brackets](#A-quantum-anchor-for-higher-Koszul-brackets)
* [Pruning Foundation Models for High Accuracy without Retraining](#Pruning-Foundation-Models-for-High-Accuracy-without-Retraining)
* [Bayesian Concept Bottleneck Models with LLM Priors](#Bayesian-Concept-Bottleneck-Models-with-LLM-Priors)


## Effective Guidance for Model Attention with Simple Yes-no Annotations

>Authors: Seongmin Lee, Ali Payani, Duen Horng, Chau

>2024-10-29

> http://arxiv.org/abs/2410.22312v1

Modern deep learning models often make predictions by focusing on irrelevant
areas, leading to biased performance and limited generalization. Existing
methods aimed at rectifying model attention require explicit labels for
irrelevant areas or complex pixel-wise ground truth attention maps. We present
CRAYON (Correcting Reasoning with Annotations of Yes Or No), offering
effective, scalable, and practical solutions to rectify model attention using
simple yes-no annotations. CRAYON empowers classical and modern model
interpretation techniques to identify and guide model reasoning:
CRAYON-ATTENTION directs classic interpretations based on saliency maps to
focus on relevant image regions, while CRAYON-PRUNING removes irrelevant
neurons identified by modern concept-based methods to mitigate their influence.
Through extensive experiments with both quantitative and human evaluation, we
showcase CRAYON's effectiveness, scalability, and practicality in refining
model attention. CRAYON achieves state-of-the-art performance, outperforming 12
methods across 3 benchmark datasets, surpassing approaches that require more
complex annotations.


## The Impact of Inference Acceleration Strategies on Bias of LLMs

>Authors: Elisabeth Kirsten, Ivan Habernal, Vedant Nanda, Muhammad Bilal Zafar

>2024-10-29

> http://arxiv.org/abs/2410.22118v1

Last few years have seen unprecedented advances in capabilities of Large
Language Models (LLMs). These advancements promise to deeply benefit a vast
array of application domains. However, due to their immense size, performing
inference with LLMs is both costly and slow. Consequently, a plethora of recent
work has proposed strategies to enhance inference efficiency, e.g.,
**quantization**, **pruning**, and caching. These acceleration strategies reduce the
inference cost and latency, often by several factors, while maintaining much of
the predictive performance measured via common benchmarks. In this work, we
explore another critical aspect of LLM performance: demographic bias in model
generations due to inference acceleration optimizations. Using a wide range of
metrics, we probe bias in model outputs from a number of angles. Analysis of
outputs before and after inference acceleration shows significant change in
bias. Worryingly, these bias effects are complex and unpredictable. A
combination of an acceleration strategy and bias type may show little bias
change in one model but may lead to a large effect in another. Our results
highlight a need for in-depth and case-by-case evaluation of model bias after
it has been modified to accelerate inference.


## HRPVT High-Resolution Pyramid Vision Transformer for medium and small-scale human pose estimation

>Authors: Zhoujie Xu

>2024-10-29

> http://arxiv.org/abs/2410.22079v1

Human pose estimation on medium and small scales has long been a significant
challenge in this field. Most existing methods focus on restoring
high-resolution feature maps by stacking multiple costly deconvolutional layers
or by continuously aggregating semantic information from low-resolution feature
maps while maintaining high-resolution ones, which can lead to information
redundancy. Additionally, due to **quantization** errors, heatmap-based methods
have certain disadvantages in accurately locating keypoints of medium and
small-scale human figures. In this paper, we propose HRPVT, which utilizes PVT
v2 as the backbone to model long-range dependencies. Building on this, we
introduce the High-Resolution Pyramid Module (HRPM), designed to generate
higher quality high-resolution representations by incorporating the intrinsic
inductive biases of Convolutional Neural Networks (CNNs) into the
high-resolution feature maps. The integration of HRPM enhances the performance
of pure transformer-based models for human pose estimation at medium and small
scales. Furthermore, we replace the heatmap-based method with SimCC approach,
which eliminates the need for costly upsampling layers, thereby allowing us to
allocate more computational resources to HRPM. To accommodate models with
varying parameter scales, we have developed two insertion strategies of HRPM,
each designed to enhancing the model's ability to perceive medium and
small-scale human poses from two distinct perspectives.


## Reliable and Compact Graph Fine-tuning via GraphSparse Prompting

>Authors: Bo Jiang, Hao Wu, Beibei Wang, Jin Tang, Bin Luo

>2024-10-29

> http://arxiv.org/abs/2410.21749v1

Recently, graph prompt learning has garnered increasing attention in adapting
pre-trained GNN models for downstream graph learning tasks. However, existing
works generally conduct prompting over all graph elements (e.g., nodes, edges,
node attributes, etc.), which is suboptimal and obviously redundant. To address
this issue, we propose exploiting **sparse** representation theory for graph
prompting and present Graph Sparse Prompting (GSP). GSP aims to adaptively and
**sparse**ly select the optimal elements (e.g., certain node attributes) to achieve
compact prompting for downstream tasks. Specifically, we propose two kinds of
GSP models, termed Graph Sparse Feature Prompting (GSFP) and Graph Sparse
multi-Feature Prompting (GSmFP). Both GSFP and GSmFP provide a general scheme
for tuning any specific pre-trained GNNs that can achieve attribute selection
and compact prompt learning simultaneously. A simple yet effective algorithm
has been designed for solving GSFP and GSmFP models. Experiments on 16
widely-used benchmark datasets validate the effectiveness and advantages of the
proposed GSFPs.


## MotionGPT-2 A General-Purpose Motion-Language Model for Motion Generation and Understanding

>Authors: Yuan Wang, Di Huang, Yaqi Zhang, Wanli Ouyang, Jile Jiao, Xuetao Feng, Yan Zhou, Pengfei Wan, Shixiang Tang, Dan Xu

>2024-10-29

> http://arxiv.org/abs/2410.21747v1

Generating lifelike human motions from descriptive texts has experienced
remarkable research focus in the recent years, propelled by the emerging
requirements of digital humans.Despite impressive advances, existing approaches
are often constrained by limited control modalities, task specificity, and
focus solely on body motion representations.In this paper, we present
MotionGPT-2, a unified Large Motion-Language Model (LMLM) that addresses these
limitations. MotionGPT-2 accommodates multiple motion-relevant tasks and
supporting multimodal control conditions through pre-trained Large Language
Models (LLMs). It **quantize**s multimodal inputs-such as text and single-frame
poses-into discrete, LLM-interpretable tokens, seamlessly integrating them into
the LLM's vocabulary. These tokens are then organized into unified prompts,
guiding the LLM to generate motion outputs through a
pretraining-then-finetuning paradigm. We also show that the proposed
MotionGPT-2 is highly adaptable to the challenging 3D holistic motion
generation task, enabled by the innovative motion discretization framework,
Part-Aware VQVAE, which ensures fine-grained representations of body and hand
movements. Extensive experiments and visualizations validate the effectiveness
of our method, demonstrating the adaptability of MotionGPT-2 across motion
generation, motion captioning, and generalized motion completion tasks.


## Quarter-quantized thermal Hall effect with parity anomaly

>Authors: Yu-Hao Wan, Qing-Feng Sun

>2024-10-29

> http://arxiv.org/abs/2410.21686v1

We show that in the proximity of s-wave superconductors, the magnetic
topological surface states can transform into Majorana surface state, featuring
a single gapless Majorana cone with parity anomaly when the superconducting
pairing gap matches the surface magnetization gap. The emergence of $N=1/2$
Majorana chiral edge current is observed at the boundaries between the gap
region and the gapless region. Additionally, in systems with a single gapless
Majorana cone, a quarter-**quantize**d thermal Hall conductance appears under the
dephasing. By mapping the system to a conductor-network model, we identify the
appearance of 1/4 chiral heat channels as the cause of the quarter-**quantize**d
Hall thermal conductance. We observe the stability of this quarter-**quantize**d
thermal conductance under temperature variations, serving as a distinctive
feature indicating the presence of a single gapless Majorana cone in the
system. Our models can be experimentally realized using magnetic topological
insulators or iron-based superconductors.


## Multiple-beam Interference Spectroscopy Instrument Analysis and Spectrum Reconstruction

>Authors: Mohamad Jouni, Daniele Picone, Mauro Dalla Mura

>2024-10-28

> http://arxiv.org/abs/2410.21586v1

Hyperspectral imaging systems based on multiple-beam interference (MBI), such
as Fabry-Perot interferometry, are attracting interest due to their compact
design, high throughput, and fine resolution. Unlike dispersive devices, which
measure spectra directly, the desired spectra in interferometric systems are
reconstructed from measured interferograms. Although the response of MBI
devices is modeled by the Airy function, existing reconstruction techniques are
often limited to Fourier-transform spectroscopy, which is tailored for two-beam
interference (TBI). These methods impose limitations for MBI and are
susceptible to non-idealities like irregular sampling and noise, highlighting
the need for an in-depth numerical framework. To fill this gap, we propose a
rigorous taxonomy of the TBI and MBI instrument description and propose a
unified Bayesian formulation which both embeds the description of existing
literature works and adds some of the real-world non-idealities of the
acquisition process. Under this framework, we provide a comprehensive review of
spectroscopy forward and inverse models. In the forward model, we propose a
thorough analysis of the discretization of the continuous model and the
ill-posedness of the problem. In the inverse model, we extend the range of
existing solutions for spectrum reconstruction, framing them as an optimization
problem. Specifically, we provide a progressive comparative analysis of
reconstruction methods from more specific to more general scenarios, up to
employing the proposed Bayesian framework with prior knowledge, such as
**sparsity** constraints. Experiments on simulated and real data demonstrate the
framework's flexibility and noise robustness. The code is available at
https://github.com/mhmdjouni/inverspyctrometry.


## Efficient Training of Sparse Autoencoders for Large Language Models via Layer Groups

>Authors: Davide Ghilardi, Federico Belotti, Marco Molinari

>2024-10-28

> http://arxiv.org/abs/2410.21508v1

Sparse AutoEnocders (SAEs) have recently been employed as an unsupervised
approach for understanding the inner workings of Large Language Models (LLMs).
They reconstruct the model's activations with a **sparse** linear combination of
interpretable features. However, training SAEs is computationally intensive,
especially as models grow in size and complexity. To address this challenge, we
propose a novel training strategy that reduces the number of trained SAEs from
one per layer to one for a given group of contiguous layers. Our experimental
results on Pythia 160M highlight a speedup of up to 6x without compromising the
reconstruction quality and performance on downstream tasks. Therefore, layer
clustering presents an efficient approach to train SAEs in modern LLMs.


## ShadowKV KV Cache in Shadows for High-Throughput Long-Context LLM Inference

>Authors: Hanshi Sun, Li-Wen Chang, Wenlei Bao, Size Zheng, Ningxin Zheng, Xin Liu, Harry Dong, Yuejie Chi, Beidi Chen

>2024-10-28

> http://arxiv.org/abs/2410.21465v1

With the widespread deployment of long-context large language models (LLMs),
there has been a growing demand for efficient support of high-throughput
inference. However, as the key-value (KV) cache expands with the sequence
length, the increasing memory footprint and the need to access it for each
token generation both result in low throughput when serving long-context LLMs.
While various dynamic **sparse** attention methods have been proposed to speed up
inference while maintaining generation quality, they either fail to
sufficiently reduce GPU memory consumption or introduce significant decoding
latency by offloading the KV cache to the CPU. We present ShadowKV, a
high-throughput long-context LLM inference system that stores the low-rank key
cache and offloads the value cache to reduce the memory footprint for larger
batch sizes and longer sequences. To minimize decoding latency, ShadowKV
employs an accurate KV selection strategy that reconstructs minimal **sparse** KV
pairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,
including RULER, LongBench, and Needle In A Haystack, and models like
Llama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and
Qwen2-7B-128K, we demonstrate that it can support up to 6$\times$ larger batch
sizes and boost throughput by up to 3.04$\times$ on an A100 GPU without
sacrificing accuracy, even surpassing the performance achievable with infinite
batch size under the assumption of infinite GPU memory. The code is available
at https://github.com/bytedance/ShadowKV.


## Constrained Transformer-Based Porous Media Generation to Spatial Distribution of Rock Properties

>Authors: Zihan Ren, Sanjay Srinivasan, Dustin Crandall

>2024-10-28

> http://arxiv.org/abs/2410.21462v1

Pore-scale modeling of rock images based on information in 3D micro-computed
tomography data is crucial for studying complex subsurface processes such as
CO2 and brine multiphase flow during Geologic Carbon Storage (GCS). While deep
learning models can generate 3D rock microstructures that match static rock
properties, they have two key limitations: they don't account for the spatial
distribution of rock properties that can have an important influence on the
flow and transport characteristics (such as permeability and relative
permeability) of the rock and they generate structures below the representative
elementary volume (REV) scale for those transport properties. Addressing these
issues is crucial for building a consistent workflow between pore-scale
analysis and field-scale modeling. To address these challenges, we propose a
two-stage modeling framework that combines a Vector Quantized Variational
Autoencoder (VQVAE) and a transformer model for spatial upscaling and
arbitrary-size 3D porous media reconstruction in an autoregressive manner. The
VQVAE first compresses and **quantize**s sub-volume training images into
low-dimensional tokens, while we train a transformer to spatially assemble
these tokens into larger images following specific spatial order. By employing
a multi-token generation strategy, our approach preserves both sub-volume
integrity and spatial relationships among these sub-image patches. We
demonstrate the effectiveness of our multi-token transformer generation
approach and validate it using real data from a test well, showcasing its
potential to generate models for the porous media at the well scale using only
a spatial porosity model. The interpolated representative porous media that
reflect field-scale geological properties accurately model transport
properties, including permeability and multiphase flow relative permeability of
CO2 and brine.


## Arithmetic Without Algorithms Language Models Solve Math With a Bag of Heuristics

>Authors: Yaniv Nikankin, Anja Reusch, Aaron Mueller, Yonatan Belinkov

>2024-10-28

> http://arxiv.org/abs/2410.21272v1

Do large language models (LLMs) solve reasoning tasks by learning robust
generalizable algorithms, or do they memorize training data? To investigate
this question, we use arithmetic reasoning as a representative task. Using
causal analysis, we identify a subset of the model (a circuit) that explains
most of the model's behavior for basic arithmetic logic and examine its
functionality. By zooming in on the level of individual circuit neurons, we
discover a **sparse** set of important neurons that implement simple heuristics.
Each heuristic identifies a numerical input pattern and outputs corresponding
answers. We hypothesize that the combination of these heuristic neurons is the
mechanism used to produce correct arithmetic answers. To test this, we
categorize each neuron into several heuristic types-such as neurons that
activate when an operand falls within a certain range-and find that the
unordered combination of these heuristic types is the mechanism that explains
most of the model's accuracy on arithmetic prompts. Finally, we demonstrate
that this mechanism appears as the main source of arithmetic accuracy early in
training. Overall, our experimental results across several LLMs show that LLMs
perform arithmetic using neither robust algorithms nor memorization; rather,
they rely on a "bag of heuristics".


## EoRA Training-free Compensation for Compressed LLM with Eigenspace Low-Rank Approximation

>Authors: Shih-Yang Liu, Huck Yang, Chein-Yi Wang, Nai Chit Fung, Hongxu Yin, Charbel Sakr, Saurav Muralidharan, Kwang-Ting Cheng, Jan Kautz, Yu-Chiang Frank Wang, Pavlo Molchanov, Min-Hung Chen

>2024-10-28

> http://arxiv.org/abs/2410.21271v1

In this work, we re-formulate the model compression problem into the
customized compensation problem: Given a compressed model, we aim to introduce
residual low-rank paths to compensate for compression errors under customized
requirements from users (e.g., tasks, compression ratios), resulting in greater
flexibility in adjusting overall capacity without being constrained by specific
compression formats. However, naively applying SVD to derive residual paths
causes suboptimal utilization of the low-rank representation capacity. Instead,
we propose Training-free Eigenspace Low-Rank Approximation (EoRA), a method
that directly minimizes compression-induced errors without requiring
gradient-based training, achieving fast optimization in minutes using a small
amount of calibration data. EoRA projects compression errors into the
eigenspace of input activations, leveraging eigenvalues to effectively
prioritize the reconstruction of high-importance error components. Moreover,
EoRA can be seamlessly integrated with fine-tuning and **quantization** to further
improve effectiveness and efficiency. EoRA consistently outperforms previous
methods in compensating errors for compressed LLaMA2/3 models on various tasks,
such as language generation, commonsense reasoning, and math reasoning tasks
(e.g., 31.31%/12.88% and 9.69% improvements on ARC-Easy/ARC-Challenge and
MathQA when compensating LLaMA3-8B that is **quantize**d to 4-bit and pruned to 2:4
**sparsity**). EoRA offers a scalable, training-free solution to compensate for
compression errors, making it a powerful tool to deploy LLMs in various
capacity and efficiency requirements.


## Reconstructing dynamics from sparse observations with no training on target system

>Authors: Zheng-Meng Zhai, Jun-Yin Huang, Benjamin D. Stern, Ying-Cheng Lai

>2024-10-28

> http://arxiv.org/abs/2410.21222v1

In applications, an anticipated situation is where the system of interest has
never been encountered before and **sparse** observations can be made only once.
Can the dynamics be faithfully reconstructed from the limited observations
without any training data? This problem defies any known traditional methods of
nonlinear time-series analysis as well as existing machine-learning methods
that typically require extensive data from the target system for training. We
address this challenge by developing a hybrid transformer and
reservoir-computing machine-learning scheme. The key idea is that, for a
complex and nonlinear target system, the training of the transformer can be
conducted not using any data from the target system, but with essentially
unlimited synthetic data from known chaotic systems. The trained transformer is
then tested with the **sparse** data from the target system. The output of the
transformer is further fed into a reservoir computer for predicting the
long-term dynamics or the attractor of the target system. The power of the
proposed hybrid machine-learning framework is demonstrated using a large number
of prototypical nonlinear dynamical systems, with high reconstruction accuracy
even when the available data is only 20% of that required to faithfully
represent the dynamical behavior of the underlying system. The framework
provides a paradigm of reconstructing complex and nonlinear dynamics in the
extreme situation where training data does not exist and the observations are
random and **sparse**.


## EMOCPD Efficient Attention-based Models for Computational Protein Design Using Amino Acid Microenvironment

>Authors: Xiaoqi Ling, Cheng Cai, Demin Kong, Zhisheng Wei, Jing Wu, Lei Wang, Zhaohong Deng

>2024-10-28

> http://arxiv.org/abs/2410.21069v2

Computational protein design (CPD) refers to the use of computational methods
to design proteins. Traditional methods relying on energy functions and
heuristic algorithms for sequence design are inefficient and do not meet the
demands of the big data era in biomolecules, with their accuracy limited by the
energy functions and search algorithms. Existing deep learning methods are
constrained by the learning capabilities of the networks, failing to extract
effective information from **sparse** protein structures, which limits the accuracy
of protein design. To address these shortcomings, we developed an Efficient
attention-based Models for Computational Protein Design using amino acid
microenvironment (EMOCPD). It aims to predict the category of each amino acid
in a protein by analyzing the three-dimensional atomic environment surrounding
the amino acids, and optimize the protein based on the predicted
high-probability potential amino acid categories. EMOCPD employs a multi-head
attention mechanism to focus on important features in the **sparse** protein
microenvironment and utilizes an inverse residual structure to optimize the
network architecture. The proposed EMOCPD achieves over 80% accuracy on the
training set and 68.33% and 62.32% accuracy on two independent test sets,
respectively, surpassing the best comparative methods by over 10%. In protein
design, the thermal stability and protein expression of the predicted mutants
from EMOCPD show significant improvements compared to the wild type,
effectively validating EMOCPD's potential in designing superior proteins.
Furthermore, the predictions of EMOCPD are influenced positively, negatively,
or have minimal impact based on the content of the 20 amino acids, categorizing
amino acids as positive, negative, or neutral. Research findings indicate that
EMOCPD is more suitable for designing proteins with lower contents of negative
amino acids.


## SparseTem Boosting the Efficiency of CNN-Based Video Encoders by Exploiting Temporal Continuity

>Authors: Kunyun Wang, Jieru Zhao, Shuo Yang, Wenchao Ding, Minyi Guo

>2024-10-28

> http://arxiv.org/abs/2410.20790v1

Deep learning models have become pivotal in the field of video processing and
is increasingly critical in practical applications such as autonomous driving
and object detection. Although Vision Transformers (ViTs) have demonstrated
their power, Convolutional Neural Networks (CNNs) remain a highly efficient and
high-performance choice for feature extraction and encoding. However, the
intensive computational demands of convolution operations hinder its broader
adoption as a video encoder. Given the inherent temporal continuity in video
frames, changes between consecutive frames are minimal, allowing for the
skipping of redundant computations. This technique, which we term as Diff
Computation, presents two primary challenges. First, Diff Computation requires
to cache intermediate feature maps to ensure the correctness of non-linear
computations, leading to significant memory consumption. Second, the imbalance
of **sparsity** among layers, introduced by Diff Computation, incurs accuracy
degradation. To address these issues, we propose a memory-efficient scheduling
method to eliminate memory overhead and an online adjustment mechanism to
minimize accuracy degradation. We integrate these techniques into our
framework, SparseTem, to seamlessly support various CNN-based video encoders.
SparseTem achieves speedup of 1.79x for EfficientDet and 4.72x for CRNN, with
minimal accuracy drop and no additional memory overhead. Extensive experimental
results demonstrate that SparseTem sets a new state-of-the-art by effectively
utilizing temporal continuity to accelerate CNN-based video encoders.


## Statistical Inference in High-dimensional Poisson Regression with Applications to Mediation Analysis

>Authors: Prabrisha Rakshit, Zijian Guo

>2024-10-28

> http://arxiv.org/abs/2410.20671v1

Large-scale datasets with count outcome variables are widely present in
various applications, and the Poisson regression model is among the most
popular models for handling count outcomes. This paper considers the
high-dimensional **sparse** Poisson regression model and proposes bias-corrected
estimators for both linear and quadratic transformations of high-dimensional
regression vectors. We establish the asymptotic normality of the estimators,
construct asymptotically valid confidence intervals, and conduct related
hypothesis testing. We apply the devised methodology to high-dimensional
mediation analysis with count outcome, with particular application of testing
for the existence of interaction between the treatment variable and
high-dimensional mediators. We demonstrate the proposed methods through
extensive simulation studies and application to real-world epigenetic data.


## Unsupervised Panoptic Interpretation of Latent Spaces in GANs Using Space-Filling Vector Quantization

>Authors: Mohammad Hassan Vali, Tom Bäckström

>2024-10-27

> http://arxiv.org/abs/2410.20573v1

Generative adversarial networks (GANs) learn a latent space whose samples can
be mapped to real-world images. Such latent spaces are difficult to interpret.
Some earlier supervised methods aim to create an interpretable latent space or
discover interpretable directions that require exploiting data labels or
annotated synthesized samples for training. However, we propose using a
modification of vector **quantization** called space-filling vector **quantization**
(SFVQ), which **quantize**s the data on a piece-wise linear curve. SFVQ can capture
the underlying morphological structure of the latent space and thus make it
interpretable. We apply this technique to model the latent space of pretrained
StyleGAN2 and BigGAN networks on various datasets. Our experiments show that
the SFVQ curve yields a general interpretable model of the latent space that
determines which part of the latent space corresponds to what specific
generative factors. Furthermore, we demonstrate that each line of SFVQ's curve
can potentially refer to an interpretable direction for applying intelligible
image transformations. We also showed that the points located on an SFVQ line
can be used for controllable data augmentation.


## ARLON Boosting Diffusion Transformers with Autoregressive Models for Long Video Generation

>Authors: Zongyi Li, Shujie Hu, Shujie Liu, Long Zhou, Jeongsoo Choi, Lingwei Meng, Xun Guo, Jinyu Li, Hefei Ling, Furu Wei

>2024-10-27

> http://arxiv.org/abs/2410.20502v1

Text-to-video models have recently undergone rapid and substantial
advancements. Nevertheless, due to limitations in data and computational
resources, achieving efficient generation of long videos with rich motion
dynamics remains a significant challenge. To generate high-quality, dynamic,
and temporally consistent long videos, this paper presents ARLON, a novel
framework that boosts diffusion Transformers with autoregressive models for
long video generation, by integrating the coarse spatial and long-range
temporal information provided by the AR model to guide the DiT model.
Specifically, ARLON incorporates several key innovations: 1) A latent Vector
Quantized Variational Autoencoder (VQ-VAE) compresses the input latent space of
the DiT model into compact visual tokens, bridging the AR and DiT models and
balancing the learning complexity and information density; 2) An adaptive
norm-based semantic injection module integrates the coarse discrete visual
units from the AR model into the DiT model, ensuring effective guidance during
video generation; 3) To enhance the tolerance capability of noise introduced
from the AR inference, the DiT model is trained with coarser visual latent
tokens incorporated with an uncertainty sampling module. Experimental results
demonstrate that ARLON significantly outperforms the baseline OpenSora-V1.2 on
eight out of eleven metrics selected from VBench, with notable improvements in
dynamic degree and aesthetic quality, while delivering competitive results on
the remaining three and simultaneously accelerating the generation process. In
addition, ARLON achieves state-of-the-art performance in long video generation.
Detailed analyses of the improvements in inference efficiency are presented,
alongside a practical application that demonstrates the generation of long
videos using progressive text prompts. See demos of ARLON at
\url{http://aka.ms/arlon}.


## Uncovering Capabilities of Model Pruning in Graph Contrastive Learning

>Authors: Wu Junran, Chen Xueyuan, Li Shangzhe

>2024-10-27

> http://arxiv.org/abs/2410.20356v1

Graph contrastive learning has achieved great success in pre-training graph
neural networks without ground-truth labels. Leading graph contrastive learning
follows the classical scheme of contrastive learning, forcing model to identify
the essential information from augmented views. However, general augmented
views are produced via random corruption or learning, which inevitably leads to
semantics alteration. Although domain knowledge guided augmentations alleviate
this issue, the generated views are domain specific and undermine the
generalization. In this work, motivated by the firm representation ability of
**sparse** model from **pruning**, we reformulate the problem of graph contrastive
learning via contrasting different model versions rather than augmented views.
We first theoretically reveal the superiority of model **pruning** in contrast to
data augmentations. In practice, we take original graph as input and
dynamically generate a perturbed graph encoder to contrast with the original
encoder by **pruning** its transformation weights. Furthermore, considering the
integrity of node embedding in our method, we are capable of developing a local
contrastive loss to tackle the hard negative samples that disturb the model
training. We extensively validate our method on various benchmarks regarding
graph classification via unsupervised and transfer learning. Compared to the
state-of-the-art (SOTA) works, better performance can always be obtained by the
proposed method.


## Accelerating Direct Preference Optimization with Prefix Sharing

>Authors: Franklin Wang, Sumanth Hegde

>2024-10-27

> http://arxiv.org/abs/2410.20305v1

Offline paired preference optimization algorithms have become a popular
approach for fine-tuning on preference data, outperforming traditional
supervised fine-tuning in various tasks. However, traditional implementations
often involve redundant computations, especially for tasks with long shared
prompts. We introduce prefix sharing for preference tuning, a novel technique
that processes chosen and rejected responses as one sequence with a shared
prefix. To prevent cross-response contamination, we use a custom block-**sparse**
attention mask. Our method achieves $1.1$-$1.5\times$ improvement in training
throughput on popular DPO datasets, without any effect on convergence. When
combined with sequence packing, we observe consistent $1.3$-$1.6\times$
speedups, benefiting even datasets with smaller sequence lengths. While we
focus on Direct Preference Optimization (DPO), our approach is applicable to
other paired preference tuning methods. By enhancing computational efficiency,
our work contributes to making preference-based fine-tuning more accessible for
a wider range of applications and model sizes. We open-source our code at
https://github.com/frankxwang/dpo-prefix-sharing.


## Sparse Decomposition of Graph Neural Networks

>Authors: Yaochen Hu, Mai Zeng, Ge Zhang, Pavel Rumiantsev, Liheng Ma, Yingxue Zhang, Mark Coates

>2024-10-25

> http://arxiv.org/abs/2410.19723v1

Graph Neural Networks (GNN) exhibit superior performance in graph
representation learning, but their inference cost can be high, due to an
aggregation operation that can require a memory fetch for a very large number
of nodes. This inference cost is the major obstacle to deploying GNN models
with \emph{online prediction} to reflect the potentially dynamic node features.
To address this, we propose an approach to reduce the number of nodes that are
included during aggregation. We achieve this through a **sparse** decomposition,
learning to approximate node representations using a weighted sum of linearly
transformed features of a carefully selected subset of nodes within the
extended neighbourhood. The approach achieves linear complexity with respect to
the average node degree and the number of layers in the graph neural network.
We introduce an algorithm to compute the optimal parameters for the **sparse**
decomposition, ensuring an accurate approximation of the original GNN model,
and present effective strategies to reduce the training time and improve the
learning process. We demonstrate via extensive experiments that our method
outperforms other baselines designed for inference speedup, achieving
significant accuracy gains with comparable inference times for both node
classification and spatio-temporal forecasting tasks.


## GeoLLaVA Efficient Fine-Tuned Vision-Language Models for Temporal Change Detection in Remote Sensing

>Authors: Hosam Elgendy, Ahmed Sharshar, Ahmed Aboeitta, Yasser Ashraf, Mohsen Guizani

>2024-10-25

> http://arxiv.org/abs/2410.19552v1

Detecting temporal changes in geographical landscapes is critical for
applications like environmental monitoring and urban planning. While remote
sensing data is abundant, existing vision-language models (VLMs) often fail to
capture temporal dynamics effectively. This paper addresses these limitations
by introducing an annotated dataset of video frame pairs to track evolving
geographical patterns over time. Using fine-tuning techniques like Low-Rank
Adaptation (LoRA), **quantize**d LoRA (QLoRA), and model **pruning** on models such as
Video-LLaVA and LLaVA-NeXT-Video, we significantly enhance VLM performance in
processing remote sensing temporal changes. Results show significant
improvements, with the best performance achieving a BERT score of 0.864 and
ROUGE-1 score of 0.576, demonstrating superior accuracy in describing land-use
transformations.


## Learning ID-free Item Representation with Token Crossing for Multimodal Recommendation

>Authors: Kangning Zhang, Jiarui Jin, Yingjie Qin, Ruilong Su, Jianghao Lin, Yong Yu, Weinan Zhang

>2024-10-25

> http://arxiv.org/abs/2410.19276v1

Current multimodal recommendation models have extensively explored the
effective utilization of multimodal information; however, their reliance on ID
embeddings remains a performance bottleneck. Even with the assistance of
multimodal information, optimizing ID embeddings remains challenging for
ID-based Multimodal Recommender when interaction data is **sparse**. Furthermore,
the unique nature of item-specific ID embeddings hinders the information
exchange among related items and the spatial requirement of ID embeddings
increases with the scale of item. Based on these limitations, we propose an
ID-free MultimOdal TOken Representation scheme named MOTOR that represents each
item using learnable multimodal tokens and connects them through shared tokens.
Specifically, we first employ product **quantization** to discretize each item's
multimodal features (e.g., images, text) into discrete token IDs. We then
interpret the token embeddings corresponding to these token IDs as implicit
item features, introducing a new Token Cross Network to capture the implicit
interaction patterns among these tokens. The resulting representations can
replace the original ID embeddings and transform the original ID-based
multimodal recommender into ID-free system, without introducing any additional
loss design. MOTOR reduces the overall space requirements of these models,
facilitating information interaction among related items, while also
significantly enhancing the model's recommendation capability. Extensive
experiments on nine mainstream models demonstrate the significant performance
improvement achieved by MOTOR, highlighting its effectiveness in enhancing
multimodal recommendation systems.


## Ripple Accelerating LLM Inference on Smartphones with Correlation-Aware Neuron Management

>Authors: Tuowei Wang, Ruwen Fan, Minxing Huang, Zixu Hao, Kun Li, Ting Cao, Youyou Lu, Yaoxue Zhang, Ju Ren

>2024-10-25

> http://arxiv.org/abs/2410.19274v2

Large Language Models (LLMs) have achieved remarkable success across various
domains, yet deploying them on mobile devices remains an arduous challenge due
to their extensive computational and memory demands. While lightweight LLMs
have been developed to fit mobile environments, they suffer from degraded model
accuracy. In contrast, **sparsity**-based techniques minimize DRAM usage by
selectively transferring only relevant neurons to DRAM while retaining the full
model in external storage, such as flash. However, such approaches are
critically limited by numerous I/O operations, particularly on smartphones with
severe IOPS constraints.
  In this paper, we propose Ripple, a novel approach that accelerates LLM
inference on smartphones by optimizing neuron placement in flash memory. Ripple
leverages the concept of Neuron Co-Activation, where neurons frequently
activated together are linked to facilitate continuous read access and optimize
data transfer efficiency. Our approach incorporates a two-stage solution: an
offline stage that reorganizes neuron placement based on co-activation
patterns, and an online stage that employs tailored data access and caching
strategies to align well with hardware characteristics. Evaluations conducted
on a variety of smartphones and LLMs demonstrate that Ripple achieves up to
5.93x improvements in I/O latency compared to the state-of-the-art. As the
first solution to optimize storage placement under **sparsity**, Ripple explores a
new optimization space at the intersection of **sparsity**-driven algorithm and
storage-level system co-design in LLM inference.


## Read-ME Refactorizing LLMs as Router-Decoupled Mixture of Experts with System Co-Design

>Authors: Ruisi Cai, Yeonju Ro, Geon-Woo Kim, Peihao Wang, Babak Ehteshami Bejnordi, Aditya Akella, Zhangyang Wang

>2024-10-24

> http://arxiv.org/abs/2410.19123v1

The proliferation of large language models (LLMs) has led to the adoption of
Mixture-of-Experts (MoE) architectures that dynamically leverage specialized
subnetworks for improved efficiency and performance. Despite their benefits,
MoE models face significant challenges during inference, including inefficient
memory management and suboptimal batching, due to misaligned design choices
between the model architecture and the system policies. Furthermore, the
conventional approach of training MoEs from scratch is increasingly prohibitive
in terms of cost. In this paper, we propose a novel framework Read-ME that
transforms pre-trained dense LLMs into smaller MoE models (in contrast to
"upcycling" generalist MoEs), avoiding the high costs of ground-up training.
Our approach employs activation **sparsity** to extract experts. To compose
experts, we examine the widely-adopted layer-wise router design and show its
redundancy, and thus we introduce the pre-gating router decoupled from the MoE
backbone that facilitates system-friendly pre-computing and lookahead
scheduling, enhancing expert-aware batching and caching. Our codesign therefore
addresses critical gaps on both the algorithmic and system fronts, establishing
a scalable and efficient alternative for LLM inference in resource-constrained
settings. Read-ME outperforms other popular open-source dense models of similar
scales, achieving improvements of up to 10.1% on MMLU, and improving mean
end-to-end latency up to 6.1%. Codes are available at:
https://github.com/VITA-Group/READ-ME.


## TesseraQ Ultra Low-Bit LLM Post-Training Quantization with Block Reconstruction

>Authors: Yuhang Li, Priyadarshini Panda

>2024-10-24

> http://arxiv.org/abs/2410.19103v1

Large language models (LLMs) have revolutionized natural language processing,
albeit at the cost of immense memory and computation requirements.
Post-training **quantization** (PTQ) is becoming the de facto method to reduce the
memory footprint and improve the inference throughput of LLMs. In this work, we
aim to push the upper limit of LLM PTQ by optimizing the weight rounding
parameters with the block reconstruction technique, a predominant method in
previous vision models. We propose TesseraQ, a new state-of-the-art PTQ
technique, to **quantize** the weights of LLMs to ultra-low bits. To effectively
optimize the rounding in LLMs and stabilize the reconstruction process, we
introduce progressive adaptive rounding. This approach iteratively transits the
soft rounding variables to hard variables during the reconstruction process.
Additionally, we optimize the de**quantization** scale parameters to fully leverage
the block reconstruction technique. We demonstrate that TesseraQ can be
seamlessly integrated with existing scaling or clipping-based PTQ algorithms
such as AWQ and OmniQuant, significantly enhancing their performance and
establishing a new state-of-the-art. For instance, when compared to AWQ,
TesseraQ improves the wikitext2 perplexity from 14.65 to 6.82 and average
downstream accuracy from 50.52 to 59.27 with 2-bit weight-only **quantization** of
LLaMA-2-7B. Across a range of **quantization** schemes, including W2A16, W3A16,
W3A3, and W4A4, TesseraQ consistently exhibits superior performance.


## PixelGaussian Generalizable 3D Gaussian Reconstruction from Arbitrary Views

>Authors: Xin Fei, Wenzhao Zheng, Yueqi Duan, Wei Zhan, Masayoshi Tomizuka, Kurt Keutzer, Jiwen Lu

>2024-10-24

> http://arxiv.org/abs/2410.18979v1

We propose PixelGaussian, an efficient feed-forward framework for learning
generalizable 3D Gaussian reconstruction from arbitrary views. Most existing
methods rely on uniform pixel-wise Gaussian representations, which learn a
fixed number of 3D Gaussians for each view and cannot generalize well to more
input views. Differently, our PixelGaussian dynamically adapts both the
Gaussian distribution and quantity based on geometric complexity, leading to
more efficient representations and significant improvements in reconstruction
quality. Specifically, we introduce a Cascade Gaussian Adapter to adjust
Gaussian distribution according to local geometry complexity identified by a
keypoint scorer. CGA leverages deformable attention in context-aware
hypernetworks to guide Gaussian **pruning** and splitting, ensuring accurate
representation in complex regions while reducing redundancy. Furthermore, we
design a transformer-based Iterative Gaussian Refiner module that refines
Gaussian representations through direct image-Gaussian interactions. Our
PixelGaussian can effectively reduce Gaussian redundancy as input views
increase. We conduct extensive experiments on the large-scale ACID and
RealEstate10K datasets, where our method achieves state-of-the-art performance
with good generalization to various numbers of views. Code:
https://github.com/Barrybarry-Smith/PixelGaussian.


## Large Spatial Model End-to-end Unposed Images to Semantic 3D

>Authors: Zhiwen Fan, Jian Zhang, Wenyan Cong, Peihao Wang, Renjie Li, Kairun Wen, Shijie Zhou, Achuta Kadambi, Zhangyang Wang, Danfei Xu, Boris Ivanovic, Marco Pavone, Yue Wang

>2024-10-24

> http://arxiv.org/abs/2410.18956v1

Reconstructing and understanding 3D structures from a limited number of
images is a well-established problem in computer vision. Traditional methods
usually break this task into multiple subtasks, each requiring complex
transformations between different data representations. For instance, dense
reconstruction through Structure-from-Motion (SfM) involves converting images
into key points, optimizing camera parameters, and estimating structures.
Afterward, accurate **sparse** reconstructions are required for further dense
modeling, which is subsequently fed into task-specific neural networks. This
multi-step process results in considerable processing time and increased
engineering complexity.
  In this work, we present the Large Spatial Model (LSM), which processes
unposed RGB images directly into semantic radiance fields. LSM simultaneously
estimates geometry, appearance, and semantics in a single feed-forward
operation, and it can generate versatile label maps by interacting with
language at novel viewpoints. Leveraging a Transformer-based architecture, LSM
integrates global geometry through pixel-aligned point maps. To enhance spatial
attribute regression, we incorporate local context aggregation with multi-scale
fusion, improving the accuracy of fine local details. To tackle the scarcity of
labeled 3D semantic data and enable natural language-driven scene manipulation,
we incorporate a pre-trained 2D language-based segmentation model into a
3D-consistent semantic feature field. An efficient decoder then parameterizes a
set of semantic anisotropic Gaussians, facilitating supervised end-to-end
learning. Extensive experiments across various tasks show that LSM unifies
multiple 3D vision tasks directly from unposed images, achieving real-time
semantic 3D reconstruction for the first time.


## Dynamic Vocabulary Pruning in Early-Exit LLMs

>Authors: Jort Vincenti, Karim Abdel Sadek, Joan Velja, Matteo Nulli, Metod Jazbec

>2024-10-24

> http://arxiv.org/abs/2410.18952v1

Increasing the size of large language models (LLMs) has been shown to lead to
better performance. However, this comes at the cost of slower and more
expensive inference. Early-exiting is a promising approach for improving the
efficiency of LLM inference by enabling next token prediction at intermediate
layers. Yet, the large vocabulary size in modern LLMs makes the confidence
estimation required for exit decisions computationally expensive, diminishing
the efficiency gains. To address this, we propose dynamically **pruning** the
vocabulary at test time for each token. Specifically, the vocabulary is pruned
at one of the initial layers, and the smaller vocabulary is then used
throughout the rest of the forward pass. Our experiments demonstrate that such
post-hoc dynamic vocabulary **pruning** improves the efficiency of confidence
estimation in early-exit LLMs while maintaining competitive performance.


## Dynamic 3D Gaussian Tracking for Graph-Based Neural Dynamics Modeling

>Authors: Mingtong Zhang, Kaifeng Zhang, Yunzhu Li

>2024-10-24

> http://arxiv.org/abs/2410.18912v1

Videos of robots interacting with objects encode rich information about the
objects' dynamics. However, existing video prediction approaches typically do
not explicitly account for the 3D information from videos, such as robot
actions and objects' 3D states, limiting their use in real-world robotic
applications. In this work, we introduce a framework to learn object dynamics
directly from multi-view RGB videos by explicitly considering the robot's
action trajectories and their effects on scene dynamics. We utilize the 3D
Gaussian representation of 3D Gaussian Splatting (3DGS) to train a
particle-based dynamics model using Graph Neural Networks. This model operates
on **sparse** control particles downsampled from the densely tracked 3D Gaussian
reconstructions. By learning the neural dynamics model on offline robot
interaction data, our method can predict object motions under varying initial
configurations and unseen robot actions. The 3D transformations of Gaussians
can be interpolated from the motions of control particles, enabling the
rendering of predicted future object states and achieving action-conditioned
video prediction. The dynamics model can also be applied to model-based
planning frameworks for object manipulation tasks. We conduct experiments on
various kinds of deformable materials, including ropes, clothes, and stuffed
animals, demonstrating our framework's ability to model complex shapes and
dynamics. Our project page is available at https://gs-dynamics.github.io.


## Hypersurface deformations

>Authors: Martin Bojowald, Erick I. Duque, Aiden Shah

>2024-10-24

> http://arxiv.org/abs/2410.18807v1

Deformations of spacelike hypersurfaces in space-time play an important role
in discussions of general covariance and slicing independence in gravitational
theories. In a canonical formulation, they provide the geometrical meaning of
gauge transformations generated by the diffeomorphism and Hamiltonian
constraints. However, it has been known for some time that the relationship
between hypersurface deformations and general covariance is not a kinematical
equivalence but holds only on the solution space of the constraints and
requires their gauge equations and equations of motion to be used. The
off-shell behavior of hypersurface deformations on their own, without imposing
constraint and gauge equations, is therefore different from space-time
diffeomorphisms. Its complete understanding is important for potential
**quantization**s or modifications of general relativity in canonical form and of
compatible space-time geometries that may be implied by them. Here, a
geometrical analysis of hypersurface deformations is performed, allowing for a
dependence of hypersurface deformation generators (the lapse function and the
shift vector) on the phase-space degrees of freedom given by the geometry of an
embedded spacelike hypersurface. The result is compared in detail with Poisson
brackets of the gravitational constraints. As a new implication of physical
relevance, covariance conditions are obtained for theories of emergent modified
gravity without symmetry restrictions.


## Sliding DFT-based Signal Recovery for Modulo ADC with 1-bit Folding Information

>Authors: Neil Irwin Bernardo

>2024-10-24

> http://arxiv.org/abs/2410.18757v1

The modulo analog-to-digital converter (ADC) is a promising solution to
resolve the limited dynamic range (DR) issue of conventional ADCs. However, a
modulo ADC requires an unfolding scheme to correct the nonlinear distortion
introduced by the modulo operation. This paper presents a sliding discrete
Fourier Transform (DFT)-based method for fast signal reconstruction given the
modulo ADC output sequence and a 1-bit folding information sequence. In
contrast to existing DFT-based signal recovery techniques for modulo ADCs, our
proposed sliding DFT method reduces the required observation time and minimizes
the spectral leakage effects via proper choice of window function parameters. A
mean squared error (MSE) performance guarantee is established for the proposed
signal recovery algorithm. More precisely, we derive sufficient conditions for
the oversampling factor ($\mathrm{OF}$) and the number of **quantization** bits
($b$) to obtain a specific MSE performance. Our numerical results demonstrate
that modulo ADCs equipped with our proposed recovery method can outperform
conventional ADCs without modulo for $\mathrm{OF} \geq 4$ and $b \geq 4$. The
impact of spectral leakage on the MSE performance of the proposed sliding DFT
recovery method is also quantified.


## PESFormer Boosting Macro- and Micro-expression Spotting with Direct Timestamp Encoding

>Authors: Wang-Wang Yu, Kai-Fu Yang, Xiangrui Hu, Jingwen Jiang, Hong-Mei Yan, Yong-Jie Li

>2024-10-24

> http://arxiv.org/abs/2410.18695v1

The task of macro- and micro-expression spotting aims to precisely localize
and categorize temporal expression instances within untrimmed videos. Given the
**sparse** distribution and varying durations of expressions, existing anchor-based
methods often represent instances by encoding their deviations from predefined
anchors. Additionally, these methods typically slice the untrimmed videos into
fixed-length sliding windows. However, anchor-based encoding often fails to
capture all training intervals, and slicing the original video as sliding
windows can result in valuable training intervals being discarded. To overcome
these limitations, we introduce PESFormer, a simple yet effective model based
on the vision transformer architecture to achieve point-to-interval expression
spotting. PESFormer employs a direct timestamp encoding (DTE) approach to
replace anchors, enabling binary classification of each timestamp instead of
optimizing entire ground truths. Thus, all training intervals are retained in
the form of discrete timestamps. To maximize the utilization of training
intervals, we enhance the preprocessing process by replacing the short videos
produced through the sliding window method.Instead, we implement a strategy
that involves zero-padding the untrimmed training videos to create uniform,
longer videos of a predetermined duration. This operation efficiently preserves
the original training intervals and eliminates video slice
enhancement.Extensive qualitative and quantitative evaluations on three
datasets -- CAS(ME)^2, CAS(ME)^3 and SAMM-LV -- demonstrate that our PESFormer
outperforms existing techniques, achieving the best performance.


## The Nature of Mathematical Modeling and Probabilistic Optimization Engineering in Generative AI

>Authors: Fulu Li

>2024-10-24

> http://arxiv.org/abs/2410.18441v1

In this paper, we give an in-depth analysis on the mathematical problem
formulations and the probabilistic optimization explorations for some of the
key components in Transformer model [33] in the field of generative AI. We
explore and discuss some potential further enhancement for current state of the
art methods for some key underlying technologies of generative AI models from
algorithmic and probabilistic optimization perspective. In particular, we
present an optimal solution for sub-word encoding (SWE) based on similar
initial settings as that of byte-pair encoding (BPE) algorithm in [9] with
similar objectives as that of WordPiece approach in [28, 31] to maximize the
likelihood of the training data. We also present cross entropy optimization
method to optimize hyperparameters for word2vec model [17]. In addition, we
propose a factored combination of rotary positional encoding (RoPE) [32] and
attention with linear biases (ALiBi) [23] with a harmonic series. We also
present a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a
probability distribution over block distances in the matrix to decide which
block is likely to participate in a given round of attention computation while
maintaining the lower triangle shape of the tensor for autoregressive language
models by re-shaping the tensors. Finally, we present staircase adaptive
**quantization** (SAQ) of key-value (KV) cache for multi-query attention (MQA)
based on the framework presented in [16] to have gradual **quantization**
degradation while achieving reasonable model quality and cost savings.


## Causal Order Discovery based on Monotonic SCMs

>Authors: Ali Izadi, Martin Ester

>2024-10-24

> http://arxiv.org/abs/2410.19870v1

In this paper, we consider the problem of causal order discovery within the
framework of monotonic Structural Causal Models (SCMs), which have gained
attention for their potential to enable causal inference and causal discovery
from observational data. While existing approaches either assume prior
knowledge about the causal order or use complex optimization techniques to
impose **sparsity** in the Jacobian of Triangular Monotonic Increasing maps, our
work introduces a novel sequential procedure that directly identifies the
causal order by iteratively detecting the root variable. This method eliminates
the need for **sparsity** assumptions and the associated optimization challenges,
enabling the identification of a unique SCM without the need for multiple
independence tests to break the Markov equivalence class. We demonstrate the
effectiveness of our approach in sequentially finding the root variable,
comparing it to methods that maximize Jacobian **sparsity**.


## CoreInfer Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation

>Authors: Qinsi Wang, Saeed Vahidian, Hancheng Ye, Jianyang Gu, Jianyi Zhang, Yiran Chen

>2024-10-23

> http://arxiv.org/abs/2410.18311v1

Large language models (LLMs) with billions of parameters have sparked a new
wave of exciting AI applications. However, their high computational costs and
memory demands during inference pose significant challenges. Adaptive **sparse**
activation inference, which activates only a small number of neurons for each
token, offers a novel way to accelerate model inference without degrading
performance, showing great potential for resource-constrained hardware devices.
Nevertheless, existing methods predict activated neurons based on individual
tokens with additional MLP, which involve frequent changes in activation maps
and resource calls, limiting the acceleration benefits of **sparse** activation. In
this paper, we introduce CoreInfer, an MLP-free adaptive **sparse** activation
inference method based on sentence-level prediction. Specifically, we propose
the concept of sentence-wise core neurons, which refers to the subset of
neurons most critical for a given sentence, and empirically demonstrate its
effectiveness. To determine the core neurons, we explore the correlation
between core neurons and the sentence's semantics. Remarkably, we discovered
that core neurons exhibit both stability and similarity in relation to the
sentence's semantics -- an insight overlooked by previous studies. Building on
this finding, we further design two semantic-based methods for predicting core
neurons to fit different input scenarios. In CoreInfer, the core neurons are
determined during the pre-filling stage and fixed during the encoding stage,
enabling zero-cost **sparse** inference. We evaluated the model generalization and
task generalization of CoreInfer across various models and tasks. Notably, on
an NVIDIA TITAN XP GPU, CoreInfer achieved a 10.33 times and 2.72 times speedup
compared to the Huggingface implementation and PowerInfer, respectively.


## LEGO Language Model Building Blocks

>Authors: Shrenik Bhansali, Alwin Jin, Tyler Lizzo, Larry Heck

>2024-10-23

> http://arxiv.org/abs/2410.18287v1

Large language models (LLMs) are essential in natural language processing
(NLP) but are costly in data collection, pre-training, fine-tuning, and
inference. Task-specific small language models (SLMs) offer a cheaper
alternative but lack robustness and generalization. This paper proposes LEGO, a
novel technique to extract SLMs from an LLM and recombine them. Using
state-of-the-art LLM **pruning** strategies, we can create task- and user-specific
SLM building blocks that are efficient for fine-tuning and inference while also
preserving user data privacy. LEGO utilizes Federated Learning and a novel
aggregation scheme for the LLM reconstruction, maintaining robustness without
high costs and preserving user data privacy. We experimentally demonstrate the
versatility of LEGO, showing its ability to enable model heterogeneity and
mitigate the effects of data heterogeneity while maintaining LLM robustness.


## Leveraging Skills from Unlabeled Prior Data for Efficient Online Exploration

>Authors: Max Wilcoxson, Qiyang Li, Kevin Frans, Sergey Levine

>2024-10-23

> http://arxiv.org/abs/2410.18076v1

Unsupervised pretraining has been transformative in many supervised domains.
However, applying such ideas to reinforcement learning (RL) presents a unique
challenge in that fine-tuning does not involve mimicking task-specific data,
but rather exploring and locating the solution through iterative
self-improvement. In this work, we study how unlabeled prior trajectory data
can be leveraged to learn efficient exploration strategies. While prior data
can be used to pretrain a set of low-level skills, or as additional off-policy
data for online RL, it has been unclear how to combine these ideas effectively
for online exploration. Our method SUPE (Skills from Unlabeled Prior data for
Exploration) demonstrates that a careful combination of these ideas compounds
their benefits. Our method first extracts low-level skills using a variational
autoencoder (VAE), and then pseudo-relabels unlabeled trajectories using an
optimistic reward model, transforming prior data into high-level, task-relevant
examples. Finally, SUPE uses these transformed examples as additional
off-policy data for online RL to learn a high-level policy that composes
pretrained low-level skills to explore efficiently. We empirically show that
SUPE reliably outperforms prior strategies, successfully solving a suite of
long-horizon, **sparse**-reward tasks. Code: https://github.com/rail-berkeley/supe.


## ExpertFlow Optimized Expert Activation and Token Allocation for Efficient Mixture-of-Experts Inference

>Authors: Xin He, Shunkang Zhang, Yuxin Wang, Haiyan Yin, Zihao Zeng, Shaohuai Shi, Zhenheng Tang, Xiaowen Chu, Ivor Tsang, Ong Yew Soon

>2024-10-23

> http://arxiv.org/abs/2410.17954v1

Sparse Mixture of Experts (MoE) models, while outperforming dense Large
Language Models (LLMs) in terms of performance, face significant deployment
challenges during inference due to their high memory demands. Existing
offloading techniques, which involve swapping activated and idle experts
between the GPU and CPU, often suffer from rigid expert caching mechanisms.
These mechanisms fail to adapt to dynamic routing, leading to inefficient cache
utilization, or incur prohibitive costs for prediction training. To tackle
these inference-specific challenges, we introduce ExpertFlow, a comprehensive
system specifically designed to enhance inference efficiency by accommodating
flexible routing and enabling efficient expert scheduling between CPU and GPU.
This reduces overhead and boosts system performance. Central to our approach is
a predictive routing path-based offloading mechanism that utilizes a
lightweight predictor to accurately forecast routing paths before computation
begins. This proactive strategy allows for real-time error correction in expert
caching, significantly increasing cache hit ratios and reducing the frequency
of expert transfers, thereby minimizing I/O overhead. Additionally, we
implement a dynamic token scheduling strategy that optimizes MoE inference by
rearranging input tokens across different batches. This method not only reduces
the number of activated experts per batch but also improves computational
efficiency. Our extensive experiments demonstrate that ExpertFlow achieves up
to 93.72\% GPU memory savings and enhances inference speed by 2 to 10 times
compared to baseline methods, highlighting its effectiveness and utility as a
robust solution for resource-constrained inference scenarios.


## Quantizable Ghost-ridden theories using Kinetic Positivity Constraints

>Authors: Sukanta Panda, Archit Vidyarthi

>2024-10-23

> http://arxiv.org/abs/2410.17924v1

We present a novel way to constrain the ghost field with respect to other
physical fields present in a given theory such that the theory becomes
quantizable. This is achieved by imposing positivity of the total kinetic
energy of the system and performing Lorentz transformations in the field space
manifold to arrive at an effective Lagrangian containing only physical degrees
of freedom. Since models containing ghost fields such as quintom models are
relevant in the cosmological context, this method can help ensure that such
theories don't violate unitarity and can be treated as realistic candidates
without the need to completely eliminate ghost(s).


## Identifiable Representation and Model Learning for Latent Dynamic Systems

>Authors: Congxi Zhang, Yongchun Xie

>2024-10-23

> http://arxiv.org/abs/2410.17882v1

Learning identifiable representations and models from low-level observations
is useful for an intelligent spacecraft to reliability finish downstream tasks.
For temporal observations, to ensure that the data generating process is
provably inverted, most existing works either assume the noise variables in the
dynamic mechanisms are (conditionally) independent, or require interventions
which can directly affect each latent variable. However, in practice, the
relationship between the exogenous inputs/interventions and the latent
variables may follow some complex deterministic mechanisms. In this work, we
study the problem of identifiable representation and model learning for latent
dynamic systems. The key idea is that we use an inductive bias inspired by
controllable canonical forms, which is invariant, **sparse**, and input dependent
by definition. We prove that, for linear or affine nonlinear latent dynamic
systems, it is possible to identify the representations up to scaling and
determine the models up to some simple transformations. The results have
potential to provide some theoretical guarantees for developing more
trustworthy decision-making and control methods for intelligent spacecrafts.


## Att2CPC Attention-Guided Lossy Attribute Compression of Point Clouds

>Authors: Kai Liu, Kang You, Pan Gao, Manoranjan Paul

>2024-10-23

> http://arxiv.org/abs/2410.17823v1

With the great progress of 3D sensing and acquisition technology, the volume
of point cloud data has grown dramatically, which urges the development of
efficient point cloud compression methods. In this paper, we focus on the task
of learned lossy point cloud attribute compression (PCAC). We propose an
efficient attention-based method for lossy compression of point cloud
attributes leveraging on an autoencoder architecture. Specifically, at the
encoding side, we conduct multiple downsampling to best exploit the local
attribute patterns, in which effective External Cross Attention (ECA) is
devised to hierarchically aggregate features by intergrating attributes and
geometry contexts. At the decoding side, the attributes of the point cloud are
progressively reconstructed based on the multi-scale representation and the
zero-padding upsampling tactic. To the best of our knowledge, this is the first
approach to introduce attention mechanism to point-based lossy PCAC task. We
verify the compression efficiency of our model on various sequences, including
human body frames, **sparse** objects, and large-scale point cloud scenes.
Experiments show that our method achieves an average improvement of 1.15 dB and
2.13 dB in BD-PSNR of Y channel and YUV channel, respectively, when comparing
with the state-of-the-art point-based method Deep-PCAC. Codes of this paper are
available at https://github.com/I2-Multimedia-Lab/Att2CPC.


## Anomaly Resilient Temporal QoS Prediction using Hypergraph Convoluted Transformer Network

>Authors: Suraj Kumar, Soumi Chattopadhyay, Chandranath Adak

>2024-10-23

> http://arxiv.org/abs/2410.17762v1

Quality-of-Service (QoS) prediction is a critical task in the service
lifecycle, enabling precise and adaptive service recommendations by
anticipating performance variations over time in response to evolving network
uncertainties and user preferences. However, contemporary QoS prediction
methods frequently encounter data **sparsity** and cold-start issues, which hinder
accurate QoS predictions and limit the ability to capture diverse user
preferences. Additionally, these methods often assume QoS data reliability,
neglecting potential credibility issues such as outliers and the presence of
greysheep users and services with atypical invocation patterns. Furthermore,
traditional approaches fail to leverage diverse features, including
domain-specific knowledge and complex higher-order patterns, essential for
accurate QoS predictions. In this paper, we introduce a real-time, trust-aware
framework for temporal QoS prediction to address the aforementioned challenges,
featuring an end-to-end deep architecture called the Hypergraph Convoluted
Transformer Network (HCTN). HCTN combines a hypergraph structure with graph
convolution over hyper-edges to effectively address high-**sparsity** issues by
capturing complex, high-order correlations. Complementing this, the transformer
network utilizes multi-head attention along with parallel 1D convolutional
layers and fully connected dense blocks to capture both fine-grained and
coarse-grained dynamic patterns. Additionally, our approach includes a
**sparsity**-resilient solution for detecting greysheep users and services,
incorporating their unique characteristics to improve prediction accuracy.
Trained with a robust loss function resistant to outliers, HCTN demonstrated
state-of-the-art performance on the large-scale WSDREAM-2 datasets for response
time and throughput.


## Escaping the Forest Sparse Interpretable Neural Networks for Tabular Data

>Authors: Salvatore Raieli, Abdulrahman Altahhan, Nathalie Jeanray, Stéphane Gerart, Sebastien Vachenc

>2024-10-23

> http://arxiv.org/abs/2410.17758v1

Tabular datasets are widely used in scientific disciplines such as biology.
While these disciplines have already adopted AI methods to enhance their
findings and analysis, they mainly use tree-based methods due to their
interpretability. At the same time, artificial neural networks have been shown
to offer superior flexibility and depth for rich and complex non-tabular
problems, but they are falling behind tree-based models for tabular data in
terms of performance and interpretability. Although **sparsity** has been shown to
improve the interpretability and performance of ANN models for complex
non-tabular datasets, enforcing **sparsity** structurally and formatively for
tabular data before training the model, remains an open question. To address
this question, we establish a method that infuses **sparsity** in neural networks
by utilising attention mechanisms to capture the features' importance in
tabular datasets. We show that our models, Sparse TABular NET or sTAB-Net with
attention mechanisms, are more effective than tree-based models, reaching the
state-of-the-art on biological datasets. They further permit the extraction of
insights from these datasets and achieve better performance than post-hoc
methods like SHAP.


## Beware of Calibration Data for Pruning Large Language Models

>Authors: Yixin Ji, Yang Xiang, Juntao Li, Qingrong Xia, Ping Li, Xinyu Duan, Zhefeng Wang, Min Zhang

>2024-10-23

> http://arxiv.org/abs/2410.17711v1

As large language models (LLMs) are widely applied across various fields,
model compression has become increasingly crucial for reducing costs and
improving inference efficiency. Post-training **pruning** is a promising method
that does not require resource-intensive iterative training and only needs a
small amount of calibration data to assess the importance of parameters.
Previous research has primarily focused on designing advanced **pruning** methods,
while different calibration data's impact on **pruning** performance still lacks
systematical exploration. We fill this blank and surprisingly observe that the
effects of calibration data even value more than designing advanced **pruning**
strategies, especially for high **sparsity**. Our preliminary exploration also
discloses that using calibration data similar to the training data can yield
better performance. As pre-training data is usually inaccessible for advanced
LLMs, we further provide a self-generating calibration data synthesis strategy
to construct feasible calibration data. We conduct experiments on the recent
strong open-source LLMs (e.g., DCLM, and LLaMA-3), and the results show that
the proposed method outperforms commonly used calibration data and can
effectively enhance strong **pruning** methods (e.g., Wanda, OWL).


## PETAH Parameter Efficient Task Adaptation for Hybrid Transformers in a resource-limited Context

>Authors: Maximilian Augustin, Syed Shakib Sarwar, Mostafa Elhoushi, Sai Qian Zhang, Yuecheng Li, Barbara De Salvo

>2024-10-23

> http://arxiv.org/abs/2410.17661v1

Following their success in natural language processing (NLP), there has been
a shift towards transformer models in computer vision. While transformers
perform well and offer promising multi-tasking performance, due to their high
compute requirements, many resource-constrained applications still rely on
convolutional or hybrid models that combine the benefits of convolution and
attention layers and achieve the best results in the sub 100M parameter range.
Simultaneously, task adaptation techniques that allow for the use of one shared
transformer backbone for multiple downstream tasks, resulting in great storage
savings at negligible cost in performance, have not yet been adopted for hybrid
transformers. In this work, we investigate how to achieve the best
task-adaptation performance and introduce PETAH: Parameter Efficient Task
Adaptation for Hybrid Transformers. We further combine PETAH adaptation with
**pruning** to achieve highly performant and storage friendly models for
multi-tasking. In our extensive evaluation on classification and other vision
tasks, we demonstrate that our PETAH-adapted hybrid models outperform
established task-adaptation techniques for ViTs while requiring fewer
parameters and being more efficient on mobile hardware.


## AutoRNet Automatically Optimizing Heuristics for Robust Network Design via Large Language Models

>Authors: He Yu, Jing Liu

>2024-10-23

> http://arxiv.org/abs/2410.17656v1

Achieving robust networks is a challenging problem due to its NP-hard nature
and complex solution space. Current methods, from handcrafted feature
extraction to deep learning, have made progress but remain rigid, requiring
manual design and large labeled datasets. To address these issues, we propose
AutoRNet, a framework that integrates large language models (LLMs) with
evolutionary algorithms to generate heuristics for robust network design. We
design network optimization strategies to provide domain-specific prompts for
LLMs, utilizing domain knowledge to generate advanced heuristics. Additionally,
we introduce an adaptive fitness function to balance convergence and diversity
while maintaining degree distributions. AutoRNet is evaluated on **sparse** and
dense scale-free networks, outperforming current methods by reducing the need
for manual design and large datasets.


## Process Supervision-Guided Policy Optimization for Code Generation

>Authors: Ning Dai, Zheng Wu, Renjie Zheng, Ziyun Wei, Wenlei Shi, Xing Jin, Guanlin Liu, Chen Dun, Liang Huang, Lin Yan

>2024-10-23

> http://arxiv.org/abs/2410.17621v1

Reinforcement Learning (RL) with unit test feedback has enhanced large
language models (LLMs) code generation, but relies on **sparse** rewards provided
only after complete code evaluation, limiting learning efficiency and
incremental improvements. When generated code fails all unit tests, no learning
signal is received, hindering progress on complex tasks. To address this, we
propose a Process Reward Model (PRM) that delivers dense, line-level feedback
on code correctness during generation, mimicking human code refinement and
providing immediate guidance. We explore various strategies for training PRMs
and integrating them into the RL framework, finding that using PRMs both as
dense rewards and for value function initialization significantly boosts
performance. Our approach increases our in-house LLM's pass rate from 28.2% to
29.8% on LiveCodeBench and from 31.8% to 35.8% on our internal benchmark. Our
experimental results highlight the effectiveness of PRMs in enhancing RL-driven
code generation, especially for long-horizon scenarios.


## Adaptive Wireless Image Semantic Transmission Design, Simulation, and Prototype Validation

>Authors: Jiarun Ding, Peiwen Jiang, Chao-Kai Wen, Shi Jin

>2024-10-23

> http://arxiv.org/abs/2410.17536v1

The rapid development of artificial intelligence has significantly advanced
semantic communications, particularly in wireless image transmission. However,
most existing approaches struggle to precisely distinguish and prioritize image
content, and they do not sufficiently incorporate semantic priorities into
system design. In this study, we propose an adaptive wireless image semantic
transmission scheme called ASCViT-JSCC, which utilizes vision transformer-based
joint source-channel coding (JSCC). This scheme prioritizes different image
regions based on their importance, identified through object and feature point
detection. Unimportant background sections are masked, enabling them to be
recovered at the receiver, while the freed resources are allocated to enhance
object protection via the JSCC network. We also integrate **quantization** modules
to enable compatibility with quadrature amplitude modulation, commonly used in
modern wireless communications. To address frequency-selective fading channels,
we introduce CSIPA-Net, which allocates power based on channel information,
further improving performance. Notably, we conduct over-the-air testing on a
prototype platform composed of a software-defined radio and embedded graphics
processing unit systems, validating our methods. Both simulations and
real-world measurements demonstrate that ASCViT-JSCC effectively prioritizes
object protection according to channel conditions, significantly enhancing
image reconstruction quality, especially in challenging channel environments.


## Congestion Forecast for Trains with Railroad-Graph-based Semi-Supervised Learning using Sparse Passenger Reports

>Authors: Soto Anno, Kota Tsubouchi, Masamichi Shimosaka

>2024-10-23

> http://arxiv.org/abs/2410.17510v1

Forecasting rail congestion is crucial for efficient mobility in transport
systems. We present rail congestion forecasting using reports from passengers
collected through a transit application. Although reports from passengers have
received attention from researchers, ensuring a sufficient volume of reports is
challenging due to passenger's reluctance. The limited number of reports
results in the **sparsity** of the congestion label, which can be an issue in
building a stable prediction model. To address this issue, we propose a
semi-supervised method for congestion forecasting for trains, or SURCONFORT.
Our key idea is twofold: firstly, we adopt semi-supervised learning to leverage
**sparse**ly labeled data and many unlabeled data. Secondly, in order to complement
the unlabeled data from nearby stations, we design a railway network-oriented
graph and apply the graph to semi-supervised graph regularization. Empirical
experiments with actual reporting data show that SURCONFORT improved the
forecasting performance by 14.9% over state-of-the-art methods under the label
**sparsity**.


## PtychoFormer A Transformer-based Model for Ptychographic Phase Retrieval

>Authors: Ryuma Nakahata, Shehtab Zaman, Mingyuan Zhang, Fake Lu, Kenneth Chiu

>2024-10-22

> http://arxiv.org/abs/2410.17377v1

Ptychography is a computational method of microscopy that recovers
high-resolution transmission images of samples from a series of diffraction
patterns. While conventional phase retrieval algorithms can iteratively recover
the images, they require oversampled diffraction patterns, incur significant
computational costs, and struggle to recover the absolute phase of the sample's
transmission function. Deep learning algorithms for ptychography are a
promising approach to resolving the limitations of iterative algorithms. We
present PtychoFormer, a hierarchical transformer-based model for data-driven
single-shot ptychographic phase retrieval. PtychoFormer processes subsets of
diffraction patterns, generating local inferences that are seamlessly stitched
together to produce a high-quality reconstruction. Our model exhibits tolerance
to **sparse**ly scanned diffraction patterns and achieves up to 3600 times faster
imaging speed than the extended ptychographic iterative engine (ePIE). We also
propose the extended-PtychoFormer (ePF), a hybrid approach that combines the
benefits of PtychoFormer with the ePIE. ePF minimizes global phase shifts and
significantly enhances reconstruction quality, achieving state-of-the-art phase
retrieval in ptychography.


## LVSM A Large View Synthesis Model with Minimal 3D Inductive Bias

>Authors: Haian Jin, Hanwen Jiang, Hao Tan, Kai Zhang, Sai Bi, Tianyuan Zhang, Fujun Luan, Noah Snavely, Zexiang Xu

>2024-10-22

> http://arxiv.org/abs/2410.17242v1

We propose the Large View Synthesis Model (LVSM), a novel transformer-based
approach for scalable and generalizable novel view synthesis from **sparse**-view
inputs. We introduce two architectures: (1) an encoder-decoder LVSM, which
encodes input image tokens into a fixed number of 1D latent tokens, functioning
as a fully learned scene representation, and decodes novel-view images from
them; and (2) a decoder-only LVSM, which directly maps input images to
novel-view outputs, completely eliminating intermediate scene representations.
Both models bypass the 3D inductive biases used in previous methods -- from 3D
representations (e.g., NeRF, 3DGS) to network designs (e.g., epipolar
projections, plane sweeps) -- addressing novel view synthesis with a fully
data-driven approach. While the encoder-decoder model offers faster inference
due to its independent latent representation, the decoder-only LVSM achieves
superior quality, scalability, and zero-shot generalization, outperforming
previous state-of-the-art methods by 1.5 to 3.5 dB PSNR. Comprehensive
evaluations across multiple datasets demonstrate that both LVSM variants
achieve state-of-the-art novel view synthesis quality. Notably, our models
surpass all previous methods even with reduced computational resources (1-2
GPUs). Please see our website for more details:
https://haian-jin.github.io/projects/LVSM/ .


## Can General-Purpose Large Language Models Generalize to English-Thai Machine Translation ?

>Authors: Jirat Chiaranaipanich, Naiyarat Hanmatheekuna, Jitkapat Sawatphol, Krittamate Tiankanon, Jiramet Kinchagawat, Amrest Chinkamol, Parinthapat Pengpun, Piyalitt Ittichaiwong, Peerat Limkonchotiwat

>2024-10-22

> http://arxiv.org/abs/2410.17145v1

Large language models (LLMs) perform well on common tasks but struggle with
generalization in low-resource and low-computation settings. We examine this
limitation by testing various LLMs and specialized translation models on
English-Thai machine translation and code-switching datasets. Our findings
reveal that under more strict computational constraints, such as 4-bit
**quantization**, LLMs fail to translate effectively. In contrast, specialized
models, with comparable or lower computational requirements, consistently
outperform LLMs. This underscores the importance of specialized models for
maintaining performance under resource constraints.


## Math Neurosurgery Isolating Language Models' Math Reasoning Abilities Using Only Forward Passes

>Authors: Bryan R. Christ, Zack Gottesman, Jonathan Kropko, Thomas Hartvigsen

>2024-10-22

> http://arxiv.org/abs/2410.16930v1

Math reasoning is a highly active area of Large Language Model (LLM) research
because it is a hallmark of artificial intelligence. However, few works have
explored how math reasoning is encoded within LLM parameters and if it is a
skill that can be isolated within a model. Doing so could allow targeted
intervention to improve math performance without altering non-math behavior and
foster understanding of how models encode math reasoning. We introduce Math
Neurosurgery (MathNeuro), a method for isolating math-specific parameters in
LLMs using only forward passes. MathNeuro builds on existing work by using
weights and activations to calculate parameter importance, but isolates
math-specific parameters by removing those important for general language
tasks. Pruning parameters MathNeuro identifies deletes a LLM's math reasoning
ability without destroying its general language ability. Scaling these
parameters by a small constant improves a pretrained or instruction-tuned LLM's
performance by 4-17% on GSM8K while leaving non-math behavior unaltered.
MathNeuro is also data efficient: most of its effectiveness holds when
identifying math-specific parameters using a single sample. MathNeuro
highlights the potential for future work to intervene on math-specific
parameters.


## Pyramid Vector Quantization for LLMs

>Authors: Tycho F. A. van der Ouderaa, Maximilian L. Croci, Agrin Hilmkil, James Hensman

>2024-10-22

> http://arxiv.org/abs/2410.16926v1

Recent works on compression of large language models (LLM) using **quantization**
considered reparameterizing the architecture such that weights are distributed
on the sphere. This demonstratively improves the ability to **quantize** by
increasing the mathematical notion of coherence, resulting in fewer weight
outliers without affecting the network output. In this work, we aim to further
exploit this spherical geometry of the weights when performing **quantization** by
considering Pyramid Vector Quantization (PVQ) for large language models.
Arranging points evenly on the sphere is notoriously difficult, especially in
high dimensions, and in case approximate solutions exists, representing points
explicitly in a codebook is typically not feasible due to its additional memory
cost. Instead, PVQ uses a fixed integer lattice on the sphere by projecting
points onto the 1-sphere, which allows for efficient encoding and decoding
without requiring an explicit codebook in memory. To obtain a practical
algorithm, we propose to combine PVQ with scale **quantization** for which we
derive theoretically optimal **quantization**s, under empirically verified
assumptions. Further, we extend pyramid vector **quantization** to use Hessian
information to minimize **quantization** error under expected feature activations,
instead of only relying on weight magnitudes. Experimentally, we achieves
state-of-the-art **quantization** performance with pareto-optimal trade-off between
performance and bits per weight and bits per activation, compared to compared
methods. On weight-only, we find that we can **quantize** a Llama-3 70B model to
3.25 bits per weight and retain 98\% accuracy on downstream tasks.


## Semantic-guided Search for Efficient Program Repair with Large Language Models

>Authors: Thanh Le-Cong, Bach Le, Toby Murray

>2024-10-22

> http://arxiv.org/abs/2410.16655v1

In this paper, we first show that increases in beam size of even just
small-sized LLM (1B-7B parameters) require an extensive GPU resource
consumption, leading to up to 80% of recurring crashes due to memory overloads
in LLM-based APR. Seemingly simple solutions to reduce memory consumption are
(1) to **quantize** LLM models, i.e., converting the weights of a LLM from
high-precision values to lower-precision ones. and (2) to make beam search
sequential, i.e., forwarding each beam through the model sequentially and then
concatenate them back into a single model output. However, we show that these
approaches still do not work via both theoretical analysis and experiments. To
address this, we introduce FLAMES, a novel LLM-based APR technique that employs
semantic-guided patch generation to enhance repair effectiveness and memory
efficiency. Unlike conventional methods that rely on beam search, FLAMES
utilizes greedy decoding to enhance memory efficiency while steering the search
to more potentially good repair candidates via a semantic-guided best-first
search algorithm. At each decoding step, FLAMES uses semantic feedback from
test validation such as the number of passing and failing test cases to select
the most promising token to explore further. Our empirical evaluation on the
Defects4J and HumanEval-Java datasets shows that FLAMES not only substantially
reduces memory consumption by up to 83% compared to conventional LLM-based APR,
but also accelerates the repair process. Remarkably, FLAMES successfully
generated 133 and 103 correct fixes for 333 and 163 bugs in the Defects4J and
HumanEval-Java datasets, respectively. This suggests that FLAMES is not only
more efficient but also outperforms state-of-the-art techniques, fixing at
least 10 and 11 more bugs than SOTA baselines in the Defects4J and
HumanEval-Java datasets, respectively.


## Bayesian High-dimensional Linear Regression with Sparse Projection-posterior

>Authors: Samhita Pal, Subhashis Ghoshal

>2024-10-21

> http://arxiv.org/abs/2410.16577v2

We consider a novel Bayesian approach to estimation, uncertainty
quantification, and variable selection for a high-dimensional linear regression
model under **sparsity**. The number of predictors can be nearly exponentially
large relative to the sample size. We put a conjugate normal prior initially
disregarding **sparsity**, but for making an inference, instead of the original
multivariate normal posterior, we use the posterior distribution induced by a
map transforming the vector of regression coefficients to a **sparse** vector
obtained by minimizing the sum of squares of deviations plus a suitably scaled
$\ell_1$-penalty on the vector. We show that the resulting **sparse**
projection-posterior distribution contracts around the true value of the
parameter at the optimal rate adapted to the **sparsity** of the vector. We show
that the true **sparsity** structure gets a large **sparse** projection-posterior
probability. We further show that an appropriately recentred credible ball has
the correct asymptotic frequentist coverage. Finally, we describe how the
computational burden can be distributed to many machines, each dealing with
only a small fraction of the whole dataset. We conduct a comprehensive
simulation study under a variety of settings and found that the proposed method
performs well for finite sample sizes. We also apply the method to several real
datasets, including the ADNI data, and compare its performance with the
state-of-the-art methods. We implemented the method in the \texttt{R} package
called \texttt{**sparse**Proj}, and all computations have been carried out using
this package.


## Does your LLM truly unlearn? An embarrassingly simple approach to recover unlearned knowledge

>Authors: Zhiwei Zhang, Fali Wang, Xiaomin Li, Zongyu Wu, Xianfeng Tang, Hui Liu, Qi He, Wenpeng Yin, Suhang Wang

>2024-10-21

> http://arxiv.org/abs/2410.16454v1

Large language models (LLMs) have shown remarkable proficiency in generating
text, benefiting from extensive training on vast textual corpora. However, LLMs
may also acquire unwanted behaviors from the diverse and sensitive nature of
their training data, which can include copyrighted and private content. Machine
unlearning has been introduced as a viable solution to remove the influence of
such problematic content without the need for costly and time-consuming
retraining. This process aims to erase specific knowledge from LLMs while
preserving as much model utility as possible. Despite the effectiveness of
current unlearning methods, little attention has been given to whether existing
unlearning methods for LLMs truly achieve forgetting or merely hide the
knowledge, which current unlearning benchmarks fail to detect. This paper
reveals that applying **quantization** to models that have undergone unlearning can
restore the "forgotten" information. To thoroughly evaluate this phenomenon, we
conduct comprehensive experiments using various **quantization** techniques across
multiple precision levels. We find that for unlearning methods with utility
constraints, the unlearned model retains an average of 21\% of the intended
forgotten knowledge in full precision, which significantly increases to 83\%
after 4-bit **quantization**. Based on our empirical findings, we provide a
theoretical explanation for the observed phenomenon and propose a
**quantization**-robust unlearning strategy to mitigate this intricate issue...


## Improving Neuron-level Interpretability with White-box Language Models

>Authors: Hao Bai, Yi Ma

>2024-10-21

> http://arxiv.org/abs/2410.16443v1

Neurons in auto-regressive language models like GPT-2 can be interpreted by
analyzing their activation patterns. Recent studies have shown that techniques
such as dictionary learning, a form of post-hoc **sparse** coding, enhance this
neuron-level interpretability. In our research, we are driven by the goal to
fundamentally improve neural network interpretability by embedding **sparse**
coding directly within the model architecture, rather than applying it as an
afterthought. In our study, we introduce a white-box transformer-like
architecture named Coding RAte TransformEr (CRATE), explicitly engineered to
capture **sparse**, low-dimensional structures within data distributions. Our
comprehensive experiments showcase significant improvements (up to 103%
relative improvement) in neuron-level interpretability across a variety of
evaluation metrics. Detailed investigations confirm that this enhanced
interpretability is steady across different layers irrespective of the model
size, underlining CRATE's robust performance in enhancing neural network
interpretability. Further analysis shows that CRATE's increased
interpretability comes from its enhanced ability to consistently and
distinctively activate on relevant tokens. These findings point towards a
promising direction for creating white-box foundation models that excel in
neuron-level interpretation.


## MagicPIG LSH Sampling for Efficient LLM Generation

>Authors: Zhuoming Chen, Ranajoy Sadhukhan, Zihao Ye, Yang Zhou, Jianyu Zhang, Niklas Nolte, Yuandong Tian, Matthijs Douze, Leon Bottou, Zhihao Jia, Beidi Chen

>2024-10-21

> http://arxiv.org/abs/2410.16179v2

Large language models (LLMs) with long context windows have gained
significant attention. However, the KV cache, stored to avoid re-computation,
becomes a bottleneck. Various dynamic **sparse** or TopK-based attention
approximation methods have been proposed to leverage the common insight that
attention is **sparse**. In this paper, we first show that TopK attention itself
suffers from quality degradation in certain downstream tasks because attention
is not always as **sparse** as expected. Rather than selecting the keys and values
with the highest attention scores, sampling with theoretical guarantees can
provide a better estimation for attention output. To make the sampling-based
approximation practical in LLM generation, we propose MagicPIG, a heterogeneous
system based on Locality Sensitive Hashing (LSH). MagicPIG significantly
reduces the workload of attention computation while preserving high accuracy
for diverse tasks. MagicPIG stores the LSH hash tables and runs the attention
computation on the CPU, which allows it to serve longer contexts and larger
batch sizes with high approximation accuracy. MagicPIG can improve decoding
throughput by $1.9\sim3.9\times$ across various GPU hardware and achieve 110ms
decoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a
context of 96k tokens. The code is available at
\url{https://github.com/Infini-AI-Lab/MagicPIG}.


## PODTILE Facilitating Podcast Episode Browsing with Auto-generated Chapters

>Authors: Azin Ghazimatin, Ekaterina Garmash, Gustavo Penha, Kristen Sheets, Martin Achenbach, Oguz Semerci, Remi Galvez, Marcus Tannenberg, Sahitya Mantravadi, Divya Narayanan, Ofeliya Kalaydzhyan, Douglas Cole, Ben Carterette, Ann Clifton, Paul N. Bennett, Claudia Hauff, Mounia Lalmas

>2024-10-21

> http://arxiv.org/abs/2410.16148v1

Listeners of long-form talk-audio content, such as podcast episodes, often
find it challenging to understand the overall structure and locate relevant
sections. A practical solution is to divide episodes into
chapters--semantically coherent segments labeled with titles and timestamps.
Since most episodes on our platform at Spotify currently lack creator-provided
chapters, automating the creation of chapters is essential. Scaling the
chapterization of podcast episodes presents unique challenges. First, episodes
tend to be less structured than written texts, featuring spontaneous
discussions with nuanced transitions. Second, the transcripts are usually
lengthy, averaging about 16,000 tokens, which necessitates efficient processing
that can preserve context. To address these challenges, we introduce PODTILE, a
fine-tuned encoder-decoder transformer to segment conversational data. The
model simultaneously generates chapter transitions and titles for the input
transcript. To preserve context, each input text is augmented with global
context, including the episode's title, description, and previous chapter
titles. In our intrinsic evaluation, PODTILE achieved an 11% improvement in
ROUGE score over the strongest baseline. Additionally, we provide insights into
the practical benefits of auto-generated chapters for listeners navigating
episode content. Our findings indicate that auto-generated chapters serve as a
useful tool for engaging with less popular podcasts. Finally, we present
empirical evidence that using chapter titles can enhance effectiveness of
**sparse** retrieval in search tasks.


## Beyond 24 exploring VNM sparsity for efficient transformer inference on GPUs

>Authors: Kang Zhao, Tao Yuan, Han Bao, Zhenfeng Su, Chang Gao, Zhaofeng Sun, Zichen Liang, Liping Jing, Jianfei Chen

>2024-10-21

> http://arxiv.org/abs/2410.16135v1

To date, 2:4 **sparsity** has stood as the only **sparse** pattern that can be
accelerated using **sparse** tensor cores on GPUs. In practice, 2:4 **sparsity** often
possesses low actual speedups ($\leq 1.3$) and requires fixed **sparse** ratios,
meaning that other ratios, such as 4:8, 8:16, or those exceeding 50% **sparsity**,
do not incur any speedups on GPUs. Recent studies suggest that V:N:M **sparsity**
is promising in addressing these limitations of 2:4 **sparsity**. However,
regarding accuracy, the effects of V:N:M **sparsity** on broader Transformer
models, such as vision Transformers and large language models (LLMs), are
largely unexamined. Moreover, Some specific issues related to V:N:M **sparsity**,
such as how to select appropriate V and M values, remain unresolved. In this
study, we thoroughly investigate the application of V:N:M **sparsity** in vision
models and LLMs across multiple tasks, from pertaining to downstream tasks. We
propose three key approaches to enhance the applicability and accuracy of
V:N:M-**sparse** Transformers, including heuristic V and M selection,
V:N:M-specific channel permutation, and three-staged LoRA training techniques.
Experimental results show that, with our methods, the DeiT-small achieves
lossless accuracy at 64:2:5 **sparsity**, while the DeiT-base maintains accuracy
even at 64:2:8 **sparsity**. In addition, the fine-tuned LLama2-7B at 64:2:5
**sparsity** performs comparably or better than training-free 2:4 **sparse**
alternatives on downstream tasks. More importantly, V:N:M-**sparse** Transformers
offer a wider range of speedup-accuracy trade-offs compared to 2:4 **sparsity**.
Overall, our exploration largely facilitates the V:N:M **sparsity** to act as a
truly effective acceleration solution for Transformers in cost-sensitive
inference scenarios.


## Continuous Speech Synthesis using per-token Latent Diffusion

>Authors: Arnon Turetzky, Nimrod Shabtay, Slava Shechtman, Hagai Aronowitz, David Haws, Ron Hoory, Avihu Dekel

>2024-10-21

> http://arxiv.org/abs/2410.16048v1

The success of autoregressive transformer models with discrete tokens has
inspired **quantization**-based approaches for continuous modalities, though these
often limit reconstruction quality. We therefore introduce SALAD, a per-token
latent diffusion model for zero-shot text-to-speech, that operates on
continuous representations. SALAD builds upon the recently proposed expressive
diffusion head for image generation, and extends it to generate variable-length
outputs. Our approach utilizes semantic tokens for providing contextual
information and determining the stopping condition. We suggest three continuous
variants for our method, extending popular discrete speech synthesis
techniques. Additionally, we implement discrete baselines for each variant and
conduct a comparative analysis of discrete versus continuous speech modeling
techniques. Our results demonstrate that both continuous and discrete
approaches are highly competent, and that SALAD achieves a superior
intelligibility score while obtaining speech quality and speaker similarity on
par with the ground-truth audio.


## Steering Knowledge Selection Behaviours in LLMs via SAE-Based Representation Engineering

>Authors: Yu Zhao, Alessio Devoto, Giwon Hong, Xiaotang Du, Aryo Pradipta Gema, Hongru Wang, Xuanli He, Kam-Fai Wong, Pasquale Minervini

>2024-10-21

> http://arxiv.org/abs/2410.15999v2

Large language models (LLMs) can store a significant amount of factual
knowledge in their parameters. However, their parametric knowledge may conflict
with the information provided in the context -- this phenomenon, known as
\emph{context-memory knowledge conflicts}, can lead to undesirable model
behaviour, such as reliance on outdated or incorrect information. Analysing the
internal activations of LLMs, we find that they can internally register the
signals of knowledge conflict at mid-layers. Such signals allow us to detect
whether a knowledge conflict occurs and use \emph{inference-time} intervention
strategies to resolve it. In this work, we propose \textsc{SpARE}, a
\emph{training-free} representation engineering method that uses pre-trained
**sparse** auto-encoders (SAEs) to control the knowledge selection behaviour of
LLMs. \textsc{SpARE} identifies the functional features that control the
knowledge selection behaviours and applies them to edit the internal
activations of LLMs at inference time. Our experimental results show that
\textsc{SpARE} can effectively control the usage of either knowledge source to
resolve knowledge conflict in open-domain question-answering tasks, surpassing
existing representation engineering methods ($+10\%$) as well as contrastive
decoding methods ($+15\%$).


## Residual vector quantization for KV cache compression in large language model

>Authors: Ankur Kumar

>2024-10-21

> http://arxiv.org/abs/2410.15704v1

KV cache compression methods have mainly relied on scalar **quantization**
techniques to reduce the memory requirements during decoding. In this work, we
apply residual vector **quantization**, which has been widely used for high
fidelity audio compression, to compress KV cache in large language models
(LLM). We adapt the standard recipe with minimal changes to compress the output
of any key or value projection matrix in a pretrained LLM: we scale the vector
by its standard deviation, divide channels into groups and then **quantize** each
group with the same residual vector **quantize**r. We learn the codebook using
exponential moving average and there are no other learnable parameters
including the input and output projections normally used in a vector
**quantization** set up. We find that a residual depth of 8 recovers most of the
performance of the un**quantize**d model. We also find that grouping non-contiguous
channels together works better than grouping contiguous channels for
compressing key matrix and the method further benefits from a light weight
finetuning of LLM together with the **quantization**. Overall, the proposed
technique is competitive with existing **quantization** methods while being much
simpler and results in 5.5x compression compared to half precision.


## A quantum anchor for higher Koszul brackets

>Authors: Ekaterina Shemyakova, Yagmur Yilmaz

>2024-10-21

> http://arxiv.org/abs/2410.15664v1

It is well known that the chain map between the de Rham and Poisson complexes
on a Poisson manifold also maps the Koszul bracket of differential forms into
the Schouten bracket of multivector fields.
  In the generalized case of a $P_\infty$-structure, where a Poisson bivector
$P$ is replaced by an arbitrary even multivector obeying $[[P,P]]=0$, an analog
of the chain map and an $L_\infty$-morphism from the higher Koszul brackets
into the Schouten bracket are also known; however, they differ significantly in
nature.
  In the present paper, we address the problem of quantizing this picture. In
particular, we show that the $L_\infty$-morphism is **quantize**d into a single
linear operator, which is a formal Fourier integral operator.
  This paper employs Voronov's thick morphism technique and quantum
Mackenzie-Xu transformations in the framework of $L_\infty$-algebroids.


## Pruning Foundation Models for High Accuracy without Retraining

>Authors: Pu Zhao, Fei Sun, Xuan Shen, Pinrui Yu, Zhenglun Kong, Yanzhi Wang, Xue Lin

>2024-10-21

> http://arxiv.org/abs/2410.15567v1

Despite the superior performance, it is challenging to deploy foundation
models or large language models (LLMs) due to their massive parameters and
computations. While **pruning** is a promising technique to reduce model size and
accelerate the inference, the traditional **pruning** techniques can hardly be
applied for LLMs as they need to finetune the model on the full dataset with
multiple epochs consuming massive data and hardware resources. To deal with
this problem, post-training **pruning** methods are proposed to prune LLMs in
one-shot without retraining. However, their accuracy after **pruning** may suffer
from certain performance degradation due to the lack of retraining with massive
data. To address this issue, in this paper, we first formulate the
post-training problem for layer-wise LLM compression to simultaneously prune
multiple weights in LLMs. Next, we provide an optimal solution for this problem
and design our post-training **pruning** algorithm for both unstructured and
semi-structured **sparsity**. Our extensive experiments demonstrate the superior
performance of the proposed methods in comparison to SOTA baselines across
various LLM families including transformer-based LLMs and Mamba-based LLMs.
Code link: https://github.com/piuzha/APT


## Bayesian Concept Bottleneck Models with LLM Priors

>Authors: Jean Feng, Avni Kothari, Luke Zier, Chandan Singh, Yan Shuo Tan

>2024-10-21

> http://arxiv.org/abs/2410.15555v1

Concept Bottleneck Models (CBMs) have been proposed as a compromise between
white-box and black-box models, aiming to achieve interpretability without
sacrificing accuracy. The standard training procedure for CBMs is to predefine
a candidate set of human-interpretable concepts, extract their values from the
training data, and identify a **sparse** subset as inputs to a transparent
prediction model. However, such approaches are often hampered by the tradeoff
between enumerating a sufficiently large set of concepts to include those that
are truly relevant versus controlling the cost of obtaining concept
extractions. This work investigates a novel approach that sidesteps these
challenges: BC-LLM iteratively searches over a potentially infinite set of
concepts within a Bayesian framework, in which Large Language Models (LLMs)
serve as both a concept extraction mechanism and prior. BC-LLM is broadly
applicable and multi-modal. Despite imperfections in LLMs, we prove that BC-LLM
can provide rigorous statistical inference and uncertainty quantification. In
experiments, it outperforms comparator methods including black-box models,
converges more rapidly towards relevant concepts and away from spuriously
correlated ones, and is more robust to out-of-distribution samples.

