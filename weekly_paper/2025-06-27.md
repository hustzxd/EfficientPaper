# 2025-06-27

# Table of Contents
* [mTSBench Benchmarking Multivariate Time Series Anomaly Detection and Model Selection at Scale](#mTSBench-Benchmarking-Multivariate-Time-Series-Anomaly-Detection-and-Model-Selection-at-Scale)
* [TopK Language Models](#TopK-Language-Models)
* [Early Stopping Tabular In-Context Learning](#Early-Stopping-Tabular-In-Context-Learning)
* [Computational Design of Two-Dimensional MoSi$_2$N$_4$ Family Field-Effect Transistor for Future Ångström-Scale CMOS Technology Nodes](#Computational-Design-of-Two-Dimensional-MoSi$_2$N$_4$-Family-Field-Effect-Transistor-for-Future-Ångström-Scale-CMOS-Technology-Nodes)
* [Active Inference AI Systems for Scientific Discovery](#Active-Inference-AI-Systems-for-Scientific-Discovery)
* [Diverse polymorphs and phase transitions in van der Waals In$_2$Se$_3$](#Diverse-polymorphs-and-phase-transitions-in-van-der-Waals-In$_2$Se$_3$)
* [Uncover Treasures in DCT Advancing JPEG Quality Enhancement by Exploiting Latent Correlations](#Uncover-Treasures-in-DCT-Advancing-JPEG-Quality-Enhancement-by-Exploiting-Latent-Correlations)
* [Learning to Skip the Middle Layers of Transformers](#Learning-to-Skip-the-Middle-Layers-of-Transformers)
* [CovDocker Benchmarking Covalent Drug Design with Tasks, Datasets, and Solutions](#CovDocker-Benchmarking-Covalent-Drug-Design-with-Tasks,-Datasets,-and-Solutions)
* [Little By Little Continual Learning via Self-Activated Sparse Mixture-of-Rank Adaptive Learning](#Little-By-Little-Continual-Learning-via-Self-Activated-Sparse-Mixture-of-Rank-Adaptive-Learning)
* [Forecasting Geopolitical Events with a Sparse Temporal Fusion Transformer and Gaussian Process Hybrid A Case Study in Middle Eastern and U.S. Conflict Dynamics](#Forecasting-Geopolitical-Events-with-a-Sparse-Temporal-Fusion-Transformer-and-Gaussian-Process-Hybrid-A-Case-Study-in-Middle-Eastern-and-U.S.-Conflict-Dynamics)
* [Interpretable Representation Learning for Additive Rule Ensembles](#Interpretable-Representation-Learning-for-Additive-Rule-Ensembles)
* [Free Electron Paths from Dirac's Wave Equation Elucidating Zitterbewegung and Spin](#Free-Electron-Paths-from-Dirac's-Wave-Equation-Elucidating-Zitterbewegung-and-Spin)
* [Efficient Training for Optical Computing](#Efficient-Training-for-Optical-Computing)
* [MultiFinRAG An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering](#MultiFinRAG-An-Optimized-Multimodal-Retrieval-Augmented-Generation-(RAG)-Framework-for-Financial-Question-Answering)
* [RAG-VisualRec An Open Resource for Vision- and Text-Enhanced Retrieval-Augmented Generation in Recommendation](#RAG-VisualRec-An-Open-Resource-for-Vision--and-Text-Enhanced-Retrieval-Augmented-Generation-in-Recommendation)
* [FINN-GL Generalized Mixed-Precision Extensions for FPGA-Accelerated LSTMs](#FINN-GL-Generalized-Mixed-Precision-Extensions-for-FPGA-Accelerated-LSTMs)
* [GPU Kernel Scientist An LLM-Driven Framework for Iterative Kernel Optimization](#GPU-Kernel-Scientist-An-LLM-Driven-Framework-for-Iterative-Kernel-Optimization)
* [The Ideation-Execution Gap Execution Outcomes of LLM-Generated versus Human Research Ideas](#The-Ideation-Execution-Gap-Execution-Outcomes-of-LLM-Generated-versus-Human-Research-Ideas)
* [Characterization and Mitigation of Training Instabilities in Microscaling Formats](#Characterization-and-Mitigation-of-Training-Instabilities-in-Microscaling-Formats)
* [AI Assistants to Enhance and Exploit the PETSc Knowledge Base](#AI-Assistants-to-Enhance-and-Exploit-the-PETSc-Knowledge-Base)
* [Fine-Tuning and Prompt Engineering of LLMs, for the Creation of Multi-Agent AI for Addressing Sustainable Protein Production Challenges](#Fine-Tuning-and-Prompt-Engineering-of-LLMs,-for-the-Creation-of-Multi-Agent-AI-for-Addressing-Sustainable-Protein-Production-Challenges)
* [Reinforcement Learning Increases Wind Farm Power Production by Enabling Closed-Loop Collaborative Control](#Reinforcement-Learning-Increases-Wind-Farm-Power-Production-by-Enabling-Closed-Loop-Collaborative-Control)
* [GPTailor Large Language Model Pruning Through Layer Cutting and Stitching](#GPTailor-Large-Language-Model-Pruning-Through-Layer-Cutting-and-Stitching)
* [SV-LLM An Agentic Approach for SoC Security Verification using Large Language Models](#SV-LLM-An-Agentic-Approach-for-SoC-Security-Verification-using-Large-Language-Models)
* [Exploiting Lightweight Hierarchical ViT and Dynamic Framework for Efficient Visual Tracking](#Exploiting-Lightweight-Hierarchical-ViT-and-Dynamic-Framework-for-Efficient-Visual-Tracking)
* [DipSVD Dual-importance Protected SVD for Efficient LLM Compression](#DipSVD-Dual-importance-Protected-SVD-for-Efficient-LLM-Compression)
* [Solver Performance of Accelerated MoM for Connected Arrays](#Solver-Performance-of-Accelerated-MoM-for-Connected-Arrays)
* [IMC-PINN-FE A Physics-Informed Neural Network for Patient-Specific Left Ventricular Finite Element Modeling with Image Motion Consistency and Biomechanical Parameter Estimation](#IMC-PINN-FE-A-Physics-Informed-Neural-Network-for-Patient-Specific-Left-Ventricular-Finite-Element-Modeling-with-Image-Motion-Consistency-and-Biomechanical-Parameter-Estimation)
* [Breaking Spatial Boundaries Spectral-Domain Registration Guided Hyperspectral and Multispectral Blind Fusion](#Breaking-Spatial-Boundaries-Spectral-Domain-Registration-Guided-Hyperspectral-and-Multispectral-Blind-Fusion)
* [Q-resafe Assessing Safety Risks and Quantization-aware Safety Patching for Quantized Large Language Models](#Q-resafe-Assessing-Safety-Risks-and-Quantization-aware-Safety-Patching-for-Quantized-Large-Language-Models)
* [DuoGPT Training-free Dual Sparsity through Activation-aware Pruning in LLMs](#DuoGPT-Training-free-Dual-Sparsity-through-Activation-aware-Pruning-in-LLMs)
* [Breaking the Boundaries of Long-Context LLM Inference Adaptive KV Management on a Single Commodity GPU](#Breaking-the-Boundaries-of-Long-Context-LLM-Inference-Adaptive-KV-Management-on-a-Single-Commodity-GPU)
* [The Impact of the Russia-Ukraine Conflict on the Cloud Computing Risk Landscape](#The-Impact-of-the-Russia-Ukraine-Conflict-on-the-Cloud-Computing-Risk-Landscape)
* [Bridging Compositional and Distributional Semantics A Survey on Latent Semantic Geometry via AutoEncoder](#Bridging-Compositional-and-Distributional-Semantics-A-Survey-on-Latent-Semantic-Geometry-via-AutoEncoder)
* [ToSA Token Merging with Spatial Awareness](#ToSA-Token-Merging-with-Spatial-Awareness)
* [Learning Instruction-Following Policies through Open-Ended Instruction Relabeling with Large Language Models](#Learning-Instruction-Following-Policies-through-Open-Ended-Instruction-Relabeling-with-Large-Language-Models)
* [MegaFold System-Level Optimizations for Accelerating Protein Structure Prediction Models](#MegaFold-System-Level-Optimizations-for-Accelerating-Protein-Structure-Prediction-Models)
* [Cross-Layer Discrete Concept Discovery for Interpreting Language Models](#Cross-Layer-Discrete-Concept-Discovery-for-Interpreting-Language-Models)
* [Radial Attention $O(n\log n)$ Sparse Attention with Energy Decay for Long Video Generation](#Radial-Attention-$O(n\log-n)$-Sparse-Attention-with-Energy-Decay-for-Long-Video-Generation)
* [Orthogonal Finetuning Made Scalable](#Orthogonal-Finetuning-Made-Scalable)
* [JoyAgents-R1 Joint Evolution Dynamics for Versatile Multi-LLM Agents with Reinforcement Learning](#JoyAgents-R1-Joint-Evolution-Dynamics-for-Versatile-Multi-LLM-Agents-with-Reinforcement-Learning)
* [SimpleGVR A Simple Baseline for Latent-Cascaded Video Super-Resolution](#SimpleGVR-A-Simple-Baseline-for-Latent-Cascaded-Video-Super-Resolution)
* [Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large Language Models](#Outlier-Safe-Pre-Training-for-Robust-4-Bit-Quantization-of-Large-Language-Models)
* [Performance Analysis of OAMP Detection for ODDM Modulation in Satellite Communications](#Performance-Analysis-of-OAMP-Detection-for-ODDM-Modulation-in-Satellite-Communications)
* [Tensor-Parallelism with Partially Synchronized Activations](#Tensor-Parallelism-with-Partially-Synchronized-Activations)
* [Position Intelligent Science Laboratory Requires the Integration of Cognitive and Embodied AI](#Position-Intelligent-Science-Laboratory-Requires-the-Integration-of-Cognitive-and-Embodied-AI)
* [Programming Geotechnical Reliability Algorithms using Generative AI](#Programming-Geotechnical-Reliability-Algorithms-using-Generative-AI)
* [AnTKV Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models](#AnTKV-Anchor-Token-Aware-Sub-Bit-Vector-Quantization-for-KV-Cache-in-Large-Language-Models)
* [Mem4Nav Boosting Vision-and-Language Navigation in Urban Environments with a Hierarchical Spatial-Cognition Long-Short Memory System](#Mem4Nav-Boosting-Vision-and-Language-Navigation-in-Urban-Environments-with-a-Hierarchical-Spatial-Cognition-Long-Short-Memory-System)
* [Center of Gravity-Guided Focusing Influence Mechanism for Multi-Agent Reinforcement Learning](#Center-of-Gravity-Guided-Focusing-Influence-Mechanism-for-Multi-Agent-Reinforcement-Learning)
* [Measuring and Guiding Monosemanticity](#Measuring-and-Guiding-Monosemanticity)
* [Tunable phase transitions from semimetals to Chern insulators in two-dimensional quadratic-band-crossing materials](#Tunable-phase-transitions-from-semimetals-to-Chern-insulators-in-two-dimensional-quadratic-band-crossing-materials)
* [WebGuard++Interpretable Malicious URL Detection via Bidirectional Fusion of HTML Subgraphs and Multi-Scale Convolutional BERT](#WebGuard++Interpretable-Malicious-URL-Detection-via-Bidirectional-Fusion-of-HTML-Subgraphs-and-Multi-Scale-Convolutional-BERT)
* [MNN-AECS Energy Optimization for LLM Decoding on Mobile Devices via Adaptive Core Selection](#MNN-AECS-Energy-Optimization-for-LLM-Decoding-on-Mobile-Devices-via-Adaptive-Core-Selection)
* [Skywork-SWE Unveiling Data Scaling Laws for Software Engineering in LLMs](#Skywork-SWE-Unveiling-Data-Scaling-Laws-for-Software-Engineering-in-LLMs)
* [HARPT A Corpus for Analyzing Consumers' Trust and Privacy Concerns in Mobile Health Apps](#HARPT-A-Corpus-for-Analyzing-Consumers'-Trust-and-Privacy-Concerns-in-Mobile-Health-Apps)
* [Video-XL-2 Towards Very Long-Video Understanding Through Task-Aware KV Sparsification](#Video-XL-2-Towards-Very-Long-Video-Understanding-Through-Task-Aware-KV-Sparsification)
* [VHU-Net Variational Hadamard U-Net for Body MRI Bias Field Correction](#VHU-Net-Variational-Hadamard-U-Net-for-Body-MRI-Bias-Field-Correction)
* [Black-Box Test Code Fault Localization Driven by Large Language Models and Execution Estimation](#Black-Box-Test-Code-Fault-Localization-Driven-by-Large-Language-Models-and-Execution-Estimation)
* [CommVQ Commutative Vector Quantization for KV Cache Compression](#CommVQ-Commutative-Vector-Quantization-for-KV-Cache-Compression)
* [Focus Your Attention Towards Data-Intuitive Lightweight Vision Transformers](#Focus-Your-Attention-Towards-Data-Intuitive-Lightweight-Vision-Transformers)
* [SWA-SOP Spatially-aware Window Attention for Semantic Occupancy Prediction in Autonomous Driving](#SWA-SOP-Spatially-aware-Window-Attention-for-Semantic-Occupancy-Prediction-in-Autonomous-Driving)
* [Universal Solvability for Robot Motion Planning on Graphs](#Universal-Solvability-for-Robot-Motion-Planning-on-Graphs)
* [Harnessing the Power of Reinforcement Learning for Language-Model-Based Information Retriever via Query-Document Co-Augmentation](#Harnessing-the-Power-of-Reinforcement-Learning-for-Language-Model-Based-Information-Retriever-via-Query-Document-Co-Augmentation)
* [ReDit Reward Dithering for Improved LLM Policy Optimization](#ReDit-Reward-Dithering-for-Improved-LLM-Policy-Optimization)
* [Learning Point Correspondences In Radar 3D Point Clouds For Radar-Inertial Odometry](#Learning-Point-Correspondences-In-Radar-3D-Point-Clouds-For-Radar-Inertial-Odometry)
* [Security Assessment of DeepSeek and GPT Series Models against Jailbreak Attacks](#Security-Assessment-of-DeepSeek-and-GPT-Series-Models-against-Jailbreak-Attacks)
* [LLMs on a Budget? Say HOLA](#LLMs-on-a-Budget?-Say-HOLA)
* [A Large Language Model-based Multi-Agent Framework for Analog Circuits' Sizing Relationships Extraction](#A-Large-Language-Model-based-Multi-Agent-Framework-for-Analog-Circuits'-Sizing-Relationships-Extraction)
* [SlimMoE Structured Compression of Large MoE Models via Expert Slimming and Distillation](#SlimMoE-Structured-Compression-of-Large-MoE-Models-via-Expert-Slimming-and-Distillation)
* [ARD-LoRA Dynamic Rank Allocation for Parameter-Efficient Fine-Tuning of Foundation Models with Heterogeneous Adaptation Needs](#ARD-LoRA-Dynamic-Rank-Allocation-for-Parameter-Efficient-Fine-Tuning-of-Foundation-Models-with-Heterogeneous-Adaptation-Needs)
* [Make It Efficient Dynamic Sparse Attention for Autoregressive Image Generation](#Make-It-Efficient-Dynamic-Sparse-Attention-for-Autoregressive-Image-Generation)
* [From Pixels and Words to Waves A Unified Framework for Spectral Dictionary vLLMs](#From-Pixels-and-Words-to-Waves-A-Unified-Framework-for-Spectral-Dictionary-vLLMs)
* [HE-LRM Encrypted Deep Learning Recommendation Models using Fully Homomorphic Encryption](#HE-LRM-Encrypted-Deep-Learning-Recommendation-Models-using-Fully-Homomorphic-Encryption)
* [Routing Mamba Scaling State Space Models with Mixture-of-Experts Projection](#Routing-Mamba-Scaling-State-Space-Models-with-Mixture-of-Experts-Projection)
* [Sparse Feature Coactivation Reveals Composable Semantic Modules in Large Language Models](#Sparse-Feature-Coactivation-Reveals-Composable-Semantic-Modules-in-Large-Language-Models)
* [Auto-Regressive Surface Cutting](#Auto-Regressive-Surface-Cutting)
* [Cost-Effective Optimization and Implementation of the CRT-Paillier Decryption Algorithm for Enhanced Performance](#Cost-Effective-Optimization-and-Implementation-of-the-CRT-Paillier-Decryption-Algorithm-for-Enhanced-Performance)
* [Evolving Prompts In-Context An Open-ended, Self-replicating Perspective](#Evolving-Prompts-In-Context-An-Open-ended,-Self-replicating-Perspective)
* [PlanMoGPT Flow-Enhanced Progressive Planning for Text to Motion Synthesis](#PlanMoGPT-Flow-Enhanced-Progressive-Planning-for-Text-to-Motion-Synthesis)
* [Robust PDE discovery under sparse and highly noisy conditions via attention neural networks](#Robust-PDE-discovery-under-sparse-and-highly-noisy-conditions-via-attention-neural-networks)
* [StainPIDR A Pathological Image Decouplingand Reconstruction Method for Stain Normalization Based on Color Vector Quantization and Structure Restaining](#StainPIDR-A-Pathological-Image-Decouplingand-Reconstruction-Method-for-Stain-Normalization-Based-on-Color-Vector-Quantization-and-Structure-Restaining)
* [Expanding Relevance Judgments for Medical Case-based Retrieval Task with Multimodal LLMs](#Expanding-Relevance-Judgments-for-Medical-Case-based-Retrieval-Task-with-Multimodal-LLMs)
* [Particle-in-cell simulations of plasma wakefield formation in microwave waveguides](#Particle-in-cell-simulations-of-plasma-wakefield-formation-in-microwave-waveguides)
* [Safe Pruning LoRA Robust Distance-Guided Pruning for Safety Alignment in Adaptation of LLMs](#Safe-Pruning-LoRA-Robust-Distance-Guided-Pruning-for-Safety-Alignment-in-Adaptation-of-LLMs)
* [The Evolution of Natural Language Processing How Prompt Optimization and Language Models are Shaping the Future](#The-Evolution-of-Natural-Language-Processing-How-Prompt-Optimization-and-Language-Models-are-Shaping-the-Future)
* [MDSAMMemory-Driven Sparse Attention Matrix for LVLMs Hallucination Mitigation](#MDSAMMemory-Driven-Sparse-Attention-Matrix-for-LVLMs-Hallucination-Mitigation)
* [EQuARX Efficient Quantized AllReduce in XLA for Distributed Machine Learning Acceleration](#EQuARX-Efficient-Quantized-AllReduce-in-XLA-for-Distributed-Machine-Learning-Acceleration)
* [TyphoFormer Language-Augmented Transformer for Accurate Typhoon Track Forecasting](#TyphoFormer-Language-Augmented-Transformer-for-Accurate-Typhoon-Track-Forecasting)
* [A novel fast short-time root music method for vibration monitoring of high-speed spindles](#A-novel-fast-short-time-root-music-method-for-vibration-monitoring-of-high-speed-spindles)
* [Towards Deeper GCNs Alleviating Over-smoothing via Iterative Training and Fine-tuning](#Towards-Deeper-GCNs-Alleviating-Over-smoothing-via-Iterative-Training-and-Fine-tuning)
* [A Grassroots Network and Community Roadmap for Interconnected Autonomous Science Laboratories for Accelerated Discovery](#A-Grassroots-Network-and-Community-Roadmap-for-Interconnected-Autonomous-Science-Laboratories-for-Accelerated-Discovery)
* [From Unstructured Communication to Intelligent RAG Multi-Agent Automation for Supply Chain Knowledge Bases](#From-Unstructured-Communication-to-Intelligent-RAG-Multi-Agent-Automation-for-Supply-Chain-Knowledge-Bases)
* [Code Generation for Near-Roofline Finite Element Actions on GPUs from Symbolic Variational Forms](#Code-Generation-for-Near-Roofline-Finite-Element-Actions-on-GPUs-from-Symbolic-Variational-Forms)
* [Trans${^2}$-CBCT A Dual-Transformer Framework for Sparse-View CBCT Reconstruction](#Trans${^2}$-CBCT-A-Dual-Transformer-Framework-for-Sparse-View-CBCT-Reconstruction)
* [Cache Me If You Can How Many KVs Do You Need for Effective Long-Context LMs?](#Cache-Me-If-You-Can-How-Many-KVs-Do-You-Need-for-Effective-Long-Context-LMs?)
* [Cross-Modal Epileptic Signal Harmonization Frequency Domain Mapping Quantization for Pre-training a Unified Neurophysiological Transformer](#Cross-Modal-Epileptic-Signal-Harmonization-Frequency-Domain-Mapping-Quantization-for-Pre-training-a-Unified-Neurophysiological-Transformer)
* [The Hidden Cost of an Image Quantifying the Energy Consumption of AI Image Generation](#The-Hidden-Cost-of-an-Image-Quantifying-the-Energy-Consumption-of-AI-Image-Generation)
* [RocketStack A level-aware deep recursive ensemble learning framework with exploratory feature fusion and model pruning dynamics](#RocketStack-A-level-aware-deep-recursive-ensemble-learning-framework-with-exploratory-feature-fusion-and-model-pruning-dynamics)
* [EHCube4P Learning Epistatic Patterns Through Hypercube Graph Convolution Neural Network for Protein Fitness Function Estimation](#EHCube4P-Learning-Epistatic-Patterns-Through-Hypercube-Graph-Convolution-Neural-Network-for-Protein-Fitness-Function-Estimation)
* [Vision-Based Multirotor Control for Spherical Target Tracking A Bearing-Angle Approach](#Vision-Based-Multirotor-Control-for-Spherical-Target-Tracking-A-Bearing-Angle-Approach)
* [eSapiens A Real-World NLP Framework for Multimodal Document Understanding and Enterprise Knowledge Processing](#eSapiens-A-Real-World-NLP-Framework-for-Multimodal-Document-Understanding-and-Enterprise-Knowledge-Processing)
* [Fast and Stable Diffusion Planning through Variational Adaptive Weighting](#Fast-and-Stable-Diffusion-Planning-through-Variational-Adaptive-Weighting)
* [A Simple Contrastive Framework Of Item Tokenization For Generative Recommendation](#A-Simple-Contrastive-Framework-Of-Item-Tokenization-For-Generative-Recommendation)


## mTSBench Benchmarking Multivariate Time Series Anomaly Detection and Model Selection at Scale

>Authors: Xiaona Zhou, Constantin Brif, Ismini Lourentzou

>2025-06-26

> http://arxiv.org/abs/2506.21550v1

Multivariate time series anomaly detection (MTS-AD) is critical in domains
like healthcare, cybersecurity, and industrial monitoring, yet remains
challenging due to complex inter-variable dependencies, temporal dynamics, and
**sparse** anomaly labels. We introduce mTSBench, the largest benchmark to date for
MTS-AD and unsupervised model selection, spanning 344 labeled time series
across 19 datasets and 12 diverse application domains. mTSBench evaluates 24
anomaly detection methods, including large language model (LLM)-based detectors
for multivariate time series, and systematically benchmarks unsupervised model
selection techniques under standardized conditions. Consistent with prior
findings, our results confirm that no single detector excels across datasets,
underscoring the importance of model selection. However, even state-of-the-art
selection methods remain far from optimal, revealing critical gaps. mTSBench
provides a unified evaluation suite to enable rigorous, reproducible
comparisons and catalyze future advances in adaptive anomaly detection and
robust model selection.


## TopK Language Models

>Authors: Ryosuke Takahashi, Tatsuro Inaba, Kentaro Inui, Benjamin Heinzerling

>2025-06-26

> http://arxiv.org/abs/2506.21468v1

Sparse autoencoders (SAEs) have become an important tool for analyzing and
interpreting the activation space of transformer-based language models (LMs).
However, SAEs suffer several shortcomings that diminish their utility and
internal validity. Since SAEs are trained post-hoc, it is unclear if the
failure to discover a particular concept is a failure on the SAE's side or due
to the underlying LM not representing this concept. This problem is exacerbated
by training conditions and architecture choices affecting which features an SAE
learns. When tracing how LMs learn concepts during training, the lack of
feature stability also makes it difficult to compare SAEs features across
different checkpoints. To address these limitations, we introduce a
modification to the transformer architecture that incorporates a TopK
activation function at chosen layers, making the model's hidden states
equivalent to the latent features of a TopK SAE. This approach eliminates the
need for post-hoc training while providing interpretability comparable to SAEs.
The resulting TopK LMs offer a favorable trade-off between model size,
computational efficiency, and interpretability. Despite this simple
architectural change, TopK LMs maintain their original capabilities while
providing robust interpretability benefits. Our experiments demonstrate that
the **sparse** representations learned by TopK LMs enable successful steering
through targeted neuron interventions and facilitate detailed analysis of
neuron formation processes across checkpoints and layers. These features make
TopK LMs stable and reliable tools for understanding how language models learn
and represent concepts, which we believe will significantly advance future
research on model interpretability and controllability.


## Early Stopping Tabular In-Context Learning

>Authors: Jaris Küken, Lennart Purucker, Frank Hutter

>2025-06-26

> http://arxiv.org/abs/2506.21387v1

Tabular foundation models have shown strong performance across various
tabular learning tasks via in-context learning, offering robust generalization
without any downstream finetuning. However, their inference-time costs remain
high, particularly for larger datasets. To address this, we propose
early-stopping the in-context learning process. We achieve this by dynamically
evaluating whether to stop in-context learning after each Transformer encoder
layer. Once stopped, we decode the embedding using a pre-trained layer-wise
decoder. Experiments across 34 small classification tasks size show that early
stopping in-context learning accelerates inference by up to x1.3 with
negligible degradation in predictive performance. To assess scalability, we
further evaluate our method on five larger classification tasks, achieving
speedups of up to x2.2. Our results demonstrate the potential of early exiting
as an effective and practical strategy for improving the efficiency of tabular
in-context learning.


## Computational Design of Two-Dimensional MoSi$_2$N$_4$ Family Field-Effect Transistor for Future Ångström-Scale CMOS Technology Nodes

>Authors: Che Chen Tho, Zongmeng Yang, Shibo Fang, Shiying Guo, Liemao Cao, Chit Siong Lau, Fei Liu, Shengli Zhang, Jing Lu, L. K. Ang, Lain-Jong Li, Yee Sin Ang

>2025-06-26

> http://arxiv.org/abs/2506.21366v1

Advancing complementary metal-oxide-semiconductor (CMOS) technology into the
sub-1-nm angstr\"om-scale technology nodes is expected to involve alternative
semiconductor channel materials, as silicon transistors encounter severe
performance degradation at physical gate lengths below 10 nm. Two-dimensional
(2D) semiconductors have emerged as strong candidates for overcoming
short-channel effects due to their atomically thin bodies, which inherently
suppress electrostatic leakage and improve gate control in aggressively scaled
field-effect transistors (FETs). Among the growing library of 2D materials, the
MoSi$_2$N$_4$ family -- a synthetic septuple-layered materials -- has attracted
increasing attention for its remarkable ambient stability, suitable bandgaps,
and favorable carrier transport characteristics, making it a promising platform
for next-generation transistors. While experimental realization of sub-10-nm 2D
FETs remains technologically demanding, computational device simulation using
first-principles density functional theory combined with nonequilibrium Green's
function transport simulations provide a powerful and cost-effective route for
exploring the performance limits and optimal design of ultrascaled FET. This
review consolidates the current progress in the computational design of
MoSi$_2$N$_4$ family FETs. We review the physical properties of MoSi$_2$N$_4$
that makes them compelling candidates for transistor applications, as well as
the simulated device performance and optimization strategy of MoSi$_2$N$_4$
family FETs. Finally, we identify key challenges and research gaps, and outline
future directions that could accelerate the practical deployment of
MoSi$_2$N$_4$ family FET in the angstr\"om-scale CMOS era.


## Active Inference AI Systems for Scientific Discovery

>Authors: Karthik Duraisamy

>2025-06-26

> http://arxiv.org/abs/2506.21329v1

The rapid evolution of artificial intelligence has led to expectations of
transformative scientific discovery, yet current systems remain fundamentally
limited by their operational architectures, brittle reasoning mechanisms, and
their separation from experimental reality. Building on earlier work, we
contend that progress in AI-driven science now depends on closing three
fundamental gaps -- the abstraction gap, the reasoning gap, and the reality gap
-- rather than on model size/data/test time compute. Scientific reasoning
demands internal representations that support simulation of actions and
response, causal structures that distinguish correlation from mechanism, and
continuous calibration. We define active inference AI systems for scientific
discovery as those that (i) maintain long-lived research memories grounded in
causal self-supervised foundation models, (ii) symbolic or neuro-symbolic
planners equipped with Bayesian guardrails, (iii) grow persistent knowledge
graphs where thinking generates novel conceptual nodes, reasoning establishes
causal edges, and real-world interaction prunes false connections while
strengthening verified pathways, and (iv) refine their internal representations
through closed-loop interaction with both high-fidelity simulators and
automated laboratories - an operational loop where mental simulation guides
action and empirical surprise reshapes understanding. In essence, we outline an
architecture where discovery arises from the interplay between internal models
that enable counterfactual reasoning and external validation that grounds
hypotheses in reality. It is also argued that the inherent ambiguity in
feedback from simulations and experiments, and underlying uncertainties makes
human judgment indispensable, not as a temporary scaffold but as a permanent
architectural component.


## Diverse polymorphs and phase transitions in van der Waals In$_2$Se$_3$

>Authors: Mingfeng Liu, Jiantao Wang, Peitao Liu, Qiang Wang, Zhibo Liu, Yan Sun, Xing-Qiu Chen

>2025-06-26

> http://arxiv.org/abs/2506.21248v1

Van der Waals In$_2$Se$_3$ has garnered significant attention due to its
unique properties and wide applications associated with its rich polymorphs and
polymorphic phase transitions. Despite extensive studies, the vast complex
polymorphic phase space remains largely unexplored, and the underlying
microscopic mechanism for their phase transformations remains elusive. Here, we
develop a highly accurate, efficient, and reliable machine-learning potential
(MLP), which not only facilitates accurate exploration of the intricate
potential energy surface (PES), but also enables us to conduct large-scale
molecular dynamics (MD) simulations with first-principles accuracy. We identify
the accurate structure of the $\beta''$ polymorph and uncover several
previously unreported $\beta'$ polymorph variants exhibiting dynamic stability
and competing energies, which are elucidated by characteristic flat imaginary
phonon bands and the distinctive Mexican-hat-like PES in the $\beta$ polymorph.
Through the MLP-accelerated MD simulations, we directly observe the polymorphic
phase transformations among the $\alpha$, $\beta$, $\beta'$, and $\beta''$
polymorphs under varying temperature and pressure conditions, and build for the
first time an ab initio temperature-pressure phase diagram, showing good
agreement with experiments. Furthermore, our MD simulations reveal a novel
strain-induced reversible phase transition between the $\beta'$ and $\beta''$
polymorphs. This work not only unveils diverse polymorphs in van der Waals
In$_2$Se$_3$, but also provides crucial atomic insights into their phase
transitions, opening new avenues for the design of novel functional electronic
devices.


## Uncover Treasures in DCT Advancing JPEG Quality Enhancement by Exploiting Latent Correlations

>Authors: Jing Yang, Qunliang Xing, Mai Xu, Minglang Qiao

>2025-06-26

> http://arxiv.org/abs/2506.21171v1

Joint Photographic Experts Group (JPEG) achieves data compression by
quantizing Discrete Cosine Transform (DCT) coefficients, which inevitably
introduces compression artifacts. Most existing JPEG quality enhancement
methods operate in the pixel domain, suffering from the high computational
costs of decoding. Consequently, direct enhancement of JPEG images in the DCT
domain has gained increasing attention. However, current DCT-domain methods
often exhibit limited performance. To address this challenge, we identify two
critical types of correlations within the DCT coefficients of JPEG images.
Building on this insight, we propose an Advanced DCT-domain JPEG Quality
Enhancement (AJQE) method that fully exploits these correlations. The AJQE
method enables the adaptation of numerous well-established pixel-domain models
to the DCT domain, achieving superior performance with reduced computational
complexity. Compared to the pixel-domain counterparts, the DCT-domain models
derived by our method demonstrate a 0.35 dB improvement in PSNR and a 60.5%
increase in enhancement throughput on average.


## Learning to Skip the Middle Layers of Transformers

>Authors: Tim Lawson, Laurence Aitchison

>2025-06-26

> http://arxiv.org/abs/2506.21103v1

Conditional computation is a popular strategy to make Transformers more
efficient. Existing methods often target individual modules (e.g.,
mixture-of-experts layers) or skip layers independently of one another.
However, interpretability research has demonstrated that the middle layers of
Transformers exhibit greater redundancy, and that early layers aggregate
information into token positions. Guided by these insights, we propose a novel
architecture that dynamically skips a variable number of layers from the middle
outward. In particular, a learned gating mechanism determines whether to bypass
a symmetric span of central blocks based on the input, and a gated attention
mechanism prevents subsequent tokens from attending to skipped token positions.
Residual norms are controlled with a 'sandwich' or 'perilayernorm' scheme and
gate **sparsity** with an adaptive regularization loss. We had aimed to reduce
compute requirements for 'simpler' tokens and potentially foster an emergent
multi-level representational hierarchy but, at the scales investigated, our
approach does not achieve improvements in the trade-off between validation
cross-entropy and estimated FLOPs compared to dense baselines with fewer
layers. We release our code at https://github.com/tim-lawson/skip-middle.


## CovDocker Benchmarking Covalent Drug Design with Tasks, Datasets, and Solutions

>Authors: Yangzhe Peng, Kaiyuan Gao, Liang He, Yuheng Cong, Haiguang Liu, Kun He, Lijun Wu

>2025-06-26

> http://arxiv.org/abs/2506.21085v1

Molecular docking plays a crucial role in predicting the binding mode of
ligands to target proteins, and covalent interactions, which involve the
formation of a covalent bond between the ligand and the target, are
particularly valuable due to their strong, enduring binding nature. However,
most existing docking methods and deep learning approaches hardly account for
the formation of covalent bonds and the associated structural changes. To
address this gap, we introduce a comprehensive benchmark for covalent docking,
CovDocker, which is designed to better capture the complexities of covalent
binding. We decompose the covalent docking process into three main tasks:
reactive location prediction, covalent reaction prediction, and covalent
docking. By adapting state-of-the-art models, such as Uni-Mol and Chemformer,
we establish baseline performances and demonstrate the effectiveness of the
benchmark in accurately predicting interaction sites and modeling the molecular
transformations involved in covalent binding. These results confirm the role of
the benchmark as a rigorous framework for advancing research in covalent drug
design. It underscores the potential of data-driven approaches to accelerate
the discovery of selective covalent inhibitors and addresses critical
challenges in therapeutic development.


## Little By Little Continual Learning via Self-Activated Sparse Mixture-of-Rank Adaptive Learning

>Authors: Haodong Lu, Chongyang Zhao, Jason Xue, Lina Yao, Kristen Moore, Dong Gong

>2025-06-26

> http://arxiv.org/abs/2506.21035v1

Continual learning (CL) with large pre-trained models is challenged by
catastrophic forgetting and task interference. Existing LoRA-based
Mixture-of-Experts (MoE) approaches mitigate forgetting by assigning and
freezing task-specific adapters, but suffer from interference, redundancy, and
ambiguous routing due to coarse adapter-level selection. However, this design
introduces three key challenges: 1) Interference: Activating full LoRA experts
per input leads to subspace interference and prevents selective reuse of useful
components across tasks. 2) Redundancy: Newly added experts often duplicate or
contradict existing knowledge due to unnecessary activation of unrelated ranks
and insufficient reuse of relevant ones. 3) Ambiguity: Overlapping features
across tasks confuse the router, resulting in unstable expert assignments. As
more experts accumulate, earlier task routing degrades, accelerating
forgetting. We propose MoRA, a Mixture-of-Rank Adaptive learning approach with
self-activated and **sparse** rank activation for CL. Unlike mixing multiple
low-rank matrices, MoRA decomposes each rank-r update into r rank-1 components,
each treated as an independent expert, enabling fine-grained mixture of rank-1
expert utilization while mitigating interference and redundancy. To avoid
ambiguous routing, we propose that each rank-1 expert can infer its own
relevance via intermediate activations. Coupled with our proposed rank **pruning**
and activation budgets, MoRA adaptively selects a **sparse** mixture of ranks per
input. We validate MoRA on continual learning tasks with CLIP and large
language models (LLMs), analyzing both in-domain learning and out-of-domain
forgetting/generalization during fine-tuning. MoRA shows significant
effectiveness on enhancing CL with PTMs, and improving generalization while
mitigating forgetting.


## Forecasting Geopolitical Events with a Sparse Temporal Fusion Transformer and Gaussian Process Hybrid A Case Study in Middle Eastern and U.S. Conflict Dynamics

>Authors: Hsin-Hsiung Huang, Hayden Hampton

>2025-06-26

> http://arxiv.org/abs/2506.20935v1

Forecasting geopolitical conflict from data sources like the Global Database
of Events, Language, and Tone (GDELT) is a critical challenge for national
security. The inherent **sparsity**, burstiness, and overdispersion of such data
cause standard deep learning models, including the Temporal Fusion Transformer
(TFT), to produce unreliable long-horizon predictions. We introduce STFT-VNNGP,
a hybrid architecture that won the 2023 Algorithms for Threat Detection (ATD)
competition by overcoming these limitations. Designed to bridge this gap, our
model employs a two-stage process: first, a TFT captures complex temporal
dynamics to generate multi-quantile forecasts. These quantiles then serve as
informed inputs for a Variational Nearest Neighbor Gaussian Process (VNNGP),
which performs principled spatiotemporal smoothing and uncertainty
quantification. In a case study forecasting conflict dynamics in the Middle
East and the U.S., STFT-VNNGP consistently outperforms a standalone TFT,
showing a superior ability to predict the timing and magnitude of bursty event
periods, particularly at long-range horizons. This work offers a robust
framework for generating more reliable and actionable intelligence from
challenging event data, with all code and workflows made publicly available to
ensure reproducibility.


## Interpretable Representation Learning for Additive Rule Ensembles

>Authors: Shahrzad Behzadimanesh, Pierre Le Bodic, Geoffrey I. Webb, Mario Boley

>2025-06-26

> http://arxiv.org/abs/2506.20927v1

Small additive ensembles of symbolic rules offer interpretable prediction
models. Traditionally, these ensembles use rule conditions based on
conjunctions of simple threshold propositions $x \geq t$ on a single input
variable $x$ and threshold $t$, resulting geometrically in axis-parallel
polytopes as decision regions. While this form ensures a high degree of
interpretability for individual rules and can be learned efficiently using the
gradient boosting approach, it relies on having access to a curated set of
expressive and ideally independent input features so that a small ensemble of
axis-parallel regions can describe the target variable well. Absent such
features, reaching sufficient accuracy requires increasing the number and
complexity of individual rules, which diminishes the interpretability of the
model. Here, we extend classical rule ensembles by introducing logical
propositions with learnable **sparse** linear transformations of input variables,
i.e., propositions of the form $\mathbf{x}^\mathrm{T}\mathbf{w} \geq t$, where
$\mathbf{w}$ is a learnable **sparse** weight vector, enabling decision regions as
general polytopes with oblique faces. We propose a learning method using
sequential greedy optimization based on an iteratively reweighted formulation
of logistic regression. Experimental results demonstrate that the proposed
method efficiently constructs rule ensembles with the same test risk as
state-of-the-art methods while significantly reducing model complexity across
ten benchmark datasets.


## Free Electron Paths from Dirac's Wave Equation Elucidating Zitterbewegung and Spin

>Authors: James L Beck

>2025-06-25

> http://arxiv.org/abs/2506.20857v1

Despite the widespread belief that Dirac's wave equation does not exhibit
electron paths, they are hiding in plain sight. The worldline of a free
electron is revealed by applying Dirac's velocity operator to its wave function
whose space-time arguments are expressed in a proper time by a Lorentz
transformation. This motion can be decomposed into two parts: the electron's
global motion of its inertia (or spin) center and an inherent local periodic
motion about this point that produces the electron's spin. The latter has the
ultra-high ZBW (zitterbewegung) frequency $\omega_0$ found by Schr\"{o}dinger
in his operator analysis of Dirac's equation and so is appropriately called the
\emph{zitter motion}. The decomposition corresponds to Gordon's decomposition
of Dirac's current where the so-called polarization and magnetization currents
are due to the zitter motion. In an inertial "rest"-frame fixed at the inertia
center, Dirac's wave function corresponding to the electron spin in a specified
direction implies that a free electron of mass $m$ moves in an inherent
perpetual zitter motion at the speed of light $c$ in a circle of radius
$c/\omega_0 = \hbar /(2mc)$ about the inertia center in a plane orthogonal to
this spin direction. The electron continuously accelerates about the spin
center without any external force because the inertia is effective at the spin
center, rather than at its charge center where the electron interacts with the
electro-magnetic potential. This analysis confirms the nature of ZBW directly
from Dirac's equation, agreeing with the conclusions of Barut and Zanghi, Beck,
Hestenes, Rivas and Salesi from their classical electron models. Furthermore,
these five classical models are equivalent and express the same free electron
dynamics as Dirac's equation.


## Efficient Training for Optical Computing

>Authors: Manon P. Bart, Nick Sparks, Ryan T. Glasser

>2025-06-25

> http://arxiv.org/abs/2506.20833v1

Diffractive optical information processors have demonstrated significant
promise in delivering high-speed, parallel, and energy efficient inference for
scaling machine learning tasks. Training, however, remains a major
computational bottleneck, compounded by large datasets and many simulations
required for state-of-the-art classification models. The underlying linear
transformations in such systems are inherently constrained to compositions of
circulant and diagonal matrix factors, representing free-space propagation and
phase and/or amplitude modulation of light, respectively. While theoretically
established that an arbitrary linear transformation can be generated by such
factors, only upper bounds on the number of factors exist, which are
experimentally unfeasible. Additionally, physical parameters such as
inter-layer distance, number of layers, and phase-only modulation further
restrict the solution space. Without tractable analytical decompositions, prior
works have implemented various constrained minimization techniques. As
trainable elements occupy a small subset of the overall transformation,
existing techniques incur unnecessary computational overhead, limiting
scalability. In this work, we demonstrate significant reduction in training
time by exploiting the structured and **sparse** nature of diffractive systems in
training and inference. We introduce a novel backpropagation algorithm that
incorporates plane wave decomposition via the Fourier transform, computing
gradients across all trainable elements in a given layer simultaneously, using
only change-of-basis and element wise multiplication. Given the lack of a
closed-form mathematical decomposition for realizable optical architectures,
this approach is not only valuable for machine learning tasks but broadly
applicable for the generation of arbitrary linear transformations, wavefront
shaping, and other signal processing tasks.


## MultiFinRAG An Optimized Multimodal Retrieval-Augmented Generation (RAG) Framework for Financial Question Answering

>Authors: Chinmay Gondhalekar, Urjitkumar Patel, Fang-Chun Yeh

>2025-06-25

> http://arxiv.org/abs/2506.20821v1

Financial documents--such as 10-Ks, 10-Qs, and investor presentations--span
hundreds of pages and combine diverse modalities, including dense narrative
text, structured tables, and complex figures. Answering questions over such
content often requires joint reasoning across modalities, which strains
traditional large language models (LLMs) and retrieval-augmented generation
(RAG) pipelines due to token limitations, layout loss, and fragmented
cross-modal context. We introduce MultiFinRAG, a retrieval-augmented generation
framework purpose-built for financial QA. MultiFinRAG first performs multimodal
extraction by grouping table and figure images into batches and sending them to
a lightweight, **quantize**d open-source multimodal LLM, which produces both
structured JSON outputs and concise textual summaries. These outputs, along
with narrative text, are embedded and indexed with modality-aware similarity
thresholds for precise retrieval. A tiered fallback strategy then dynamically
escalates from text-only to text+table+image contexts when necessary, enabling
cross-modal reasoning while reducing irrelevant context. Despite running on
commodity hardware, MultiFinRAG achieves 19 percentage points higher accuracy
than ChatGPT-4o (free-tier) on complex financial QA tasks involving text,
tables, images, and combined multimodal reasoning.


## RAG-VisualRec An Open Resource for Vision- and Text-Enhanced Retrieval-Augmented Generation in Recommendation

>Authors: Ali Tourani, Fatemeh Nazary, Yashar Deldjoo

>2025-06-25

> http://arxiv.org/abs/2506.20817v1

This paper addresses the challenge of developing multimodal recommender
systems for the movie domain, where limited metadata (e.g., title, genre) often
hinders the generation of robust recommendations. We introduce a resource that
combines LLM-generated plot descriptions with trailer-derived visual embeddings
in a unified pipeline supporting both Retrieval-Augmented Generation (RAG) and
collaborative filtering. Central to our approach is a data augmentation step
that transforms **sparse** metadata into richer textual signals, alongside fusion
strategies (e.g., PCA, CCA) that integrate visual cues. Experimental
evaluations demonstrate that CCA-based fusion significantly boosts recall
compared to unimodal baselines, while an LLM-driven re-ranking step further
improves NDCG, particularly in scenarios with limited textual data. By
releasing this framework, we invite further exploration of multi-modal
recommendation techniques tailored to cold-start, novelty-focused, and
domain-specific settings. All code, data, and detailed documentation are
publicly available at: https://github.com/RecSys-lab/RAG-VisualRec


## FINN-GL Generalized Mixed-Precision Extensions for FPGA-Accelerated LSTMs

>Authors: Shashwat Khandelwal, Jakoba Petri-Koenig, Thomas B. Preußer, Michaela Blott, Shreejith Shanker

>2025-06-25

> http://arxiv.org/abs/2506.20810v1

Recurrent neural networks (RNNs), particularly LSTMs, are effective for
time-series tasks like sentiment analysis and short-term stock prediction.
However, their computational complexity poses challenges for real-time
deployment in resource constrained environments. While FPGAs offer a promising
platform for energy-efficient AI **acceleration**, existing tools mainly target
feed-forward networks, and LSTM **acceleration** typically requires full custom
implementation. In this paper, we address this gap by leveraging the
open-source and extensible FINN framework to enable the generalized deployment
of LSTMs on FPGAs. Specifically, we leverage the Scan operator from the Open
Neural Network Exchange (ONNX) specification to model the recurrent nature of
LSTM computations, enabling support for mixed quantisation within them and
functional verification of LSTM-based models. Furthermore, we introduce custom
transformations within the FINN compiler to map the quantised ONNX computation
graph to hardware blocks from the HLS kernel library of the FINN compiler and
Vitis HLS. We validate the proposed tool-flow by training a quantised ConvLSTM
model for a mid-price stock prediction task using the widely used dataset and
generating a corresponding hardware IP of the model using our flow, targeting
the XCZU7EV device. We show that the generated quantised ConvLSTM accelerator
through our flow achieves a balance between performance (latency) and resource
consumption, while matching (or bettering) inference accuracy of
state-of-the-art models with reduced precision. We believe that the
generalisable nature of the proposed flow will pave the way for
resource-efficient RNN accelerator designs on FPGAs.


## GPU Kernel Scientist An LLM-Driven Framework for Iterative Kernel Optimization

>Authors: Martin Andrews, Sam Witteveen

>2025-06-25

> http://arxiv.org/abs/2506.20807v1

Optimizing GPU kernels for high performance is a complex task, often
demanding deep architectural knowledge, extensive profiling, and iterative
experimentation. This challenge is amplified when targeting newer or
less-documented GPU architectures where traditional development aids are
scarce. This paper introduces an LLM-powered "GPU Kernel Scientist," an
automated methodology for iteratively refining accelerator kernels.
  Our methodology employs LLMs in a multi-stage, evolutionary process: (a)
strategically selecting promising prior code versions as a basis for new
iterations; (b) generating hypotheses for optimization experiments, based on
existing code and assimilated knowledge from general GPU literature; and (c)
autonomously implementing these experiments through code modification and
subsequent submission to an external evaluation system, using only observed
timing data as performance feedback. We detail how this approach navigates the
challenges of the AMD MI300 target architecture and leverages LLMs to
compensate for limited domain-specific human expertise.
  Since quantitative results from an ongoing performance competition were
embargoed on paper submission date, we present the architectural design,
operational workflow, and qualitative insights, highlighting the potential of
LLM-driven agents to democratise and accelerate GPU kernel optimization,
especially in resource-constrained or rapidly evolving hardware environments.


## The Ideation-Execution Gap Execution Outcomes of LLM-Generated versus Human Research Ideas

>Authors: Chenglei Si, Tatsunori Hashimoto, Diyi Yang

>2025-06-25

> http://arxiv.org/abs/2506.20803v1

Large Language Models (LLMs) have shown promise in accelerating the
scientific research pipeline. A key capability for this process is the ability
to generate novel research ideas, and prior studies have found settings in
which LLM-generated research ideas were judged as more novel than human-expert
ideas. However, a good idea should not simply appear to be novel, it should
also result in better research after being executed. To test whether
AI-generated ideas lead to better research outcomes, we conduct an execution
study by recruiting 43 expert researchers to execute randomly-assigned ideas,
either written by experts or generated by an LLM. Each expert spent over 100
hours implementing the idea and wrote a 4-page short paper to document the
experiments. All the executed projects are then reviewed blindly by expert NLP
researchers. Comparing the review scores of the same ideas before and after
execution, the scores of the LLM-generated ideas decrease significantly more
than expert-written ideas on all evaluation metrics (novelty, excitement,
effectiveness, and overall; p < 0.05), closing the gap between LLM and human
ideas observed at the ideation stage. When comparing the aggregated review
scores from the execution study, we even observe that for many metrics there is
a flip in rankings where human ideas score higher than LLM ideas. This
ideation-execution gap highlights the limitations of current LLMs in generating
truly effective research ideas and the challenge of evaluating research ideas
in the absence of execution outcomes.


## Characterization and Mitigation of Training Instabilities in Microscaling Formats

>Authors: Huangyuan Su, Mujin Kwun, Stephanie Gil, Sham Kakade, Nikhil Anand

>2025-06-25

> http://arxiv.org/abs/2506.20752v1

Training large language models is an expensive, compute-bound process that
must be repeated as models scale, algorithms improve, and new data is
collected. To address this, next-generation hardware accelerators increasingly
support lower-precision arithmetic formats, such as the Microscaling (MX)
formats introduced in NVIDIA's Blackwell architecture. These formats use a
shared scale within blocks of parameters to extend representable range and
perform forward/backward GEMM operations in reduced precision for efficiency
gains. In this work, we investigate the challenges and viability of
block-scaled precision formats during model training. Across nearly one
thousand language models trained from scratch -- spanning compute budgets from
$2 \times 10^{17}$ to $4.8 \times 10^{19}$ FLOPs and sweeping over a broad
range of weight-activation precision combinations -- we consistently observe
that training in MX formats exhibits sharp, stochastic instabilities in the
loss, particularly at larger compute scales. To explain this phenomenon, we
conduct controlled experiments and ablations on a smaller proxy model that
exhibits similar behavior as the language model, sweeping across architectural
settings, hyperparameters, and precision formats. These experiments motivate a
simple model in which multiplicative gradient bias introduced by the
**quantization** of layer-norm affine parameters and a small fraction of
activations can trigger runaway divergence. Through \emph{in situ} intervention
experiments on our proxy model, we demonstrate that instabilities can be
averted or delayed by modifying precision schemes mid-training. Guided by these
findings, we evaluate stabilization strategies in the LLM setting and show that
certain hybrid configurations recover performance competitive with
full-precision training. We release our code at
https://github.com/Hither1/systems-scaling.


## AI Assistants to Enhance and Exploit the PETSc Knowledge Base

>Authors: Barry Smith, Junchao Zhang, Hong Zhang, Lois Curfman McInnes, Murat Keceli, Archit Vasan, Satish Balay, Toby Isaac, Le Chen, Venkatram Vishwanath

>2025-06-25

> http://arxiv.org/abs/2506.20608v1

Generative AI, especially through large language models (LLMs), is
transforming how technical knowledge can be accessed, reused, and extended.
PETSc, a widely used numerical library for high-performance scientific
computing, has accumulated a rich but fragmented knowledge base over its three
decades of development, spanning source code, documentation, mailing lists,
GitLab issues, Discord conversations, technical papers, and more. Much of this
knowledge remains informal and inaccessible to users and new developers. To
activate and utilize this knowledge base more effectively, the PETSc team has
begun building an LLM-powered system that combines PETSc content with custom
LLM tools -- including retrieval-augmented generation (RAG), reranking
algorithms, and chatbots -- to assist users, support developers, and propose
updates to formal documentation. This paper presents initial experiences
designing and evaluating these tools, focusing on system architecture, using
RAG and reranking for PETSc-specific information, evaluation methodologies for
various LLMs and embedding models, and user interface design. Leveraging the
Argonne Leadership Computing Facility resources, we analyze how LLM responses
can enhance the development and use of numerical software, with an initial
focus on scalable Krylov solvers. Our goal is to establish an extensible
framework for knowledge-centered AI in scientific software, enabling scalable
support, enriched documentation, and enhanced workflows for research and
development. We conclude by outlining directions for expanding this system into
a robust, evolving platform that advances software ecosystems to accelerate
scientific discovery.


## Fine-Tuning and Prompt Engineering of LLMs, for the Creation of Multi-Agent AI for Addressing Sustainable Protein Production Challenges

>Authors: Alexander D. Kalian, Jaewook Lee, Stefan P. Johannesson, Lennart Otte, Christer Hogstrand, Miao Guo

>2025-06-25

> http://arxiv.org/abs/2506.20598v1

The global demand for sustainable protein sources has accelerated the need
for intelligent tools that can rapidly process and synthesise domain-specific
scientific knowledge. In this study, we present a proof-of-concept multi-agent
Artificial Intelligence (AI) framework designed to support sustainable protein
production research, with an initial focus on microbial protein sources. Our
Retrieval-Augmented Generation (RAG)-oriented system consists of two GPT-based
LLM agents: (1) a literature search agent that retrieves relevant scientific
literature on microbial protein production for a specified microbial strain,
and (2) an information extraction agent that processes the retrieved content to
extract relevant biological and chemical information. Two parallel
methodologies, fine-tuning and prompt engineering, were explored for agent
optimisation. Both methods demonstrated effectiveness at improving the
performance of the information extraction agent in terms of transformer-based
cosine similarity scores between obtained and ideal outputs. Mean cosine
similarity scores were increased by up to 25%, while universally reaching mean
scores of $\geq 0.89$ against ideal output text. Fine-tuning overall improved
the mean scores to a greater extent (consistently of $\geq 0.94$) compared to
prompt engineering, although lower statistical uncertainties were observed with
the latter approach. A user interface was developed and published for enabling
the use of the multi-agent AI system, alongside preliminary exploration of
additional chemical safety-based search capabilities


## Reinforcement Learning Increases Wind Farm Power Production by Enabling Closed-Loop Collaborative Control

>Authors: Andrew Mole, Max Weissenbacher, Georgios Rigas, Sylvain Laizet

>2025-06-25

> http://arxiv.org/abs/2506.20554v1

Traditional wind farm control operates each turbine independently to maximize
individual power output. However, coordinated wake steering across the entire
farm can substantially increase the combined wind farm energy production.
Although dynamic closed-loop control has proven effective in flow control
applications, wind farm optimization has relied primarily on static,
low-fidelity simulators that ignore critical turbulent flow dynamics. In this
work, we present the first reinforcement learning (RL) controller integrated
directly with high-fidelity large-eddy simulation (LES), enabling real-time
response to atmospheric turbulence through collaborative, dynamic control
strategies. Our RL controller achieves a 4.30% increase in wind farm power
output compared to baseline operation, nearly doubling the 2.19% gain from
static optimal yaw control obtained through Bayesian optimization. These
results establish dynamic flow-responsive control as a transformative approach
to wind farm optimization, with direct implications for accelerating renewable
energy deployment to net-zero targets.


## GPTailor Large Language Model Pruning Through Layer Cutting and Stitching

>Authors: Guinan Su, Li Shen, Lu Yin, Shiwei Liu, Yanwu Yang, Jonas Geiping

>2025-06-25

> http://arxiv.org/abs/2506.20480v1

Large language models (LLMs) have shown remarkable capabilities in language
understanding and generation. However, such impressive capability typically
comes with a substantial model size, which presents significant challenges in
deployment and inference. While structured **pruning** of model parameters offers a
promising way to reduce computational costs at deployment time, current methods
primarily focus on single model **pruning**. In this work, we develop a novel
strategy to compress models by strategically combining or merging layers from
finetuned model variants, which preserves the original model's abilities by
aggregating capabilities accentuated in different finetunes. We pose the
optimal tailoring of these LLMs as a zero-order optimization problem, adopting
a search space that supports three different operations: (1) Layer removal, (2)
Layer selection from different candidate models, and (3) Layer merging. Our
experiments demonstrate that this approach leads to competitive model **pruning**,
for example, for the Llama2-13B model families, our compressed models maintain
approximately 97.3\% of the original performance while removing $\sim25\%$ of
parameters, significantly outperforming previous state-of-the-art methods. The
code is available at https://github.com/Guinan-Su/auto-merge-llm.


## SV-LLM An Agentic Approach for SoC Security Verification using Large Language Models

>Authors: Dipayan Saha, Shams Tarek, Hasan Al Shaikh, Khan Thamid Hasan, Pavan Sai Nalluri, Md. Ajoad Hasan, Nashmin Alam, Jingbo Zhou, Sujan Kumar Saha, Mark Tehranipoor, Farimah Farahmandi

>2025-06-25

> http://arxiv.org/abs/2506.20415v1

Ensuring the security of complex system-on-chips (SoCs) designs is a critical
imperative, yet traditional verification techniques struggle to keep pace due
to significant challenges in automation, scalability, comprehensiveness, and
adaptability. The advent of large language models (LLMs), with their remarkable
capabilities in natural language understanding, code generation, and advanced
reasoning, presents a new paradigm for tackling these issues. Moving beyond
monolithic models, an agentic approach allows for the creation of multi-agent
systems where specialized LLMs collaborate to solve complex problems more
effectively. Recognizing this opportunity, we introduce SV-LLM, a novel
multi-agent assistant system designed to automate and enhance SoC security
verification. By integrating specialized agents for tasks like verification
question answering, security asset identification, threat modeling, test plan
and property generation, vulnerability detection, and simulation-based bug
validation, SV-LLM streamlines the workflow. To optimize their performance in
these diverse tasks, agents leverage different learning paradigms, such as
in-context learning, fine-tuning, and retrieval-augmented generation (RAG). The
system aims to reduce manual intervention, improve accuracy, and accelerate
security analysis, supporting proactive identification and mitigation of risks
early in the design cycle. We demonstrate its potential to transform hardware
security practices through illustrative case studies and experiments that
showcase its applicability and efficacy.


## Exploiting Lightweight Hierarchical ViT and Dynamic Framework for Efficient Visual Tracking

>Authors: Ben Kang, Xin Chen, Jie Zhao, Chunjuan Bo, Dong Wang, Huchuan Lu

>2025-06-25

> http://arxiv.org/abs/2506.20381v1

Transformer-based visual trackers have demonstrated significant advancements
due to their powerful modeling capabilities. However, their practicality is
limited on resource-constrained devices because of their slow processing
speeds. To address this challenge, we present HiT, a novel family of efficient
tracking models that achieve high performance while maintaining fast operation
across various devices. The core innovation of HiT lies in its Bridge Module,
which connects lightweight transformers to the tracking framework, enhancing
feature representation quality. Additionally, we introduce a dual-image
position encoding approach to effectively encode spatial information. HiT
achieves an impressive speed of 61 frames per second (fps) on the NVIDIA Jetson
AGX platform, alongside a competitive AUC of 64.6% on the LaSOT benchmark,
outperforming all previous efficient trackers.Building on HiT, we propose
DyHiT, an efficient dynamic tracker that flexibly adapts to scene complexity by
selecting routes with varying computational requirements. DyHiT uses search
area features extracted by the backbone network and inputs them into an
efficient dynamic router to classify tracking scenarios. Based on the
classification, DyHiT applies a divide-and-conquer strategy, selecting
appropriate routes to achieve a superior trade-off between accuracy and speed.
The fastest version of DyHiT achieves 111 fps on NVIDIA Jetson AGX while
maintaining an AUC of 62.4% on LaSOT.Furthermore, we introduce a training-free
**acceleration** method based on the dynamic routing architecture of DyHiT. This
method significantly improves the execution speed of various high-performance
trackers without sacrificing accuracy. For instance, our **acceleration** method
enables the state-of-the-art tracker SeqTrack-B256 to achieve a 2.68 times
speedup on an NVIDIA GeForce RTX 2080 Ti GPU while maintaining the same AUC of
69.9% on the LaSOT.


## DipSVD Dual-importance Protected SVD for Efficient LLM Compression

>Authors: Xuan Ding, Rui Sun, Yunjian Zhang, Xiu Yan, Yueqi Zhou, Kaihao Huang, Suzhong Fu, Chuanlong Xie, Yao Zhu

>2025-06-25

> http://arxiv.org/abs/2506.20353v1

The ever-increasing computational demands and deployment costs of large
language models (LLMs) have spurred numerous compressing methods. Compared to
**quantization** and unstructured **pruning**, SVD compression offers superior hardware
compatibility and theoretical guarantees. However, existing SVD-based methods
focus on the overall discrepancy between the original and compressed matrices
while overlooking the protection of critical components within the matrix,
which leads to inferior performance in the compressed models. This paper
proposes a dual-level importance protection mechanism to enhance SVD-based
compression methods: (1) local importance protection: preserving the most
critical singular vectors within each weight matrix through channel-weighted
data whitening; and (2) global importance protection: enabling less important
layers to bear a greater portion of the compression burden through either a
heuristic or optimization-based approach, thereby minimizing the impact of
compression on critical layers. Extensive experiments demonstrate that DipSVD
outperforms existing SVD-based compression approaches across multiple
benchmarks, achieving superior model performance especially at high model
compression ratios.


## Solver Performance of Accelerated MoM for Connected Arrays

>Authors: Harald Hultin, Lucas Åkerstedt, B. L. G. Jonsson

>2025-06-25

> http://arxiv.org/abs/2506.20350v1

Simulating and developing large rectangularly shaped arrays with equidistant
interspacing is challenging as the computational complexity grows quickly with
array size. However, the geometrical shape of the array, appropriately meshed,
leads to a multilevel Toeplitz structure in the RWG-based Method of Moment
impedance matrix representation that can be used to mitigate the increased
complexity. This paper develops, presents and compares two different
accelerated solvers that both utilize the matrix structure to determine antenna
properties. Both methods use a novel mesh-partitioning algorithm and its
associated data representation, reducing storage and computational costs. The
first solver is an iterative method based on multilevel fast Fourier transform
to accelerate matrix multiplications. The second solver approach is based on an
extension of a fast direct Toeplitz solver, adapted to a block-matrix
structure. This fast direct solver is demonstrated to have close to machine
epsilon accuracy. Both accelerated methods are evaluated on two different array
element types, for arrays with up to 900 elements. The results are compared
with conventional direct and iterative matrix solvers. Improvements are seen in
both the time and required storage to solve the problem. The choice of the most
efficient method depends on the residual thresholds in the iterative method,
geometry of the element and frequency. Two different preconditioners for the
iterative method are investigated to evaluate their performance. The two
accelerated methods vastly outperform regular matrix inversion methods.


## IMC-PINN-FE A Physics-Informed Neural Network for Patient-Specific Left Ventricular Finite Element Modeling with Image Motion Consistency and Biomechanical Parameter Estimation

>Authors: Siyu Mu, Wei Xuan Chan, Choon Hwai Yap

>2025-06-25

> http://arxiv.org/abs/2506.20696v1

Elucidating the biomechanical behavior of the myocardium is crucial for
understanding cardiac physiology, but cannot be directly inferred from clinical
imaging and typically requires finite element (FE) simulations. However,
conventional FE methods are computationally expensive and often fail to
reproduce observed cardiac motions. We propose IMC-PINN-FE, a physics-informed
neural network (PINN) framework that integrates imaged motion consistency (IMC)
with FE modeling for patient-specific left ventricular (LV) biomechanics.
Cardiac motion is first estimated from MRI or echocardiography using either a
pre-trained attention-based network or an unsupervised cyclic-regularized
network, followed by extraction of motion modes. IMC-PINN-FE then rapidly
estimates myocardial stiffness and active tension by fitting clinical pressure
measurements, accelerating computation from hours to seconds compared to
traditional inverse FE. Based on these parameters, it performs FE modeling
across the cardiac cycle at 75x speedup. Through motion constraints, it matches
imaged displacements more accurately, improving average Dice from 0.849 to
0.927, while preserving realistic pressure-volume behavior. IMC-PINN-FE
advances previous PINN-FE models by introducing back-computation of material
properties and better motion fidelity. Using motion from a single subject to
reconstruct shape modes also avoids the need for large datasets and improves
patient specificity. IMC-PINN-FE offers a robust and efficient approach for
rapid, personalized, and image-consistent cardiac biomechanical modeling.


## Breaking Spatial Boundaries Spectral-Domain Registration Guided Hyperspectral and Multispectral Blind Fusion

>Authors: Kunjing Yang, Libin Zheng, Minru Bai, Ting Lu, Leyuan Fang

>2025-06-25

> http://arxiv.org/abs/2506.20293v1

The blind fusion of unregistered hyperspectral images (HSIs) and
multispectral images (MSIs) has attracted growing attention recently. To
address the registration challenge, most existing methods employ spatial
transformations on the HSI to achieve alignment with the MSI. However, due to
the substantial differences in spatial resolution of the images, the
performance of these methods is often unsatisfactory. Moreover, the
registration process tends to be time-consuming when dealing with large-sized
images in remote sensing. To address these issues, we propose tackling the
registration problem from the spectral domain. Initially, a lightweight
Spectral Prior Learning (SPL) network is developed to extract spectral features
from the HSI and enhance the spectral resolution of the MSI. Following this,
the obtained image undergoes spatial downsampling to produce the registered
HSI. In this process, subspace representation and cyclic training strategy are
employed to improve spectral accuracy of the registered HSI obtained. Next, we
propose a blind **sparse** fusion (BSF) method, which utilizes group **sparsity**
regularization to equivalently promote the low-rankness of the image. This
approach not only circumvents the need for rank estimation, but also reduces
computational complexity. Then, we employ the Proximal Alternating Optimization
(PAO) algorithm to solve the BSF model, and present its convergence analysis.
Finally, extensive numerical experiments on simulated and real datasets are
conducted to verify the effectiveness of our method in registration and fusion.
We also demonstrate its efficacy in enhancing classification performance.


## Q-resafe Assessing Safety Risks and Quantization-aware Safety Patching for Quantized Large Language Models

>Authors: Kejia Chen, Jiawen Zhang, Jiacong Hu, Yu Wang, Jian Lou, Zunlei Feng, Mingli Song

>2025-06-25

> http://arxiv.org/abs/2506.20251v1

Quantized large language models (LLMs) have gained increasing attention and
significance for enabling deployment in resource-constrained environments.
However, emerging studies on a few calibration dataset-free **quantization**
methods suggest that **quantization** may compromise the safety capabilities of
LLMs, underscoring the urgent need for systematic safety evaluations and
effective mitigation strategies. In this paper, we present comprehensive safety
evaluations across various mainstream **quantization** techniques and diverse
calibration datasets, utilizing widely accepted safety benchmarks. To address
the identified safety vulnerabilities, we propose a **quantization**-aware safety
patching framework, Q-resafe, to efficiently restore the safety capabilities of
**quantize**d LLMs while minimizing any adverse impact on utility. Extensive
experimental results demonstrate that Q-resafe successfully re-aligns the
safety of **quantize**d LLMs with their pre-**quantization** counterparts, even under
challenging evaluation scenarios. Project page is available at:
https://github.com/Thecommonirin/Qresafe.


## DuoGPT Training-free Dual Sparsity through Activation-aware Pruning in LLMs

>Authors: Ruokai Yin, Yuhang Li, Donghyun Lee, Priyadarshini Panda

>2025-06-25

> http://arxiv.org/abs/2506.20194v1

Large language models (LLMs) deliver strong performance but are difficult to
deploy due to high memory and compute costs. While **pruning** reduces these
demands, most methods ignore activation **sparsity** observed at runtime. We
reinterpret activation **sparsity** as dynamic structured weight **sparsity** and
propose DuoGPT, a unified framework that constructs dual-**sparse** (spMspV)
workloads by combining unstructured weight **pruning** with activation **sparsity**. To
preserve accuracy, we extend the Optimal Brain Compression (OBC) framework with
activation-aware calibration and introduce output residuals from the dense
model as correction terms. We further optimize the solution for efficient GPU
execution, enabling scalability to billion-parameter LLMs. Evaluations on
LLaMA-2 and LLaMA-3 show that DuoGPT outperforms state-of-the-art structured
**pruning** methods by up to 9.17% accuracy at an iso-speedup of 1.39$\times$
compared to the baseline dense model.


## Breaking the Boundaries of Long-Context LLM Inference Adaptive KV Management on a Single Commodity GPU

>Authors: He Sun, Li Li, Mingjun Xiao, Chengzhong Xu

>2025-06-25

> http://arxiv.org/abs/2506.20187v1

Advanced Large Language Models (LLMs) have achieved impressive performance
across a wide range of complex and long-context natural language tasks.
However, performing long-context LLM inference locally on a commodity GPU (a
PC) with privacy concerns remains challenging due to the increasing memory
demands of the key-value (**KV**) cache. Existing systems typically identify
important tokens and selectively offload their **KV** data to GPU and CPU memory.
The **KV** data needs to be offloaded to disk due to the limited memory on a
commodity GPU, but the process is bottlenecked by token importance evaluation
overhead and the disk's low bandwidth. In this paper, we present LeoAM, the
first efficient importance-aware long-context LLM inference system for a single
commodity GPU with adaptive hierarchical GPU-CPU-Disk **KV** management. Our system
employs an adaptive **KV** management strategy that partitions **KV** data into
variable-sized chunks based on the skewed distribution of attention weights
across different layers to reduce computational and additional transmission
overheads. Moreover, we propose a lightweight **KV** abstract method, which
minimizes transmission latency by storing and extracting the **KV** abstract of
each chunk on disk instead of the full **KV** data. LeoAM also leverages the
dynamic compression and pipeline techniques to further accelerate inference.
Experimental results demonstrate that LongInfer achieves an average inference
latency speedup of 3.46x, while maintaining comparable LLM response quality. In
scenarios with larger batch sizes, it achieves up to a 5.47x speedup.


## The Impact of the Russia-Ukraine Conflict on the Cloud Computing Risk Landscape

>Authors: Malikussaid, Sutiyo

>2025-06-25

> http://arxiv.org/abs/2506.20104v1

The Russian invasion of Ukraine has fundamentally altered the information
technology (IT) risk landscape, particularly in cloud computing environments.
This paper examines how this geopolitical conflict has accelerated data
sovereignty concerns, transformed cybersecurity paradigms, and reshaped cloud
infrastructure strategies worldwide. Through an analysis of documented cyber
operations, regulatory responses, and organizational adaptations between 2022
and early 2025, this research demonstrates how the conflict has served as a
catalyst for a broader reassessment of IT risk. The research reveals that while
traditional IT risk frameworks offer foundational guidance, their standard
application may inadequately address the nuances of state-sponsored threats,
conflicting data governance regimes, and the weaponization of digital
dependencies without specific geopolitical augmentation. The contribution of
this paper lies in its focused synthesis and strategic adaptation of existing
best practices into a multi-layered approach. This approach uniquely synergizes
resilient cloud architectures (including sovereign and hybrid models), enhanced
data-centric security strategies (such as advanced encryption and
privacy-enhancing technologies), and geopolitically-informed governance to
build digital resilience. The interplay between these layers, emphasizing how
geopolitical insights directly shape architectural and security choices beyond
standard best practices-particularly by integrating the human element,
including personnel vulnerabilities and expertise, as a core consideration in
technical design and operational management-offers a more robust defense
against the specific, multifaceted risks arising from geopolitical conflict in
increasingly fractured digital territories.


## Bridging Compositional and Distributional Semantics A Survey on Latent Semantic Geometry via AutoEncoder

>Authors: Yingji Zhang, Danilo S. Carvalho, André Freitas

>2025-06-25

> http://arxiv.org/abs/2506.20083v1

Integrating compositional and symbolic properties into current distributional
semantic spaces can enhance the interpretability, controllability,
compositionality, and generalisation capabilities of Transformer-based
auto-regressive language models (LMs). In this survey, we offer a novel
perspective on latent space geometry through the lens of compositional
semantics, a direction we refer to as \textit{semantic representation
learning}. This direction enables a bridge between symbolic and distributional
semantics, helping to mitigate the gap between them. We review and compare
three mainstream autoencoder architectures-Variational AutoEncoder (VAE),
Vector Quantised VAE (VQVAE), and Sparse AutoEncoder (SAE)-and examine the
distinctive latent geometries they induce in relation to semantic structure and
interpretability.


## ToSA Token Merging with Spatial Awareness

>Authors: Hsiang-Wei Huang, Wenhao Chai, Kuang-Ming Chen, Cheng-Yen Yang, Jenq-Neng Hwang

>2025-06-24

> http://arxiv.org/abs/2506.20066v1

Token merging has emerged as an effective strategy to accelerate Vision
Transformers (ViT) by reducing computational costs. However, existing methods
primarily rely on the visual token's feature similarity for token merging,
overlooking the potential of integrating spatial information, which can serve
as a reliable criterion for token merging in the early layers of ViT, where the
visual tokens only possess weak visual information. In this paper, we propose
ToSA, a novel token merging method that combines both semantic and spatial
awareness to guide the token merging process. ToSA leverages the depth image as
input to generate pseudo spatial tokens, which serve as auxiliary spatial
information for the visual token merging process. With the introduced spatial
awareness, ToSA achieves a more informed merging strategy that better preserves
critical scene structure. Experimental results demonstrate that ToSA
outperforms previous token merging methods across multiple benchmarks on visual
and embodied question answering while largely reducing the runtime of the ViT,
making it an efficient solution for ViT **acceleration**. The code will be
available at: https://github.com/hsiangwei0903/ToSA


## Learning Instruction-Following Policies through Open-Ended Instruction Relabeling with Large Language Models

>Authors: Zhicheng Zhang, Ziyan Wang, Yali Du, Fei Fang

>2025-06-24

> http://arxiv.org/abs/2506.20061v1

Developing effective instruction-following policies in reinforcement learning
remains challenging due to the reliance on extensive human-labeled instruction
datasets and the difficulty of learning from **sparse** rewards. In this paper, we
propose a novel approach that leverages the capabilities of large language
models (LLMs) to automatically generate open-ended instructions retrospectively
from previously collected agent trajectories. Our core idea is to employ LLMs
to relabel unsuccessful trajectories by identifying meaningful subtasks the
agent has implicitly accomplished, thereby enriching the agent's training data
and substantially alleviating reliance on human annotations. Through this
open-ended instruction relabeling, we efficiently learn a unified
instruction-following policy capable of handling diverse tasks within a single
policy. We empirically evaluate our proposed method in the challenging Craftax
environment, demonstrating clear improvements in sample efficiency, instruction
coverage, and overall policy performance compared to state-of-the-art
baselines. Our results highlight the effectiveness of utilizing LLM-guided
open-ended instruction relabeling to enhance instruction-following
reinforcement learning.


## MegaFold System-Level Optimizations for Accelerating Protein Structure Prediction Models

>Authors: Hoa La, Ahan Gupta, Alex Morehead, Jianlin Cheng, Minjia Zhang

>2025-06-24

> http://arxiv.org/abs/2506.20686v1

Protein structure prediction models such as AlphaFold3 (AF3) push the
frontier of biomolecular modeling by incorporating science-informed
architectural changes to the transformer architecture. However, these advances
come at a steep system cost, introducing: compute- and memory-intensive
operators, 2D attention mechanisms, and retrieval-augmented data pipelines,
which collectively hinder the scalability of AF3 training. In this work, we
present MegaFold, a cross-platform system to accelerate AF3 training. MegaFold
tackles key bottlenecks through ahead-of-time caching to eliminate GPU idle
time from the retrieval-augmented data pipeline, Triton-based kernels for
memory-efficient EvoAttention on heterogeneous devices, and deep fusion for
common and critical small operators in AF3. Evaluation on both NVIDIA H200 and
AMD MI250 GPUs shows that MegaFold reduces peak memory usage of AF3 training by
up to 1.23$\times$ and improves per-iteration training time by up-to
1.73$\times$ and 1.62$\times$ respectively. More importantly, MegaFold enables
training on 1.35$\times$ longer sequence lengths compared to PyTorch baselines
without running out-of-memory, significantly improving the scalability of
modern protein folding models. We open source our code at
https://github.com/Supercomputing-System-AI-Lab/MegaFold/.


## Cross-Layer Discrete Concept Discovery for Interpreting Language Models

>Authors: Ankur Garg, Xuemin Yu, Hassan Sajjad, Samira Ebrahimi Kahou

>2025-06-24

> http://arxiv.org/abs/2506.20040v1

Uncovering emergent concepts across transformer layers remains a significant
challenge because the residual stream linearly mixes and duplicates
information, obscuring how features evolve within large language models.
Current research efforts primarily inspect neural representations at single
layers, thereby overlooking this cross-layer superposition and the redundancy
it introduces. These representations are typically either analyzed directly for
activation patterns or passed to probing classifiers that map them to a limited
set of predefined concepts. To address these limitations, we propose
\gls{clvqvae}, a framework that uses vector **quantization** to map representations
across layers and in the process collapse duplicated residual-stream features
into compact, interpretable concept vectors. Our approach uniquely combines
top-$k$ temperature-based sampling during **quantization** with EMA codebook
updates, providing controlled exploration of the discrete latent space while
maintaining code-book diversity. We further enhance the framework with
scaled-spherical k-means++ for codebook initialization, which clusters by
directional similarity rather than magnitude, better aligning with semantic
structure in word embedding space.


## Radial Attention $O(n\log n)$ Sparse Attention with Energy Decay for Long Video Generation

>Authors: Xingyang Li, Muyang Li, Tianle Cai, Haocheng Xi, Shuo Yang, Yujun Lin, Lvmin Zhang, Songlin Yang, Jinbo Hu, Kelly Peng, Maneesh Agrawala, Ion Stoica, Kurt Keutzer, Song Han

>2025-06-24

> http://arxiv.org/abs/2506.19852v1

Recent advances in diffusion models have enabled high-quality video
generation, but the additional temporal dimension significantly increases
computational costs, making training and inference on long videos prohibitively
expensive. In this paper, we identify a phenomenon we term Spatiotemporal
Energy Decay in video diffusion models: post-softmax attention scores diminish
as spatial and temporal distance between tokens increase, akin to the physical
decay of signal or waves over space and time in nature. Motivated by this, we
propose Radial Attention, a scalable **sparse** attention mechanism with $O(n \log
n)$ complexity that translates energy decay into exponentially decaying compute
density, which is significantly more efficient than standard $O(n^2)$ dense
attention and more expressive than linear attention. Specifically, Radial
Attention employs a simple, static attention mask where each token attends to
spatially nearby tokens, with the attention window size shrinking with temporal
distance. Moreover, it allows pre-trained video diffusion models to extend
their generation length with efficient LoRA-based fine-tuning. Extensive
experiments show that Radial Attention maintains video quality across
Wan2.1-14B, HunyuanVideo, and Mochi 1, achieving up to a 1.9$\times$ speedup
over the original dense attention. With minimal tuning, it enables video
generation up to 4$\times$ longer while reducing training costs by up to
4.4$\times$ compared to direct fine-tuning and accelerating inference by up to
3.7$\times$ compared to dense attention inference.


## Orthogonal Finetuning Made Scalable

>Authors: Zeju Qiu, Weiyang Liu, Adrian Weller, Bernhard Schölkopf

>2025-06-24

> http://arxiv.org/abs/2506.19847v1

Orthogonal finetuning (OFT) offers highly parameter-efficient adaptation
while preventing catastrophic forgetting, but its high runtime and memory
demands limit practical deployment. We identify the core computational
bottleneck in OFT as its weight-centric implementation, which relies on costly
matrix-matrix multiplications with cubic complexity. To overcome this, we
propose OFTv2, an input-centric reformulation that instead uses matrix-vector
multiplications (i.e., matrix-free computation), reducing the computational
cost to quadratic. We further introduce the Cayley-Neumann parameterization, an
efficient orthogonal parameterization that approximates the matrix inversion in
Cayley transform via a truncated Neumann series. These modifications allow
OFTv2 to achieve up to 10x faster training and 3x lower GPU memory usage
without compromising performance. In addition, we extend OFTv2 to support
finetuning **quantize**d foundation models and show that it outperforms the popular
QLoRA in training stability, efficiency, and memory usage.


## JoyAgents-R1 Joint Evolution Dynamics for Versatile Multi-LLM Agents with Reinforcement Learning

>Authors: Ai Han, Junxing Hu, Pu Wei, Zhiqian Zhang, Yuhang Guo, Jiawei Lu, Zicheng Zhang

>2025-06-24

> http://arxiv.org/abs/2506.19846v1

Multi-agent reinforcement learning (MARL) has emerged as a prominent paradigm
for increasingly complex tasks. However, joint evolution across heterogeneous
agents remains challenging due to cooperative inefficiency and training
instability. In this paper, we propose the joint evolution dynamics for MARL
called JoyAgents-R1, which first applies Group Relative Policy Optimization
(GRPO) to the joint training of heterogeneous multi-agents. By iteratively
refining agents' large language models (LLMs) and memories, the method achieves
holistic equilibrium with optimal decision-making and memory capabilities.
Specifically, JoyAgents-R1 first implements node-wise Monte Carlo sampling on
the behavior of each agent across entire reasoning trajectories to enhance GRPO
sampling efficiency while maintaining policy diversity. Then, our marginal
benefit-driven selection strategy identifies top-$K$ sampling groups with
maximal reward fluctuations, enabling targeted agent model updates that improve
training stability and maximize joint benefits through cost-effective parameter
adjustments. Meanwhile, JoyAgents-R1 introduces an adaptive memory evolution
mechanism that repurposes GRPO rewards as cost-free supervisory signals to
eliminate repetitive reasoning and accelerate convergence. Experiments across
general and domain-specific scenarios demonstrate that JoyAgents-R1 achieves
performance comparable to that of larger LLMs while built on smaller
open-source models.


## SimpleGVR A Simple Baseline for Latent-Cascaded Video Super-Resolution

>Authors: Liangbin Xie, Yu Li, Shian Du, Menghan Xia, Xintao Wang, Fanghua Yu, Ziyan Chen, Pengfei Wan, Jiantao Zhou, Chao Dong

>2025-06-24

> http://arxiv.org/abs/2506.19838v1

Latent diffusion models have emerged as a leading paradigm for efficient
video generation. However, as user expectations shift toward higher-resolution
outputs, relying solely on latent computation becomes inadequate. A promising
approach involves decoupling the process into two stages: semantic content
generation and detail synthesis. The former employs a computationally intensive
base model at lower resolutions, while the latter leverages a lightweight
cascaded video super-resolution (VSR) model to achieve high-resolution output.
In this work, we focus on studying key design principles for latter cascaded
VSR models, which are underexplored currently. First, we propose two
degradation strategies to generate training pairs that better mimic the output
characteristics of the base model, ensuring alignment between the VSR model and
its upstream generator. Second, we provide critical insights into VSR model
behavior through systematic analysis of (1) timestep sampling strategies, (2)
noise augmentation effects on low-resolution (LR) inputs. These findings
directly inform our architectural and training innovations. Finally, we
introduce interleaving temporal unit and **sparse** local attention to achieve
efficient training and inference, drastically reducing computational overhead.
Extensive experiments demonstrate the superiority of our framework over
existing methods, with ablation studies confirming the efficacy of each design
choice. Our work establishes a simple yet effective baseline for cascaded video
super-resolution generation, offering practical insights to guide future
advancements in efficient cascaded synthesis systems.


## Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large Language Models

>Authors: Jungwoo Park, Taewhoo Lee, Chanwoong Yoon, Hyeon Hwang, Jaewoo Kang

>2025-06-24

> http://arxiv.org/abs/2506.19697v1

Extreme activation outliers in Large Language Models (LLMs) critically
degrade **quantization** performance, hindering efficient on-device deployment.
While channel-wise operations and adaptive gradient scaling are recognized
causes, practical mitigation remains challenging. We introduce Outlier-Safe
Pre-Training (OSP), a practical guideline that proactively prevents outlier
formation rather than relying on post-hoc mitigation. OSP combines three key
innovations: (1) the Muon optimizer, eliminating privileged bases while
maintaining training efficiency; (2) Single-Scale RMSNorm, preventing
channel-wise amplification; and (3) a learnable embedding projection,
redistributing activation magnitudes originating from embedding matrices. We
validate OSP by training a 1.4B-parameter model on 1 trillion tokens, which is
the first production-scale LLM trained without such outliers. Under aggressive
4-bit **quantization**, our OSP model achieves a 35.7 average score across 10
benchmarks (compared to 26.5 for an Adam-trained model), with only a 2%
training overhead. Remarkably, OSP models exhibit near-zero excess kurtosis
(0.04) compared to extreme values (1818.56) in standard models, fundamentally
altering LLM **quantization** behavior. Our work demonstrates that outliers are not
inherent to LLMs but are consequences of training strategies, paving the way
for more efficient LLM deployment. The source code and pretrained checkpoints
are available at https://github.com/dmis-lab/Outlier-Safe-Pre-Training.


## Performance Analysis of OAMP Detection for ODDM Modulation in Satellite Communications

>Authors: Yu Liu, Cunhua Pan, Tantao Gong, Yinlu Wang, Ming Chen

>2025-06-24

> http://arxiv.org/abs/2506.19688v1

Towards future 6G wireless networks, low earth orbit (LEO) satellites have
been widely considered as a promising component to enhance the terrestrial
communications. To ensure the link reliability of high-mobility satellite
communication scenarios, the emerging orthogonal delay-Doppler division
multiplexing (ODDM) modulation has attracted significant research attention. In
this paper, we study the diversity gain achieved by ODDM modulation along with
the mathematical analysis and numerical simulations. Additionally, we propose
an orthogonal approximate message passing (OAMP) algorithm based detector to
harvest the diversity gain promised by ODDM modulation. By operating the linear
and non-linear estimator iteratively, the orthogonal approximate message
passing (OAMP) detector can utilize the **sparsity** of the effective delay-Doppler
(DD) domain channel and extract the full diversity. Simulation results reveal
the relationship between diversity gain and system parameters, and demonstrate
that our proposed detector can achieve better performance than the conventional
message passing methods with significantly reduced complexity.


## Tensor-Parallelism with Partially Synchronized Activations

>Authors: Itay Lamprecht, Asaf Karnieli, Yair Hanani, Niv Giladi, Daniel Soudry

>2025-06-24

> http://arxiv.org/abs/2506.19645v1

Training and inference of Large Language Models (LLMs) with
tensor-parallelism requires substantial communication to synchronize
activations. Our findings suggest that with a few minor adjustments to current
practices, LLMs can be trained without fully synchronizing activations,
reducing bandwidth demands. We name this "Communication-Aware Architecture for
Tensor-parallelism" (CAAT-Net). We train 1B and 7B parameter CAAT-Net models,
with a 50% reduction in tensor-parallel communication and no significant drop
in pretraining accuracy. Furthermore, we demonstrate how CAAT-Net accelerates
both training and inference workloads.


## Position Intelligent Science Laboratory Requires the Integration of Cognitive and Embodied AI

>Authors: Sha Zhang, Suorong Yang, Tong Xie, Xiangyuan Xue, Zixuan Hu, Rui Li, Wenxi Qu, Zhenfei Yin, Tianfan Fu, Di Hu, Andres M Bran, Nian Ran, Bram Hoex, Wangmeng Zuo, Philippe Schwaller, Wanli Ouyang, Lei Bai, Yanyong Zhang, Lingyu Duan, Shixiang Tang, Dongzhan Zhou

>2025-06-24

> http://arxiv.org/abs/2506.19613v1

Scientific discovery has long been constrained by human limitations in
expertise, physical capability, and sleep cycles. The recent rise of AI
scientists and automated laboratories has accelerated both the cognitive and
operational aspects of research. However, key limitations persist: AI systems
are often confined to virtual environments, while automated laboratories lack
the flexibility and autonomy to adaptively test new hypotheses in the physical
world. Recent advances in embodied AI, such as generalist robot foundation
models, diffusion-based action policies, fine-grained manipulation learning,
and sim-to-real transfer, highlight the promise of integrating cognitive and
embodied intelligence. This convergence opens the door to closed-loop systems
that support iterative, autonomous experimentation and the possibility of
serendipitous discovery. In this position paper, we propose the paradigm of
Intelligent Science Laboratories (ISLs): a multi-layered, closed-loop framework
that deeply integrates cognitive and embodied intelligence. ISLs unify
foundation models for scientific reasoning, agent-based workflow orchestration,
and embodied agents for robust physical experimentation. We argue that such
systems are essential for overcoming the current limitations of scientific
discovery and for realizing the full transformative potential of AI-driven
science.


## Programming Geotechnical Reliability Algorithms using Generative AI

>Authors: Atma Sharma, Jie Zhang, Meng Lu, Shuangyi Wu, Baoxiang Li

>2025-06-24

> http://arxiv.org/abs/2506.19536v1

Programming reliability algorithms is crucial for risk assessment in
geotechnical engineering. This study explores the possibility of automating and
accelerating this task using Generative AI based on Large Language Models
(LLMs). Specifically, the most popular LLM, i.e., ChatGPT, is used to test the
ability to generate MATLAB codes for four classical reliability algorithms. The
four specific examples considered in this study are: (1) First Order
Reliability Method (FORM); (2) Subset simulation; (3) Random field simulation;
and (4) Bayesian update using Gibbs sampling. The results obtained using the
generated codes are compared with benchmark methods. It is found that the use
of LLMs can be promising for generating reliability codes. Failure,
limitations, and challenges of adopting LLMs are also discussed. Overall, this
study demonstrates that existing LLMs can be leveraged powerfully and can
contribute toward accelerating the adoption of reliability techniques in
routine geotechnical engineering.


## AnTKV Anchor Token-Aware Sub-Bit Vector Quantization for KV Cache in Large Language Models

>Authors: Zeyu Li, Chuanfu Xiao, Yang Wang, Xiang Liu, Zhenheng Tang, Baotong Lu, Mao Yang, Xinyu Chen, Xiaowen Chu

>2025-06-24

> http://arxiv.org/abs/2506.19505v1

Quantization has emerged as an effective and lightweight solution to reduce
the memory footprint of the **KV** cache in Large Language Models (LLMs).
Nevertheless, minimizing the performance degradation caused by ultra-**low-bit** **KV**
cache **quantization** remains a significant challenge. We observe that quantizing
the **KV** cache of different tokens has varying impacts on the quality of
attention outputs. To systematically investigate this phenomenon, we perform
forward error propagation analysis on attention and propose the Anchor Score
(AnS) that quantifies the sensitivity of each token's **KV** cache to
**quantization**-induced error. Our analysis reveals significant disparities in AnS
across tokens, suggesting that preserving a small subset with full precision
(FP16) of high-AnS tokens can greatly mitigate accuracy loss in aggressive
**quantization** scenarios. Based on this insight, we introduce AnT**KV**, a novel
framework that leverages Anchor Token-aware Vector Quantization to compress the
**KV** cache. Furthermore, to support efficient deployment, we design and develop a
triton kernel that is fully compatible with FlashAttention, enabling fast
online Anchor Token selection. AnT**KV** enables LLaMA-3-8B to handle context
lengths up to 840K tokens on a single 80GB A100 GPU, while achieving up to 3.5x
higher decoding throughput compared to the FP16 baseline. Our experiment
results demonstrate that AnT**KV** matches or outperforms prior works such as KIVI,
S**KV**Q, **KV**Quant, and CQ under 4-bit settings. More importantly, AnT**KV** achieves
significantly lower perplexity under ultra-**low-bit** **quantization** on Mistral-7B,
with only 6.32 at 1-bit and 8.87 at 0.375-bit, compared to the FP16 baseline of
4.73.


## Mem4Nav Boosting Vision-and-Language Navigation in Urban Environments with a Hierarchical Spatial-Cognition Long-Short Memory System

>Authors: Lixuan He, Haoyu Dong, Zhenxing Chen, Yangcheng Yu, Jie Feng, Yong Li

>2025-06-24

> http://arxiv.org/abs/2506.19433v1

Vision-and-Language Navigation (VLN) in large-scale urban environments
requires embodied agents to ground linguistic instructions in complex scenes
and recall relevant experiences over extended time horizons. Prior modular
pipelines offer interpretability but lack unified memory, while end-to-end
(M)LLM agents excel at fusing vision and language yet remain constrained by
fixed context windows and implicit spatial reasoning. We introduce
\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system
that can augment any VLN backbone. Mem4Nav fuses a **sparse** octree for
fine-grained voxel indexing with a semantic topology graph for high-level
landmark connectivity, storing both in trainable memory tokens embedded via a
reversible Transformer. Long-term memory (LTM) compresses and retains
historical observations at both octree and graph nodes, while short-term memory
(STM) caches recent multimodal entries in relative coordinates for real-time
obstacle avoidance and local planning. At each step, STM retrieval sharply
prunes dynamic context, and, when deeper history is needed, LTM tokens are
decoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and
Map2Seq across three backbones (modular, state-of-the-art VLN with prompt-based
LLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13
pp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW
improvement. Ablations confirm the indispensability of both the hierarchical
map and dual memory modules. Our codes are open-sourced via
https://github.com/tsinghua-fib-lab/Mem4Nav.


## Center of Gravity-Guided Focusing Influence Mechanism for Multi-Agent Reinforcement Learning

>Authors: Yisak Park, Sunwoo Lee, Seungyul Han

>2025-06-24

> http://arxiv.org/abs/2506.19417v1

Cooperative multi-agent reinforcement learning (MARL) under **sparse** rewards
presents a fundamental challenge due to limited exploration and insufficient
coordinated attention among agents. In this work, we propose the Focusing
Influence Mechanism (FIM), a novel framework that enhances cooperation by
directing agent influence toward task-critical elements, referred to as Center
of Gravity (CoG) state dimensions, inspired by Clausewitz's military theory.
FIM consists of three core components: (1) identifying CoG state dimensions
based on their stability under agent behavior, (2) designing counterfactual
intrinsic rewards to promote meaningful influence on these dimensions, and (3)
encouraging persistent and synchronized focus through eligibility-trace-based
credit accumulation. These mechanisms enable agents to induce more targeted and
effective state transitions, facilitating robust cooperation even in extremely
**sparse** reward settings. Empirical evaluations across diverse MARL benchmarks
demonstrate that the proposed FIM significantly improves cooperative
performance compared to baselines.


## Measuring and Guiding Monosemanticity

>Authors: Ruben Härle, Felix Friedrich, Manuel Brack, Stephan Wäldchen, Björn Deiseroth, Patrick Schramowski, Kristian Kersting

>2025-06-24

> http://arxiv.org/abs/2506.19382v1

There is growing interest in leveraging mechanistic interpretability and
controllability to better understand and influence the internal dynamics of
large language models (LLMs). However, current methods face fundamental
challenges in reliably localizing and manipulating feature representations.
Sparse Autoencoders (SAEs) have recently emerged as a promising direction for
feature extraction at scale, yet they, too, are limited by incomplete feature
isolation and unreliable monosemanticity. To systematically quantify these
limitations, we introduce Feature Monosemanticity Score (FMS), a novel metric
to quantify feature monosemanticity in latent representation. Building on these
insights, we propose Guided Sparse Autoencoders (G-SAE), a method that
conditions latent representations on labeled concepts during training. We
demonstrate that reliable localization and disentanglement of target concepts
within the latent space improve interpretability, detection of behavior, and
control. Specifically, our evaluations on toxicity detection, writing style
identification, and privacy attribute recognition show that G-SAE not only
enhances monosemanticity but also enables more effective and fine-grained
steering with less quality degradation. Our findings provide actionable
guidelines for measuring and advancing mechanistic interpretability and control
of LLMs.


## Tunable phase transitions from semimetals to Chern insulators in two-dimensional quadratic-band-crossing materials

>Authors: Wen-Hao Bian, Jing Wang

>2025-06-24

> http://arxiv.org/abs/2506.19378v1

We systematically investigate how static symmetry-breaking perturbations and
dynamic Floquet terms via a polarized light manipulate the topological phase
transitions in the two-dimensional quadratic-band-crossing-point (QBCP)
materials. The Berry curvature shows distinct behavior in such two situations.
It is linearly and quadratically proportional to the product of microstructural
parameters $t_{x,z}$ for the former and the latter, respectively. The static
perturbation eliminates the QBCP and opens an energy gap, which leads to the
momentum-inversion symmetry of Berry curvature. This yields a nontrivial Chern
number determined by the microstructural parameters. In contrast, we
demonstrate that either a circularly or an elliptically polarized light breaks
the time-reversal symmetry, transforming the QBCP semimetal into a Chern
insulator with a **quantize**d anomalous Hall conductivity $\sigma_{xy} =
Ce^2/\hbar$, where the Chern number is governed by the polarization angle.
Moreover, the linear polarization preserves the central antisymmetry of the
Berry curvature, giving rise to a topological trivial insulator. These results
establish a tunable topological phase transition from a QBCP semimetal to Chern
insulator in the two-dimensional QBCP materials.


## WebGuard++Interpretable Malicious URL Detection via Bidirectional Fusion of HTML Subgraphs and Multi-Scale Convolutional BERT

>Authors: Ye Tian, Zhang Yumin, Yifan Jia, Jianguo Sun, Yanbin Wang

>2025-06-24

> http://arxiv.org/abs/2506.19356v1

URL+HTML feature fusion shows promise for robust malicious URL detection,
since attacker artifacts persist in DOM structures. However, prior work suffers
from four critical shortcomings: (1) incomplete URL modeling, failing to
jointly capture lexical patterns and semantic context; (2) HTML graph **sparsity**,
where threat-indicative nodes (e.g., obfuscated scripts) are isolated amid
benign content, causing signal dilution during graph aggregation; (3)
unidirectional analysis, ignoring URL-HTML feature bidirectional interaction;
and (4) opaque decisions, lacking attribution to malicious DOM components. To
address these challenges, we present WebGuard++, a detection framework with 4
novel components: 1) Cross-scale URL Encoder: Hierarchically learns
local-to-global and coarse to fine URL features based on Transformer network
with dynamic convolution. 2) Subgraph-aware HTML Encoder: Decomposes DOM graphs
into interpretable substructures, amplifying **sparse** threat signals via
Hierarchical feature fusion. 3) Bidirectional Coupling Module: Aligns URL and
HTML embeddings through cross-modal contrastive learning, optimizing
inter-modal consistency and intra-modal specificity. 4) Voting Module:
Localizes malicious regions through consensus voting on malicious subgraph
predictions. Experiments show WebGuard++ achieves significant improvements over
state-of-the-art baselines, achieving 1.1x-7.9x higher TPR at fixed FPR of
0.001 and 0.0001 across both datasets.


## MNN-AECS Energy Optimization for LLM Decoding on Mobile Devices via Adaptive Core Selection

>Authors: Zhengxiang Huang, Chaoyue Niu, Zhaode Wang, Jiarui Xue, Hanming Zhang, Yugang Wang, Zewei Xin, Xiaotang Jiang, Chengfei Lv, Fan Wu, Guihai Chen

>2025-06-24

> http://arxiv.org/abs/2506.19884v1

As the demand for on-device Large Language Model (LLM) inference grows,
energy efficiency has become a major concern, especially for battery-limited
mobile devices. Our analysis shows that the memory-bound LLM decode phase
dominates energy use, and yet most existing works focus on accelerating the
prefill phase, neglecting energy concerns. We introduce Adaptive Energy-Centric
Core Selection (AECS) and integrate it into MNN to create the energy-efficient
version, MNN-AECS, the first engine-level system solution without requiring
root access or OS modifications for energy-efficient LLM decoding. MNN-AECS is
designed to reduce LLM decoding energy while keeping decode speed within an
acceptable slowdown threshold by dynamically selecting low-power CPU cores.
MNN-AECS is evaluated across 5 Android and 2 iOS devices on 5 popular LLMs of
various sizes. Compared to original MNN, MNN-AECS cuts down energy use by 23%
without slowdown averaged over all 7 devices and 4 datasets. Against other
engines, including llama.cpp, executorch, mllm, and MediaPipe, MNN-AECS
delivers 39% to 78% energy saving and 12% to 363% speedup on average.


## Skywork-SWE Unveiling Data Scaling Laws for Software Engineering in LLMs

>Authors: Liang Zeng, Yongcong Li, Yuzhen Xiao, Changshi Li, Chris Yuhao Liu, Rui Yan, Tianwen Wei, Jujie He, Xuchen Song, Yang Liu, Yahui Zhou

>2025-06-24

> http://arxiv.org/abs/2506.19290v1

Software engineering (SWE) has recently emerged as a crucial testbed for
next-generation LLM agents, demanding inherent capabilities in two critical
dimensions: sustained iterative problem-solving (e.g., >50 interaction rounds)
and long-context dependency resolution (e.g., >32k tokens). However, the data
curation process in SWE remains notoriously time-consuming, as it heavily
relies on manual annotation for code file filtering and the setup of dedicated
runtime environments to execute and validate unit tests. Consequently, most
existing datasets are limited to only a few thousand GitHub-sourced instances.
To this end, we propose an incremental, automated data-curation pipeline that
systematically scales both the volume and diversity of SWE datasets. Our
dataset comprises 10,169 real-world Python task instances from 2,531 distinct
GitHub repositories, each accompanied by a task specified in natural language
and a dedicated runtime-environment image for automated unit-test validation.
We have carefully curated over 8,000 successfully runtime-validated training
trajectories from our proposed SWE dataset. When fine-tuning the Skywork-SWE
model on these trajectories, we uncover a striking data scaling phenomenon: the
trained model's performance for software engineering capabilities in LLMs
continues to improve as the data size increases, showing no signs of
saturation. Notably, our Skywork-SWE model achieves 38.0% pass@1 accuracy on
the SWE-bench Verified benchmark without using verifiers or multiple rollouts,
establishing a new state-of-the-art (SOTA) among the Qwen2.5-Coder-32B-based
LLMs built on the OpenHands agent framework. Furthermore, with the
incorporation of test-time scaling techniques, the performance further improves
to 47.0% accuracy, surpassing the previous SOTA results for sub-32B parameter
models. We release the Skywork-SWE-32B model checkpoint to accelerate future
research.


## HARPT A Corpus for Analyzing Consumers' Trust and Privacy Concerns in Mobile Health Apps

>Authors: Timoteo Kelly, Abdulkadir Korkmaz, Samuel Mallet, Connor Souders, Sadra Aliakbarpour, Praveen Rao

>2025-06-24

> http://arxiv.org/abs/2506.19268v2

We present HARPT, a large-scale annotated corpus of mobile health app store
reviews aimed at advancing research in user privacy and trust. The dataset
comprises over 480,000 user reviews labeled into seven categories that capture
critical aspects of trust in applications, trust in providers and privacy
concerns. Creating HARPT required addressing multiple complexities, such as
defining a nuanced label schema, isolating relevant content from large volumes
of noisy data, and designing an annotation strategy that balanced scalability
with accuracy. This strategy integrated rule-based filtering, iterative manual
labeling with review, targeted data augmentation, and weak supervision using
transformer-based classifiers to accelerate coverage. In parallel, a carefully
curated subset of 7,000 reviews was manually annotated to support model
development and evaluation. We benchmark a broad range of classification
models, demonstrating that strong performance is achievable and providing a
baseline for future research. HARPT is released as a public resource to support
work in health informatics, cybersecurity, and natural language processing.


## Video-XL-2 Towards Very Long-Video Understanding Through Task-Aware KV Sparsification

>Authors: Minghao Qin, Xiangrui Liu, Zhengyang Liang, Yan Shu, Huaying Yuan, Juenjie Zhou, Shitao Xiao, Bo Zhao, Zheng Liu

>2025-06-24

> http://arxiv.org/abs/2506.19225v1

Multi-modal large language models (MLLMs) models have made significant
progress in video understanding over the past few years. However, processing
long video inputs remains a major challenge due to high memory and
computational costs. This makes it difficult for current models to achieve both
strong performance and high efficiency in long video understanding. To address
this challenge, we propose Video-XL-2, a novel MLLM that delivers superior
cost-effectiveness for long-video understanding based on task-aware **KV**
sparsification. The proposed framework operates with two key steps: chunk-based
pre-filling and bi-level key-value decoding. Chunk-based pre-filling divides
the visual token sequence into chunks, applying full attention within each
chunk and **sparse** attention across chunks. This significantly reduces
computational and memory overhead. During decoding, bi-level key-value decoding
selectively reloads either dense or **sparse** key-values for each chunk based on
its relevance to the task. This approach further improves memory efficiency and
enhances the model's ability to capture fine-grained information. Video-XL-2
achieves state-of-the-art performance on various long video understanding
benchmarks, outperforming existing open-source lightweight models. It also
demonstrates exceptional efficiency, capable of processing over 10,000 frames
on a single NVIDIA A100 (80GB) GPU and thousands of frames in just a few
seconds.


## VHU-Net Variational Hadamard U-Net for Body MRI Bias Field Correction

>Authors: Xin Zhu

>2025-06-23

> http://arxiv.org/abs/2506.19181v1

Bias field artifacts in magnetic resonance imaging (MRI) scans introduce
spatially smooth intensity inhomogeneities that degrade image quality and
hinder downstream analysis. To address this challenge, we propose a novel
variational Hadamard U-Net (VHU-Net) for effective body MRI bias field
correction. The encoder comprises multiple convolutional Hadamard transform
blocks (ConvHTBlocks), each integrating convolutional layers with a Hadamard
transform (HT) layer. Specifically, the HT layer performs channel-wise
frequency decomposition to isolate low-frequency components, while a subsequent
scaling layer and semi-soft thresholding mechanism suppress redundant
high-frequency noise. To compensate for the HT layer's inability to model
inter-channel dependencies, the decoder incorporates an inverse
HT-reconstructed transformer block, enabling global, frequency-aware attention
for the recovery of spatially consistent bias fields. The stacked decoder
ConvHTBlocks further enhance the capacity to reconstruct the underlying
ground-truth bias field. Building on the principles of variational inference,
we formulate a new evidence lower bound (ELBO) as the training objective,
promoting **sparsity** in the latent space while ensuring accurate bias field
estimation. Comprehensive experiments on abdominal and prostate MRI datasets
demonstrate the superiority of VHU-Net over existing state-of-the-art methods
in terms of intensity uniformity, signal fidelity, and tissue contrast.
Moreover, the corrected images yield substantial downstream improvements in
segmentation accuracy. Our framework offers computational efficiency,
interpretability, and robust performance across multi-center datasets, making
it suitable for clinical deployment.


## Black-Box Test Code Fault Localization Driven by Large Language Models and Execution Estimation

>Authors: Ahmadreza Saboor Yaraghi, Golnaz Gharachorlu, Sakina Fatima, Lionel C. Briand, Ruiyuan Wan, Ruifeng Gao

>2025-06-23

> http://arxiv.org/abs/2506.19045v1

Fault localization (FL) is a critical step in debugging which typically
relies on repeated executions to pinpoint faulty code regions. However,
repeated executions can be impractical in the presence of non-deterministic
failures or high execution costs. While recent efforts have leveraged Large
Language Models (LLMs) to aid execution-free FL, these have primarily focused
on identifying faults in the system under test (SUT) rather than in the often
complex system test code. However, the latter is also important as, in
practice, many failures are triggered by faulty test code. To overcome these
challenges, we introduce a fully static, LLM-driven approach for system test
code fault localization (TCFL) that does not require executing the test case.
Our method uses a single failure execution log to estimate the test's execution
trace through three novel algorithms that identify only code statements likely
involved in the failure. This pruned trace, combined with the error message, is
used to prompt the LLM to rank potential faulty locations. Our black-box,
system-level approach requires no access to the SUT source code and is
applicable to large test scripts that assess full system behavior. We evaluate
our technique at function, block, and line levels using an industrial dataset
of faulty test cases not previously used in pre-training LLMs. Results show
that our best estimated trace closely match actual traces, with an F1 score of
around 90%. Additionally, **pruning** the complex system test code reduces the
LLM's inference time by up to 34% without any loss in FL performance. Our
results further suggest that block-level TCFL offers a practical balance,
narrowing the search space while preserving useful context, achieving an 81%
hit rate at top-3 (Hit@3).


## CommVQ Commutative Vector Quantization for KV Cache Compression

>Authors: Junyan Li, Yang Zhang, Muhammad Yusuf Hassan, Talha Chafekar, Tianle Cai, Zhile Ren, Pengsheng Guo, Foroozan Karimzadeh, Colorado Reed, Chong Wang, Chuang Gan

>2025-06-23

> http://arxiv.org/abs/2506.18879v1

Large Language Models (LLMs) are increasingly used in applications requiring
long context lengths, but the key-value (**KV**) cache often becomes a memory
bottleneck on GPUs as context grows. To address this, we propose Commutative
Vector Quantization (CommVQ) to significantly reduce memory usage for
long-context LLM inference. We first introduce additive **quantization** with a
lightweight encoder and codebook to compress the **KV** cache, which can be decoded
via simple matrix multiplication. To further reduce computational costs during
decoding, we design the codebook to be commutative with Rotary Position
Embedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.
This enables efficient integration of decoding into the self-attention
mechanism. Our approach achieves high accuracy with additive **quantization** and
low overhead via the RoPE-commutative codebook. Experiments on long-context
benchmarks and GSM8K show that our method reduces FP16 **KV** cache size by 87.5%
with 2-bit **quantization**, while outperforming state-of-the-art **KV** cache
**quantization** methods. Notably, it enables 1-bit **KV** cache **quantization** with
minimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context
length on a single RTX 4090 GPU. The source code is available at:
https://github.com/UMass-Embodied-AGI/CommVQ.


## Focus Your Attention Towards Data-Intuitive Lightweight Vision Transformers

>Authors: Suyash Gaurav, Muhammad Farhan Humayun, Jukka Heikkonen, Jatin Chaudhary

>2025-06-23

> http://arxiv.org/abs/2506.18791v1

The evolution of Vision Transformers has led to their widespread adaptation
to different domains. Despite large-scale success, there remain significant
challenges including their reliance on extensive computational and memory
resources for pre-training on huge datasets as well as difficulties in
task-specific transfer learning. These limitations coupled with energy
inefficiencies mainly arise due to the computation-intensive self-attention
mechanism. To address these issues, we propose a novel Super-Pixel Based Patch
Pooling (SPPP) technique that generates context-aware, semantically rich, patch
embeddings to effectively reduce the architectural complexity and improve
efficiency. Additionally, we introduce the Light Latent Attention (LLA) module
in our pipeline by integrating latent tokens into the attention mechanism
allowing cross-attention operations to significantly reduce the time and space
complexity of the attention module. By leveraging the data-intuitive patch
embeddings coupled with dynamic positional encodings, our approach adaptively
modulates the cross-attention process to focus on informative regions while
maintaining the global semantic structure. This targeted attention improves
training efficiency and accelerates convergence. Notably, the SPPP module is
lightweight and can be easily integrated into existing transformer
architectures. Extensive experiments demonstrate that our proposed architecture
provides significant improvements in terms of computational efficiency while
achieving comparable results with the state-of-the-art approaches, highlighting
its potential for energy-efficient transformers suitable for edge deployment.
(The code is available on our GitHub repository:
https://github.com/zser092/Focused-Attention-ViT).


## SWA-SOP Spatially-aware Window Attention for Semantic Occupancy Prediction in Autonomous Driving

>Authors: Helin Cao, Rafael Materla, Sven Behnke

>2025-06-23

> http://arxiv.org/abs/2506.18785v1

Perception systems in autonomous driving rely on sensors such as LiDAR and
cameras to perceive the 3D environment. However, due to occlusions and data
**sparsity**, these sensors often fail to capture complete information. Semantic
Occupancy Prediction (SOP) addresses this challenge by inferring both occupancy
and semantics of unobserved regions. Existing transformer-based SOP methods
lack explicit modeling of spatial structure in attention computation, resulting
in limited geometric awareness and poor performance in **sparse** or occluded
areas. To this end, we propose Spatially-aware Window Attention (SWA), a novel
mechanism that incorporates local spatial context into attention. SWA
significantly improves scene completion and achieves state-of-the-art results
on LiDAR-based SOP benchmarks. We further validate its generality by
integrating SWA into a camera-based SOP pipeline, where it also yields
consistent gains across modalities.


## Universal Solvability for Robot Motion Planning on Graphs

>Authors: Anubhav Dhar, Ashlesha Hota, Sudeshna Kolay, Pranav Nyati, Tanishq Prasad

>2025-06-23

> http://arxiv.org/abs/2506.18755v1

We study the Universal Solvability of Robot Motion Planning on Graphs (USolR)
problem: given an undirected graph G = (V, E) and p robots, determine whether
any arbitrary configuration of the robots can be transformed into any other
arbitrary configuration via a sequence of valid, collision-free moves. We
design a canonical accumulation procedure that maps arbitrary configurations to
configurations that occupy a fixed subset of vertices, enabling us to analyze
configuration reachability in terms of equivalence classes. We prove that in
instances that are not universally solvable, at least half of all
configurations are unreachable from a given one, and leverage this to design an
efficient randomized algorithm with one-sided error, which can be derandomized
with a blow-up in the running time by a factor of p. Further, we optimize our
deterministic algorithm by using the structure of the input graph G = (V, E),
achieving a running time of O(p * (|V| + |E|)) in **sparse** graphs and O(|V| +
|E|) in dense graphs. Finally, we consider the Graph Edge Augmentation for
Universal Solvability (EAUS) problem, where given a connected graph G that is
not universally solvable for p robots, the question is to check if for a given
budget b, at most b edges can be added to G to make it universally solvable for
p robots. We provide an upper bound of p - 2 on b for general graphs. On the
other hand, we also provide examples of graphs that require Theta(p) edges to
be added. We further study the Graph Vertex and Edge Augmentation for Universal
Solvability (VEAUS) problem, where a vertices and b edges can be added, and we
provide lower bounds on a and b.


## Harnessing the Power of Reinforcement Learning for Language-Model-Based Information Retriever via Query-Document Co-Augmentation

>Authors: Jingming Liu, Yumeng Li, Wei Shi, Yao-Xiang Ding, Hui Su, Kun Zhou

>2025-06-23

> http://arxiv.org/abs/2506.18670v1

Recent studies have proposed leveraging Large Language Models (LLMs) as
information retrievers through query rewriting. However, for challenging
corpora, we argue that enhancing queries alone is insufficient for robust
semantic matching; the LLM should also have sufficient understanding of the
corpus by directly handling and augmenting the documents themselves. To this
end, we present an LLM-based retriever empowered to augment both user queries
and corpus documents, with its policy fully explored via reinforcement learning
(RL) and minimal human inductive bias. Notably, we find that simply allowing
the LLM to modify documents yields little benefit unless paired with our
carefully designed bidirectional RL framework, which enables the LLM to
simultaneously learn and collaborate on both query and document augmentation
policies. A key technical challenge in realizing such a framework lies in
jointly updating both policies during training, where the rewards for the two
directions depend on each other, making their entangled reward intractable. Our
approach addresses this by introducing a reward sampling strategy and a
specifically designed RL algorithm that enables effective training with these
sampled rewards. Experimental results demonstrate that our approach
significantly enhances LLM-based retrieval performance in both **sparse** and dense
settings, particularly in difficult retrieval domains, and achieves strong
cross-benchmark generalization. Our code is released at
https://github.com/liujm2001/CoAugRetriever.


## ReDit Reward Dithering for Improved LLM Policy Optimization

>Authors: Chenxing Wei, Jiarui Yu, Ying Tiffany He, Hande Dong, Yao Shu, Fei Yu

>2025-06-23

> http://arxiv.org/abs/2506.18631v2

DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning
capabilities through its rule-based reward system. While it's a ''perfect''
reward system that effectively mitigates reward hacking, such reward functions
are often discrete. Our experimental observations suggest that discrete rewards
can lead to gradient anomaly, unstable optimization, and slow convergence. To
address this issue, we propose ReDit (Reward Dithering), a method that dithers
the discrete reward signal by adding simple random noise. With this perturbed
reward, exploratory gradients are continuously provided throughout the learning
process, enabling smoother gradient updates and accelerating convergence. The
injected noise also introduces stochasticity into flat reward regions,
encouraging the model to explore novel policies and escape local optima.
Experiments across diverse tasks demonstrate the effectiveness and efficiency
of ReDit. On average, ReDit achieves performance comparable to vanilla GRPO
with only approximately 10% the training steps, and furthermore, still exhibits
a 4% performance improvement over vanilla GRPO when trained for a similar
duration. Visualizations confirm significant mitigation of gradient issues with
ReDit. Moreover, theoretical analyses are provided to further validate these
advantages.


## Learning Point Correspondences In Radar 3D Point Clouds For Radar-Inertial Odometry

>Authors: Jan Michalczyk, Stephan Weiss, Jan Steinbrener

>2025-06-23

> http://arxiv.org/abs/2506.18580v1

Using 3D point clouds in odometry estimation in robotics often requires
finding a set of correspondences between points in subsequent scans. While
there are established methods for point clouds of sufficient quality,
state-of-the-art still struggles when this quality drops. Thus, this paper
presents a novel learning-based framework for predicting robust point
correspondences between pairs of noisy, **sparse** and unstructured 3D point clouds
from a light-weight, low-power, inexpensive, consumer-grade System-on-Chip
(SoC) Frequency Modulated Continuous Wave (FMCW) radar sensor. Our network is
based on the transformer architecture which allows leveraging the attention
mechanism to discover pairs of points in consecutive scans with the greatest
mutual affinity. The proposed network is trained in a self-supervised way using
set-based multi-label classification cross-entropy loss, where the ground-truth
set of matches is found by solving the Linear Sum Assignment (LSA) optimization
problem, which avoids tedious hand annotation of the training data.
Additionally, posing the loss calculation as multi-label classification permits
supervising on point correspondences directly instead of on odometry error,
which is not feasible for **sparse** and noisy data from the SoC radar we use. We
evaluate our method with an open-source state-of-the-art Radar-Inertial
Odometry (RIO) framework in real-world Unmanned Aerial Vehicle (UAV) flights
and with the widely used public Coloradar dataset. Evaluation shows that the
proposed method improves the position estimation accuracy by over 14 % and 19 %
on average, respectively. The open source code and datasets can be found here:
https://github.com/aau-cns/radar_transformer.


## Security Assessment of DeepSeek and GPT Series Models against Jailbreak Attacks

>Authors: Xiaodong Wu, Xiangman Li, Jianbing Ni

>2025-06-23

> http://arxiv.org/abs/2506.18543v1

The widespread deployment of large language models (LLMs) has raised critical
concerns over their vulnerability to jailbreak attacks, i.e., adversarial
prompts that bypass alignment mechanisms and elicit harmful or policy-violating
outputs. While proprietary models like GPT-4 have undergone extensive
evaluation, the robustness of emerging open-source alternatives such as
DeepSeek remains largely underexplored, despite their growing adoption in
real-world applications. In this paper, we present the first systematic
jailbreak evaluation of DeepSeek-series models, comparing them with GPT-3.5 and
GPT-4 using the HarmBench benchmark. We evaluate seven representative attack
strategies across 510 harmful behaviors categorized by both function and
semantic domain. Our analysis reveals that DeepSeek's Mixture-of-Experts (MoE)
architecture introduces routing **sparsity** that offers selective robustness
against optimization-based attacks such as TAP-T, but leads to significantly
higher vulnerability under prompt-based and manually engineered attacks. In
contrast, GPT-4 Turbo demonstrates stronger and more consistent safety
alignment across diverse behaviors, likely due to its dense Transformer design
and reinforcement learning from human feedback. Fine-grained behavioral
analysis and case studies further show that DeepSeek often routes adversarial
prompts to under-aligned expert modules, resulting in inconsistent refusal
behaviors. These findings highlight a fundamental trade-off between
architectural efficiency and alignment generalization, emphasizing the need for
targeted safety tuning and modular alignment strategies to ensure secure
deployment of open-source LLMs.


## LLMs on a Budget? Say HOLA

>Authors: Zohaib Hasan Siddiqui, Jiechao Gao, Ebad Shabbir, Mohammad Anas Azeez, Rafiq Ali, Gautam Siddharth Kashyap, Usman Naseem

>2025-06-23

> http://arxiv.org/abs/2506.18952v1

Running Large Language Models (LLMs) on edge devices is constrained by high
compute and memory demands posing a barrier for real-time applications in
sectors like healthcare, education, and embedded systems. Current solutions
such as **quantization**, **pruning**, and retrieval-augmented generation (RAG) offer
only partial optimizations and often compromise on speed or accuracy. We
introduce HOLA, an end-to-end optimization framework for efficient LLM
deployment. Internally, it leverages Hierarchical Speculative Decoding (HSD)
for faster inference without quality loss. Externally, AdaComp-RAG adjusts
retrieval complexity based on context needs. Together with LoBi, which blends
structured **pruning** (LoRA) and **quantization**, HOLA delivers significant gains:
17.6% EMA on GSM8K, 10.5% MCA on ARC, and reduced latency and memory on edge
devices like Jetson Nano--proving both scalable and production-ready.


## A Large Language Model-based Multi-Agent Framework for Analog Circuits' Sizing Relationships Extraction

>Authors: Chengjie Liu, Weiyu Chen, Huiyao Xu, Yuan Du, Jun Yang, Li Du

>2025-06-23

> http://arxiv.org/abs/2506.18424v1

In the design process of the analog circuit pre-layout phase, device sizing
is an important step in determining whether an analog circuit can meet the
required performance metrics. Many existing techniques extract the circuit
sizing task as a mathematical optimization problem to solve and continuously
improve the optimization efficiency from a mathematical perspective. But they
ignore the automatic introduction of prior knowledge, fail to achieve effective
**pruning** of the search space, which thereby leads to a considerable compression
margin remaining in the search space. To alleviate this problem, we propose a
large language model (LLM)-based multi-agent framework for analog circuits'
sizing relationships extraction from academic papers. The search space in the
sizing process can be effectively pruned based on the sizing relationship
extracted by this framework. Eventually, we conducted tests on 3 types of
circuits, and the optimization efficiency was improved by $2.32 \sim 26.6
\times$. This work demonstrates that the LLM can effectively prune the search
space for analog circuit sizing, providing a new solution for the combination
of LLMs and conventional analog circuit design automation methods.


## SlimMoE Structured Compression of Large MoE Models via Expert Slimming and Distillation

>Authors: Zichong Li, Chen Liang, Zixuan Zhang, Ilgee Hong, Young Jin Kim, Weizhu Chen, Tuo Zhao

>2025-06-23

> http://arxiv.org/abs/2506.18349v1

The Mixture of Experts (MoE) architecture has emerged as a powerful paradigm
for scaling large language models (LLMs) while maintaining inference
efficiency. However, their enormous memory requirements make them prohibitively
expensive to fine-tune or deploy in resource-constrained environments. To
address this challenge, we introduce SlimMoE, a multi-stage compression
framework for transforming large MoE models into much smaller, efficient
variants without incurring the prohibitive costs of training from scratch. Our
method systematically reduces parameter counts by slimming experts and
transferring knowledge through intermediate stages, effectively mitigating the
performance degradation common in one-shot **pruning** approaches. Using this
framework, we compress Phi 3.5-MoE (41.9B total/6.6B activated parameters) to
create Phi-mini-MoE (7.6B total/2.4B activated parameters) and Phi-tiny-MoE
(3.8B total/1.1B activated parameters) using only 400B tokens--less than 10% of
the original model's training data. These compressed models can be fine-tuned
on a single GPU (A100 for Phi-mini-MoE, A6000 for Phi-tiny-MoE), making them
highly suitable for academic and resource-limited settings. Our experiments
demonstrate that these compressed models outperform others of similar size and
remain competitive with larger models. For instance, Phi-mini-MoE achieves
similar or better performance to Phi-3-mini using only 2/3 of the activated
parameters and yields comparable MMLU scores to Llama 3.1 8B despite having
significantly lower latency. Our findings demonstrate that structured **pruning**
combined with staged distillation offers an effective path to creating
high-quality, compact MoE models, paving the way for broader adoption of MoE
architectures. We make our models publicly available at
https://huggingface.co/microsoft/Phi-mini-MoE-instruct and
https://huggingface.co/microsoft/Phi-tiny-MoE-instruct .


## ARD-LoRA Dynamic Rank Allocation for Parameter-Efficient Fine-Tuning of Foundation Models with Heterogeneous Adaptation Needs

>Authors: Haseeb Ullah Khan Shinwari, Muhammad Usama

>2025-06-23

> http://arxiv.org/abs/2506.18267v1

Conventional Low-Rank Adaptation (LoRA) methods employ a fixed rank, imposing
uniform adaptation across transformer layers and attention heads despite their
heterogeneous learning dynamics. This paper introduces Adaptive Rank Dynamic
LoRA (ARD-LoRA), a novel framework that automates rank allocation through
learnable scaling factors. These factors are optimized via a meta-objective
balancing task performance and parameter efficiency, incorporating $\ell_1$
**sparsity** for minimal rank and Total Variation regularization for stable rank
transitions. ARD-LoRA enables continuous, differentiable, per-head rank
adaptation. Experiments on LLAMA-3.1-70B and PaliGemma-2 demonstrate ARD-LoRA's
efficacy, achieving up to 99.3% of full fine-tuning performance with only 0.32%
trainable parameters, outperforming strong baselines like DoRA and AdaLoRA.
Furthermore, it reduces multimodal adaptation memory by 41%. These results
establish dynamic, fine-grained rank allocation as a critical paradigm for
efficient foundation model adaptation.


## Make It Efficient Dynamic Sparse Attention for Autoregressive Image Generation

>Authors: Xunzhi Xiang, Qi Fan

>2025-06-23

> http://arxiv.org/abs/2506.18226v1

Autoregressive conditional image generation models have emerged as a dominant
paradigm in text-to-image synthesis. These methods typically convert images
into one-dimensional token sequences and leverage the self-attention mechanism,
which has achieved remarkable success in natural language processing, to
capture long-range dependencies, model global context, and ensure semantic
coherence. However, excessively long contexts during inference lead to
significant memory overhead caused by **KV**-cache and computational delays. To
alleviate these challenges, we systematically analyze how global semantics,
spatial layouts, and fine-grained textures are formed during inference, and
propose a novel training-free context optimization method called Adaptive
Dynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies
historical tokens crucial for maintaining local texture consistency and those
essential for ensuring global semantic coherence, thereby efficiently
streamlining attention computation. Additionally, we introduce a dynamic
**KV**-cache update mechanism tailored for ADSA, reducing GPU memory consumption
during inference by approximately $50\%$. Extensive qualitative and
quantitative experiments demonstrate the effectiveness and superiority of our
approach in terms of both generation quality and resource efficiency.


## From Pixels and Words to Waves A Unified Framework for Spectral Dictionary vLLMs

>Authors: Andrew Kiruluta, Priscilla Burity

>2025-06-22

> http://arxiv.org/abs/2506.18943v1

Vision-language models (VLMs) unify computer vision and natural language
processing in a single architecture capable of interpreting and describing
images. Most state-of-the-art systems rely on two computationally intensive
components: convolutions in the vision encoder and quadratic self-attention for
multimodal fusion. This work removes both by introducing a spectral dictionary
token mixer, which represents each image patch or wordpiece as a **sparse**
combination of learnable frequency atoms. Our 1.1B-parameter prototype,
SDict-VLM, achieves BLEU-4 of 39.2, CIDEr of 127.5, and SPICE of 27.0 on
MS-COCO captioning, along with 50.3 percent accuracy on VQAv2. These results
close approximately 85 percent of the performance gap to BLIP-2 while using 60
percent fewer parameters, 2.3 times less peak GPU memory, and 2.2 times faster
inference than PaLI-3. To our knowledge, this is the first VLM to eliminate
both convolutions and self-attention while matching mid-scale transformer
baselines. In addition to its O(L log L) complexity, the shared frequency
dictionary enables transparent cross-modal alignment and offers a tunable
trade-off between accuracy and compute, paving the way for efficient and
interpretable VLMs.


## HE-LRM Encrypted Deep Learning Recommendation Models using Fully Homomorphic Encryption

>Authors: Karthik Garimella, Austin Ebel, Gabrielle De Micheli, Brandon Reagen

>2025-06-22

> http://arxiv.org/abs/2506.18150v1

Fully Homomorphic Encryption (FHE) is an encryption scheme that not only
encrypts data but also allows for computations to be applied directly on the
encrypted data. While computationally expensive, FHE can enable
privacy-preserving neural inference in the client-server setting: a client
encrypts their input with FHE and sends it to an untrusted server. The server
then runs neural inference on the encrypted data and returns the encrypted
results. The client decrypts the output locally, keeping both the input and
result private from the server. Private inference has focused on networks with
dense inputs such as image classification, and less attention has been given to
networks with **sparse** features. Unlike dense inputs, **sparse** features require
efficient encrypted lookup operations into large embedding tables, which
present computational and memory constraints for FHE.
  In this paper, we explore the challenges and opportunities when applying FHE
to Deep Learning Recommendation Models (DLRM) from both a compiler and systems
perspective. DLRMs utilize conventional MLPs for dense features and embedding
tables to map **sparse**, categorical features to dense vector representations. We
develop novel methods for performing compressed embedding lookups in order to
reduce FHE computational costs while keeping the underlying model performant.
Our embedding lookup improves upon a state-of-the-art approach by $77 \times$.
Furthermore, we present an efficient multi-embedding packing strategy that
enables us to perform a 44 million parameter embedding lookup under FHE.
Finally, we integrate our solutions into the open-source Orion framework and
present HE-LRM, an end-to-end encrypted DLRM. We evaluate HE-LRM on UCI (health
prediction) and Criteo (click prediction), demonstrating that with the right
compression and packing strategies, encrypted inference for recommendation
systems is practical.


## Routing Mamba Scaling State Space Models with Mixture-of-Experts Projection

>Authors: Zheng Zhan, Liliang Ren, Shuohang Wang, Liyuan Liu, Yang Liu, Yeyun Gong, Yanzhi Wang, Yelong Shen

>2025-06-22

> http://arxiv.org/abs/2506.18145v1

Linear State Space Models (SSMs) offer remarkable performance gains in
efficient sequence modeling, with constant inference-time computation and
memory complexity. Recent advances, such as Mamba, further enhance SSMs with
input-dependent gating and hardware-aware implementations, positioning them as
strong alternatives to Transformers for long sequence modeling. However,
efficiently scaling the expressive power of SSMs, particularly with Mixture of
Experts (MoE), remains challenging, as naive integration attempts often falter
or degrade performance. In this work, we introduce Routing Mamba (RoM), a novel
approach that scales SSM parameters using **sparse** mixtures of linear projection
experts. By sharing routing decisions between projection layers and lightweight
sub-modules within Mamba across experts, RoM leverages synergies among linear
projection experts for effective and efficient **sparse** scaling of Mamba layers.
At a scale of 1.3B active parameters (10B total) and 16K training sequence
length, RoM achieves language modeling performance equivalent to a dense Mamba
model requiring over 2.3x more active parameters, and demonstrates consistent
perplexity across context lengths. Experimental results further show RoM
effectively scales hybrid language models, yielding a 23% FLOPS saving compared
to dense Mamba scaling for similar performance.


## Sparse Feature Coactivation Reveals Composable Semantic Modules in Large Language Models

>Authors: Ruixuan Deng, Xiaoyang Hu, Miles Gilberti, Shane Storks, Aman Taxali, Mike Angstadt, Chandra Sripada, Joyce Chai

>2025-06-22

> http://arxiv.org/abs/2506.18141v1

We identify semantically coherent, context-consistent network components in
large language models (LLMs) using coactivation of **sparse** autoencoder (SAE)
features collected from just a handful of prompts. Focusing on country-relation
tasks, we show that ablating semantic components for countries and relations
changes model outputs in predictable ways, while amplifying these components
induces counterfactual responses. Notably, composing relation and country
components yields compound counterfactual outputs. We find that, whereas most
country components emerge from the very first layer, the more abstract relation
components are concentrated in later layers. Furthermore, within relation
components themselves, nodes from later layers tend to have a stronger causal
impact on model outputs. Overall, these findings suggest a modular organization
of knowledge within LLMs and advance methods for efficient, targeted model
manipulation.


## Auto-Regressive Surface Cutting

>Authors: Yang Li, Victor Cheung, Xinhai Liu, Yuguang Chen, Zhongjin Luo, Biwen Lei, Haohan Weng, Zibo Zhao, Jingwei Huang, Zhuo Chen, Chunchao Guo

>2025-06-22

> http://arxiv.org/abs/2506.18017v1

Surface cutting is a fundamental task in computer graphics, with applications
in UV parameterization, texture mapping, and mesh decomposition. However,
existing methods often produce technically valid but overly fragmented atlases
that lack semantic coherence. We introduce SeamGPT, an auto-regressive model
that generates cutting seams by mimicking professional workflows. Our key
technical innovation lies in formulating surface cutting as a next token
prediction task: sample point clouds on mesh vertices and edges, encode them as
shape conditions, and employ a GPT-style transformer to sequentially predict
seam segments with **quantize**d 3D coordinates. Our approach achieves exceptional
performance on UV unwrapping benchmarks containing both manifold and
non-manifold meshes, including artist-created, and 3D-scanned models. In
addition, it enhances existing 3D segmentation tools by providing clean
boundaries for part decomposition.


## Cost-Effective Optimization and Implementation of the CRT-Paillier Decryption Algorithm for Enhanced Performance

>Authors: Zhengwu Huang, Ding Deng, Pengyue Sun, Guangfu Sun, Xiaomei Tang

>2025-06-22

> http://arxiv.org/abs/2506.17935v1

To address the privacy protection problem in cloud computing, privacy
enhancement techniques such as the Paillier additive homomorphism algorithm are
receiving widespread attention. Paillier algorithm allows addition and scalar
multiplication operations in dencrypted state, which can effectively protect
privacy. However, its computational efficiency is limited by complex modulo
operations due to the ciphertext expansion followed by encryption. To
accelerate its decryption operation, the Chinese Remainder Theorem (CRT) is
often used to optimize these modulo operations, which lengthens the decryption
computation chain in turn. To address this issue, we propose an eCRT-Paillier
decryption algorithm that shortens the decryption computation chain by
combining precomputed parameters and eliminating extra judgment operations
introduced by Montgomery modular multiplications. These two improvements reduce
50% modular multiplications and 60% judgment operations in the postprocessing
of the CRT-Paillier decryption algorithm. Based on these improvements, we
propose a highly parallel full-pipeline architecture to eliminate stalls caused
by multiplier reuse in traditional modular exponentiation operations. This
architecture also adopts some optimizations such as simplifying modular
exponentiation units by dividing the exponent into segments and parallelizing
data flow by multi-core instantiation. Finally, a high-throughput and efficient
Paillier accelerator named MESA was implemented on the Xilinx Virtex-7 FPGA for
evaluation, which can complete a decryption using 2048-bit key within 0.577ms
under 100 MHz clock frequency. Compared to prior works, MESA demonstrates a
throughput improvement of 1.16 to 313.21 under identical conditions, also with
enhancements in area efficiency for LUT, DSP, and FF of 3.32 to 117.55, 1.49 to
1.64, and 2.94 to 9.94, respectively.


## Evolving Prompts In-Context An Open-ended, Self-replicating Perspective

>Authors: Jianyu Wang, Zhiqiang Hu, Lidong Bing

>2025-06-22

> http://arxiv.org/abs/2506.17930v1

We propose a novel prompt design paradigm that challenges conventional wisdom
in large language model (LLM) prompting. While conventional wisdom prioritizes
well-crafted instructions and demonstrations for in-context learning (ICL), we
show that **pruning** random demonstrations into seemingly incoherent "gibberish"
can remarkably improve performance across diverse tasks. Notably, the
"gibberish" always matches or surpasses state-of-the-art automatic prompt
optimization techniques, achieving substantial gains regardless of LLM
alignment. Nevertheless, discovering an effective **pruning** strategy is
non-trivial, as existing attribution methods and prompt compression algorithms
fail to deliver robust results, let alone human intuition. In terms of this, we
propose a self-discover prompt optimization framework, PromptQuine, an
evolutionary search framework that automatically searches for the **pruning**
strategy by itself using only low-data regimes. Much like the emergent
complexity in nature--such as symbiosis and self-organization--arising in
response to resource constraints, our framework evolves and refines
unconventional yet highly effective prompts by leveraging only the tokens
present within the context. We demonstrate its effectiveness across
classification, multi-choice question answering, generation and math reasoning
tasks across LLMs, while achieving decent runtime efficiency. We hope our
findings can guide mechanistic studies on in-context learning, and provide a
call to action, to pave the way for more open-ended search algorithms for more
effective LLM prompting.


## PlanMoGPT Flow-Enhanced Progressive Planning for Text to Motion Synthesis

>Authors: Chuhao Jin, Haosen Li, Bingzi Zhang, Che Liu, Xiting Wang, Ruihua Song, Wenbing Huang, Ying Qin, Fuzheng Zhang, Di Zhang

>2025-06-22

> http://arxiv.org/abs/2506.17912v1

Recent advances in large language models (LLMs) have enabled breakthroughs in
many multimodal generation tasks, but a significant performance gap still
exists in text-to-motion generation, where LLM-based methods lag far behind
non-LLM methods. We identify the granularity of motion tokenization as a
critical bottleneck: fine-grained tokenization induces local dependency issues,
where LLMs overemphasize short-term coherence at the expense of global semantic
alignment, while coarse-grained tokenization sacrifices motion details. To
resolve this issue, we propose PlanMoGPT, an LLM-based framework integrating
progressive planning and flow-enhanced fine-grained motion tokenization. First,
our progressive planning mechanism leverages LLMs' autoregressive capabilities
to hierarchically generate motion tokens by starting from **sparse** global plans
and iteratively refining them into full sequences. Second, our flow-enhanced
tokenizer doubles the downsampling resolution and expands the codebook size by
eight times, minimizing detail loss during discretization, while a
flow-enhanced decoder recovers motion nuances. Extensive experiments on
text-to-motion benchmarks demonstrate that it achieves state-of-the-art
performance, improving FID scores by 63.8% (from 0.380 to 0.141) on
long-sequence generation while enhancing motion diversity by 49.9% compared to
existing methods. The proposed framework successfully resolves the
diversity-quality trade-off that plagues current non-LLM approaches,
establishing new standards for text-to-motion generation.


## Robust PDE discovery under sparse and highly noisy conditions via attention neural networks

>Authors: Shilin Zhang, Yunqing Huang, Nianyu Yi, shihan Zhang

>2025-06-22

> http://arxiv.org/abs/2506.17908v1

The discovery of partial differential equations (PDEs) from experimental data
holds great promise for uncovering predictive models of complex physical
systems. In this study, we introduce an efficient automatic model discovery
framework, ANN-PYSR, which integrates attention neural networks with the
state-of-the-art PySR symbolic regression library. Our approach successfully
identifies the governing PDE in six benchmark examples. Compared to the DLGA
framework, numerical experiments demonstrate ANN-PYSR can extract the
underlying dynamic model more efficiently and robustly from **sparse**, highly
noisy data (noise level up to 200%, 5000 sampling points). It indicates an
extensive variety of practical applications of ANN-PYSR, particularly in
conditions with **sparse** sensor networks and high noise levels, where traditional
methods frequently fail.


## StainPIDR A Pathological Image Decouplingand Reconstruction Method for Stain Normalization Based on Color Vector Quantization and Structure Restaining

>Authors: Zheng Chen

>2025-06-22

> http://arxiv.org/abs/2506.17879v1

The color appearance of a pathological image is highly related to the imaging
protocols, the proportion of different dyes, and the scanning devices.
Computer-aided diagnostic systems may deteriorate when facing these
color-variant pathological images. In this work, we propose a stain
normalization method called StainPIDR. We try to eliminate this color
discrepancy by decoupling the image into structure features and
vector-**quantize**d color features, restaining the structure features with the
target color features, and decoding the stained structure features to
normalized pathological images. We assume that color features decoupled by
different images with the same color should be exactly the same. Under this
assumption, we train a fixed color vector codebook to which the decoupled color
features will map. In the restaining part, we utilize the cross-attention
mechanism to efficiently stain the structure features. As the target color
(decoupled from a selected template image) will also affect the performance of
stain normalization, we further design a template image selection algorithm to
select a template from a given dataset. In our extensive experiments, we
validate the effectiveness of StainPIDR and the template image selection
algorithm. All the results show that our method can perform well in the stain
normalization task. The code of StainPIDR will be publicly available later.


## Expanding Relevance Judgments for Medical Case-based Retrieval Task with Multimodal LLMs

>Authors: Catarina Pires, Sérgio Nunes, Luís Filipe Teixeira

>2025-06-21

> http://arxiv.org/abs/2506.17782v1

Evaluating Information Retrieval (IR) systems relies on high-quality manual
relevance judgments (qrels), which are costly and time-consuming to obtain.
While pooling reduces the annotation effort, it results in only partially
labeled datasets. Large Language Models (LLMs) offer a promising alternative to
reducing reliance on manual judgments, particularly in complex domains like
medical case-based retrieval, where relevance assessment requires analyzing
both textual and visual information. In this work, we explore using a
Multimodal Large Language Model (MLLM) to expand relevance judgments, creating
a new dataset of automated judgments. Specifically, we employ Gemini 1.5 Pro on
the ImageCLEFmed 2013 case-based retrieval task, simulating human assessment
through an iteratively refined, structured prompting strategy that integrates
binary scoring, instruction-based evaluation, and few-shot learning. We
systematically experimented with various prompt configurations to maximize
agreement with human judgments. To evaluate agreement between the MLLM and
human judgments, we use Cohen's Kappa, achieving a substantial agreement score
of 0.6, comparable to inter-annotator agreement typically observed in
multimodal retrieval tasks. Starting from the original 15,028 manual judgments
(4.72% relevant) across 35 topics, our MLLM-based approach expanded the dataset
by over 37x to 558,653 judgments, increasing relevant annotations to 5,950. On
average, each medical case query received 15,398 new annotations, with
approximately 99% being non-relevant, reflecting the high **sparsity** typical in
this domain. Our results demonstrate the potential of MLLMs to scale relevance
judgment collection, offering a promising direction for supporting retrieval
evaluation in medical and multimodal IR tasks.


## Particle-in-cell simulations of plasma wakefield formation in microwave waveguides

>Authors: Jesús E. López, Eduardo A. Orozco

>2025-06-21

> http://arxiv.org/abs/2506.17752v1

The **acceleration** of charged particles is fundamental not only for
experimental studies in particle physics but also for applications in fields
such as semiconductor manufacturing and medical therapies. However,
conventional accelerators face limitations due to their large size, driven by
low **acceleration** gradients. Plasma-based accelerators have emerged as a
promising alternative, offering ultrahigh **acceleration** gradients, though their
implementation is often limited by the need for high-intensity, femtosecond
laser systems and sophisticated diagnostics. As a more accessible alternative,
the use of microwave pulses to excite plasma wakefields in waveguides filled
with low-density plasma has gained attention. In this study, we perform
three-dimensional particle-in-cell simulations to investigate the formation and
structure of electrostatic wakefields driven by short microwave pulses in
rectangular plasma waveguides. The results establish a theoretical basis for
evaluating the feasibility and potential applications of microwave-driven
plasma **acceleration** schemes.


## Safe Pruning LoRA Robust Distance-Guided Pruning for Safety Alignment in Adaptation of LLMs

>Authors: Shuang Ao, Yi Dong, Jinwei Hu, Sarvapali Ramchurn

>2025-06-21

> http://arxiv.org/abs/2506.18931v1

Fine-tuning Large Language Models (LLMs) with Low-Rank Adaptation (LoRA)
enhances adaptability while reducing computational costs. However, fine-tuning
can compromise safety alignment, even with benign data, increasing
susceptibility to harmful outputs. Existing safety alignment methods struggle
to capture complex parameter shifts, leading to suboptimal safety-utility
trade-offs. To address this issue, we propose Safe Pruning LoRA (SPLoRA), a
novel **pruning**-based approach that selectively removes LoRA layers that weaken
safety alignment, improving safety while preserving performance. At its core,
we introduce Empirical-DIEM (E-DIEM), a dimension-insensitive similarity metric
that effectively detects safety misalignment in LoRA-adapted models. We conduct
extensive experiments on LLMs fine-tuned with mixed of benign and malicious
data, and purely benign datasets, evaluating SPLoRA across utility, safety, and
reliability metrics. Results demonstrate that SPLoRA outperforms
state-of-the-art safety alignment techniques, significantly reducing safety
risks while maintaining or improving model performance and reliability.
Additionally, SPLoRA reduces inference overhead, making it a scalable and
efficient solution for deploying safer and more reliable LLMs. The code is
available at https://github.com/AoShuang92/SPLoRA.


## The Evolution of Natural Language Processing How Prompt Optimization and Language Models are Shaping the Future

>Authors: Summra Saleem, Muhammad Nabeel Asim, Shaista Zulfiqar, Andreas Dengel

>2025-06-21

> http://arxiv.org/abs/2506.17700v1

Large Language Models (LLMs) have revolutionized the field of Natural
Language Processing (NLP) by automating traditional labor-intensive tasks and
consequently accelerated the development of computer-aided applications. As
researchers continue to advance this field with the introduction of novel
language models and more efficient training/finetuning methodologies, the idea
of prompt engineering and subsequent optimization strategies with LLMs has
emerged as a particularly impactful trend to yield a substantial performance
boost across diverse NLP tasks. To best of our knowledge numerous review
articles have explored prompt engineering, however, a critical gap exists in
comprehensive analyses of prompt optimization strategies. To bridge this gap
this paper provides unique and comprehensive insights about the potential of
diverse prompt optimization strategies. It analyzes their underlying working
paradigms and based on these principles, categorizes them into 11 distinct
classes. Moreover, the paper provides details about various NLP tasks where
these prompt optimization strategies have been employed, along with details of
different LLMs and benchmark datasets used for evaluation. This comprehensive
compilation lays a robust foundation for future comparative studies and enables
rigorous assessment of prompt optimization and LLM-based predictive pipelines
under consistent experimental settings: a critical need in the current
landscape. Ultimately, this research will centralize diverse strategic
knowledge to facilitate the adaptation of existing prompt optimization
strategies for development of innovative predictors across unexplored tasks.


## MDSAMMemory-Driven Sparse Attention Matrix for LVLMs Hallucination Mitigation

>Authors: Shuaiye Lu, Linjiang Zhou, Xiaochuan Shi

>2025-06-21

> http://arxiv.org/abs/2506.17664v1

Hallucinations in large vision-language models (LVLMs) often stem from the
model's sensitivity to image tokens during decoding, as evidenced by attention
peaks observed when generating both real and hallucinated entities. To address
this, we propose Memory-Driven Sparse Attention Matrix (MDSAM) , a novel
training-free approach that dynamically captures and refines the attention
allocated to image tokens at each layer. MDSAM memorizes attention patterns and
activates updates through alignment during decoding, enhancing focus on
relevant image tokens while effectively reducing hallucinations. We evaluate
MDSAM on multiple benchmarks for tasks such as image captioning and visual
question answering, demonstrating its ability to consistently reduce
hallucinations and improve reliability. Compatible with various LVLM
architectures, MDSAM highlights its adaptability and effectiveness in
mitigating hallucinations without requiring additional training or external
tools.


## EQuARX Efficient Quantized AllReduce in XLA for Distributed Machine Learning Acceleration

>Authors: Ibrahim Ahmed, Clemens Schaefer, Gil Tabak, Denis Vnukov, Zenong Zhang, Felix chern, Anatoliy Yevtushenko, Andy Davis

>2025-06-21

> http://arxiv.org/abs/2506.17615v1

While Large Language Models (LLMs) have become highly influential, their
enormous scale presents significant deployment challenges. Efficiently serving
these models typically requires distributing them across numerous accelerator
devices, which introduces substantial performance overhead from inter-device
communication (collectives). While model **quantization** has been widely adopted
to reduce the memory and compute requirements of LLM weights and activations
with minimal quality impact, applying **quantization** directly to collectives like
AllReduce is inherently difficult due to the inter-device summation involved,
which can lead to numerical instability or significant error accumulation. In
this work, we present a native dynamic block-wise efficient **quantize**d AllReduce
within the XLA compiler for TPUs (EQuARX). By using TPU-friendly **quantization**
and deep pipelining of communication and compute, EQuARX with int8 precision
achieves a 1.8X speedup over baseline BF16 AllReduce across various network
topologies. Furthermore, EQuARX accelerates the prefill stage of Gemma 3 27B by
1.25X and Gemma 3 12B by 1.1X, respectively, with small to negligible impact on
quality.


## TyphoFormer Language-Augmented Transformer for Accurate Typhoon Track Forecasting

>Authors: Lincan Li, Eren Erman Ozguven, Yue Zhao, Guang Wang, Yiqun Xie, Yushun Dong

>2025-06-21

> http://arxiv.org/abs/2506.17609v1

Accurate typhoon track forecasting is crucial for early system warning and
disaster response. While Transformer-based models have demonstrated strong
performance in modeling the temporal dynamics of dense trajectories of humans
and vehicles in smart cities, they usually lack access to broader contextual
knowledge that enhances the forecasting reliability of **sparse** meteorological
trajectories, such as typhoon tracks. To address this challenge, we propose
TyphoFormer, a novel framework that incorporates natural language descriptions
as auxiliary prompts to improve typhoon trajectory forecasting. For each time
step, we use Large Language Model (LLM) to generate concise textual
descriptions based on the numerical attributes recorded in the North Atlantic
hurricane database. The language descriptions capture high-level meteorological
semantics and are embedded as auxiliary special tokens prepended to the
numerical time series input. By integrating both textual and sequential
information within a unified Transformer encoder, TyphoFormer enables the model
to leverage contextual cues that are otherwise inaccessible through numerical
features alone. Extensive experiments are conducted on HURDAT2 benchmark,
results show that TyphoFormer consistently outperforms other state-of-the-art
baseline methods, particularly under challenging scenarios involving nonlinear
path shifts and limited historical observations.


## A novel fast short-time root music method for vibration monitoring of high-speed spindles

>Authors: Huiguang Zhang, Baoguo Liu, Wei Feng, Zongtang Li

>2025-06-21

> http://arxiv.org/abs/2506.17600v1

Ultra-high-speed spindle bearings challenge traditional vibration monitoring
due to broadband noise, non-stationarity, and limited time-frequency
resolution. We present a fast Short-Time Root-MUSIC (fSTrM) algorithm that
exploits
  FFT-accelerated Lanczos bidiagonalization to reduce computational complexity
from $\mathcal{O}(N^3)$ to $SN\log_2N+S^2(N+S)+M^2(N+M)$
  while preserving parametric super-resolution. The method constructs Hankel
matrices from 16 ms signal frames and extracts fault frequencies through
polynomial rooting on the unit circle. Experimental validation on the
Politecnico di Torino bearing dataset demonstrates breakthrough micro-defect
detection capabilities. The algorithm reliably identifies 150 $\mu$m defects --
previously undetectable by conventional methods -- providing 72+ hours
additional warning time. Compared to STFT and wavelet methods, fSTrM achieves
1.2 Hz frequency resolution (vs. 12.5 Hz), 93\% detection rate at $-$5 dB SNR,
and quantifies defect severity through harmonic content analysis. Critically,
the algorithm processes each frame in 2.4 ms on embedded ARM Cortex-M7
hardware, enabling real-time deployment. This advancement transforms bearing
monitoring from failure prevention to continuous degradation assessment,
establishing a new paradigm for predictive maintenance in aerospace and
precision machining.


## Towards Deeper GCNs Alleviating Over-smoothing via Iterative Training and Fine-tuning

>Authors: Furong Peng, Jinzhen Gao, Xuan Lu, Kang Liu, Yifan Huo, Sheng Wang

>2025-06-21

> http://arxiv.org/abs/2506.17576v1

Graph Convolutional Networks (GCNs) suffer from severe performance
degradation in deep architectures due to over-smoothing. While existing studies
primarily attribute the over-smoothing to repeated applications of graph
Laplacian operators, our empirical analysis reveals a critical yet overlooked
factor: trainable linear transformations in GCNs significantly exacerbate
feature collapse, even at moderate depths (e.g., 8 layers). In contrast,
Simplified Graph Convolution (SGC), which removes these transformations,
maintains stable feature diversity up to 32 layers, highlighting linear
transformations' dual role in facilitating expressive power and inducing
over-smoothing. However, completely removing linear transformations weakens the
model's expressive capacity.
  To address this trade-off, we propose Layer-wise Gradual Training (LGT), a
novel training strategy that progressively builds deep GCNs while preserving
their expressiveness. LGT integrates three complementary components: (1)
layer-wise training to stabilize optimization from shallow to deep layers, (2)
low-rank adaptation to fine-tune shallow layers and accelerate training, and
(3) identity initialization to ensure smooth integration of new layers and
accelerate convergence. Extensive experiments on benchmark datasets demonstrate
that LGT achieves state-of-the-art performance on vanilla GCN, significantly
improving accuracy even in 32-layer settings. Moreover, as a training method,
LGT can be seamlessly combined with existing methods such as PairNorm and
ContraNorm, further enhancing their performance in deeper networks. LGT offers
a general, architecture-agnostic training framework for scalable deep GCNs. The
code is available at [https://github.com/jfklasdfj/LGT_GCN].


## A Grassroots Network and Community Roadmap for Interconnected Autonomous Science Laboratories for Accelerated Discovery

>Authors: Rafael Ferreira da Silva, Milad Abolhasani, Dionysios A. Antonopoulos, Laura Biven, Ryan Coffee, Ian T. Foster, Leslie Hamilton, Shantenu Jha, Theresa Mayer, Benjamin Mintz, Robert G. Moore, Salahudin Nimer, Noah Paulson, Woong Shin, Frederic Suter, Mitra Taheri, Michela Taufer, Newell R. Washburn

>2025-06-20

> http://arxiv.org/abs/2506.17510v1

Scientific discovery is being revolutionized by AI and autonomous systems,
yet current autonomous laboratories remain isolated islands unable to
collaborate across institutions. We present the Autonomous Interconnected
Science Lab Ecosystem (AISLE), a grassroots network transforming fragmented
capabilities into a unified system that shorten the path from ideation to
innovation to impact and accelerates discovery from decades to months. AISLE
addresses five critical dimensions: (1) cross-institutional equipment
orchestration, (2) intelligent data management with FAIR compliance, (3)
AI-agent driven orchestration grounded in scientific principles, (4)
interoperable agent communication interfaces, and (5) AI/ML-integrated
scientific education. By connecting autonomous agents across institutional
boundaries, autonomous science can unlock research spaces inaccessible to
traditional approaches while democratizing cutting-edge technologies. This
paradigm shift toward collaborative autonomous science promises breakthroughs
in sustainable energy, materials development, and public health.


## From Unstructured Communication to Intelligent RAG Multi-Agent Automation for Supply Chain Knowledge Bases

>Authors: Yao Zhang, Zaixi Shang, Silpan Patel, Mikel Zuniga

>2025-06-20

> http://arxiv.org/abs/2506.17484v1

Supply chain operations generate vast amounts of operational data; however,
critical knowledge such as system usage practices, troubleshooting workflows,
and resolution techniques often remains buried within unstructured
communications like support tickets, emails, and chat logs. While RAG systems
aim to leverage such communications as a knowledge base, their effectiveness is
limited by raw data challenges: support tickets are typically noisy,
inconsistent, and incomplete, making direct retrieval suboptimal. Unlike
existing RAG approaches that focus on runtime optimization, we introduce a
novel offline-first methodology that transforms these communications into a
structured knowledge base. Our key innovation is a LLMs-based multi-agent
system orchestrating three specialized agents: Category Discovery for taxonomy
creation, Categorization for ticket grouping, and Knowledge Synthesis for
article generation. Applying our methodology to real-world support tickets with
resolution notes and comments, our system creates a compact knowledge base -
reducing total volume to just 3.4% of original ticket data while improving
quality. Experiments demonstrate that our prebuilt knowledge base in RAG
systems significantly outperforms traditional RAG implementations (48.74% vs.
38.60% helpful answers) and achieves a 77.4% reduction in unhelpful responses.
By automating institutional knowledge capture that typically remains siloed in
experts' heads, our solution translates to substantial operational efficiency:
reducing support workload, accelerating resolution times, and creating
self-improving systems that automatically resolve approximately 50% of future
supply chain tickets. Our approach addresses a key gap in knowledge management
by transforming transient communications into structured, reusable knowledge
through intelligent offline processing rather than latency-inducing runtime
architectures.


## Code Generation for Near-Roofline Finite Element Actions on GPUs from Symbolic Variational Forms

>Authors: Kaushik Kulkarni, Andreas Klöckner

>2025-06-20

> http://arxiv.org/abs/2506.17471v1

We present a novel parallelization strategy for evaluating Finite Element
Method (FEM) variational forms on GPUs, focusing on those that are expressible
through the Unified Form Language (UFL) on simplex meshes. We base our approach
on code transformations, wherein we construct a space of scheduling candidates
and rank them via a heuristic cost model to effectively handle the large
diversity of computational workloads that can be expressed in this way. We
present a design of a search space to which the cost model is applied, along
with an associated **pruning** strategy to limit the number of configurations that
need to be empirically evaluated. The goal of our design is to strike a balance
between the device's latency-hiding capabilities and the amount of state space,
a key factor in attaining near-roofline performance.
  To make our work widely available, we have prototyped our parallelization
strategy within the \textsc{Firedrake} framework, a UFL-based FEM solver. We
evaluate the performance of our parallelization scheme on two generations of
Nvidia GPUs, specifically the Titan V (Volta architecture) and Tesla K40c
(Kepler architecture), across a range of operators commonly used in
applications, including fluid dynamics, wave propagation, and structural
mechanics, in 2D and 3D geometries. Our results demonstrate that our proposed
algorithm achieves more than $50\%$ roofline performance in $65\%$ of the test
cases on both devices.


## Trans${^2}$-CBCT A Dual-Transformer Framework for Sparse-View CBCT Reconstruction

>Authors: Minmin Yang, Huantao Ren, Senem Velipasalar

>2025-06-20

> http://arxiv.org/abs/2506.17425v1

Cone-beam computed tomography (CBCT) using only a few X-ray projection views
enables faster scans with lower radiation dose, but the resulting severe
under-sampling causes strong artifacts and poor spatial coverage. We address
these challenges in a unified framework. First, we replace conventional
UNet/ResNet encoders with TransUNet, a hybrid CNN-Transformer model.
Convolutional layers capture local details, while self-attention layers enhance
global context. We adapt TransUNet to CBCT by combining multi-scale features,
querying view-specific features per 3D point, and adding a lightweight
attenuation-prediction head. This yields Trans-CBCT, which surpasses prior
baselines by 1.17 dB PSNR and 0.0163 SSIM on the LUNA16 dataset with six views.
Second, we introduce a neighbor-aware Point Transformer to enforce volumetric
coherence. This module uses 3D positional encoding and attention over k-nearest
neighbors to improve spatial consistency. The resulting model, Trans$^2$-CBCT,
provides an additional gain of 0.63 dB PSNR and 0.0117 SSIM. Experiments on
LUNA16 and ToothFairy show consistent gains from six to ten views, validating
the effectiveness of combining CNN-Transformer features with point-based
geometry reasoning for **sparse**-view CBCT reconstruction.


## Cache Me If You Can How Many KVs Do You Need for Effective Long-Context LMs?

>Authors: Adithya Bhaskar, Alexander Wettig, Tianyu Gao, Yihe Dong, Danqi Chen

>2025-06-20

> http://arxiv.org/abs/2506.17121v1

Language models handle increasingly long contexts for tasks such as book
summarization, but this leads to growing memory costs for the key-value (**KV**)
cache. Many prior works have proposed ways of discarding **KV**s from memory, but
their approaches are tailored to favorable settings, obscuring caveats like
high peak memory and performance degradation, and a fair comparison between
methods is difficult. In this paper, we propose the ***KV** footprint* as a unified
metric, which accounts for both the amount of **KV** entries stored and their
lifespan in memory. We evaluate methods based on the smallest footprint they
attain while preserving performance in both long-context understanding and
generation, with context lengths of up to 128K tokens. This metric reveals the
high peak memory of prior **KV** eviction methods. One class of methods --
*post-fill eviction* -- has a high footprint due to being incompatible with
eviction during pre-filling. We adapt these methods to be able to evict **KV**s
during pre-filling, achieving substantially lower **KV** footprints. We then turn
to *recency eviction* methods, wherein we propose PruLong, an end-to-end
optimization method for learning which attention heads need to retain the full
**KV** cache and which do not. PruLong saves memory while preserving long-context
performance, achieving 12% smaller **KV** footprint than prior methods while
retaining performance in challenging recall tasks. Our paper clarifies the
complex tangle of long-context inference methods and paves the way for future
development to minimize the **KV** footprint.


## Cross-Modal Epileptic Signal Harmonization Frequency Domain Mapping Quantization for Pre-training a Unified Neurophysiological Transformer

>Authors: Runkai Zhang, Hua Yu, John Q. Gan, Haixian Wang

>2025-06-20

> http://arxiv.org/abs/2506.17068v1

Scalp electroencephalography (EEG) and intracranial EEG (iEEG) are vital for
epilepsy diagnosis and treatment. Their unified analysis offers the potential
to harness the complementary strengths of each modality but is challenging due
to variations in recording montages, amplitude and signal-to-noise ratio (SNR),
and frequency components. To address the aforementioned challenges, this paper
introduces EpiNT, a novel Transformer-based pre-trained model for unified EEG
and iEEG analysis. EpiNT employs channel-independent modeling with masked
autoencoders (MAE) and vector **quantization** (VQ), along with a frequency domain
mapping **quantize**r to capture crucial frequency features. Pre-trained on over
2,700 hours of multi-modal clinical neurophysiological data from 1,199
patients, EpiNT outperformed both randomly initialized models and other
pre-trained methods on six downstream classification tasks, demonstrating
robust representation learning capabilities. This work presents a promising
approach for unified epilepsy neurophysiology analysis.


## The Hidden Cost of an Image Quantifying the Energy Consumption of AI Image Generation

>Authors: Giulia Bertazzini, Chiara Albisani, Daniele Baracchi, Dasara Shullani, Roberto Verdecchia

>2025-06-20

> http://arxiv.org/abs/2506.17016v1

With the growing adoption of AI image generation, in conjunction with the
ever-increasing environmental resources demanded by AI, we are urged to answer
a fundamental question: What is the environmental impact hidden behind each
image we generate? In this research, we present a comprehensive empirical
experiment designed to assess the energy consumption of AI image generation.
Our experiment compares 17 state-of-the-art image generation models by
considering multiple factors that could affect their energy consumption, such
as model **quantization**, image resolution, and prompt length. Additionally, we
consider established image quality metrics to study potential trade-offs
between energy consumption and generated image quality. Results show that image
generation models vary drastically in terms of the energy they consume, with up
to a 46x difference. Image resolution affects energy consumption
inconsistently, ranging from a 1.3x to 4.7x increase when doubling resolution.
U-Net-based models tend to consume less than Transformer-based one. Model
**quantization** instead results to deteriorate the energy efficiency of most
models, while prompt length and content have no statistically significant
impact. Improving image quality does not always come at the cost of a higher
energy consumption, with some of the models producing the highest quality
images also being among the most energy efficient ones.


## RocketStack A level-aware deep recursive ensemble learning framework with exploratory feature fusion and model pruning dynamics

>Authors: Çağatay Demirel

>2025-06-20

> http://arxiv.org/abs/2506.16965v1

Ensemble learning remains a cornerstone of machine learning, with stacking
used to integrate predictions from multiple base learners through a meta-model.
However, deep stacking remains rare, as most designs prioritize horizontal
diversity over recursive depth due to model complexity, feature redundancy, and
computational burden. To address these challenges, RocketStack, a level-aware
recursive ensemble framework, is introduced and explored up to ten stacking
levels, extending beyond prior architectures. The framework incrementally
prunes weaker learners at each level, enabling deeper stacking without
excessive complexity. To mitigate early performance saturation, mild Gaussian
noise is added to out-of-fold (OOF) scores before **pruning**, and compared against
strict OOF **pruning**. Further both per-level and periodic feature compressions
are explored using attention-based selection, Simple, Fast, Efficient (SFE)
filter, and autoencoders. Across 33 datasets (23 binary, 10 multi-class),
linear-trend tests confirmed rising accuracy with depth in most variants, and
the top performing meta-model at each level increasingly outperformed the
strongest standalone ensemble. In the binary subset, periodic SFE with mild
OOF-score randomization reached 97.08% at level 10, 5.14% above the
strict-**pruning** configuration and cut runtime by 10.5% relative to no
compression. In the multi-class subset, periodic attention selection reached
98.60% at level 10, exceeding the strongest baseline by 6.11%, while reducing
runtime by 56.1% and feature dimensionality by 74% compared to no compression.
These findings highlight mild randomization as an effective regularizer and
periodic compression as a stabilizer. Echoing the design of multistage rockets
in aerospace (prune, compress, propel) RocketStack achieves deep recursive
ensembling with tractable complexity.


## EHCube4P Learning Epistatic Patterns Through Hypercube Graph Convolution Neural Network for Protein Fitness Function Estimation

>Authors: Muhammad Daud, Philippe Charton, Cedric Damour, Jingbo Wang, Frederic Cadet

>2025-06-20

> http://arxiv.org/abs/2506.16921v1

Understanding the relationship between protein sequences and their functions
is fundamental to protein engineering, but this task is hindered by the
combinatorially vast sequence space and the experimental noise inherent in
fitness measurements. In this study, we present a novel framework that models
the sequence landscape as a hypercube $H(k,2)$ and integrates wavelet-based
signal denoising with a graph convolutional neural network (GCN) to predict
protein fitness across rugged fitness landscapes. Using a dataset of 419
experimentally measured mutant sequences of the Tobacco 5-Epi-Aristolochene
Synthase (TEAS) enzyme, we preprocess the fitness signals using a 1-D discrete
wavelet transform with a Daubechies-3 basis to suppress experimental noise
while preserving local epistatic patterns. Our model comprises two GCN layers,
allowing for beyond pairwise aggregation, followed by a multi-layer perceptron
(MLP). We show that our approach, EHCube4P, generalizes well across different
enzyme activity datasets and effectively captures higher-order mutational
interactions. Performance varies with the ruggedness of the fitness landscape,
with smoother signals yielding higher test set $r^2$ scores. These results
demonstrate that combining wavelet preprocessing with graph-based deep learning
enhances the robustness and generalization of fitness prediction, particularly
for **sparse** and noisy biological datasets. The approach provides a scalable and
interpretable framework for protein fitness estimation applicable to a broad
range of combinatorial biological systems.


## Vision-Based Multirotor Control for Spherical Target Tracking A Bearing-Angle Approach

>Authors: Marcelo Jacinto, Rita Cunha

>2025-06-20

> http://arxiv.org/abs/2506.16870v1

This work addresses the problem of designing a visual servo controller for a
multirotor vehicle, with the end goal of tracking a moving spherical target
with unknown radius. To address this problem, we first transform two bearing
measurements provided by a camera sensor into a bearing-angle pair. We then use
this information to derive the system's dynamics in a new set of coordinates,
where the angle measurement is used to quantify a relative distance to the
target. Building on this system representation, we design an adaptive nonlinear
control algorithm that takes advantage of the properties of the new system
geometry and assumes that the target follows a constant **acceleration** model.
Simulation results illustrate the performance of the proposed control
algorithm.


## eSapiens A Real-World NLP Framework for Multimodal Document Understanding and Enterprise Knowledge Processing

>Authors: Isaac Shi, Zeyuan Li, Wenli Wang, Lewei He, Yang Yang, Tianyu Shi

>2025-06-20

> http://arxiv.org/abs/2506.16768v1

We introduce eSapiens, a unified question-answering system designed for
enterprise settings, which bridges structured databases and unstructured
textual corpora via a dual-module architecture. The system combines a
Text-to-SQL planner with a hybrid Retrieval-Augmented Generation (RAG)
pipeline, enabling natural language access to both relational data and
free-form documents. To enhance answer faithfulness, the RAG module integrates
dense and **sparse** retrieval, commercial reranking, and a citation verification
loop that ensures grounding consistency. We evaluate eSapiens on the RAGTruth
benchmark across five leading large language models (LLMs), analyzing
performance across key dimensions such as completeness, hallucination, and
context utilization. Results demonstrate that eSapiens outperforms a FAISS
baseline in contextual relevance and generation quality, with optional
strict-grounding controls for high-stakes scenarios. This work provides a
deployable framework for robust, citation-aware question answering in
real-world enterprise applications.


## Fast and Stable Diffusion Planning through Variational Adaptive Weighting

>Authors: Zhiying Qiu, Tao Lin

>2025-06-20

> http://arxiv.org/abs/2506.16688v1

Diffusion models have recently shown promise in offline RL. However, these
methods often suffer from high training costs and slow convergence,
particularly when using transformer-based denoising backbones. While several
optimization strategies have been proposed -- such as modified noise schedules,
auxiliary prediction targets, and adaptive loss weighting -- challenges remain
in achieving stable and efficient training. In particular, existing loss
weighting functions typically rely on neural network approximators, which can
be ineffective in early training phases due to limited generalization capacity
of MLPs when exposed to **sparse** feedback in the early training stages. In this
work, we derive a variationally optimal uncertainty-aware weighting function
and introduce a closed-form polynomial approximation method for its online
estimation under the flow-based generative modeling framework. We integrate our
method into a diffusion planning pipeline and evaluate it on standard offline
RL benchmarks. Experimental results on Maze2D and Kitchen tasks show that our
method achieves competitive performance with up to 10 times fewer training
steps, highlighting its practical effectiveness.


## A Simple Contrastive Framework Of Item Tokenization For Generative Recommendation

>Authors: Penglong Zhai, Yifang Yuan, Fanyi Di, Jie Li, Yue Liu, Chen Li, Jie Huang, Sicong Wang, Yao Xu, Xin Li

>2025-06-20

> http://arxiv.org/abs/2506.16683v1

Generative retrieval-based recommendation has emerged as a promising paradigm
aiming at directly generating the identifiers of the target candidates.
However, in large-scale recommendation systems, this approach becomes
increasingly cumbersome due to the redundancy and sheer scale of the token
space. To overcome these limitations, recent research has explored the use of
semantic tokens as an alternative to ID tokens, which typically leveraged
reconstruction-based strategies, like RQ-VAE, to **quantize** content embeddings
and significantly reduce the embedding size. However, reconstructive
**quantization** aims for the precise reconstruction of each item embedding
independently, which conflicts with the goal of generative retrieval tasks
focusing more on differentiating among items. Moreover, multi-modal side
information of items, such as descriptive text and images, geographical
knowledge in location-based recommendation services, has been shown to be
effective in improving recommendations by providing richer contexts for
interactions. Nevertheless, effectively integrating such complementary
knowledge into existing generative recommendation frameworks remains
challenging. To overcome these challenges, we propose a novel unsupervised deep
**quantization** exclusively based on contrastive learning, named SimCIT (a Simple
Contrastive Item Tokenization framework). Specifically, different from existing
reconstruction-based strategies, SimCIT propose to use a learnable residual
**quantization** module to align with the signals from different modalities of the
items, which combines multi-modal knowledge alignment and semantic tokenization
in a mutually beneficial contrastive learning framework. Extensive experiments
across public datasets and a large-scale industrial dataset from various
domains demonstrate SimCIT's effectiveness in LLM-based generative
recommendation.

