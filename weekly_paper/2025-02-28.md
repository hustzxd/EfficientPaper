# 2025-02-28

# Table of Contents
* [A Sliding Layer Merging Method for Efficient Depth-Wise Pruning in LLMs](#A-Sliding-Layer-Merging-Method-for-Efficient-Depth-Wise-Pruning-in-LLMs)
* [Sparse Brains are Also Adaptive Brains Cognitive-Load-Aware Dynamic Activation for LLMs](#Sparse-Brains-are-Also-Adaptive-Brains-Cognitive-Load-Aware-Dynamic-Activation-for-LLMs)
* [Foundation Inference Models for Stochastic Differential Equations A Transformer-based Approach for Zero-shot Function Estimation](#Foundation-Inference-Models-for-Stochastic-Differential-Equations-A-Transformer-based-Approach-for-Zero-shot-Function-Estimation)
* [Binary Neural Networks for Large Language Model A Survey](#Binary-Neural-Networks-for-Large-Language-Model-A-Survey)
* [The Sharpness Disparity Principle in Transformers for Accelerating Language Model Pre-Training](#The-Sharpness-Disparity-Principle-in-Transformers-for-Accelerating-Language-Model-Pre-Training)
* [One Set to Rule Them All How to Obtain General Chemical Conditions via Bayesian Optimization over Curried Functions](#One-Set-to-Rule-Them-All-How-to-Obtain-General-Chemical-Conditions-via-Bayesian-Optimization-over-Curried-Functions)
* [A Novel Topology Recovery Method for Low Voltage Distribution Networks](#A-Novel-Topology-Recovery-Method-for-Low-Voltage-Distribution-Networks)
* [Sparse Alignment Enhanced Latent Diffusion Transformer for Zero-Shot Speech Synthesis](#Sparse-Alignment-Enhanced-Latent-Diffusion-Transformer-for-Zero-Shot-Speech-Synthesis)
* [From Hours to Minutes Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens](#From-Hours-to-Minutes-Lossless-Acceleration-of-Ultra-Long-Sequence-Generation-up-to-100K-Tokens)
* [On Pruning State-Space LLMs](#On-Pruning-State-Space-LLMs)
* [SE(3)-Equivariant Ternary Complex Prediction Towards Target Protein Degradation](#SE(3)-Equivariant-Ternary-Complex-Prediction-Towards-Target-Protein-Degradation)
* [Sliding Window Attention Training for Efficient Large Language Models](#Sliding-Window-Attention-Training-for-Efficient-Large-Language-Models)
* [Grad-ECLIP Gradient-based Visual and Textual Explanations for CLIP](#Grad-ECLIP-Gradient-based-Visual-and-Textual-Explanations-for-CLIP)
* [Seeing the Forest for the Trees A Large Scale, Continuously Updating Meta-Analysis of Frontier LLMs](#Seeing-the-Forest-for-the-Trees-A-Large-Scale,-Continuously-Updating-Meta-Analysis-of-Frontier-LLMs)
* [M-ANT Efficient Low-bit Group Quantization for LLMs via Mathematically Adaptive Numerical Type](#M-ANT-Efficient-Low-bit-Group-Quantization-for-LLMs-via-Mathematically-Adaptive-Numerical-Type)
* [Automatic Prompt Optimization via Heuristic Search A Survey](#Automatic-Prompt-Optimization-via-Heuristic-Search-A-Survey)
* [Adaptive conditional latent diffusion maps beam loss to 2D phase space projections](#Adaptive-conditional-latent-diffusion-maps-beam-loss-to-2D-phase-space-projections)
* [Scaffolding Empathy Training Counselors with Simulated Patients and Utterance-level Performance Visualizations](#Scaffolding-Empathy-Training-Counselors-with-Simulated-Patients-and-Utterance-level-Performance-Visualizations)
* [Steered Generation via Gradient Descent on Sparse Features](#Steered-Generation-via-Gradient-Descent-on-Sparse-Features)
* [Transfer Learning Assisted Fast Design Migration Over Technology Nodes A Study on Transformer Matching Network](#Transfer-Learning-Assisted-Fast-Design-Migration-Over-Technology-Nodes-A-Study-on-Transformer-Matching-Network)
* [PacQ A SIMT Microarchitecture for Efficient Dataflow in Hyper-asymmetric GEMMs](#PacQ-A-SIMT-Microarchitecture-for-Efficient-Dataflow-in-Hyper-asymmetric-GEMMs)
* [DRAMA Diverse Augmentation from Large Language Models to Smaller Dense Retrievers](#DRAMA-Diverse-Augmentation-from-Large-Language-Models-to-Smaller-Dense-Retrievers)
* [Jacobian Sparse Autoencoders Sparsify Computations, Not Just Activations](#Jacobian-Sparse-Autoencoders-Sparsify-Computations,-Not-Just-Activations)
* [LevelRAG Enhancing Retrieval-Augmented Generation with Multi-hop Logic Planning over Rewriting Augmented Searchers](#LevelRAG-Enhancing-Retrieval-Augmented-Generation-with-Multi-hop-Logic-Planning-over-Rewriting-Augmented-Searchers)
* [SpargeAttn Accurate Sparse Attention Accelerating Any Model Inference](#SpargeAttn-Accurate-Sparse-Attention-Accelerating-Any-Model-Inference)
* [Inverse Materials Design by Large Language Model-Assisted Generative Framework](#Inverse-Materials-Design-by-Large-Language-Model-Assisted-Generative-Framework)
* [HyperG Hypergraph-Enhanced LLMs for Structured Knowledge](#HyperG-Hypergraph-Enhanced-LLMs-for-Structured-Knowledge)
* [Escaping The Big Data Paradigm in Self-Supervised Representation Learning](#Escaping-The-Big-Data-Paradigm-in-Self-Supervised-Representation-Learning)
* [Weyl Quantization of Exponential Lie Groups for Square Integrable Representations](#Weyl-Quantization-of-Exponential-Lie-Groups-for-Square-Integrable-Representations)
* [Optimal Brain Apoptosis](#Optimal-Brain-Apoptosis)
* [Novel quantum circuit for image compression utilizing modified Toffoli gate and quantized transformed coefficient alongside a novel reset gate](#Novel-quantum-circuit-for-image-compression-utilizing-modified-Toffoli-gate-and-quantized-transformed-coefficient-alongside-a-novel-reset-gate)
* [ELMo-Tune-V2 LLM-Assisted Full-Cycle Auto-Tuning to Optimize LSM-Based Key-Value Stores](#ELMo-Tune-V2-LLM-Assisted-Full-Cycle-Auto-Tuning-to-Optimize-LSM-Based-Key-Value-Stores)
* [MEDA Dynamic KV Cache Allocation for Efficient Multimodal Long-Context Inference](#MEDA-Dynamic-KV-Cache-Allocation-for-Efficient-Multimodal-Long-Context-Inference)
* [LongSpec Long-Context Speculative Decoding with Efficient Drafting and Verification](#LongSpec-Long-Context-Speculative-Decoding-with-Efficient-Drafting-and-Verification)
* [A Concise Lyapunov Analysis of Nesterov's Accelerated Gradient Method](#A-Concise-Lyapunov-Analysis-of-Nesterov's-Accelerated-Gradient-Method)
* [Delta Decompression for MoE-based LLMs Compression](#Delta-Decompression-for-MoE-based-LLMs-Compression)
* [GaussianFlowOcc Sparse and Weakly Supervised Occupancy Estimation using Gaussian Splatting and Temporal Flow](#GaussianFlowOcc-Sparse-and-Weakly-Supervised-Occupancy-Estimation-using-Gaussian-Splatting-and-Temporal-Flow)
* [The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?](#The-Lottery-LLM-Hypothesis,-Rethinking-What-Abilities-Should-LLM-Compression-Preserve?)
* [Evaluating Expert Contributions in a MoE LLM for Quiz-Based Tasks](#Evaluating-Expert-Contributions-in-a-MoE-LLM-for-Quiz-Based-Tasks)
* [CodeSwift Accelerating LLM Inference for Efficient Code Generation](#CodeSwift-Accelerating-LLM-Inference-for-Efficient-Code-Generation)
* [Enhancing Image Matting in Real-World Scenes with Mask-Guided Iterative Refinement](#Enhancing-Image-Matting-in-Real-World-Scenes-with-Mask-Guided-Iterative-Refinement)
* [Systematic Weight Evaluation for Pruning Large Language Models Enhancing Performance and Sustainability](#Systematic-Weight-Evaluation-for-Pruning-Large-Language-Models-Enhancing-Performance-and-Sustainability)
* [Random Projections and Natural Sparsity in Time-Series Classification A Theoretical Analysis](#Random-Projections-and-Natural-Sparsity-in-Time-Series-Classification-A-Theoretical-Analysis)
* [Active Learning for Conditional Inverse Design with Crystal Generation and Foundation Atomic Models](#Active-Learning-for-Conditional-Inverse-Design-with-Crystal-Generation-and-Foundation-Atomic-Models)
* [Make LLM Inference Affordable to Everyone Augmenting GPU Memory with NDP-DIMM](#Make-LLM-Inference-Affordable-to-Everyone-Augmenting-GPU-Memory-with-NDP-DIMM)
* [Unconventional topological Weyl-dipole phonon](#Unconventional-topological-Weyl-dipole-phonon)
* [Atten-Transformer A Deep Learning Framework for User App Usage Prediction](#Atten-Transformer-A-Deep-Learning-Framework-for-User-App-Usage-Prediction)
* [DBudgetKV Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance](#DBudgetKV-Dynamic-Budget-in-KV-Cache-Compression-for-Ensuring-Optimal-Performance)
* [CORAL Learning Consistent Representations across Multi-step Training with Lighter Speculative Drafter](#CORAL-Learning-Consistent-Representations-across-Multi-step-Training-with-Lighter-Speculative-Drafter)
* [APINT A Full-Stack Framework for Acceleration of Privacy-Preserving Inference of Transformers based on Garbled Circuits](#APINT-A-Full-Stack-Framework-for-Acceleration-of-Privacy-Preserving-Inference-of-Transformers-based-on-Garbled-Circuits)
* [The Role of Sparsity for Length Generalization in Transformers](#The-Role-of-Sparsity-for-Length-Generalization-in-Transformers)
* [AlphaAgent LLM-Driven Alpha Mining with Regularized Exploration to Counteract Alpha Decay](#AlphaAgent-LLM-Driven-Alpha-Mining-with-Regularized-Exploration-to-Counteract-Alpha-Decay)
* [CipherPrune Efficient and Scalable Private Transformer Inference](#CipherPrune-Efficient-and-Scalable-Private-Transformer-Inference)
* [Splitting Regularized Wasserstein Proximal Algorithms for Nonsmooth Sampling Problems](#Splitting-Regularized-Wasserstein-Proximal-Algorithms-for-Nonsmooth-Sampling-Problems)
* [Layer-Wise Evolution of Representations in Fine-Tuned Transformers Insights from Sparse AutoEncoders](#Layer-Wise-Evolution-of-Representations-in-Fine-Tuned-Transformers-Insights-from-Sparse-AutoEncoders)
* [A Matter-Wave Quantum Superposition of Inertial and Constant Acceleration Motions](#A-Matter-Wave-Quantum-Superposition-of-Inertial-and-Constant-Acceleration-Motions)
* [Rewards-based image analysis in microscopy](#Rewards-based-image-analysis-in-microscopy)
* [Are Sparse Autoencoders Useful? A Case Study in Sparse Probing](#Are-Sparse-Autoencoders-Useful?-A-Case-Study-in-Sparse-Probing)
* [Automatic Joint Structured Pruning and Quantization for Efficient Neural Network Training and Compression](#Automatic-Joint-Structured-Pruning-and-Quantization-for-Efficient-Neural-Network-Training-and-Compression)
* [Energy-Efficient Transformer Inference Optimization Strategies for Time Series Classification](#Energy-Efficient-Transformer-Inference-Optimization-Strategies-for-Time-Series-Classification)
* [TerEffic Highly Efficient Ternary LLM Inference on FPGA](#TerEffic-Highly-Efficient-Ternary-LLM-Inference-on-FPGA)
* [Swallowing the Poison Pills Insights from Vulnerability Disparity Among LLMs](#Swallowing-the-Poison-Pills-Insights-from-Vulnerability-Disparity-Among-LLMs)
* [Towards Fully-Automated Materials Discovery via Large-Scale Synthesis Dataset and Expert-Level LLM-as-a-Judge](#Towards-Fully-Automated-Materials-Discovery-via-Large-Scale-Synthesis-Dataset-and-Expert-Level-LLM-as-a-Judge)
* [Advances in Continuous Variable Measurement-Device-Independent Quantum Key Distribution](#Advances-in-Continuous-Variable-Measurement-Device-Independent-Quantum-Key-Distribution)
* [Compression Scaling LawsUnifying Sparsity and Quantization](#Compression-Scaling-LawsUnifying-Sparsity-and-Quantization)
* [Vision Transformer Accelerator ASIC for Real-Time, Low-Power Sleep Staging](#Vision-Transformer-Accelerator-ASIC-for-Real-Time,-Low-Power-Sleep-Staging)
* [SAE-V Interpreting Multimodal Models for Enhanced Alignment](#SAE-V-Interpreting-Multimodal-Models-for-Enhanced-Alignment)
* [Dynamic Parallel Tree Search for Efficient LLM Reasoning](#Dynamic-Parallel-Tree-Search-for-Efficient-LLM-Reasoning)
* [The Lovász number of random circulant graphs](#The-Lovász-number-of-random-circulant-graphs)
* [LLMKey LLM-Powered Wireless Key Generation Scheme for Next-Gen IoV Systems](#LLMKey-LLM-Powered-Wireless-Key-Generation-Scheme-for-Next-Gen-IoV-Systems)
* [ZiGong 1.0 A Large Language Model for Financial Credit](#ZiGong-1.0-A-Large-Language-Model-for-Financial-Credit)
* [ZIA A Theoretical Framework for Zero-Input AI](#ZIA-A-Theoretical-Framework-for-Zero-Input-AI)
* [Recurrent Knowledge Identification and Fusion for Language Model Continual Learning](#Recurrent-Knowledge-Identification-and-Fusion-for-Language-Model-Continual-Learning)
* [Joint Similarity Item Exploration and Overlapped User Guidance for Multi-Modal Cross-Domain Recommendation](#Joint-Similarity-Item-Exploration-and-Overlapped-User-Guidance-for-Multi-Modal-Cross-Domain-Recommendation)
* [RAG-Enhanced Collaborative LLM Agents for Drug Discovery](#RAG-Enhanced-Collaborative-LLM-Agents-for-Drug-Discovery)
* [Comprehensive Analysis of Transparency and Accessibility of ChatGPT, DeepSeek, And other SoTA Large Language Models](#Comprehensive-Analysis-of-Transparency-and-Accessibility-of-ChatGPT,-DeepSeek,-And-other-SoTA-Large-Language-Models)
* [KVLink Accelerating Large Language Models via Efficient KV Cache Reuse](#KVLink-Accelerating-Large-Language-Models-via-Efficient-KV-Cache-Reuse)
* [Text-to-SQL Domain Adaptation via Human-LLM Collaborative Data Annotation](#Text-to-SQL-Domain-Adaptation-via-Human-LLM-Collaborative-Data-Annotation)
* [Compression Barriers for Autoregressive Transformers](#Compression-Barriers-for-Autoregressive-Transformers)
* [Modality-Aware Neuron Pruning for Unlearning in Multimodal Large Language Models](#Modality-Aware-Neuron-Pruning-for-Unlearning-in-Multimodal-Large-Language-Models)
* [Probe Pruning Accelerating LLMs through Dynamic Pruning via Model-Probing](#Probe-Pruning-Accelerating-LLMs-through-Dynamic-Pruning-via-Model-Probing)
* [Interpreting and Steering LLMs with Mutual Information-based Explanations on Sparse Autoencoders](#Interpreting-and-Steering-LLMs-with-Mutual-Information-based-Explanations-on-Sparse-Autoencoders)
* [DReSD Dense Retrieval for Speculative Decoding](#DReSD-Dense-Retrieval-for-Speculative-Decoding)
* [Blockchain innovation in promoting employment](#Blockchain-innovation-in-promoting-employment)
* [PIP-KAG Mitigating Knowledge Conflicts in Knowledge-Augmented Generation via Parametric Pruning](#PIP-KAG-Mitigating-Knowledge-Conflicts-in-Knowledge-Augmented-Generation-via-Parametric-Pruning)
* [Scaling Sparse and Dense Retrieval in Decoder-Only LLMs](#Scaling-Sparse-and-Dense-Retrieval-in-Decoder-Only-LLMs)
* [Towards Swift Serverless LLM Cold Starts with ParaServe](#Towards-Swift-Serverless-LLM-Cold-Starts-with-ParaServe)
* [Verification and Validation for Trustworthy Scientific Machine Learning](#Verification-and-Validation-for-Trustworthy-Scientific-Machine-Learning)
* [Q-PETR Quant-aware Position Embedding Transformation for Multi-View 3D Object Detection](#Q-PETR-Quant-aware-Position-Embedding-Transformation-for-Multi-View-3D-Object-Detection)
* [PAPI Exploiting Dynamic Parallelism in Large Language Model Decoding with a Processing-In-Memory-Enabled Computing System](#PAPI-Exploiting-Dynamic-Parallelism-in-Large-Language-Model-Decoding-with-a-Processing-In-Memory-Enabled-Computing-System)
* [When Compression Meets Model Compression Memory-Efficient Double Compression for Large Language Models](#When-Compression-Meets-Model-Compression-Memory-Efficient-Double-Compression-for-Large-Language-Models)
* [CoKV Optimizing KV Cache Allocation via Cooperative Game](#CoKV-Optimizing-KV-Cache-Allocation-via-Cooperative-Game)
* [Tight Clusters Make Specialized Experts](#Tight-Clusters-Make-Specialized-Experts)
* [SVDq 1.25-bit and 410x Key Cache Compression for LLM Attention](#SVDq-1.25-bit-and-410x-Key-Cache-Compression-for-LLM-Attention)
* [Round Attention A Novel Round-Level Attention Mechanism to Accelerate LLM Inference](#Round-Attention-A-Novel-Round-Level-Attention-Mechanism-to-Accelerate-LLM-Inference)
* [Soybean pod and seed counting in both outdoor fields and indoor laboratories using unions of deep neural networks](#Soybean-pod-and-seed-counting-in-both-outdoor-fields-and-indoor-laboratories-using-unions-of-deep-neural-networks)
* [PPC-GPT Federated Task-Specific Compression of Large Language Models via Pruning and Chain-of-Thought Distillation](#PPC-GPT-Federated-Task-Specific-Compression-of-Large-Language-Models-via-Pruning-and-Chain-of-Thought-Distillation)
* [LightMamba Efficient Mamba Acceleration on FPGA with Quantization and Hardware Co-design](#LightMamba-Efficient-Mamba-Acceleration-on-FPGA-with-Quantization-and-Hardware-Co-design)
* [Interleaved Block-based Learned Image Compression with Feature Enhancement and Quantization Error Compensation](#Interleaved-Block-based-Learned-Image-Compression-with-Feature-Enhancement-and-Quantization-Error-Compensation)
* [CoT-ICL Lab A Petri Dish for Studying Chain-of-Thought Learning from In-Context Demonstrations](#CoT-ICL-Lab-A-Petri-Dish-for-Studying-Chain-of-Thought-Learning-from-In-Context-Demonstrations)


## A Sliding Layer Merging Method for Efficient Depth-Wise Pruning in LLMs

>Authors: Xuan Ding, Yao Zhu, Yunjian Zhang, Chuanlong Xie

>2025-02-26

> http://arxiv.org/abs/2502.19159v1

Compared to width-wise **pruning**, depth-wise **pruning** can significantly
accelerate inference in resource-constrained scenarios. Howerver, treating the
entire Transformer layer as the minimum **pruning** unit may degrade model
performance by indiscriminately discarding the entire information of the layer.
This paper reveals the "Patch-like" feature relationship between layers in
large language models by analyzing the correlation of the outputs of different
layers in the reproducing kernel Hilbert space. Building on this observation,
we proposes a sliding layer merging method that dynamically selects and fuses
consecutive layers from top to bottom according to a pre-defined similarity
threshold, thereby simplifying the model structure while maintaining its
performance. Extensive experiments on LLMs with various architectures and
different parameter scales show that our method outperforms existing **pruning**
techniques in both zero-shot inference performance and retraining recovery
quality after **pruning**. In particular, in the experiment with 35\% **pruning** on
the Vicuna-7B model, our method achieved a 1.654\% improvement in average
performance on zero-shot tasks compared to the existing method. Moreover, we
further reveal the potential of combining depth **pruning** with width **pruning** to
enhance the **pruning** effect. Our codes are available at
https://github.com/920927/SLM-a-sliding-layer-merging-method.


## Sparse Brains are Also Adaptive Brains Cognitive-Load-Aware Dynamic Activation for LLMs

>Authors: Yiheng Yang, Yujie Wang, Chi Ma, Lei Yu, Emmanuele Chersoni, Chu-Ren Huang

>2025-02-26

> http://arxiv.org/abs/2502.19078v1

Dense large language models(LLMs) face critical efficiency bottlenecks as
they rigidly activate all parameters regardless of input complexity. While
existing **sparsity** methods(static **pruning** or dynamic activation) address this
partially, they either lack adaptivity to contextual or model structural
demands or incur prohibitive computational overhead. Inspired by human brain's
dual-process mechanisms - predictive coding (N400) for backbone **sparsity** and
structural reanalysis (P600) for complex context - we propose CLADA, a
\textit{\textbf{C}ognitive-\textbf{L}oad-\textbf{A}ware \textbf{D}ynamic
\textbf{A}ctivation} framework that synergizes statistical **sparsity** with
semantic adaptability. Our key insight is that LLM activations exhibit two
complementary patterns: 1) \textit{Global statistical **sparsity**} driven by
sequence-level prefix information, and 2) \textit{Local semantic adaptability}
modulated by cognitive load metrics(e.g., surprisal and entropy). CLADA employs
a hierarchical thresholding strategy: a baseline from offline error-controlled
optimization ensures 40\%+ **sparsity**, dynamically adjusted by real-time
cognitive signals. Evaluations across six mainstream LLMs and nine benchmarks
demonstrate that CLADA achieves \textbf{~20\% average speedup with <2\%
accuracy drop}, outperforming Griffin (5\%+ degradation) and TT (negligible
speedup). Crucially, we establish the first formal connection between
neurolinguistic event-related potential (ERP) components and LLM efficiency
mechanisms through multi-level regression analysis ($R^2=0.17$ for
**sparsity**-adaptation synergy). Requiring no retraining or architectural changes,
CLADA offers a deployable solution for resource-aware LLM inference while
advancing biologically-inspired AI design. Our code is available at
\href{https://github.com/Oldify/CLADA}{CLADA}.


## Foundation Inference Models for Stochastic Differential Equations A Transformer-based Approach for Zero-shot Function Estimation

>Authors: Patrick Seifner, Kostadin Cvejoski, David Berghaus, Cesar Ojeda, Ramses J. Sanchez

>2025-02-26

> http://arxiv.org/abs/2502.19049v1

Stochastic differential equations (SDEs) describe dynamical systems where
deterministic flows, governed by a drift function, are superimposed with random
fluctuations dictated by a diffusion function. The accurate estimation (or
discovery) of these functions from data is a central problem in machine
learning, with wide application across natural and social sciences alike. Yet
current solutions are brittle, and typically rely on symbolic regression or
Bayesian non-parametrics. In this work, we introduce FIM-SDE (Foundation
Inference Model for SDEs), a transformer-based recognition model capable of
performing accurate zero-shot estimation of the drift and diffusion functions
of SDEs, from noisy and **sparse** observations on empirical processes of different
dimensionalities. Leveraging concepts from amortized inference and neural
operators, we train FIM-SDE in a supervised fashion, to map a large set of
noisy and discretely observed SDE paths to their corresponding drift and
diffusion functions. We demonstrate that one and the same (pretrained) FIM-SDE
achieves robust zero-shot function estimation (i.e. without any parameter
fine-tuning) across a wide range of synthetic and real-world processes, from
canonical SDE systems (e.g. double-well dynamics or weakly perturbed Hopf
bifurcations) to human motion recordings and oil price and wind speed
fluctuations.


## Binary Neural Networks for Large Language Model A Survey

>Authors: Liangdong Liu, Zhitong Zheng, Cong Wang, Tianhuang Su, Zhenyu Yang

>2025-02-26

> http://arxiv.org/abs/2502.19008v1

Large language models (LLMs) have wide applications in the field of natural
language processing(NLP), such as GPT-4 and Llama. However, with the
exponential growth of model parameter sizes, LLMs bring significant resource
overheads. Low-bit **quantization**, as a key technique, reduces memory usage and
computational demands by decreasing the bit-width of model parameters,
activations, and gradients. Previous **quantization** methods for LLMs have largely
employed Post-Training Quantization (PTQ) and Quantization-Aware Training
(QAT). PTQ does not require any retraining of the original model, while QAT
involves optimizing precision during training to achieve the best **quantization**
parameters. The BitNet team proposed a radically different approach, where
**quantization** is performed from the start of model training, utilizing
low-precision binary weights during the training process. This approach has led
to the emergence of many binary **quantization** techniques for large language
models. This paper provides a comprehensive review of these binary **quantization**
techniques. Specifically, we will introduce binary **quantization** techniques in
deep neural networks and further explore their application to LLMs, reviewing
their various contributions, implementations, and applications.


## The Sharpness Disparity Principle in Transformers for Accelerating Language Model Pre-Training

>Authors: Jinbo Wang, Mingze Wang, Zhanpeng Zhou, Junchi Yan, Weinan E, Lei Wu

>2025-02-26

> http://arxiv.org/abs/2502.19002v1

Transformers consist of diverse building blocks, such as embedding layers,
normalization layers, self-attention mechanisms, and point-wise feedforward
networks. Thus, understanding the differences and interactions among these
blocks is important. In this paper, we uncover a clear Sharpness Disparity
across these blocks, which emerges early in training and intriguingly persists
throughout the training process. Motivated by this finding, we propose
Blockwise Learning Rate (LR), a strategy that tailors the LR to each block's
sharpness, accelerating large language model (LLM) pre-training. By integrating
Blockwise LR into AdamW, we consistently achieve lower terminal loss and nearly
$2\times$ speedup compared to vanilla AdamW. We demonstrate this **acceleration**
across GPT-2 and LLaMA, with model sizes ranging from 0.12B to 1.1B and
datasets of OpenWebText and MiniPile. Finally, we incorporate Blockwise LR into
Adam-mini (Zhang et al., 2024), a recently proposed memory-efficient variant of
Adam, achieving a combined $2\times$ speedup and $2\times$ memory saving. These
results underscore the potential of exploiting the sharpness disparity to
improve LLM training.


## One Set to Rule Them All How to Obtain General Chemical Conditions via Bayesian Optimization over Curried Functions

>Authors: Stefan P. Schmid, Ella Miray Rajaonson, Cher Tian Ser, Mohammad Haddadnia, Shi Xuan Leong, Alán Aspuru-Guzik, Agustinus Kristiadi, Kjell Jorner, Felix Strieth-Kalthoff

>2025-02-26

> http://arxiv.org/abs/2502.18966v1

General parameters are highly desirable in the natural sciences - e.g.,
chemical reaction conditions that enable high yields across a range of related
transformations. This has a significant practical impact since those general
parameters can be transferred to related tasks without the need for laborious
and time-intensive re-optimization. While Bayesian optimization (BO) is widely
applied to find optimal parameter sets for specific tasks, it has remained
underused in experiment planning towards such general optima. In this work, we
consider the real-world problem of condition optimization for chemical
reactions to study how performing generality-oriented BO can accelerate the
identification of general optima, and whether these optima also translate to
unseen examples. This is achieved through a careful formulation of the problem
as an optimization over curried functions, as well as systematic evaluations of
generality-oriented strategies for optimization tasks on real-world
experimental data. We find that for generality-oriented optimization, simple
myopic optimization strategies that decouple parameter and task selection
perform comparably to more complex ones, and that effective optimization is
merely determined by an effective exploration of both parameter and task space.


## A Novel Topology Recovery Method for Low Voltage Distribution Networks

>Authors: Sina Mohammadi, Van-Hai Bui, Wencong Su

>2025-02-26

> http://arxiv.org/abs/2502.18939v1

Low voltage distribution networks (LVDNs) suffer from limited visibility due
to **sparse** or nonexistent measurement systems, leaving distribution network
service providers with incomplete data. Maintenance activities, such as
transformer upgrades and power line replacements, sometimes go undocumented,
leading to unmonitored topology changes. This lack of oversight hinders network
optimization, fault detection, and outage management, as utilities cannot fully
monitor or control the system. With the rise of electric vehicles, having an
accurate understanding of LVDN topology is crucial to avoid infrastructure
damage from potential overloads. This paper introduces a method to reconstruct
LVDN topology using incremental voltage and current changes from smart meters
at customer endpoints. The approach identifies and maps network topologies with
high accuracy, overcoming limitations of prior methods by discarding
unrealistic assumptions. Specifically, it addresses grids with fewer than three
pole connections and employs an AC power flow model over simplified DC
approximations. Simulations across diverse configurations validate the method's
effectiveness in accurately reconstructing LVDN topologies, enhancing
real-world applicability.


## Sparse Alignment Enhanced Latent Diffusion Transformer for Zero-Shot Speech Synthesis

>Authors: Ziyue Jiang, Yi Ren, Ruiqi Li, Shengpeng Ji, Zhenhui Ye, Chen Zhang, Bai Jionghao, Xiaoda Yang, Jialong Zuo, Yu Zhang, Rui Liu, Xiang Yin, Zhou Zhao

>2025-02-26

> http://arxiv.org/abs/2502.18924v1

While recent zero-shot text-to-speech (TTS) models have significantly
improved speech quality and expressiveness, mainstream systems still suffer
from issues related to speech-text alignment modeling: 1) models without
explicit speech-text alignment modeling exhibit less robustness, especially for
hard sentences in practical applications; 2) predefined alignment-based models
suffer from naturalness constraints of forced alignments. This paper introduces
\textit{S-DiT}, a TTS system featuring an innovative **sparse** alignment algorithm
that guides the latent diffusion transformer (DiT). Specifically, we provide
**sparse** alignment boundaries to S-DiT to reduce the difficulty of alignment
learning without limiting the search space, thereby achieving high naturalness.
Moreover, we employ a multi-condition classifier-free guidance strategy for
accent intensity adjustment and adopt the piecewise rectified flow technique to
accelerate the generation process. Experiments demonstrate that S-DiT achieves
state-of-the-art zero-shot TTS speech quality and supports highly flexible
control over accent intensity. Notably, our system can generate high-quality
one-minute speech with only 8 sampling steps. Audio samples are available at
https://sditdemo.github.io/sditdemo/.


## From Hours to Minutes Lossless Acceleration of Ultra Long Sequence Generation up to 100K Tokens

>Authors: Tong Wu, Junzhe Shen, Zixia Jia, Yuxuan Wang, Zilong Zheng

>2025-02-26

> http://arxiv.org/abs/2502.18890v1

Generating ultra-long sequences with large language models (LLMs) has become
increasingly crucial but remains a highly time-intensive task, particularly for
sequences up to 100K tokens. While traditional speculative decoding methods
exist, simply extending their generation limits fails to accelerate the process
and can be detrimental. Through an in-depth analysis, we identify three major
challenges hindering efficient generation: frequent model reloading, dynamic
key-value (**KV**) management and repetitive generation. To address these issues,
we introduce TOKENSWIFT, a novel framework designed to substantially accelerate
the generation process of ultra-long sequences while maintaining the target
model's inherent quality. Experimental results demonstrate that TOKENSWIFT
achieves over 3 times speedup across models of varying scales (1.5B, 7B, 8B,
14B) and architectures (MHA, GQA). This **acceleration** translates to hours of
time savings for ultra-long sequence generation, establishing TOKENSWIFT as a
scalable and effective solution at unprecedented lengths. Code can be found at
https://github.com/bigai-nlco/TokenSwift.


## On Pruning State-Space LLMs

>Authors: Tamer Ghattas, Michael Hassid, Roy Schwartz

>2025-02-26

> http://arxiv.org/abs/2502.18886v1

Recent work proposed state-space models (SSMs) as an efficient alternative to
transformer-based LLMs. Can these models be pruned to further reduce their
computation costs? We adapt several **pruning** methods to the SSM structure, and
apply them to four SSM-based LLMs across multiple tasks. We find that such
models are quite robust to some **pruning** methods (e.g. WANDA), while using other
methods lead to fast performance degradation.


## SE(3)-Equivariant Ternary Complex Prediction Towards Target Protein Degradation

>Authors: Fanglei Xue, Meihan Zhang, Shuqi Li, Xinyu Gao, James A. Wohlschlegel, Wenbing Huang, Yi Yang, Weixian Deng

>2025-02-26

> http://arxiv.org/abs/2502.18875v1

Targeted protein degradation (TPD) induced by small molecules has emerged as
a rapidly evolving modality in drug discovery, targeting proteins traditionally
considered "undruggable". Proteolysis-targeting chimeras (PROTACs) and
molecular glue degraders (MGDs) are the primary small molecules that induce
TPD. Both types of molecules form a ternary complex linking an E3 ligase with a
target protein, a crucial step for drug discovery. While significant advances
have been made in binary structure prediction for proteins and small molecules,
ternary structure prediction remains challenging due to obscure interaction
mechanisms and insufficient training data. Traditional methods relying on
manually assigned rules perform poorly and are computationally demanding due to
extensive random sampling. In this work, we introduce DeepTernary, a novel deep
learning-based approach that directly predicts ternary structures in an
end-to-end manner using an encoder-decoder architecture. DeepTernary leverages
an SE(3)-equivariant graph neural network (GNN) with both intra-graph and
ternary inter-graph attention mechanisms to capture intricate ternary
interactions from our collected high-quality training dataset, TernaryDB. The
proposed query-based Pocket Points Decoder extracts the 3D structure of the
final binding ternary complex from learned ternary embeddings, demonstrating
state-of-the-art accuracy and speed in existing PROTAC benchmarks without prior
knowledge from known PROTACs. It also achieves notable accuracy on the more
challenging MGD benchmark under the blind docking protocol. Remarkably, our
experiments reveal that the buried surface area calculated from predicted
structures correlates with experimentally obtained degradation potency-related
metrics. Consequently, DeepTernary shows potential in effectively assisting and
accelerating the development of TPDs for previously undruggable targets.


## Sliding Window Attention Training for Efficient Large Language Models

>Authors: Zichuan Fu, Wentao Song, Yejing Wang, Xian Wu, Yefeng Zheng, Yingying Zhang, Derong Xu, Xuetao Wei, Tong Xu, Xiangyu Zhao

>2025-02-26

> http://arxiv.org/abs/2502.18845v1

Recent advances in transformer-based Large Language Models (LLMs) have
demonstrated remarkable capabilities across various tasks. However, their
quadratic computational complexity concerning sequence length remains a
significant bottleneck for processing long documents. As a result, many efforts
like **sparse** attention and state space models have been proposed to improve the
efficiency of LLMs over long sequences. Though effective, these approaches
compromise the performance or introduce structural complexity. This calls for a
simple yet efficient model that preserves the fundamental Transformer
architecture. To this end, we introduce SWAT, which enables efficient
long-context handling via Sliding Window Attention Training. This paper first
attributes the inefficiency of Transformers to the attention sink phenomenon
resulting from the high variance of softmax operation. Then, we replace softmax
with the sigmoid function and utilize a balanced ALiBi and Rotary Position
Embedding for efficient information compression and retention. Experiments
demonstrate that SWAT achieves SOTA performance compared with state-of-the-art
linear recurrent architectures on eight benchmarks. Code is available at
https://anonymous.4open.science/r/SWAT-attention.


## Grad-ECLIP Gradient-based Visual and Textual Explanations for CLIP

>Authors: Chenyang Zhao, Kun Wang, Janet H. Hsiao, Antoni B. Chan

>2025-02-26

> http://arxiv.org/abs/2502.18816v1

Significant progress has been achieved on the improvement and downstream
usages of the Contrastive Language-Image Pre-training (CLIP) vision-language
model, while less attention is paid to the interpretation of CLIP. We propose a
Gradient-based visual and textual Explanation method for CLIP (Grad-ECLIP),
which interprets the matching result of CLIP for specific input image-text
pair. By decomposing the architecture of the encoder and discovering the
relationship between the matching similarity and intermediate spatial features,
Grad-ECLIP produces effective heat maps that show the influence of image
regions or words on the CLIP results. Different from the previous Transformer
interpretation methods that focus on the utilization of self-attention maps,
which are typically extremely **sparse** in CLIP, we produce high-quality visual
explanations by applying channel and spatial weights on token features.
Qualitative and quantitative evaluations verify the effectiveness and
superiority of Grad-ECLIP compared with the state-of-the-art methods.
Furthermore, a series of analysis are conducted based on our visual and textual
explanation results, from which we explore the working mechanism of image-text
matching, the strengths and limitations in attribution identification of CLIP,
and the relationship between the concreteness/abstractness of a word and its
usage in CLIP. Finally, based on the ability of explanation map that indicates
text-specific saliency region of input image, we also propose an application
with Grad-ECLIP, which is adopted to boost the fine-grained alignment in the
CLIP fine-tuning. The code of Grad-ECLIP is available here:
https://github.com/Cyang-Zhao/Grad-Eclip.


## Seeing the Forest for the Trees A Large Scale, Continuously Updating Meta-Analysis of Frontier LLMs

>Authors: Jungsoo Park, Junmo Kang, Gabriel Stanovsky, Alan Ritter

>2025-02-26

> http://arxiv.org/abs/2502.18791v1

The surge of LLM studies makes synthesizing their findings challenging.
Meta-analysis can uncover important trends across studies, but its use is
limited by the time-consuming nature of manual data extraction. Our study
presents a semi-automated approach for meta-analysis that accelerates data
extraction using LLMs. It automatically identifies relevant arXiv papers,
extracts experimental results and related attributes, and organizes them into a
structured dataset. We conduct a comprehensive meta-analysis of frontier LLMs
using an automatically extracted dataset, reducing the effort of paper
surveying and data extraction by more than 93\% compared to manual approaches.
We validate our dataset by showing that it reproduces key findings from a
recent manual meta-analysis about Chain-of-Thought (CoT), and also uncovers new
insights that go beyond it, showing for example that in-context examples
benefit multimodal tasks but offer limited gains in mathematical tasks compared
to CoT. Our automatically updatable dataset enables continuous tracking of
target models by extracting evaluation studies as new data becomes available.
Through our scientific artifacts and empirical analysis, we provide novel
insights into LLMs while facilitating ongoing meta-analyses of their behavior.


## M-ANT Efficient Low-bit Group Quantization for LLMs via Mathematically Adaptive Numerical Type

>Authors: Weiming Hu, Haoyan Zhang, Cong Guo, Yu Feng, Renyang Guan, Zhendong Hua, Zihan Liu, Yue Guan, Minyi Guo, Jingwen Leng

>2025-02-26

> http://arxiv.org/abs/2502.18755v1

Large language models (LLMs) are one of the most important killer computer
applications. The recent algorithmic advancement proposes a fine-grained
group-wise **quantization** for LLMs, which treats a small set (e.g., 64) of values
in a tensor as a compression unit. It effectively preserves the model accuracy
without retraining, and has become the standard approach to efficiently deploy
LLMs. On the other hand, there are works that propose various adaptive data
types to better adapt to different distributions and further reduce the
required bit length for LLMs. In this work, our detailed analysis unveils a key
finding that while different tensors exhibit similar distributions, small
groups can have markedly different distributions. As such, the group-level
diversity requires a new level of adaptivity for which existing adaptive data
types fail to provide.
  In this paper, we propose MANT, a mathematically adaptive numeric type,
featuring a more flexible encoding paradigm with a wider range of data
distribution and more efficient decodingcomputation fusion mechanism to address
these challenges. Based on MANT, we develop a supporting framework to assign
the appropriate data type for each group adaptively. Meanwhile, the dynamically
generated Key-Value (**KV**) caches in LLMs introduce further complexity for
real-time **quantization**. To tackle this, we propose an efficient real-time
**quantization** mechanism. Besides, we implement a specific processing element
(PE) to efficiently support MANT and incorporate a real-time **quantization** unit.
By integrating these components into a systolic array, MANT unifies the
group-wise weight and **KV** cache **quantization** and addresses the associated
challenges. Our evaluation shows achieving, on average, 2.99x (up to 4.46x)
speedup and 2.81x (up to 4.10x) energy reduction to the state-of-the-art LLM
accelerator.


## Automatic Prompt Optimization via Heuristic Search A Survey

>Authors: Wendi Cui, Jiaxin Zhang, Zhuohang Li, Hao Sun, Damien Lopez, Kamalika Das, Bradley A. Malin, Sricharan Kumar

>2025-02-26

> http://arxiv.org/abs/2502.18746v1

Recent advances in Large Language Models have led to remarkable achievements
across a variety of Natural Language Processing tasks, making prompt
engineering increasingly central to guiding model outputs. While manual methods
can be effective, they typically rely on intuition and do not automatically
refine prompts over time. In contrast, automatic prompt optimization employing
heuristic-based search algorithms can systematically explore and improve
prompts with minimal human oversight. This survey proposes a comprehensive
taxonomy of these methods, categorizing them by where optimization occurs, what
is optimized, what criteria drive the optimization, which operators generate
new prompts, and which iterative search algorithms are applied. We further
highlight specialized datasets and tools that support and accelerate automated
prompt refinement. We conclude by discussing key open challenges pointing
toward future opportunities for more robust and versatile LLM applications.


## Adaptive conditional latent diffusion maps beam loss to 2D phase space projections

>Authors: Alexander Scheinker, Alan Williams

>2025-02-25

> http://arxiv.org/abs/2502.18684v1

Beam loss (BLM) and beam current monitors (BCM) are ubiquitous at particle
accelerator around the world. These simple devices provide non-invasive high
level beam measurements, but give no insight into the detailed 6D
(x,y,z,px,py,pz) beam phase space distributions or dynamics. We show that
generative conditional latent diffusion models can learn intricate patterns to
map waveforms of tens of BLMs or BCMs along an accelerator to detailed 2D
projections of a charged particle beam's 6D phase space density. This
transformational method can be used at any particle accelerator to transform
simple non-invasive devices into detailed beam phase space diagnostics. We
demonstrate this concept via multi-particle simulations of the high intensity
beam in the kilometer-long LANSCE linear proton accelerator.


## Scaffolding Empathy Training Counselors with Simulated Patients and Utterance-level Performance Visualizations

>Authors: Ian Steenstra, Farnaz Nouraei, Timothy W. Bickmore

>2025-02-25

> http://arxiv.org/abs/2502.18673v1

Learning therapeutic counseling involves significant role-play experience
with mock patients, with current manual training methods providing only
intermittent granular feedback. We seek to accelerate and optimize counselor
training by providing frequent, detailed feedback to trainees as they interact
with a simulated patient. Our first application domain involves training
motivational interviewing skills for counselors. Motivational interviewing is a
collaborative counseling style in which patients are guided to talk about
changing their behavior, with empathetic counseling an essential ingredient. We
developed and evaluated an LLM-powered training system that features a
simulated patient and visualizations of turn-by-turn performance feedback
tailored to the needs of counselors learning motivational interviewing. We
conducted an evaluation study with professional and student counselors,
demonstrating high usability and satisfaction with the system. We present
design implications for the development of automated systems that train users
in counseling skills and their generalizability to other types of social skills
training.


## Steered Generation via Gradient Descent on Sparse Features

>Authors: Sumanta Bhattacharyya, Pedram Rooshenas

>2025-02-25

> http://arxiv.org/abs/2502.18644v1

Large language models (LLMs) encode a diverse range of linguistic features
within their latent representations, which can be harnessed to steer their
output toward specific target characteristics. In this paper, we modify the
internal structure of LLMs by training **sparse** autoencoders to learn a **sparse**
representation of the query embedding, allowing precise control over the
model's attention distribution. We demonstrate that manipulating this **sparse**
representation effectively transforms the output toward different stylistic and
cognitive targets. Specifically, in an educational setting, we show that the
cognitive complexity of LLM-generated feedback can be systematically adjusted
by modifying the encoded query representation at a specific layer. To achieve
this, we guide the learned **sparse** embedding toward the representation of
samples from the desired cognitive complexity level, using gradient-based
optimization in the latent space.


## Transfer Learning Assisted Fast Design Migration Over Technology Nodes A Study on Transformer Matching Network

>Authors: Chenhao Chu, Yuhao Mao, Hua Wang

>2025-02-25

> http://arxiv.org/abs/2502.18636v1

In this study, we introduce an innovative methodology for the design of
mm-Wave passive networks that leverages knowledge transfer from a pre-trained
synthesis neural network (NN) model in one technology node and achieves swift
and reliable design adaptation across different integrated circuit (IC)
technologies, operating frequencies, and metal options. We prove this concept
through simulation-based demonstrations focusing on the training and comparison
of the coefficient of determination (R2) of synthesis NNs for 1:1 on-chip
transformers in GlobalFoundries(GF) 22nm FDX+ (target domain), with and without
transfer learning from a model trained in GF 45nm SOI (source domain). In the
experiments, we explore varying target data densities of 0.5%, 1%, 5%, and 100%
with a complete dataset of 0.33 million in GF 22FDX+, and for comparative
analysis, apply source data densities of 25%, 50%, 75%, and 100% with a
complete dataset of 2.5 million in GF 45SOI. With the source data only at
30GHz, the experiments span target data from two metal options in GF 22FDX+ at
frequencies of 30 and 39 GHz. The results prove that the transfer learning with
the source domain knowledge (GF 45SOI) can both accelerate the training process
in the target domain (GF 22FDX+) and improve the R2 values compared to models
without knowledge transfer. Furthermore, it is observed that a model trained
with just 5% of target data and augmented by transfer learning achieves R2
values superior to a model trained with 20% of the data without transfer,
validating the advantage seen from 1% to 5% data density. This demonstrates a
notable reduction of 4X in the necessary dataset size highlighting the efficacy
of utilizing transfer learning to mm-Wave passive network design. The PyTorch
learning and testing code is publicly available at
https://github.com/ChenhaoChu/RFIC-TL.


## PacQ A SIMT Microarchitecture for Efficient Dataflow in Hyper-asymmetric GEMMs

>Authors: Ruokai Yin, Yuhang Li, Priyadarshini Panda

>2025-02-25

> http://arxiv.org/abs/2502.18627v1

Weight-only **quantization** has been widely explored in large language models
(LLMs) to reduce memory storage and data loading overhead. During deployment on
single-instruction-multiple-threads (SIMT) architectures, weights are stored in
low-precision integer (INT) format, while activations remain in full-precision
floating-point (FP) format to preserve inference accuracy. Although memory
footprint and data loading requirements for weight matrices are reduced,
computation performance gains remain limited due to the need to convert weights
back to FP format through unpacking and de**quantization** before GEMM operations.
In this work, we investigate methods to accelerate GEMM operations involving
packed low-precision INT weights and high-precision FP activations, defining
this as the hyper-asymmetric GEMM problem. Our approach co-optimizes tile-level
packing and dataflow strategies for INT weight matrices. We further design a
specialized FP-INT multiplier unit tailored to our packing and dataflow
strategies, enabling parallel processing of multiple INT weights. Finally, we
integrate the packing, dataflow, and multiplier unit into PacQ, a SIMT
microarchitecture designed to efficiently accelerate hyper-asymmetric GEMMs. We
show that PacQ can achieve up to 1.99x speedup and 81.4% reduction in EDP
compared to weight-only **quantize**d LLM workloads running on conventional SIMT
baselines.


## DRAMA Diverse Augmentation from Large Language Models to Smaller Dense Retrievers

>Authors: Xueguang Ma, Xi Victoria Lin, Barlas Oguz, Jimmy Lin, Wen-tau Yih, Xilun Chen

>2025-02-25

> http://arxiv.org/abs/2502.18460v1

Large language models (LLMs) have demonstrated strong effectiveness and
robustness while fine-tuned as dense retrievers. However, their large parameter
size brings significant inference time computational challenges, including high
encoding costs for large-scale corpora and increased query latency, limiting
their practical deployment. While smaller retrievers offer better efficiency,
they often fail to generalize effectively with limited supervised fine-tuning
data. In this work, we introduce DRAMA, a training framework that leverages
LLMs to train smaller generalizable dense retrievers. In particular, we adopt
pruned LLMs as the backbone and train on diverse LLM-augmented data in a
single-stage contrastive learning setup. Experiments show that DRAMA offers
better multilingual and long-context capabilities than traditional
encoder-based retrievers, and achieves strong performance across multiple tasks
and languages. These highlight the potential of connecting the training of
smaller retrievers with the growing advancements in LLMs, bridging the gap
between efficiency and generalization.


## Jacobian Sparse Autoencoders Sparsify Computations, Not Just Activations

>Authors: Lucy Farnik, Tim Lawson, Conor Houghton, Laurence Aitchison

>2025-02-25

> http://arxiv.org/abs/2502.18147v1

Sparse autoencoders (SAEs) have been successfully used to discover **sparse** and
human-interpretable representations of the latent activations of LLMs. However,
we would ultimately like to understand the computations performed by LLMs and
not just their representations. The extent to which SAEs can help us understand
computations is unclear because they are not designed to "sparsify"
computations in any sense, only latent activations. To solve this, we propose
Jacobian SAEs (JSAEs), which yield not only **sparsity** in the input and output
activations of a given model component but also **sparsity** in the computation
(formally, the Jacobian) connecting them. With a na\"ive implementation, the
Jacobians in LLMs would be computationally intractable due to their size. One
key technical contribution is thus finding an efficient way of computing
Jacobians in this setup. We find that JSAEs extract a relatively large degree
of computational **sparsity** while preserving downstream LLM performance
approximately as well as traditional SAEs. We also show that Jacobians are a
reasonable proxy for computational **sparsity** because MLPs are approximately
linear when rewritten in the JSAE basis. Lastly, we show that JSAEs achieve a
greater degree of computational **sparsity** on pre-trained LLMs than on the
equivalent randomized LLM. This shows that the **sparsity** of the computational
graph appears to be a property that LLMs learn through training, and suggests
that JSAEs might be more suitable for understanding learned transformer
computations than standard SAEs.


## LevelRAG Enhancing Retrieval-Augmented Generation with Multi-hop Logic Planning over Rewriting Augmented Searchers

>Authors: Zhuocheng Zhang, Yang Feng, Min Zhang

>2025-02-25

> http://arxiv.org/abs/2502.18139v1

Retrieval-Augmented Generation (RAG) is a crucial method for mitigating
hallucinations in Large Language Models (LLMs) and integrating external
knowledge into their responses. Existing RAG methods typically employ query
rewriting to clarify the user intent and manage multi-hop logic, while using
hybrid retrieval to expand search scope. However, the tight coupling of query
rewriting to the dense retriever limits its compatibility with hybrid
retrieval, impeding further RAG performance improvements. To address this
challenge, we introduce a high-level searcher that decomposes complex queries
into atomic queries, independent of any retriever-specific optimizations.
Additionally, to harness the strengths of **sparse** retrievers for precise keyword
retrieval, we have developed a new **sparse** searcher that employs Lucene syntax
to enhance retrieval accuracy.Alongside web and dense searchers, these
components seamlessly collaborate within our proposed method,
\textbf{LevelRAG}. In LevelRAG, the high-level searcher orchestrates the
retrieval logic, while the low-level searchers (**sparse**, web, and dense) refine
the queries for optimal retrieval. This approach enhances both the completeness
and accuracy of the retrieval process, overcoming challenges associated with
current query rewriting techniques in hybrid retrieval scenarios. Empirical
experiments conducted on five datasets, encompassing both single-hop and
multi-hop question answering tasks, demonstrate the superior performance of
LevelRAG compared to existing RAG methods. Notably, LevelRAG outperforms the
state-of-the-art proprietary model, GPT4o, underscoring its effectiveness and
potential impact on the RAG field.


## SpargeAttn Accurate Sparse Attention Accelerating Any Model Inference

>Authors: Jintao Zhang, Chendong Xiang, Haofeng Huang, Jia Wei, Haocheng Xi, Jun Zhu, Jianfei Chen

>2025-02-25

> http://arxiv.org/abs/2502.18137v1

An efficient attention implementation is essential for large models due to
its quadratic time complexity. Fortunately, attention commonly exhibits
**sparsity**, i.e., many values in the attention map are near zero, allowing for
the omission of corresponding computations. Many studies have utilized the
**sparse** pattern to accelerate attention. However, most existing works focus on
optimizing attention within specific models by exploiting certain **sparse**
patterns of the attention map. A universal **sparse** attention that guarantees
both the speedup and end-to-end performance of diverse models remains elusive.
In this paper, we propose SpargeAttn, a universal **sparse** and **quantize**d
attention for any model. Our method uses a two-stage online filter: in the
first stage, we rapidly and accurately predict the attention map, enabling the
skip of some matrix multiplications in attention. In the second stage, we
design an online softmax-aware filter that incurs no extra overhead and further
skips some matrix multiplications. Experiments show that our method
significantly accelerates diverse models, including language, image, and video
generation, without sacrificing end-to-end metrics. The codes are available at
https://github.com/thu-ml/SpargeAttn.


## Inverse Materials Design by Large Language Model-Assisted Generative Framework

>Authors: Yun Hao, Che Fan, Beilin Ye, Wenhao Lu, Zhen Lu, Peilin Zhao, Zhifeng Gao, Qingyao Wu, Yanhui Liu, Tongqi Wen

>2025-02-25

> http://arxiv.org/abs/2502.18127v1

Deep generative models hold great promise for inverse materials design, yet
their efficiency and accuracy remain constrained by data scarcity and model
architecture. Here, we introduce AlloyGAN, a closed-loop framework that
integrates Large Language Model (LLM)-assisted text mining with Conditional
Generative Adversarial Networks (CGANs) to enhance data diversity and improve
inverse design. Taking alloy discovery as a case study, AlloyGAN systematically
refines material candidates through iterative screening and experimental
validation. For metallic glasses, the framework predicts thermodynamic
properties with discrepancies of less than 8% from experiments, demonstrating
its robustness. By bridging generative AI with domain knowledge and validation
workflows, AlloyGAN offers a scalable approach to accelerate the discovery of
materials with tailored properties, paving the way for broader applications in
materials science.


## HyperG Hypergraph-Enhanced LLMs for Structured Knowledge

>Authors: Sirui Huang, Hanqian Li, Yanggan Gu, Xuming Hu, Qing Li, Guandong Xu

>2025-02-25

> http://arxiv.org/abs/2502.18125v1

Given that substantial amounts of domain-specific knowledge are stored in
structured formats, such as web data organized through HTML, Large Language
Models (LLMs) are expected to fully comprehend this structured information to
broaden their applications in various real-world downstream tasks. Current
approaches for applying LLMs to structured data fall into two main categories:
serialization-based and operation-based methods. Both approaches, whether
relying on serialization or using SQL-like operations as an intermediary,
encounter difficulties in fully capturing structural relationships and
effectively handling **sparse** data. To address these unique characteristics of
structured data, we propose HyperG, a hypergraph-based generation framework
aimed at enhancing LLMs' ability to process structured knowledge. Specifically,
HyperG first augment **sparse** data with contextual information, leveraging the
generative power of LLMs, and incorporate a prompt-attentive hypergraph
learning (PHL) network to encode both the augmented information and the
intricate structural relationships within the data. To validate the
effectiveness and generalization of HyperG, we conduct extensive experiments
across two different downstream tasks requiring structured knowledge.


## Escaping The Big Data Paradigm in Self-Supervised Representation Learning

>Authors: Carlos Vélez García, Miguel Cazorla, Jorge Pomares

>2025-02-25

> http://arxiv.org/abs/2502.18056v1

The reliance on large-scale datasets and extensive computational resources
has become a major barrier to advancing representation learning in vision,
especially in data-scarce domains. In this paper, we address the critical
question: Can we escape the big data paradigm in self-supervised representation
learning from images? We introduce SCOTT (Sparse Convolutional Tokenizer for
Transformers), a shallow tokenization architecture that is compatible with
Masked Image Modeling (MIM) tasks. SCOTT injects convolutional inductive biases
into Vision Transformers (ViTs), enhancing their efficacy in small-scale data
regimes. Alongside, we propose to train on a Joint-Embedding Predictive
Architecture within a MIM framework (MIM-JEPA), operating in latent
representation space to capture more semantic features. Our approach enables
ViTs to be trained from scratch on datasets orders of magnitude smaller than
traditionally required --without relying on massive external datasets for
pretraining. We validate our method on three small-size, standard-resoultion,
fine-grained datasets: Oxford Flowers-102, Oxford IIIT Pets-37, and
ImageNet-100. Despite the challenges of limited data and high intra-class
similarity, frozen SCOTT models pretrained with MIM-JEPA significantly
outperform fully supervised methods and achieve competitive results with SOTA
approaches that rely on large-scale pretraining, complex image augmentations
and bigger model sizes. By demonstrating that robust off-the-shelf
representations can be learned with limited data, compute, and model sizes, our
work paves the way for computer applications in resource constrained
environments such as medical imaging or robotics. Our findings challenge the
prevailing notion that vast amounts of data are indispensable for effective
representation learning in vision, offering a new pathway toward more
accessible and inclusive advancements in the field.


## Weyl Quantization of Exponential Lie Groups for Square Integrable Representations

>Authors: Stine Marie Berge, Simon Halvdansson

>2025-02-25

> http://arxiv.org/abs/2502.18034v1

We construct a general **quantization** procedure for square integrable functions
on well-behaved connected exponential Lie groups. The Lie groups in question
should admit at least one co-adjoint orbit of maximal possible dimension. The
construction is based on composing the Fourier-Wigner transform with another
Fourier transform we call the Fourier-Kirillov transform. This **quantization** has
many desirable properties including respecting function translations and
inducing a well-behaved Wigner distribution.
  Moreover, we investigate the connection to the operator convolutions of
quantum harmonic analysis. This is intricately connected to Weyl **quantization**
in the Weyl-Heisenberg setting. We find that convolution relations in quantum
harmonic analysis can be written as group convolutions of Weyl **quantization**s.
This implies that the squared modulus of the wavelet transform of the
representation can be written as a convolution between two Wigner
distributions. Lastly, we look at how we can extend known results based on Weyl
**quantization** to wider classes of groups using our **quantization** procedure.


## Optimal Brain Apoptosis

>Authors: Mingyuan Sun, Zheng Fang, Jiaxu Wang, Junjie Jiang, Delei Kong, Chenming Hu, Yuetong Fang, Renjing Xu

>2025-02-25

> http://arxiv.org/abs/2502.17941v1

The increasing complexity and parameter count of Convolutional Neural
Networks (CNNs) and Transformers pose challenges in terms of computational
efficiency and resource demands. Pruning has been identified as an effective
strategy to address these challenges by removing redundant elements such as
neurons, channels, or connections, thereby enhancing computational efficiency
without heavily compromising performance. This paper builds on the foundational
work of Optimal Brain Damage (OBD) by advancing the methodology of parameter
importance estimation using the Hessian matrix. Unlike previous approaches that
rely on approximations, we introduce Optimal Brain Apoptosis (OBA), a novel
**pruning** method that calculates the Hessian-vector product value directly for
each parameter. By decomposing the Hessian matrix across network layers and
identifying conditions under which inter-layer Hessian submatrices are
non-zero, we propose a highly efficient technique for computing the
second-order Taylor expansion of parameters. This approach allows for a more
precise **pruning** process, particularly in the context of CNNs and Transformers,
as validated in our experiments including VGG19, ResNet32, ResNet50, and
ViT-B/16 on CIFAR10, CIFAR100 and Imagenet datasets. Our code is available at
https://github.com/NEU-REAL/OBA.


## Novel quantum circuit for image compression utilizing modified Toffoli gate and quantized transformed coefficient alongside a novel reset gate

>Authors: Ershadul Haque, Manoranjan Paul

>2025-02-25

> http://arxiv.org/abs/2502.17815v1

Quantum image computing has emerged as a groundbreaking field,
revolutionizing how we store and process data at speeds incomparable to
classical methods. Nevertheless, as image sizes expand, so does the complexity
of qubit connections, posing significant challenges in the efficient
representation and compression of quantum images. In response, we introduce a
modified Toffoli gate state connection using a **quantize**d transform coefficient
preparation process. This innovative strategy streamlines circuit complexity by
modifying state connection from the state connection information. In our
operational control gates, only input 1 impacts the output, allowing us to
modify the state connection and dramatically enhance the efficiency of the
proposed circuit. As a result, the proposed approach significantly reduces the
number of gates required for both image compression and representation. Our
findings reveal that it requires an impressive 44.21 percent fewer gates than
existing techniques, such as the Direct Cosine Transform Efficient Flexible
Representation of Quantum Images (DCTEFRQI), all while maintaining a consistent
peak signal-to-noise ratio (PSNR). For an image block size of 2^Sx2^Sy with q
gray levels, the complexity of our approach can be succinctly expressed as,
O[3q+log2Sx+log2Sy+2q(log2Sx+log2Sy)]. Here, Sx and Sy represent the X and Y
positional control gates while q indicates the non-zero transform coefficients.
Moreover, experimental evaluations strongly demonstrate that it excels in both
compressing and representing quantum images compared to the DCTEFRQI approach,
particularly excelling in the essential metrics of gate requirements and PSNR
performance. Embrace the future of quantum imaging with our innovative
solution, where efficiency meets excellence.


## ELMo-Tune-V2 LLM-Assisted Full-Cycle Auto-Tuning to Optimize LSM-Based Key-Value Stores

>Authors: Viraj Thakkar, Qi Lin, Kenanya Keandra Adriel Prasetyo, Raden Haryosatyo Wisjnunandono, Achmad Imam Kistijantoro, Reza Fuad Rachmadi, Zhichao Cao

>2025-02-24

> http://arxiv.org/abs/2502.17606v1

Log-Structured Merge-tree-based Key-Value Store (LSM-**KV**S) is a foundational
storage engine serving diverse modern workloads, systems, and applications. To
suit varying use cases, LSM-**KV**S allows a vast configuration space that controls
core parameters like compaction, flush, and cache sizes, each consuming a
shared pool of CPU, Memory, and Storage resources. Navigating the LSM-**KV**S
configuration space necessitates knowledge of the impact of each configuration
on the expected workload and underlying hardware. Beyond expensive and
time-intensive human-expert-based tuning, existing LSM-**KV**S tuning solutions
focus on tuning with specific workload expectations while limited to a narrow
subset of parameters.
  This paper introduces ELMo-Tune-V2, a framework that integrates Large
Language Models (LLMs) at its foundation to demonstrate the potential of
applying modern LLMs in data system optimization problems. ELMo-Tune-V2
leverages the contextual reasoning, cross-domain, and generative capabilities
of LLMs to perform 1) self-navigated characterization and modeling of LSM-**KV**S
workloads, 2) automatic tuning across a broad parameter space using
cross-domain knowledge, and 3) real-time dynamic configuration adjustments for
LSM-**KV**S. ELMo-Tune-V2 integrates three innovations: LLM-based workload
synthesis for adaptive benchmark generation, feedback-driven iterative
fine-tuning for configuration refinement, and real-time tuning to handle
evolving workloads. Through detailed evaluation using RocksDB under several
real-world applications across diverse scenarios, ELMo-Tune-V2 achieves
performance improvements up to ~14X our YCSB benchmarks compared against
default RocksDB configurations, and our end-to-end tests with upper-level
applications, NebulaGraph and Kvrocks, demonstrate performance gains of 34% and
26%, respectively.


## MEDA Dynamic KV Cache Allocation for Efficient Multimodal Long-Context Inference

>Authors: Zhongwei Wan, Hui Shen, Xin Wang, Che Liu, Zheda Mai, Mi Zhang

>2025-02-24

> http://arxiv.org/abs/2502.17599v1

Long-context Multimodal Large Language Models (MLLMs) that incorporate long
text-image and text-video modalities, demand substantial resources as their
multimodal Key-Value (**KV**) caches grow with increasing input lengths,
challenging inference efficiency. Existing methods for **KV** cache compression, in
both text-only and multimodal LLMs, have neglected attention density variations
across layers, thus often adopting uniform or progressive reduction strategies
for layer-wise cache allocation. In this work, we propose MEDA, a dynamic
layer-wise **KV** cache allocation method for efficient multimodal long-context
inference. As its core, MEDA utilizes cross-modal attention entropy to
determine the **KV** cache size at each MLLMs layer. Given the dynamically
allocated **KV** cache size at each layer, MEDA also employs a **KV** pair selection
scheme to identify which **KV** pairs to select and a **KV** pair merging strategy that
merges the selected and non-selected ones to preserve information from the
entire context. MEDA achieves up to 72% **KV** cache memory reduction and 2.82
times faster decoding speed, while maintaining or enhancing performance on
various multimodal tasks in long-context settings, including multi-images and
long-video scenarios. Our code is released at
https://github.com/AIoT-MLSys-Lab/MEDA.


## LongSpec Long-Context Speculative Decoding with Efficient Drafting and Verification

>Authors: Penghui Yang, Cunxiao Du, Fengzhuo Zhang, Haonan Wang, Tianyu Pang, Chao Du, Bo An

>2025-02-24

> http://arxiv.org/abs/2502.17421v1

Speculative decoding has become a promising technique to mitigate the high
inference latency of autoregressive decoding in Large Language Models (LLMs).
Despite its promise, the effective application of speculative decoding in LLMs
still confronts three key challenges: the increasing memory demands of the
draft model, the distribution shift between the short-training corpora and
long-context inference, and inefficiencies in attention implementation. In this
work, we enhance the performance of speculative decoding in long-context
settings by addressing these challenges. First, we propose a memory-efficient
draft model with a constant-sized Key-Value (**KV**) cache. Second, we introduce
novel position indices for short-training data, enabling seamless adaptation
from short-context training to long-context inference. Finally, we present an
innovative attention aggregation method that combines fast implementations for
prefix computation with standard attention for tree mask handling, effectively
resolving the latency and memory inefficiencies of tree decoding. Our approach
achieves strong results on various long-context tasks, including
repository-level code completion, long-context summarization, and o1-like long
reasoning tasks, demonstrating significant improvements in latency reduction.
The code is available at https://github.com/sail-sg/LongSpec.


## A Concise Lyapunov Analysis of Nesterov's Accelerated Gradient Method

>Authors: Jun Liu

>2025-02-24

> http://arxiv.org/abs/2502.17373v2

Convergence analysis of Nesterov's accelerated gradient method has attracted
significant attention over the past decades. While extensive work has explored
its theoretical properties and elucidated the intuition behind its
**acceleration**, a simple and direct proof of its convergence rates is still
lacking. We provide a concise Lyapunov analysis of the convergence rates of
Nesterov's accelerated gradient method for both general convex and strongly
convex functions.


## Delta Decompression for MoE-based LLMs Compression

>Authors: Hao Gu, Wei Li, Lujun Li, Qiyuan Zhu, Mark Lee, Shengjie Sun, Wei Xue, Yike Guo

>2025-02-24

> http://arxiv.org/abs/2502.17298v1

Mixture-of-Experts (MoE) architectures in large language models (LLMs)
achieve exceptional performance, but face prohibitive storage and memory
requirements. To address these challenges, we present $D^2$-MoE, a new delta
decompression compressor for reducing the parameters of MoE LLMs. Based on
observations of expert diversity, we decompose their weights into a shared base
weight and unique delta weights. Specifically, our method first merges each
expert's weight into the base weight using the Fisher information matrix to
capture shared components. Then, we compress delta weights through Singular
Value Decomposition (SVD) by exploiting their low-rank properties. Finally, we
introduce a semi-dynamical structured **pruning** strategy for the base weights,
combining static and dynamic redundancy analysis to achieve further parameter
reduction while maintaining input adaptivity. In this way, our $D^2$-MoE
successfully compact MoE LLMs to high compression ratios without additional
training. Extensive experiments highlight the superiority of our approach, with
over 13% performance gains than other compressors on
Mixtral|Phi-3.5|DeepSeek|Qwen2 MoE LLMs at 40$\sim$60% compression rates. Codes
are available in https://github.com/lliai/D2MoE.


## GaussianFlowOcc Sparse and Weakly Supervised Occupancy Estimation using Gaussian Splatting and Temporal Flow

>Authors: Simon Boeder, Fabian Gigengack, Benjamin Risse

>2025-02-24

> http://arxiv.org/abs/2502.17288v2

Occupancy estimation has become a prominent task in 3D computer vision,
particularly within the autonomous driving community. In this paper, we present
a novel approach to occupancy estimation, termed GaussianFlowOcc, which is
inspired by Gaussian Splatting and replaces traditional dense voxel grids with
a **sparse** 3D Gaussian representation. Our efficient model architecture based on
a Gaussian Transformer significantly reduces computational and memory
requirements by eliminating the need for expensive 3D convolutions used with
inefficient voxel-based representations that predominantly represent empty 3D
spaces. GaussianFlowOcc effectively captures scene dynamics by estimating
temporal flow for each Gaussian during the overall network training process,
offering a straightforward solution to a complex problem that is often
neglected by existing methods. Moreover, GaussianFlowOcc is designed for
scalability, as it employs weak supervision and does not require costly dense
3D voxel annotations based on additional data (e.g., LiDAR). Through extensive
experimentation, we demonstrate that GaussianFlowOcc significantly outperforms
all previous methods for weakly supervised occupancy estimation on the nuScenes
dataset while featuring an inference speed that is 50 times faster than current
SOTA.


## The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?

>Authors: Zhenheng Tang, Xiang Liu, Qian Wang, Peijie Dong, Bingsheng He, Xiaowen Chu, Bo Li

>2025-02-24

> http://arxiv.org/abs/2502.17535v1

Motivated by reducing the computational and storage costs of LLMs, model
compression and **KV** cache compression have attracted much attention from
researchers. However, current methods predominantly emphasize maintaining the
performance of compressed LLMs, as measured by perplexity or simple accuracy on
tasks of common sense knowledge QA and basic arithmetic reasoning. In this
blog, we present a brief review of recent advancements in LLMs related to
retrieval-augmented generation, multi-step reasoning, external tools, and
computational expressivity, all of which substantially enhance LLM performance.
Then, we propose a lottery LLM hypothesis suggesting that for a given LLM and
task, there exists a smaller lottery LLM capable of producing the same
performance as the original LLM with the assistance of multi-step reasoning and
external tools. Based on the review of current progress in LLMs, we discuss and
summarize the essential capabilities that the lottery LLM and **KV** cache
compression must possess, which are currently overlooked in existing methods.


## Evaluating Expert Contributions in a MoE LLM for Quiz-Based Tasks

>Authors: Andrei Chernov

>2025-02-24

> http://arxiv.org/abs/2502.17187v1

Recently, Large Language Models (LLMs) with Mixture of Experts (MoE) layers
have gained significant attention. Currently, state-of-the-art LLMs utilize
this architecture. There is a substantial amount of research on how to train
such models and how to select hyperparameters for this architecture. However,
there is a lack of studies focusing on post-evaluation analysis of MoE layer
properties. In this paper, we take a first step toward closing this gap by
evaluating expert contributions on the quiz-based MMLU benchmark. We show that
most experts were never activated during inference on this benchmark.
Additionally, the output distribution of gating networks is much closer to
uniform than **sparse**. Finally, we demonstrate that the average performance of
some experts within the same layer varies significantly.


## CodeSwift Accelerating LLM Inference for Efficient Code Generation

>Authors: Qianhui Zhao, Li Zhang, Fang Liu, Xiaoli Lian, Qiaoyuanhe Meng, Ziqian Jiao, Zetong Zhou, Borui Zhang, Runlin Guo, Jia Li

>2025-02-24

> http://arxiv.org/abs/2502.17139v1

Code generation is a latency-sensitive task that demands high timeliness, but
the autoregressive decoding mechanism of Large Language Models (LLMs) leads to
poor inference efficiency. Existing LLM inference **acceleration** methods mainly
focus on standalone functions using only built-in components. Moreover, they
treat code like natural language sequences, ignoring its unique syntax and
semantic characteristics. As a result, the effectiveness of these approaches in
code generation tasks remains limited and fails to align with real-world
programming scenarios. To alleviate this issue, we propose CodeSwift, a simple
yet highly efficient inference **acceleration** approach specifically designed for
code generation, without comprising the quality of the output. CodeSwift
constructs a multi-source datastore, providing access to both general and
project-specific knowledge, facilitating the retrieval of high-quality draft
sequences. Moreover, CodeSwift reduces retrieval cost by controlling retrieval
timing, and enhances efficiency through parallel retrieval and a context- and
LLM preference-aware cache. Experimental results show that CodeSwift can reach
up to 2.53x and 2.54x speedup compared to autoregressive decoding in
repository-level and standalone code generation tasks, respectively,
outperforming state-of-the-art inference **acceleration** approaches by up to 88%.


## Enhancing Image Matting in Real-World Scenes with Mask-Guided Iterative Refinement

>Authors: Rui Liu

>2025-02-24

> http://arxiv.org/abs/2502.17093v1

Real-world image matting is essential for applications in content creation
and augmented reality. However, it remains challenging due to the complex
nature of scenes and the scarcity of high-quality datasets. To address these
limitations, we introduce Mask2Alpha, an iterative refinement framework
designed to enhance semantic comprehension, instance awareness, and fine-detail
recovery in image matting. Our framework leverages self-supervised Vision
Transformer features as semantic priors, strengthening contextual understanding
in complex scenarios. To further improve instance differentiation, we implement
a mask-guided feature selection module, enabling precise targeting of objects
in multi-instance settings. Additionally, a **sparse** convolution-based
optimization scheme allows Mask2Alpha to recover high-resolution details
through progressive refinement,from low-resolution semantic passes to
high-resolution **sparse** reconstructions. Benchmarking across various real-world
datasets, Mask2Alpha consistently achieves state-of-the-art results, showcasing
its effectiveness in accurate and efficient image matting.


## Systematic Weight Evaluation for Pruning Large Language Models Enhancing Performance and Sustainability

>Authors: Ashhadul Islam, Samir Brahim Belhaouari, Amine Bermak

>2025-02-24

> http://arxiv.org/abs/2502.17071v1

The exponential growth of large language models (LLMs) like ChatGPT has
revolutionized artificial intelligence, offering unprecedented capabilities in
natural language processing. However, the extensive computational resources
required for training these models have significant environmental implications,
including high carbon emissions, energy consumption, and water usage. This
research presents a novel approach to LLM **pruning**, focusing on the systematic
evaluation of individual weight importance throughout the training process. By
monitoring parameter evolution over time, we propose a method that effectively
reduces model size without compromising performance. Extensive experiments with
both a scaled-down LLM and a large multimodal model reveal that moderate
**pruning** enhances efficiency and reduces loss, while excessive **pruning**
drastically deteriorates model performance. These findings highlight the
critical need for optimized AI models to ensure sustainable development,
balancing technological advancement with environmental responsibility.


## Random Projections and Natural Sparsity in Time-Series Classification A Theoretical Analysis

>Authors: Jorge Marco-Blanco, Rubén Cuevas

>2025-02-24

> http://arxiv.org/abs/2502.17061v1

Time-series classification is essential across diverse domains, including
medical diagnosis, industrial monitoring, financial forecasting, and human
activity recognition. The Rocket algorithm has emerged as a simple yet powerful
method, achieving state-of-the-art performance through random convolutional
kernels applied to time-series data, followed by non-linear transformation. Its
architecture approximates a one-hidden-layer convolutional neural network while
eliminating parameter training, ensuring computational efficiency. Despite its
empirical success, fundamental questions about its theoretical foundations
remain unexplored. We bridge theory and practice by formalizing Rocket's random
convolutional filters within the compressed sensing framework, proving that
random projections preserve discriminative patterns in time-series data. This
analysis reveals relationships between kernel parameters and input signal
characteristics, enabling more principled approaches to algorithm
configuration. Moreover, we demonstrate that its non-linearity, based on the
proportion of positive values after convolutions, expresses the inherent
**sparsity** of time-series data. Our theoretical investigation also proves that
Rocket satisfies two critical conditions: translation invariance and noise
robustness. These findings enhance interpretability and provide guidance for
parameter optimization in extreme cases, advancing both theoretical
understanding and practical application of time-series classification.


## Active Learning for Conditional Inverse Design with Crystal Generation and Foundation Atomic Models

>Authors: Zhuoyuan Li, Siyu Liu, Beilin Ye, David J. Srolovitz, Tongqi Wen

>2025-02-24

> http://arxiv.org/abs/2502.16984v1

Artificial intelligence (AI) is transforming materials science, enabling both
theoretical advancements and accelerated materials discovery. Recent progress
in crystal generation models, which design crystal structures for targeted
properties, and foundation atomic models (FAMs), which capture interatomic
interactions across the periodic table, has significantly improved inverse
materials design. However, an efficient integration of these two approaches
remains an open challenge. Here, we present an active learning framework that
combines crystal generation models and foundation atomic models to enhance the
accuracy and efficiency of inverse design. As a case study, we employ Con-CDVAE
to generate candidate crystal structures and MACE-MP-0 FAM as one of the
high-throughput screeners for bulk modulus evaluation. Through iterative active
learning, we demonstrate that Con-CDVAE progressively improves its accuracy in
generating crystals with target properties, highlighting the effectiveness of a
property-driven fine-tuning process. Our framework is general to accommodate
different crystal generation and foundation atomic models, and establishes a
scalable approach for AI-driven materials discovery. By bridging generative
modeling with atomic-scale simulations, this work paves the way for more
accurate and efficient inverse materials design.


## Make LLM Inference Affordable to Everyone Augmenting GPU Memory with NDP-DIMM

>Authors: Lian Liu, Shixin Zhao, Bing Li, Haimeng Ren, Zhaohui Xu, Mengdi Wang, Xiaowei Li, Yinhe Han, Ying Wang

>2025-02-24

> http://arxiv.org/abs/2502.16963v1

The billion-scale Large Language Models (LLMs) need deployment on expensive
server-grade GPUs with large-storage HBMs and abundant computation capability.
As LLM-assisted services become popular, achieving cost-effective LLM inference
on budget-friendly hardware becomes the trend. Extensive researches relocate
LLM parameters from expensive GPUs to host memory. However, the restricted
bandwidth between the host and GPU memory limits the inference performance.
  This work introduces Hermes, a budget-friendly system that leverages the
near-data processing (NDP) within commodity DRAM DIMMs to enhance the
performance of a single consumer-grade GPU, achieving efficient LLM inference.
The inherent activation **sparsity** in LLMs naturally divides weight parameters
into two categories, termed ``hot" and ``cold" neurons, respectively. Hot
neurons, which consist of only approximately 20\% of all weight parameters,
account for 80\% of the total computational load, while cold neurons make up
the other 80\% of parameters but are responsible for just 20\% of the
computational load. Therefore, we propose a heterogeneous computing strategy:
mapping hot neurons to a single computation-efficient GPU, while offloading
cold neurons to NDP-DIMMs, which offer large memory size but limited
computation capabilities. Meanwhile, the dynamic nature of activation **sparsity**
needs a real-time partition of hot/cold neurons and adaptive remapping of cold
neurons across multiple NDP-DIMM modules. Therefore, we introduce a lightweight
predictor optimizing real-time neuron partition and adjustment between GPU and
NDP-DIMMs. We also utilize a window-based online scheduling mechanism to
maintain load balance among NDP-DIMM modules. Hermes facilitates the deployment
of LLaMA2-70B on consumer-grade hardware at 13.75 tokens/s and realizes an
average 75.24$\times$ speedup over the state-of-the-art offloading-based
inference system.


## Unconventional topological Weyl-dipole phonon

>Authors: Jianhua Wang, Yang Wang, Feng Zhou, Wenhong Wang, Zhenxiang Cheng, Shifeng Qian, Xiaotian Wang, Zhi-Ming Yu

>2025-02-24

> http://arxiv.org/abs/2502.16958v1

A pair of Weyl points (WPs) with opposite Chern numbers ${\cal{C}}$ can
exhibit an additional higher-order $Z_2$ topological charge, giving rise to the
formation of a $Z_2$ Weyl dipole. Owing to the nontrivial topological charge,
$Z_2$ Weyl dipoles should also appear in pairs, and the WPs within each $Z_2$
Weyl dipole can not be annihilated when meeting together. As a novel
topological state, the topological Weyl-dipole phase (TWDP) has garnered
significant attention, yet its realization in crystalline materials remains a
challenge. Here, through first-principles calculations and theoretical
analysis, we demonstrate the existence of the Weyl-dipole phase in the phonon
spectra of the $P6_3$ type Y(OH)$_3$. Particularly, the Weyl dipole in this
system is protected by a **quantize**d quadrupole moment, and it distinguished from
conventional Weyl dipole, as it comprises an unconventional charge-3 WP with
${\cal{C}}=-3$ and three conventional charge-1 WPs with ${\cal{C}}=1$.
Consequently, the Weyl-dipole phase in Y(OH)$_3$ features unique
two-dimensional (2D) sextuple-helicoid Fermi-arc states on the top and bottom
surfaces, protected by the Chern number, as well as one-dimensional (1D) hinge
states that connect the two Weyl dipoles along the side hinges, guaranteed by
the **quantize**d quadrupole moment. Our findings not only introduce a novel
higher-order topological phase, but also promote Y(OH)$_3$ as a promising
platform for exploring multi-dimensional boundaries and the interaction between
first-order and second-order topologies.


## Atten-Transformer A Deep Learning Framework for User App Usage Prediction

>Authors: Longlong Li, Cunquan Qu, Guanghui Wang

>2025-02-24

> http://arxiv.org/abs/2502.16957v1

Accurately predicting smartphone app usage patterns is crucial for user
experience optimization and targeted marketing. However, existing methods
struggle to capture intricate dependencies in user behavior, particularly in
**sparse** or complex usage scenarios. To address these challenges, we introduce
Atten-Transformer, a novel model that integrates temporal attention with a
Transformer network to dynamically identify and leverage key app usage
patterns. Unlike conventional methods that primarily consider app order and
duration, our approach employs a multi-dimensional feature representation,
incorporating both feature encoding and temporal encoding to enhance predictive
accuracy. The proposed attention mechanism effectively assigns importance to
critical app usage moments, improving both model interpretability and
generalization. Extensive experiments on multiple smartphone usage datasets,
including LSapp and Tsinghua App Usage datasets, demonstrate that
Atten-Transformer consistently outperforms state-of-the-art models across
different data splits. Specifically, our model achieves a 45.24\% improvement
in HR@1 on the Tsinghua dataset (Time-based Split) and a 18.25\% improvement in
HR@1 on the LSapp dataset (Cold Start Split), showcasing its robustness across
diverse app usage scenarios. These findings highlight the potential of
integrating adaptive attention mechanisms in mobile usage forecasting, paving
the way for enhanced user engagement and resource allocation.


## DBudgetKV Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance

>Authors: Xuanfan Ni, Liyan Xu, Chenyang Lyu, Longyue Wang, Mo Yu, Lemao Liu, Fandong Meng, Jie Zhou, Piji Li

>2025-02-24

> http://arxiv.org/abs/2502.16886v1

To alleviate memory burden during inference of large language models (LLMs),
numerous studies have focused on compressing the **KV** cache by exploring aspects
such as attention **sparsity**. However, these techniques often require a
pre-defined cache budget; as the optimal budget varies with different input
lengths and task types, it limits their practical deployment accepting
open-domain instructions. To address this limitation, we propose a new **KV** cache
compression objective: to always ensure the full-cache performance regardless
of specific inputs, while maximizing **KV** cache **pruning** as much as possible. To
achieve this goal, we introduce a novel **KV** cache compression method dubbed
DBudget**KV**, which features an attention-based metric to signal when the
remaining **KV** cache is unlikely to match the full-cache performance, then
halting the **pruning** process. Empirical evaluation spanning diverse context
lengths, task types, and model sizes suggests that our method achieves lossless
**KV** **pruning** effectively and robustly, exceeding 25% compression ratio on
average. Furthermore, our method is easy to integrate within LLM inference, not
only optimizing memory space, but also showing reduced inference time compared
to existing methods.


## CORAL Learning Consistent Representations across Multi-step Training with Lighter Speculative Drafter

>Authors: Yepeng Weng, Dianwen Mei, Huishi Qiu, Xujie Chen, Li Liu, Jiang Tian, Zhongchao Shi

>2025-02-24

> http://arxiv.org/abs/2502.16880v1

Speculative decoding is a powerful technique that accelerates Large Language
Model (LLM) inference by leveraging a lightweight speculative draft model.
However, existing designs suffers in performance due to misalignment between
training and inference. Recent methods have tried to solve this issue by
adopting a multi-step training strategy, but the complex inputs of different
training steps make it harder for the draft model to converge. To address this,
we propose CORAL, a novel framework that improves both accuracy and efficiency
in speculative drafting. CORAL introduces Cross-Step Representation Alignment,
a method that enhances consistency across multiple training steps,
significantly improving speculative drafting performance. Additionally, we
identify the LM head as a major bottleneck in the inference speed of the draft
model. We introduce a weight-grouping mechanism that selectively activates a
subset of LM head parameters during inference, substantially reducing the
latency of the draft model. We evaluate CORAL on three LLM families and three
benchmark datasets, achieving speedup ratios of 2.50x-4.07x, outperforming
state-of-the-art methods such as EAGLE-2 and HASS. Our results demonstrate that
CORAL effectively mitigates training-inference misalignment and delivers
significant speedup for modern LLMs with large vocabularies.


## APINT A Full-Stack Framework for Acceleration of Privacy-Preserving Inference of Transformers based on Garbled Circuits

>Authors: Hyunjun Cho, Jaeho Jeon, Jaehoon Heo, Joo-Young Kim

>2025-02-24

> http://arxiv.org/abs/2502.16877v1

As the importance of Privacy-Preserving Inference of Transformers (PiT)
increases, a hybrid protocol that integrates Garbled Circuits (GC) and
Homomorphic Encryption (HE) is emerging for its implementation. While this
protocol is preferred for its ability to maintain accuracy, it has a severe
drawback of excessive latency. To address this, existing protocols primarily
focused on reducing HE latency, thus making GC the new latency bottleneck.
Furthermore, previous studies only focused on individual computing layers, such
as protocol or hardware accelerator, lacking a comprehensive solution at the
system level. This paper presents APINT, a full-stack framework designed to
reduce PiT's overall latency by addressing the latency problem of GC through
both software and hardware solutions. APINT features a novel protocol that
reallocates possible GC workloads to alternative methods (i.e., HE or standard
matrix operation), substantially decreasing the GC workload. It also suggests
GC-friendly circuit generation that reduces the number of AND gates at the
most, which is the expensive operator in GC. Furthermore, APINT proposes an
innovative netlist scheduling that combines coarse-grained operation mapping
and fine-grained scheduling for maximal data reuse and minimal dependency.
Finally, APINT's hardware accelerator, combined with its compiler speculation,
effectively resolves the memory stall issue. Putting it all together, APINT
achieves a remarkable end-to-end reduction in latency, outperforming the
existing protocol on CPU platform by 12.2x online and 2.2x offline. Meanwhile,
the APINT accelerator not only reduces its latency by 3.3x but also saves
energy consumption by 4.6x while operating PiT compared to the state-of-the-art
GC accelerator.


## The Role of Sparsity for Length Generalization in Transformers

>Authors: Noah Golowich, Samy Jelassi, David Brandfonbrener, Sham M. Kakade, Eran Malach

>2025-02-24

> http://arxiv.org/abs/2502.16792v1

Training large language models to predict beyond their training context
lengths has drawn much attention in recent years, yet the principles driving
such behavior of length generalization remain underexplored. We propose a new
theoretical framework to study length generalization for the next-token
prediction task, as performed by decoder-only transformers. Conceptually, we
show that length generalization occurs as long as each predicted token depends
on a small (fixed) number of previous tokens. We formalize such tasks via a
notion we call $k$-**sparse** planted correlation distributions, and show that an
idealized model of transformers which generalize attention heads successfully
length-generalize on such tasks. As a bonus, our theoretical model justifies
certain techniques to modify positional embeddings which have been introduced
to improve length generalization, such as position coupling.
  We support our theoretical results with experiments on synthetic tasks and
natural language, which confirm that a key factor driving length generalization
is a ``**sparse**'' dependency structure of each token on the previous ones.
Inspired by our theory, we introduce Predictive Position Coupling, which trains
the transformer to predict the position IDs used in a positional coupling
approach. Predictive Position Coupling thereby allows us to broaden the array
of tasks to which position coupling can successfully be applied to achieve
length generalization.


## AlphaAgent LLM-Driven Alpha Mining with Regularized Exploration to Counteract Alpha Decay

>Authors: Ziyi Tang, Zechuan Chen, Jiarui Yang, Jiayao Mai, Yongsen Zheng, Keze Wang, Jinrui Chen, Liang Lin

>2025-02-24

> http://arxiv.org/abs/2502.16789v1

Alpha mining, a critical component in quantitative investment, focuses on
discovering predictive signals for future asset returns in increasingly complex
financial markets. However, the pervasive issue of alpha decay, where factors
lose their predictive power over time, poses a significant challenge for alpha
mining. Traditional methods like genetic programming face rapid alpha decay
from overfitting and complexity, while approaches driven by Large Language
Models (LLMs), despite their promise, often rely too heavily on existing
knowledge, creating homogeneous factors that worsen crowding and accelerate
decay. To address this challenge, we propose AlphaAgent, an autonomous
framework that effectively integrates LLM agents with ad hoc regularizations
for mining decay-resistant alpha factors. AlphaAgent employs three key
mechanisms: (i) originality enforcement through a similarity measure based on
abstract syntax trees (ASTs) against existing alphas, (ii) hypothesis-factor
alignment via LLM-evaluated semantic consistency between market hypotheses and
generated factors, and (iii) complexity control via AST-based structural
constraints, preventing over-engineered constructions that are prone to
overfitting. These mechanisms collectively guide the alpha generation process
to balance originality, financial rationale, and adaptability to evolving
market conditions, mitigating the risk of alpha decay. Extensive evaluations
show that AlphaAgent outperforms traditional and LLM-based methods in
mitigating alpha decay across bull and bear markets, consistently delivering
significant alpha in Chinese CSI 500 and US S&P 500 markets over the past four
years. Notably, AlphaAgent showcases remarkable resistance to alpha decay,
elevating the potential for yielding powerful factors.


## CipherPrune Efficient and Scalable Private Transformer Inference

>Authors: Yancheng Zhang, Jiaqi Xue, Mengxin Zheng, Mimi Xie, Mingzhe Zhang, Lei Jiang, Qian Lou

>2025-02-24

> http://arxiv.org/abs/2502.16782v1

Private Transformer inference using cryptographic protocols offers promising
solutions for privacy-preserving machine learning; however, it still faces
significant runtime overhead (efficiency issues) and challenges in handling
long-token inputs (scalability issues). We observe that the Transformer's
operational complexity scales quadratically with the number of input tokens,
making it essential to reduce the input token length. Notably, each token
varies in importance, and many inputs contain redundant tokens. Additionally,
prior private inference methods that rely on high-degree polynomial
approximations for non-linear activations are computationally expensive.
Therefore, reducing the polynomial degree for less important tokens can
significantly accelerate private inference. Building on these observations, we
propose \textit{CipherPrune}, an efficient and scalable private inference
framework that includes a secure encrypted token **pruning** protocol, a polynomial
reduction protocol, and corresponding Transformer network optimizations. At the
protocol level, encrypted token **pruning** adaptively removes unimportant tokens
from encrypted inputs in a progressive, layer-wise manner. Additionally,
encrypted polynomial reduction assigns lower-degree polynomials to less
important tokens after **pruning**, enhancing efficiency without decryption. At the
network level, we introduce protocol-aware network optimization via a
gradient-based search to maximize **pruning** thresholds and polynomial reduction
conditions while maintaining the desired accuracy. Our experiments demonstrate
that CipherPrune reduces the execution overhead of private Transformer
inference by approximately $6.1\times$ for 128-token inputs and $10.6\times$
for 512-token inputs, compared to previous methods, with only a marginal drop
in accuracy. The code is publicly available at
https://github.com/UCF-Lou-Lab-PET/cipher-prune-inference.


## Splitting Regularized Wasserstein Proximal Algorithms for Nonsmooth Sampling Problems

>Authors: Fuqun Han, Stanley Osher, Wuchen Li

>2025-02-24

> http://arxiv.org/abs/2502.16773v1

Sampling from nonsmooth target probability distributions is essential in
various applications, including the Bayesian Lasso. We propose a
splitting-based sampling algorithm for the time-implicit discretization of the
probability flow for the Fokker-Planck equation, where the score function
defined as the gradient logarithm of the current probability density function,
is approximated by the regularized Wasserstein proximal. When the prior
distribution is the Laplace prior, our algorithm is explicitly formulated as a
deterministic interacting particle system, incorporating softmax operators and
shrinkage operations to efficiently compute the gradient drift vector field and
the score function. The proposed formulation introduces a particular class of
attention layers in transformer structures, which can sample **sparse** target
distributions. We verify the convergence towards target distributions regarding
R\'enyi divergences under suitable conditions. Numerical experiments in
high-dimensional nonsmooth sampling problems, such as sampling from mixed
Gaussian and Laplace distributions, logistic regressions, image restoration
with L1-TV regularization, and Bayesian neural networks, demonstrate the
efficiency and robust performance of the proposed method.


## Layer-Wise Evolution of Representations in Fine-Tuned Transformers Insights from Sparse AutoEncoders

>Authors: Suneel Nadipalli

>2025-02-23

> http://arxiv.org/abs/2502.16722v1

Fine-tuning pre-trained transformers is a powerful technique for enhancing
the performance of base models on specific tasks. From early applications in
models like BERT to fine-tuning Large Language Models (LLMs), this approach has
been instrumental in adapting general-purpose architectures for specialized
downstream tasks. Understanding the fine-tuning process is crucial for
uncovering how transformers adapt to specific objectives, retain general
representations, and acquire task-specific features. This paper explores the
underlying mechanisms of fine-tuning, specifically in the BERT transformer, by
analyzing activation similarity, training Sparse AutoEncoders (SAEs), and
visualizing token-level activations across different layers. Based on
experiments conducted across multiple datasets and BERT layers, we observe a
steady progression in how features adapt to the task at hand: early layers
primarily retain general representations, middle layers act as a transition
between general and task-specific features, and later layers fully specialize
in task adaptation. These findings provide key insights into the inner workings
of fine-tuning and its impact on representation learning within transformer
architectures.


## A Matter-Wave Quantum Superposition of Inertial and Constant Acceleration Motions

>Authors: Vlatko Vedral

>2025-02-23

> http://arxiv.org/abs/2502.16716v1

We present three different methods of calculating the non-relativistic
dynamics of a quantum matter-wave evolving in a superposition of the inertial
and accelerated motions. The relative phase between the two, which is
classically unobservable as it is a gauge transformation, can be detected in a
matter-wave interference experiment. The first method is the most
straightforward and it represents the evolution as an exponential of the
Hamiltonian. Based on the Heisenberg picture, the second method is insightful
because it gives us extra insight into the independence of the wave-packet
spreading of the magnitude of **acceleration**. Also, it demonstrates that the
Heisenberg picture is perfectly suited to capturing all aspects of quantum
interference. The final method shows the consistency with the full relativistic
treatment and we use it to make a point regarding the equivalence principle.


## Rewards-based image analysis in microscopy

>Authors: Kamyar Barakati, Yu Liu, Utkarsh Pratiush, Boris N. Slautin, Sergei V. Kalinin

>2025-02-23

> http://arxiv.org/abs/2502.18522v1

Analyzing imaging and hyperspectral data is crucial across scientific fields,
including biology, medicine, chemistry, and physics. The primary goal is to
transform high-resolution or high-dimensional data into an interpretable format
to generate actionable insights, aiding decision-making and advancing
knowledge. Currently, this task relies on complex, human-designed workflows
comprising iterative steps such as denoising, spatial sampling, keypoint
detection, feature generation, clustering, dimensionality reduction, and
physics-based deconvolutions. The introduction of machine learning over the
past decade has accelerated tasks like image segmentation and object detection
via supervised learning, and dimensionality reduction via unsupervised methods.
However, both classical and NN-based approaches still require human input,
whether for hyperparameter tuning, data labeling, or both. The growing use of
automated imaging tools, from atomically resolved imaging to biological
applications, demands unsupervised methods that optimize data representation
for human decision-making or autonomous experimentation. Here, we discuss
advances in reward-based workflows, which adopt expert decision-making
principles and demonstrate strong transfer learning across diverse tasks. We
represent image analysis as a decision-making process over possible operations
and identify desiderata and their mappings to classical decision-making
frameworks. Reward-driven workflows enable a shift from supervised, black-box
models sensitive to distribution shifts to explainable, unsupervised, and
robust optimization in image analysis. They can function as wrappers over
classical and DCNN-based methods, making them applicable to both unsupervised
and supervised workflows (e.g., classification, regression for
structure-property mapping) across imaging and hyperspectral data.


## Are Sparse Autoencoders Useful? A Case Study in Sparse Probing

>Authors: Subhash Kantamneni, Joshua Engels, Senthooran Rajamanoharan, Max Tegmark, Neel Nanda

>2025-02-23

> http://arxiv.org/abs/2502.16681v1

Sparse autoencoders (SAEs) are a popular method for interpreting concepts
represented in large language model (LLM) activations. However, there is a lack
of evidence regarding the validity of their interpretations due to the lack of
a ground truth for the concepts used by an LLM, and a growing number of works
have presented problems with current SAEs. One alternative source of evidence
would be demonstrating that SAEs improve performance on downstream tasks beyond
existing baselines. We test this by applying SAEs to the real-world task of LLM
activation probing in four regimes: data scarcity, class imbalance, label
noise, and covariate shift. Due to the difficulty of detecting concepts in
these challenging settings, we hypothesize that SAEs' basis of interpretable,
concept-level latents should provide a useful inductive bias. However, although
SAEs occasionally perform better than baselines on individual datasets, we are
unable to design ensemble methods combining SAEs with baselines that
consistently outperform ensemble methods solely using baselines. Additionally,
although SAEs initially appear promising for identifying spurious correlations,
detecting poor dataset quality, and training multi-token probes, we are able to
achieve similar results with simple non-SAE baselines as well. Though we cannot
discount SAEs' utility on other tasks, our findings highlight the shortcomings
of current SAEs and the need to rigorously evaluate interpretability methods on
downstream tasks with strong baselines.


## Automatic Joint Structured Pruning and Quantization for Efficient Neural Network Training and Compression

>Authors: Xiaoyi Qu, David Aponte, Colby Banbury, Daniel P. Robinson, Tianyu Ding, Kazuhito Koishida, Ilya Zharkov, Tianyi Chen

>2025-02-23

> http://arxiv.org/abs/2502.16638v1

Structured **pruning** and **quantization** are fundamental techniques used to reduce
the size of deep neural networks (DNNs) and typically are applied
independently. Applying these techniques jointly via co-optimization has the
potential to produce smaller, high-quality models. However, existing joint
schemes are not widely used because of (1) engineering difficulties
(complicated multi-stage processes), (2) black-box optimization (extensive
hyperparameter tuning to control the overall compression), and (3) insufficient
architecture generalization. To address these limitations, we present the
framework GETA, which automatically and efficiently performs joint structured
**pruning** and **quantization**-aware training on any DNNs. GETA introduces three key
innovations: (i) a **quantization**-aware dependency graph (QADG) that constructs a
**pruning** search space for generic **quantization**-aware DNN, (ii) a partially
projected stochastic gradient method that guarantees layerwise bit constraints
are satisfied, and (iii) a new joint learning strategy that incorporates
interpretable relationships between **pruning** and **quantization**. We present
numerical experiments on both convolutional neural networks and transformer
architectures that show that our approach achieves competitive (often superior)
performance compared to existing joint **pruning** and **quantization** methods.


## Energy-Efficient Transformer Inference Optimization Strategies for Time Series Classification

>Authors: Arshia Kermani, Ehsan Zeraatkar, Habib Irani

>2025-02-23

> http://arxiv.org/abs/2502.16627v2

The increasing computational demands of transformer models in time series
classification necessitate effective optimization strategies for
energy-efficient deployment. This paper presents a systematic investigation of
optimization techniques, focusing on structured **pruning** and **quantization**
methods for transformer architectures. Through extensive experimentation on
three distinct datasets (RefrigerationDevices, ElectricDevices, and PLAID), we
quantitatively evaluate model performance and energy efficiency across
different transformer configurations. Our experimental results demonstrate that
static **quantization** reduces energy consumption by 29.14% while maintaining
classification performance, and L1 **pruning** achieves a 63% improvement in
inference speed with minimal accuracy degradation. These findings provide
valuable insights into the effectiveness of optimization strategies for
transformer-based time series classification, establishing a foundation for
efficient model deployment in resource-constrained environments.


## TerEffic Highly Efficient Ternary LLM Inference on FPGA

>Authors: Chenyang Yin, Zhenyu Bai, Pranav Venkatram, Shivam Aggarwal, Zhaoying Li, Tulika Mitra

>2025-02-23

> http://arxiv.org/abs/2502.16473v1

Large Language Model (LLM) deployment on edge devices is typically
constrained by the need for off-chip memory access, leading to high power
consumption and limited throughput. Ternary **quantization** for LLMs is promising
in maintaining model accuracy while reducing memory footprint. However,
existing accelerators have not exploited this potential for on-chip inference.
We present TerEffic, an FPGA-based accelerator that carefully co-designs memory
architecture and computational units to unlock highly efficient LLM inference
with fully on-chip execution. Through weight compression, custom computational
units, and memory hierarchy optimization, we achieve unprecedented efficiency
by eliminating off-chip memory bandwidth bottlenecks. We propose two
architectural variants: a fully on-chip design for smaller models and an
HBM-assisted design for larger ones. When evaluated on a 370M parameter model
with single-batch inference, our on-chip design achieves 12,700 tokens/sec (149
times higher than NVIDIA's Jetson Orin Nano) with a power efficiency of 467
tokens/sec/W (19 times better than Jetson Orin Nano). The HBM-assisted design
provides 521 tokens/sec on a 2.7B parameter model (2 times higher than NVIDIA's
A100) with 33W power consumption, achieving a power efficiency of 16
tokens/sec/W (8 times better than A100).


## Swallowing the Poison Pills Insights from Vulnerability Disparity Among LLMs

>Authors: Peng Yifeng, Wu Zhizheng, Chen Chen

>2025-02-23

> http://arxiv.org/abs/2502.18518v1

Modern large language models (LLMs) exhibit critical vulnerabilities to
poison pill attacks: localized data poisoning that alters specific factual
knowledge while preserving overall model utility. We systematically demonstrate
these attacks exploit inherent architectural properties of LLMs, achieving
54.6% increased retrieval inaccuracy on long-tail knowledge versus dominant
topics and up to 25.5% increase retrieval inaccuracy on compressed models
versus original architectures. Through controlled mutations (e.g.,
temporal/spatial/entity alterations) and, our method induces localized
memorization deterioration with negligible impact on models' performance on
regular standard benchmarks (e.g., <2% performance drop on MMLU/GPQA), leading
to potential detection evasion. Our findings suggest: (1) Disproportionate
vulnerability in long-tail knowledge may result from reduced parameter
redundancy; (2) Model compression may increase attack surfaces, with
pruned/distilled models requiring 30% fewer poison samples for equivalent
damage; (3) Associative memory enables both spread of collateral damage to
related concepts and amplification of damage from simultaneous attack,
particularly for dominant topics. These findings raise concerns over current
scaling paradigms since attack costs are lowering while defense complexity is
rising. Our work establishes poison pills as both a security threat and
diagnostic tool, revealing critical security-efficiency trade-offs in language
model compression that challenges prevailing safety assumptions.


## Towards Fully-Automated Materials Discovery via Large-Scale Synthesis Dataset and Expert-Level LLM-as-a-Judge

>Authors: Heegyu Kim, Taeyang Jeon, Seungtaek Choi, Jihoon Hong, Dongwon Jeon, Sungbum Cho, Ga-Yeon Baek, Kyung-Won Kwak, Dong-Hee Lee, Sun-Jin Choi, Jisu Bae, Chihoon Lee, Yunseo Kim, Jinsung Park, Hyunsouk Cho

>2025-02-23

> http://arxiv.org/abs/2502.16457v1

Materials synthesis is vital for innovations such as energy storage,
catalysis, electronics, and biomedical devices. Yet, the process relies heavily
on empirical, trial-and-error methods guided by expert intuition. Our work aims
to support the materials science community by providing a practical,
data-driven resource. We have curated a comprehensive dataset of 17K
expert-verified synthesis recipes from open-access literature, which forms the
basis of our newly developed benchmark, AlchemyBench. AlchemyBench offers an
end-to-end framework that supports research in large language models applied to
synthesis prediction. It encompasses key tasks, including raw materials and
equipment prediction, synthesis procedure generation, and characterization
outcome forecasting. We propose an LLM-as-a-Judge framework that leverages
large language models for automated evaluation, demonstrating strong
statistical agreement with expert assessments. Overall, our contributions offer
a supportive foundation for exploring the capabilities of LLMs in predicting
and guiding materials synthesis, ultimately paving the way for more efficient
experimental design and accelerated innovation in materials science.


## Advances in Continuous Variable Measurement-Device-Independent Quantum Key Distribution

>Authors: Pu Wang, Yan Tian, Yongmin Li

>2025-02-23

> http://arxiv.org/abs/2502.16448v1

Continuous variable quantum key distribution (CV-QKD), utilizes continuous
variables encoding such as the quadra-ture components of the **quantize**d
electromagnetic field and coherent detection decoding, offering good
compatibility with the existing telecommunications technology and components.
Continuous variable measurement-device-independent QKD (CV-MDI-QKD) can
eliminate all the security threats arising from the receiver effectively, the
crucial security loophole of CV-QKD implementations. Recently, CV-MDI-QKD has
attracted extensive attentions and witnessed rapid progress. Here, we review
the achievements that have been made in the field of CV-MDI-QKD, including the
basic principle, advancements in theoretical protocols and experimental
demonstrations. Finally, we discuss the challenges faced in practical
applications and future research directions.


## Compression Scaling LawsUnifying Sparsity and Quantization

>Authors: Elias Frantar, Utku Evci, Wonpyo Park, Neil Houlsby, Dan Alistarh

>2025-02-23

> http://arxiv.org/abs/2502.16440v1

We investigate how different compression techniques -- such as weight and
activation **quantization**, and weight **sparsity** -- affect the scaling behavior of
large language models (LLMs) during pretraining. Building on previous work
showing that weight **sparsity** acts as a constant multiplier on model size in
scaling laws, we demonstrate that this "effective parameter" scaling pattern
extends to **quantization** as well. Specifically, we establish that weight-only
**quantization** achieves strong parameter efficiency multipliers, while full
**quantization** of both weights and activations shows diminishing returns at lower
bitwidths. Our results suggest that different compression techniques can be
unified under a common scaling law framework, enabling principled comparison
and combination of these methods.


## Vision Transformer Accelerator ASIC for Real-Time, Low-Power Sleep Staging

>Authors: Tristan Robitaille, Xilin Liu

>2025-02-22

> http://arxiv.org/abs/2502.16334v1

This paper introduces a lightweight vision transformer aimed at automatic
sleep staging in a wearable device. The model is trained on the MASS SS3
dataset and achieves an accuracy of 82.9% on a 4-stage classification task with
only 31.6k weights. The model is implemented in hardware and synthesized in
65nm CMOS. The accelerator consumes 6.54mW of dynamic power and 11.0mW of
leakage power over 45.6ms. Using aggressive power gating while the accelerator
is idle, it is calculated that the effective power consumption is 0.56mW. The
accelerator uses only 0.754mm2 of silicon and has a clock frequency of 379MHz.
These metrics are possible thanks to a layer-dependent fixed-point format and
data width and a window average filter on the final softmax layer of the vision
transformer.


## SAE-V Interpreting Multimodal Models for Enhanced Alignment

>Authors: Hantao Lou, Changye Li, Jiaming Ji, Yaodong Yang

>2025-02-22

> http://arxiv.org/abs/2502.17514v1

With the integration of image modality, the semantic space of multimodal
large language models (MLLMs) is more complex than text-only models, making
their interpretability more challenging and their alignment less stable,
particularly susceptible to low-quality data, which can lead to inconsistencies
between modalities, hallucinations, and biased outputs. As a result, developing
interpretability methods for MLLMs is crucial for improving alignment quality
and efficiency. In text-only LLMs, Sparse Autoencoders (SAEs) have gained
attention for their ability to interpret latent representations. However,
extending SAEs to multimodal settings presents new challenges due to modality
fusion and the difficulty of isolating cross-modal representations. To address
these challenges, we introduce SAE-V, a mechanistic interpretability framework
that extends the SAE paradigm to MLLMs. By identifying and analyzing
interpretable features along with their corresponding data, SAE-V enables
fine-grained interpretation of both model behavior and data quality,
facilitating a deeper understanding of cross-modal interactions and alignment
dynamics. Moreover, by utilizing cross-modal feature weighting, SAE-V provides
an intrinsic data filtering mechanism to enhance model alignment without
requiring additional models. Specifically, when applied to the alignment
process of MLLMs, SAE-V-based data filtering methods could achieve more than
110% performance with less than 50% data. Our results highlight SAE-V's ability
to enhance interpretability and alignment in MLLMs, providing insights into
their internal mechanisms.


## Dynamic Parallel Tree Search for Efficient LLM Reasoning

>Authors: Yifu Ding, Wentao Jiang, Shunyu Liu, Yongcheng Jing, Jinyang Guo, Yingjie Wang, Jing Zhang, Zengmao Wang, Ziwei Liu, Bo Du, Xianglong Liu, Dacheng Tao

>2025-02-22

> http://arxiv.org/abs/2502.16235v2

Tree of Thoughts (ToT) enhances Large Language Model (LLM) reasoning by
structuring problem-solving as a spanning tree. However, recent methods focus
on search accuracy while overlooking computational efficiency. The challenges
of accelerating the ToT lie in the frequent switching of reasoning focus, and
the redundant exploration of suboptimal solutions. To alleviate this dilemma,
we propose Dynamic Parallel Tree Search (DPTS), a novel parallelism framework
that aims to dynamically optimize the reasoning path in inference. It includes
the Parallelism Streamline in the generation phase to build up a flexible and
adaptive parallelism with arbitrary paths by fine-grained cache management and
alignment. Meanwhile, the Search and Transition Mechanism filters potential
candidates to dynamically maintain the reasoning focus on more possible
solutions and have less redundancy. Experiments on Qwen-2.5 and Llama-3 with
Math500 and GSM8K datasets show that DPTS significantly improves efficiency by
2-4x on average while maintaining or even surpassing existing reasoning
algorithms in accuracy, making ToT-based reasoning more scalable and
computationally efficient.


## The Lovász number of random circulant graphs

>Authors: Afonso S. Bandeira, Jarosław Błasiok, Daniil Dmitriev, Ulysse Faure, Anastasia Kireeva, Dmitriy Kunisky

>2025-02-22

> http://arxiv.org/abs/2502.16227v1

This paper addresses the behavior of the Lov\'asz number for dense random
circulant graphs. The Lov\'asz number is a well-known semidefinite programming
upper bound on the independence number. Circulant graphs, an example of a
Cayley graph, are highly structured vertex-transitive graphs on integers modulo
$n$, where the connectivity of pairs of vertices depends only on the difference
between their labels. While for random circulant graphs the asymptotics of
fundamental quantities such as the clique and the chromatic number are
well-understood, characterizing the exact behavior of the Lov\'asz number
remains open. In this work, we provide upper and lower bounds on the expected
value of the Lov\'asz number and show that it scales as the square root of the
number of vertices, up to a log log factor. Our proof relies on a reduction of
the semidefinite program formulation of the Lov\'asz number to a linear program
with random objective and constraints via diagonalization of the adjacency
matrix of a circulant graph by the discrete Fourier transform (DFT). This leads
to a problem about controlling the norms of vectors with **sparse** Fourier
coefficients, which we study using results on the restricted isometry property
of subsampled DFT matrices.


## LLMKey LLM-Powered Wireless Key Generation Scheme for Next-Gen IoV Systems

>Authors: Huanqi Yang, Weitao Xu

>2025-02-22

> http://arxiv.org/abs/2502.16199v1

Wireless key generation holds significant promise for establishing
cryptographic keys in Next-Gen Internet of Vehicles (IoV) systems. However,
existing approaches often face inefficiencies and performance limitations
caused by frequent channel probing and ineffective **quantization**. To address
these challenges, this paper introduces LLMKey, a novel key generation system
designed to enhance efficiency and security. We identify excessive channel
probing and suboptimal **quantization** as critical bottlenecks in current methods.
To mitigate these issues, we propose an innovative large language model
(LLM)-based channel probing technique that leverages the capabilities of LLMs
to reduce probing rounds while preserving crucial channel information. Instead
of conventional **quantization**, LLMKey adopts a perturbed compressed
sensing-based key delivery mechanism, improving both robustness and security.
Extensive evaluations are conducted in four real-world scenarios, encompassing
V2I (Vehicle-to-Infrastructure) and V2V (Vehicle-to-Vehicle) settings in both
urban and rural environments. The results show that LLMKey achieves an average
key agreement rate of 98.78\%, highlighting its effectiveness and reliability
across diverse conditions.


## ZiGong 1.0 A Large Language Model for Financial Credit

>Authors: Yu Lei, Zixuan Wang, Chu Liu, Tongyao Wang

>2025-02-22

> http://arxiv.org/abs/2502.16159v1

Large Language Models (LLMs) have demonstrated strong performance across
various general Natural Language Processing (NLP) tasks. However, their
effectiveness in financial credit assessment applications remains suboptimal,
primarily due to the specialized financial expertise required for these tasks.
To address this limitation, we propose ZiGong, a Mistral-based model enhanced
through multi-task supervised fine-tuning. To specifically combat model
hallucination in financial contexts, we introduce a novel data **pruning**
methodology. Our approach utilizes a proxy model to score training samples,
subsequently combining filtered data with original datasets for model training.
This data refinement strategy effectively reduces hallucinations in LLMs while
maintaining reliability in downstream financial applications. Experimental
results show our method significantly enhances model robustness and prediction
accuracy in real-world financial scenarios.


## ZIA A Theoretical Framework for Zero-Input AI

>Authors: Aditi De, NeuroBits Labs

>2025-02-22

> http://arxiv.org/abs/2502.16124v1

Zero-Input AI (ZIA) introduces a novel framework for human-computer
interaction by enabling proactive intent prediction without explicit user
commands. It integrates gaze tracking, bio-signals (EEG, heart rate), and
contextual data (time, location, usage history) into a multi-modal model for
real-time inference, targeting <100 ms latency. The proposed architecture
employs a transformer-based model with cross-modal attention, variational
Bayesian inference for uncertainty estimation, and reinforcement learning for
adaptive optimization. To support deployment on edge devices (CPUs, TPUs,
NPUs), ZIA utilizes **quantization**, weight **pruning**, and linear attention to
reduce complexity from quadratic to linear with sequence length. Theoretical
analysis establishes an information-theoretic bound on prediction error and
demonstrates how multi-modal fusion improves accuracy over single-modal
approaches. Expected performance suggests 85-90% accuracy with EEG integration
and 60-100 ms inference latency. ZIA provides a scalable, privacy-preserving
framework for accessibility, healthcare, and consumer applications, advancing
AI toward anticipatory intelligence.


## Recurrent Knowledge Identification and Fusion for Language Model Continual Learning

>Authors: Yujie Feng, Xujia Wang, Zexin Lu, Shenghong Fu, Guangyuan Shi, Yongxin Xu, Yasha Wang, Philip S. Yu, Xu Chu, Xiao-Ming Wu

>2025-02-22

> http://arxiv.org/abs/2502.17510v1

Continual learning (CL) is crucial for deploying large language models (LLMs)
in dynamic real-world environments without costly retraining. While recent
model ensemble and model merging methods guided by parameter importance have
gained popularity, they often struggle to balance knowledge transfer and
forgetting, mainly due to the reliance on static importance estimates during
sequential training. In this paper, we present Recurrent-KIF, a novel CL
framework for Recurrent Knowledge Identification and Fusion, which enables
dynamic estimation of parameter importance distributions to enhance knowledge
transfer. Inspired by human continual learning, Recurrent-KIF employs an inner
loop that rapidly adapts to new tasks while identifying important parameters,
coupled with an outer loop that globally manages the fusion of new and
historical knowledge through redundant knowledge **pruning** and key knowledge
merging. These inner-outer loops iteratively perform multiple rounds of fusion,
allowing Recurrent-KIF to leverage intermediate training information and
adaptively adjust fusion strategies based on evolving importance distributions.
Extensive experiments on two CL benchmarks with various model sizes (from 770M
to 13B) demonstrate that Recurrent-KIF effectively mitigates catastrophic
forgetting and enhances knowledge transfer.


## Joint Similarity Item Exploration and Overlapped User Guidance for Multi-Modal Cross-Domain Recommendation

>Authors: Weiming Liu, Chaochao Chen, Jiahe Xu, Xinting Liao, Fan Wang, Xiaolin Zheng, Zhihui Fu, Ruiguang Pei, Jun Wang

>2025-02-22

> http://arxiv.org/abs/2502.16068v1

Cross-Domain Recommendation (CDR) has been widely investigated for solving
long-standing data **sparsity** problem via knowledge sharing across domains. In
this paper, we focus on the Multi-Modal Cross-Domain Recommendation (MMCDR)
problem where different items have multi-modal information while few users are
overlapped across domains. MMCDR is particularly challenging in two aspects:
fully exploiting diverse multi-modal information within each domain and
leveraging useful knowledge transfer across domains. However, previous methods
fail to cluster items with similar characteristics while filtering out inherit
noises within different modalities, hurdling the model performance. What is
worse, conventional CDR models primarily rely on overlapped users for domain
adaptation, making them ill-equipped to handle scenarios where the majority of
users are non-overlapped. To fill this gap, we propose Joint Similarity Item
Exploration and Overlapped User Guidance (SIEOUG) for solving the MMCDR
problem. SIEOUG first proposes similarity item exploration module, which not
only obtains pair-wise and group-wise item-item graph knowledge, but also
reduces irrelevant noise for multi-modal modeling. Then SIEOUG proposes
user-item collaborative filtering module to aggregate user/item embeddings with
the attention mechanism for collaborative filtering. Finally SIEOUG proposes
overlapped user guidance module with optimal user matching for knowledge
sharing across domains. Our empirical study on Amazon dataset with several
different tasks demonstrates that SIEOUG significantly outperforms the
state-of-the-art models under the MMCDR setting.


## RAG-Enhanced Collaborative LLM Agents for Drug Discovery

>Authors: Namkyeong Lee, Edward De Brouwer, Ehsan Hajiramezanali, Chanyoung Park, Gabriele Scalia

>2025-02-22

> http://arxiv.org/abs/2502.17506v1

Recent advances in large language models (LLMs) have shown great potential to
accelerate drug discovery. However, the specialized nature of biochemical data
often necessitates costly domain-specific fine-tuning, posing critical
challenges. First, it hinders the application of more flexible general-purpose
LLMs in cutting-edge drug discovery tasks. More importantly, it impedes the
rapid integration of the vast amounts of scientific data continuously generated
through experiments and research. To investigate these challenges, we propose
CLADD, a retrieval-augmented generation (RAG)-empowered agentic system tailored
to drug discovery tasks. Through the collaboration of multiple LLM agents,
CLADD dynamically retrieves information from biomedical knowledge bases,
contextualizes query molecules, and integrates relevant evidence to generate
responses -- all without the need for domain-specific fine-tuning. Crucially,
we tackle key obstacles in applying RAG workflows to biochemical data,
including data heterogeneity, ambiguity, and multi-source integration. We
demonstrate the flexibility and effectiveness of this framework across a
variety of drug discovery tasks, showing that it outperforms general-purpose
and domain-specific LLMs as well as traditional deep learning approaches.


## Comprehensive Analysis of Transparency and Accessibility of ChatGPT, DeepSeek, And other SoTA Large Language Models

>Authors: Ranjan Sapkota, Shaina Raza, Manoj Karkee

>2025-02-21

> http://arxiv.org/abs/2502.18505v1

Despite increasing discussions on open-source Artificial Intelligence (AI),
existing research lacks a discussion on the transparency and accessibility of
state-of-the-art (SoTA) Large Language Models (LLMs). The Open Source
Initiative (OSI) has recently released its first formal definition of
open-source software. This definition, when combined with standard dictionary
definitions and the **sparse** published literature, provide an initial framework
to support broader accessibility to AI models such as LLMs, but more work is
essential to capture the unique dynamics of openness in AI. In addition,
concerns about open-washing, where models claim openness but lack full
transparency, has been raised, which limits the reproducibility, bias
mitigation, and domain adaptation of these models. In this context, our study
critically analyzes SoTA LLMs from the last five years, including ChatGPT,
DeepSeek, LLaMA, and others, to assess their adherence to transparency
standards and the implications of partial openness. Specifically, we examine
transparency and accessibility from two perspectives: open-source vs.
open-weight models. Our findings reveal that while some models are labeled as
open-source, this does not necessarily mean they are fully open-sourced. Even
in the best cases, open-source models often do not report model training data,
and code as well as key metrics, such as weight accessibility, and carbon
emissions. To the best of our knowledge, this is the first study that
systematically examines the transparency and accessibility of over 100
different SoTA LLMs through the dual lens of open-source and open-weight
models. The findings open avenues for further research and call for responsible
and sustainable AI practices to ensure greater transparency, accountability,
and ethical deployment of these models.(DeepSeek transparency, ChatGPT
accessibility, open source, DeepSeek open source)


## KVLink Accelerating Large Language Models via Efficient KV Cache Reuse

>Authors: Jingbo Yang, Bairu Hou, Wei Wei, Yujia Bao, Shiyu Chang

>2025-02-21

> http://arxiv.org/abs/2502.16002v1

We describe **KV**Link, an approach for efficient key-value (**KV**) cache reuse in
large language models (LLMs). In many LLM applications, different inputs can
share overlapping context, such as the same retrieved document appearing in
multiple queries. However, the LLMs still need to encode the entire context for
each query, leading to redundant computation. In this paper, we propose a new
strategy to eliminate such inefficiency, where the **KV** cache of each document is
precomputed independently. During inference, the **KV** caches of retrieved
documents are concatenated, allowing the model to reuse cached representations
instead of recomputing them. To mitigate the performance degradation of LLMs
when using **KV** caches computed independently for each document, **KV**Link
introduces three key components: adjusting positional embeddings of the **KV**
cache at inference to match the global position after concatenation, using
trainable special tokens to restore self-attention across independently encoded
documents, and applying mixed-data fine-tuning to enhance performance while
preserving the model's original capabilities. Experiments across 7 datasets
demonstrate that **KV**Link improves question answering accuracy by an average of
4% over state-of-the-art methods. Furthermore, by leveraging precomputed **KV**
caches, our approach reduces time-to-first-token by up to 90% compared to
standard LLM inference, making it a scalable and efficient solution for context
reuse.


## Text-to-SQL Domain Adaptation via Human-LLM Collaborative Data Annotation

>Authors: Yuan Tian, Daniel Lee, Fei Wu, Tung Mai, Kun Qian, Siddhartha Sahai, Tianyi Zhang, Yunyao Li

>2025-02-21

> http://arxiv.org/abs/2502.15980v1

Text-to-SQL models, which parse natural language (NL) questions to executable
SQL queries, are increasingly adopted in real-world applications. However,
deploying such models in the real world often requires adapting them to the
highly specialized database schemas used in specific applications. We find that
existing text-to-SQL models experience significant performance drops when
applied to new schemas, primarily due to the lack of domain-specific data for
fine-tuning. This data scarcity also limits the ability to effectively evaluate
model performance in new domains. Continuously obtaining high-quality
text-to-SQL data for evolving schemas is prohibitively expensive in real-world
scenarios. To bridge this gap, we propose SQLsynth, a human-in-the-loop
text-to-SQL data annotation system. SQLsynth streamlines the creation of
high-quality text-to-SQL datasets through human-LLM collaboration in a
structured workflow. A within-subjects user study comparing SQLsynth with
manual annotation and ChatGPT shows that SQLsynth significantly accelerates
text-to-SQL data annotation, reduces cognitive load, and produces datasets that
are more accurate, natural, and diverse. Our code is available at
https://github.com/adobe/nl_sql_analyzer.


## Compression Barriers for Autoregressive Transformers

>Authors: Themistoklis Haris, Krzysztof Onak

>2025-02-21

> http://arxiv.org/abs/2502.15955v1

A key limitation of autoregressive Transformers is the large memory needed at
inference-time to cache all previous key-value (**KV**) embeddings. Prior works
address this by compressing the **KV** cache, but often assume specific structural
properties of the embeddings. This raises the following natural question: Can
truly sublinear space utilization be achieved without such assumptions? In this
work, we answer this question in the negative. Any algorithm for
attention-based token generation must use $\Theta(nd)$ space, where $n$ is the
number of tokens generated so far and $d = \Omega(\log n)$ is the dimension of
the **KV** embeddings. Our proof involves a reduction from a classic communication
complexity problem and uses a randomized construction that leverages properties
of projections in the spirit of the Johnson-Linderstrauss lemma. For the
low-dimensional regime $d = o(\log n)$, we show that any algorithm requires
$\Omega(d\cdot e^d)$ space and prove, using tight bounds on covering numbers,
that SubGen, proposed by Zandieh, Han, Mirrokni and Karbasi, matches this
bound. Further, we investigate how **sparsity** assumptions enable token generation
in truly sublinear space, presenting impossibility results and proposing a new
**KV** cache compression algorithm for sliding window attention when the value
cache outside the window is unmasked. Finally, we analyze token generation's
time complexity, using an indistinguishability argument to prove that no
non-adaptive algorithm can compute attention online in sublinear time for all
tokens.


## Modality-Aware Neuron Pruning for Unlearning in Multimodal Large Language Models

>Authors: Zheyuan Liu, Guangyao Dou, Xiangchi Yuan, Chunhui Zhang, Zhaoxuan Tan, Meng Jiang

>2025-02-21

> http://arxiv.org/abs/2502.15910v1

Generative models such as Large Language Models (LLMs) and Multimodal Large
Language Models (MLLMs) trained on massive datasets can lead them to memorize
and inadvertently reveal sensitive information, raising ethical and privacy
concerns. While some prior works have explored this issue in the context of
LLMs, it presents a unique challenge for MLLMs due to the entangled nature of
knowledge across modalities, making comprehensive unlearning more difficult. To
address this challenge, we propose Modality Aware Neuron Unlearning (MANU), a
novel unlearning framework for MLLMs designed to selectively clip neurons based
on their relative importance to the targeted forget data, curated for different
modalities. Specifically, MANU consists of two stages: important neuron
selection and selective **pruning**. The first stage identifies and collects the
most influential neurons across modalities relative to the targeted forget
knowledge, while the second stage is dedicated to **pruning** those selected
neurons. MANU effectively isolates and removes the neurons that contribute most
to the forget data within each modality, while preserving the integrity of
retained knowledge. Our experiments conducted across various MLLM architectures
illustrate that MANU can achieve a more balanced and comprehensive unlearning
in each modality without largely affecting the overall model utility.


## Probe Pruning Accelerating LLMs through Dynamic Pruning via Model-Probing

>Authors: Qi Le, Enmao Diao, Ziyan Wang, Xinran Wang, Jie Ding, Li Yang, Ali Anwar

>2025-02-21

> http://arxiv.org/abs/2502.15618v1

We introduce Probe Pruning (PP), a novel framework for online, dynamic,
structured **pruning** of Large Language Models (LLMs) applied in a batch-wise
manner. PP leverages the insight that not all samples and tokens contribute
equally to the model's output, and probing a small portion of each batch
effectively identifies crucial weights, enabling tailored dynamic **pruning** for
different batches. It comprises three main stages: probing, history-informed
**pruning**, and full inference. In the probing stage, PP selects a small yet
crucial set of hidden states, based on residual importance, to run a few model
layers ahead. During the history-informed **pruning** stage, PP strategically
integrates the probing states with historical states. Subsequently, it
structurally prunes weights based on the integrated states and the PP
importance score, a metric developed specifically to assess the importance of
each weight channel in maintaining performance. In the final stage, full
inference is conducted on the remaining weights. A major advantage of PP is its
compatibility with existing models, as it operates without requiring additional
neural network modules or fine-tuning. Comprehensive evaluations of PP on
LLaMA-2/3 and OPT models reveal that even minimal probing-using just 1.5% of
FLOPs-can substantially enhance the efficiency of structured **pruning** of LLMs.
For instance, when evaluated on LLaMA-2-7B with WikiText2, PP achieves a 2.56
times lower ratio of performance degradation per unit of runtime reduction
compared to the state-of-the-art method at a 40% **pruning** ratio. Our code is
available at https://github.com/Qi-Le1/Probe_Pruning.


## Interpreting and Steering LLMs with Mutual Information-based Explanations on Sparse Autoencoders

>Authors: Xuansheng Wu, Jiayi Yuan, Wenlin Yao, Xiaoming Zhai, Ninghao Liu

>2025-02-21

> http://arxiv.org/abs/2502.15576v1

Large language models (LLMs) excel at handling human queries, but they can
occasionally generate flawed or unexpected responses. Understanding their
internal states is crucial for understanding their successes, diagnosing their
failures, and refining their capabilities. Although **sparse** autoencoders (SAEs)
have shown promise for interpreting LLM internal representations, limited
research has explored how to better explain SAE features, i.e., understanding
the semantic meaning of features learned by SAE. Our theoretical analysis
reveals that existing explanation methods suffer from the frequency bias issue,
where they emphasize linguistic patterns over semantic concepts, while the
latter is more critical to steer LLM behaviors. To address this, we propose
using a fixed vocabulary set for feature interpretations and designing a mutual
information-based objective, aiming to better capture the semantic meaning
behind these features. We further propose two runtime steering strategies that
adjust the learned feature activations based on their corresponding
explanations. Empirical results show that, compared to baselines, our method
provides more discourse-level explanations and effectively steers LLM behaviors
to defend against jailbreak attacks. These findings highlight the value of
explanations for steering LLM behaviors in downstream applications. We will
release our code and data once accepted.


## DReSD Dense Retrieval for Speculative Decoding

>Authors: Milan Gritta, Huiyin Xue, Gerasimos Lampouras

>2025-02-21

> http://arxiv.org/abs/2502.15572v1

Speculative decoding (SD) accelerates Large Language Model (LLM) generation
by using an efficient draft model to propose the next few tokens, which are
verified by the LLM in a single forward call, reducing latency while preserving
its outputs. We focus on retrieval-based SD where the draft model retrieves the
next tokens from a non-parametric datastore. Sparse retrieval (REST), which
operates on the surface form of strings, is currently the dominant paradigm due
to its simplicity and scalability. However, its effectiveness is limited due to
the usage of short contexts and exact string matching. Instead, we introduce
Dense Retrieval for Speculative Decoding (DReSD), a novel framework that uses
approximate nearest neighbour search with contextualised token embeddings to
retrieve the most semantically relevant token sequences for SD. Extensive
experiments show that DReSD achieves (on average) 87% higher acceptance rates,
65% longer accepted tokens and 19% faster generation speeds compared to **sparse**
retrieval (REST).


## Blockchain innovation in promoting employment

>Authors: David Lee Kuo Chuen, Yang Li

>2025-02-21

> http://arxiv.org/abs/2502.15549v1

Blockchain technology, though conceptualized in the early 1990s, only gained
practical relevance with Bitcoin's launch in 2009. Recent advancements have
demonstrated its transformative potential, particularly in the digital art and
global payment sectors. Non-fungible tokens (NFTs) have redefined digital
ownership, while financial institutions use blockchain to enhance cross-border
transactions, reducing costs and settlement times. Using the
Diamond-Mortensen-Pissarides (DMP) model, this paper examines blockchain's
impact on labor markets by improving job-matching efficiency, thereby reducing
unemployment. However, high research costs and competition with incumbent
technologies hinder early-stage blockchain adoption. We extend the DMP model to
analyze the role of government intervention through tax and wage policies in
mitigating these barriers. Our findings suggest that lowering firm tax rates
can accelerate blockchain innovation, enhance labor market efficiency, and
promote employment growth, highlighting the critical balance between
technological progress and economic policy in fostering blockchain-driven
economic transformation.


## PIP-KAG Mitigating Knowledge Conflicts in Knowledge-Augmented Generation via Parametric Pruning

>Authors: Pengcheng Huang, Zhenghao Liu, Yukun Yan, Xiaoyuan Yi, Hao Chen, Zhiyuan Liu, Maosong Sun, Tong Xiao, Ge Yu, Chenyan Xiong

>2025-02-21

> http://arxiv.org/abs/2502.15543v1

Knowledge-Augmented Generation (KAG) has shown great promise in updating the
internal memory of Large Language Models (LLMs) by integrating external
knowledge. However, KAG inevitably faces knowledge conflicts when the internal
memory contradicts external information. Current approaches to mitigating these
conflicts mainly focus on improving external knowledge utilization. However,
these methods have shown only limited effectiveness in mitigating the knowledge
conflict problem, as internal knowledge continues to influence the generation
process of LLMs. In this paper, we propose a ParametrIc Pruning-based
Knowledge-Augmented Generation (PIP-KAG) approach, which prunes internal
knowledge of LLMs and incorporates a plug-and-play adaptation module to help
LLMs better leverage external sources. Additionally, we construct the
CoConflictQA benchmark based on the hallucination of LLMs to better evaluate
contextual faithfulness during answering questions. Experimental results on
CoConflictQA demonstrate that PIP-KAG significantly reduces knowledge conflicts
and improves context fidelity. Notably, PIP-KAG reduces LLM's parameters by
13%, enhancing parameter efficiency in LLMs within the KAG framework. All codes
are available at https://github.com/OpenBMB/PIP-KAG.


## Scaling Sparse and Dense Retrieval in Decoder-Only LLMs

>Authors: Hansi Zeng, Julian Killingback, Hamed Zamani

>2025-02-21

> http://arxiv.org/abs/2502.15526v1

Scaling large language models (LLMs) has shown great potential for improving
retrieval model performance; however, previous studies have mainly focused on
dense retrieval trained with contrastive loss (CL), neglecting the scaling
behavior of other retrieval paradigms and optimization techniques, such as
**sparse** retrieval and knowledge distillation (KD). In this work, we conduct a
systematic comparative study on how different retrieval paradigms (**sparse** vs.
dense) and fine-tuning objectives (CL vs. KD vs. their combination) affect
retrieval performance across different model scales. Using MSMARCO passages as
the training dataset, decoder-only LLMs (Llama-3 series: 1B, 3B, 8B), and a
fixed compute budget, we evaluate various training configurations on both
in-domain (MSMARCO, TREC DL) and out-of-domain (BEIR) benchmarks. Our key
findings reveal that: (1) Scaling behaviors emerge clearly only with CL, where
larger models achieve significant performance gains, whereas KD-trained models
show minimal improvement, performing similarly across the 1B, 3B, and 8B
scales. (2) Sparse retrieval models consistently outperform dense retrieval
across both in-domain (MSMARCO, TREC DL) and out-of-domain (BEIR) benchmarks,
and they demonstrate greater robustness to imperfect supervised signals. (3) We
successfully scale **sparse** retrieval models with the combination of CL and KD
losses at 8B scale, achieving state-of-the-art (SOTA) results in all evaluation
sets.


## Towards Swift Serverless LLM Cold Starts with ParaServe

>Authors: Chiheng Lou, Sheng Qi, Chao Jin, Dapeng Nie, Haoran Yang, Xuanzhe Liu, Xin Jin

>2025-02-21

> http://arxiv.org/abs/2502.15524v1

With the surge in number of large language models (LLMs), the industry turns
to serverless computing for LLM inference serving. However, serverless LLM
serving suffers from significant cold start latency and service level objective
(SLO) violations due to the substantial model size, which leads to prolonged
model fetching time from remote storage. We present ParaServe, a serverless LLM
serving system that minimizes cold start latency through the novel use of
pipeline parallelism. Our insight is that by distributing model parameters
across multiple GPU servers, we can utilize their aggregated network bandwidth
to concurrently fetch different parts of the model. ParaServe adopts a
two-level hierarchical design. At the cluster level, ParaServe determines the
optimal degree of parallelism based on user SLOs and carefully places GPU
workers across servers to reduce network interference. At the worker level,
ParaServe overlaps model fetching, loading, and runtime initialization to
further accelerate cold starts. Additionally, ParaServe introduces pipeline
consolidation, which merges parallel groups back to individual workers to
maintain optimal performance for warm requests. Our comprehensive evaluations
under diverse settings demonstrate that ParaServe reduces the cold start
latency by up to 4.7x and improves SLO attainment by up to 1.74x compared to
baselines.


## Verification and Validation for Trustworthy Scientific Machine Learning

>Authors: John D. Jakeman, Lorena A. Barba, Joaquim R. R. A. Martins, Thomas O'Leary-Roseberry

>2025-02-21

> http://arxiv.org/abs/2502.15496v1

Scientific machine learning (SciML) models are transforming many scientific
disciplines. However, the development of good modeling practices to increase
the trustworthiness of SciML has lagged behind its application, limiting its
potential impact. The goal of this paper is to start a discussion on
establishing consensus-based good practices for predictive SciML. We identify
key challenges in applying existing computational science and engineering
guidelines, such as verification and validation protocols, and provide
recommendations to address these challenges. Our discussion focuses on
predictive SciML, which uses machine learning models to learn, improve, and
accelerate numerical simulations of physical systems. While centered on
predictive applications, our 16 recommendations aim to help researchers conduc


## Q-PETR Quant-aware Position Embedding Transformation for Multi-View 3D Object Detection

>Authors: Jiangyong Yu, Changyong Shu, Dawei Yang, Zichen Yu, Xing Hu, Yan Chen

>2025-02-21

> http://arxiv.org/abs/2502.15488v1

PETR-based methods have dominated benchmarks in 3D perception and are
increasingly becoming a key component in modern autonomous driving systems.
However, their **quantization** performance significantly degrades when INT8
inference is required, with a degradation of 58.2% in mAP and 36.9% in NDS on
the NuScenes dataset. To address this issue, we propose a **quantization**-aware
position embedding transformation for multi-view 3D object detection, termed
Q-PETR. Q-PETR offers a **quantization**friendly and deployment-friendly
architecture while preserving the original performance of PETR. It
substantially narrows the accuracy gap between INT8 and FP32 inference for
PETR-series methods. Without bells and whistles, our approach reduces the mAP
and NDS drop to within 1% under standard 8-bit per-tensor post-training
**quantization**. Furthermore, our method exceeds the performance of the original
PETR in terms of floating-point precision. Extensive experiments across a
variety of PETR-series models demonstrate its broad generalization.


## PAPI Exploiting Dynamic Parallelism in Large Language Model Decoding with a Processing-In-Memory-Enabled Computing System

>Authors: Yintao He, Haiyu Mao, Christina Giannoula, Mohammad Sadrosadati, Juan Gómez-Luna, Huawei Li, Xiaowei Li, Ying Wang, Onur Mutlu

>2025-02-21

> http://arxiv.org/abs/2502.15470v2

Large language models (LLMs) are widely used for natural language
understanding and text generation. An LLM model relies on a time-consuming step
called LLM decoding to generate output tokens. Several prior works focus on
improving the performance of LLM decoding using parallelism techniques, such as
batching and speculative decoding. State-of-the-art LLM decoding has both
compute-bound and memory-bound kernels. Some prior works statically identify
and map these different kernels to a heterogeneous architecture consisting of
both processing-in-memory (PIM) units and computation-centric accelerators. We
observe that characteristics of LLM decoding kernels (e.g., whether or not a
kernel is memory-bound) can change dynamically due to parameter changes to meet
user and/or system demands, making (1) static kernel mapping to PIM units and
computation-centric accelerators suboptimal, and (2) one-size-fits-all approach
of designing PIM units inefficient due to a large degree of heterogeneity even
in memory-bound kernels.
  In this paper, we aim to accelerate LLM decoding while considering the
dynamically changing characteristics of the kernels involved. We propose PAPI
(PArallel Decoding with PIM), a PIM-enabled heterogeneous architecture that
exploits dynamic scheduling of compute-bound or memory-bound kernels to
suitable hardware units. PAPI has two key mechanisms: (1) online kernel
characterization to dynamically schedule kernels to the most suitable hardware
units at runtime and (2) a PIM-enabled heterogeneous computing system that
harmoniously orchestrates both computation-centric processing units and hybrid
PIM units with different computing capabilities. Our experimental results on
three broadly-used LLMs show that PAPI achieves 1.8$\times$ and 11.1$\times$
speedups over a state-of-the-art heterogeneous LLM accelerator and a
state-of-the-art PIM-only LLM accelerator, respectively.


## When Compression Meets Model Compression Memory-Efficient Double Compression for Large Language Models

>Authors: Weilan Wang, Yu Mao, Dongdong Tang, Hongchao Du, Nan Guan, Chun Jason Xue

>2025-02-21

> http://arxiv.org/abs/2502.15443v1

Large language models (LLMs) exhibit excellent performance in various tasks.
However, the memory requirements of LLMs present a great challenge when
deploying on memory-limited devices, even for **quantize**d LLMs. This paper
introduces a framework to compress LLM after **quantization** further, achieving
about 2.2x compression ratio. A compression-aware **quantization** is first
proposed to enhance model weight compressibility by re-scaling the model
parameters before **quantization**, followed by a **pruning** method to improve
further. Upon this, we notice that decompression can be a bottleneck during
practical scenarios. We then give a detailed analysis of the trade-off between
memory usage and latency brought by the proposed method. A speed-adaptive
method is proposed to overcome it. The experimental results show inference with
the compressed model can achieve a 40% reduction in memory size with negligible
loss in accuracy and inference speed.


## CoKV Optimizing KV Cache Allocation via Cooperative Game

>Authors: Qiheng Sun, Hongwei Zhang, Haocheng Xia, Jiayao Zhang, Jinfei Liu, Kui Ren

>2025-02-21

> http://arxiv.org/abs/2502.17501v1

Large language models (LLMs) have achieved remarkable success on various
aspects of human life. However, one of the major challenges in deploying these
models is the substantial memory consumption required to store key-value pairs
(**KV**), which imposes significant resource demands. Recent research has focused
on **KV** cache budget allocation, with several approaches proposing head-level
budget distribution by evaluating the importance of individual attention heads.
These methods, however, assess the importance of heads independently,
overlooking their cooperative contributions within the model, which may result
in a deviation from their true impact on model performance. In light of this
limitation, we propose Co**KV**, a novel method that models the cooperation between
heads in model inference as a cooperative game. By evaluating the contribution
of each head within the cooperative game, Co**KV** can allocate the cache budget
more effectively. Extensive experiments show that Co**KV** achieves
state-of-the-art performance on the LongBench benchmark using
LLama-3-8B-Instruct and Mistral-7B models.


## Tight Clusters Make Specialized Experts

>Authors: Stefan K. Nielsen, Rachel S. Y. Teo, Laziz U. Abdullaev, Tan M. Nguyen

>2025-02-21

> http://arxiv.org/abs/2502.15315v1

Sparse Mixture-of-Experts (MoE) architectures have emerged as a promising
approach to decoupling model capacity from computational cost. At the core of
the MoE model is the router, which learns the underlying clustering structure
of the input distribution in order to send input tokens to appropriate experts.
However, latent clusters may be unidentifiable in high dimension, which causes
slow convergence, susceptibility to data contamination, and overall degraded
representations as the router is unable to perform appropriate token-expert
matching. We examine the router through the lens of clustering optimization and
derive optimal feature weights that maximally identify the latent clusters. We
use these weights to compute the token-expert routing assignments in an
adaptively transformed space that promotes well-separated clusters, which helps
identify the best-matched expert for each token. In particular, for each expert
cluster, we compute a set of weights that scales features according to whether
that expert clusters tightly along that feature. We term this novel router the
Adaptive Clustering (AC) router. Our AC router enables the MoE model to obtain
three connected benefits: 1) faster convergence, 2) better robustness to data
corruption, and 3) overall performance improvement, as experts are specialized
in semantically distinct regions of the input space. We empirically demonstrate
the advantages of our AC router over baseline routing methods when applied on a
variety of MoE backbones for language modeling and image recognition tasks in
both clean and corrupted settings.


## SVDq 1.25-bit and 410x Key Cache Compression for LLM Attention

>Authors: Hong Yankun, Li Xing, Zhen Hui-Ling, Yu Xianzhi, Liu Wulong, Yuan Mingxuan

>2025-02-21

> http://arxiv.org/abs/2502.15304v1

For the efficient inference of Large Language Models (LLMs), the effective
compression of key-value (**KV**) cache is essential. Three main types of **KV** cache
compression techniques, namely **sparsity**, channel compression, and **quantization**,
have been identified. This study presents SVDq, a Singular Value Decomposition
(SVD) - based mixed precision **quantization** method for K cache. Initially, K
cache is transformed into latent channels using SVD basis representations.
Since the values in latent channels decay rapidly and become negligible after
only a few latent channels, our method then incorporates importance-aware
**quantization** and compression for latent channels. This enables the effective
allocation of higher precision to more significant channels. Theoretically, we
prove that SVDq results in **quantization** errors (x0.1 or even lower) that are
much lower than those of per-channel key **quantization** in the original space.
Our findings based on RULER and LongBench benchmarks demonstrate that SVDq can
achieve an equivalent key cache precision as low as 1.25-bit. When combined
with key **sparsity**, it can reach a key compression ratio of up to 410x for
attention computation, all while maintaining comparable model performance.
Notably, our method is nearly lossless for LongBench datasets. This indicates
that SVDq enables high-precision **low-bit** **quantization**, providing a more
efficient solution for **KV** cache compression in LLMs.


## Round Attention A Novel Round-Level Attention Mechanism to Accelerate LLM Inference

>Authors: Yaohua Tang, Zhicheng Hu, Kun Cheng, Fan Mo, Qiheng Lv, Hua Wang, Zhi Chen

>2025-02-21

> http://arxiv.org/abs/2502.15294v2

The increasing context window size in large language models (LLMs) has
improved their ability to handle complex, long-text tasks. However, as the
conversation rounds continue, it is required to store a large amount of **KV**
cache in GPU memory, which significantly affects the efficiency and even
availability of the model serving systems. This paper analyzes dialogue data
from real users and discovers that the LLM inference manifests a watershed
layer, after which the distribution of round-level attention shows notable
similarity. We propose Round Attention, a novel round-level attention mechanism
that only recalls and computes the **KV** cache of the most relevant rounds. The
experiments show that our method saves 55\% memory usage without compromising
model performance.


## Soybean pod and seed counting in both outdoor fields and indoor laboratories using unions of deep neural networks

>Authors: Tianyou Jiang, Mingshun Shao, Tianyi Zhang, Xiaoyu Liu, Qun Yu

>2025-02-21

> http://arxiv.org/abs/2502.15286v1

Automatic counting soybean pods and seeds in outdoor fields allows for rapid
yield estimation before harvesting, while indoor laboratory counting offers
greater accuracy. Both methods can significantly accelerate the breeding
process. However, it remains challenging for accurately counting pods and seeds
in outdoor fields, and there are still no accurate enough tools for counting
pods and seeds in laboratories. In this study, we developed efficient deep
learning models for counting soybean pods and seeds in both outdoor fields and
indoor laboratories. For outdoor fields, annotating not only visible seeds but
also occluded seeds makes YOLO have the ability to estimate the number of
soybean seeds that are occluded. Moreover, we enhanced YOLO architecture by
integrating it with HQ-SAM (YOLO-SAM), and domain adaptation techniques
(YOLO-DA), to improve model robustness and generalization across soybean images
taken in outdoor fields. Testing on soybean images from the outdoor field, we
achieved a mean absolute error (MAE) of 6.13 for pod counting and 10.05 for
seed counting. For the indoor setting, we utilized Mask-RCNN supplemented with
a Swin Transformer module (Mask-RCNN-Swin), models were trained exclusively on
synthetic training images generated from a small set of labeled data. This
approach resulted in near-perfect accuracy, with an MAE of 1.07 for pod
counting and 1.33 for seed counting across actual laboratory images from two
distinct studies.


## PPC-GPT Federated Task-Specific Compression of Large Language Models via Pruning and Chain-of-Thought Distillation

>Authors: Tao Fan, Guoqiang Ma, Yuanfeng Song, Lixin Fan, Kai Chen, Qiang Yang

>2025-02-21

> http://arxiv.org/abs/2502.15857v1

Compressing Large Language Models (LLMs) into task-specific Small Language
Models (SLMs) encounters two significant challenges: safeguarding
domain-specific knowledge privacy and managing limited resources. To tackle
these challenges, we propose PPC-GPT, a innovative privacy-preserving federated
framework specifically designed for compressing LLMs into task-specific SLMs
via **pruning** and Chain-of-Thought (COT) distillation. PPC-GPT works on a
server-client federated architecture, where the client sends differentially
private (DP) perturbed task-specific data to the server's LLM. The LLM then
generates synthetic data along with their corresponding rationales. This
synthetic data is subsequently used for both LLM **pruning** and retraining
processes. Additionally, we harness COT knowledge distillation, leveraging the
synthetic data to further improve the retraining of structurally-pruned SLMs.
Our experimental results demonstrate the effectiveness of PPC-GPT across
various text generation tasks. By compressing LLMs into task-specific SLMs,
PPC-GPT not only achieves competitive performance but also prioritizes data
privacy protection.


## LightMamba Efficient Mamba Acceleration on FPGA with Quantization and Hardware Co-design

>Authors: Renjie Wei, Songqiang Xu, Linfeng Zhong, Zebin Yang, Qingyu Guo, Yuan Wang, Runsheng Wang, Meng Li

>2025-02-21

> http://arxiv.org/abs/2502.15260v1

State space models (SSMs) like Mamba have recently attracted much attention.
Compared to Transformer-based large language models (LLMs), Mamba achieves
linear computation complexity with the sequence length and demonstrates
superior performance. However, Mamba is hard to accelerate due to the scattered
activation outliers and the complex computation dependency, rendering existing
LLM accelerators inefficient. In this paper, we propose LightMamba that
co-designs the **quantization** algorithm and FPGA accelerator architecture for
efficient Mamba inference. We first propose an FPGA-friendly post-training
**quantization** algorithm that features rotation-assisted **quantization** and
power-of-two SSM **quantization** to reduce the majority of computation to 4-bit.
We further design an FPGA accelerator that partially unrolls the Mamba
computation to balance the efficiency and hardware costs. Through computation
reordering as well as fine-grained tiling and fusion, the hardware utilization
and memory efficiency of the accelerator get drastically improved. We implement
LightMamba on Xilinx Versal VCK190 FPGA and achieve 4.65x to 6.06x higher
energy efficiency over the GPU baseline. When evaluated on Alveo U280 FPGA,
LightMamba reaches 93 tokens/s, which is 1.43x that of the GPU baseline.


## Interleaved Block-based Learned Image Compression with Feature Enhancement and Quantization Error Compensation

>Authors: Shiqi Jiang, Hui Yuan, Shuai Li, Raouf Hamzaoui, Xu Wang, Junyan Huo

>2025-02-21

> http://arxiv.org/abs/2502.15188v1

In recent years, learned image compression (LIC) methods have achieved
significant performance improvements. However, obtaining a more compact latent
representation and reducing the impact of **quantization** errors remain key
challenges in the field of LIC. To address these challenges, we propose a
feature extraction module, a feature refinement module, and a feature
enhancement module. Our feature extraction module shuffles the pixels in the
image, splits the resulting image into sub-images, and extracts coarse features
from the sub-images. Our feature refinement module stacks the coarse features
and uses an attention refinement block composed of concatenated
three-dimensional convolution residual blocks to learn more compact latent
features by exploiting correlations across channels, within sub-images
(intra-sub-image correlations), and across sub-images (inter-sub-image
correlations). Our feature enhancement module reduces information loss in the
decoded features following **quantization**. We also propose a **quantization** error
compensation module that mitigates the **quantization** mismatch between training
and testing. Our four modules can be readily integrated into state-of-the-art
LIC methods. Experiments show that combining our modules with Tiny-LIC
outperforms existing LIC methods and image compression standards in terms of
peak signal-to-noise ratio (PSNR) and multi-scale structural similarity
(MS-SSIM) on the Kodak dataset and the CLIC dataset.


## CoT-ICL Lab A Petri Dish for Studying Chain-of-Thought Learning from In-Context Demonstrations

>Authors: Vignesh Kothapalli, Hamed Firooz, Maziar Sanjabi

>2025-02-21

> http://arxiv.org/abs/2502.15132v2

We introduce CoT-ICL Lab, a framework and methodology to generate synthetic
tokenized datasets and systematically study chain-of-thought (CoT) in-context
learning (ICL) in language models. CoT-ICL Lab allows fine grained control over
the complexity of in-context examples by decoupling (1) the causal structure
involved in chain token generation from (2) the underlying token processing
functions. We train decoder-only transformers (up to 700M parameters) on these
datasets and show that CoT accelerates the accuracy transition to higher values
across model sizes. In particular, we find that model depth is crucial for
leveraging CoT with limited in-context examples, while more examples help
shallow models match deeper model performance. Additionally, limiting the
diversity of token processing functions throughout training improves causal
structure learning via ICL. We also interpret these transitions by analyzing
transformer embeddings and attention maps. Overall, CoT-ICL Lab serves as a
simple yet powerful testbed for theoretical and empirical insights into ICL and
CoT in language models.

