<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="description" content="None" />
      <link rel="shortcut icon" href="img/favicon.ico" />
    <title>Efficient Paper</title>
    <link rel="stylesheet" href="css/theme.css" />
    <link rel="stylesheet" href="css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="css/extra.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Home";
        var mkdocs_page_input_path = "index.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="." class="icon icon-home"> Efficient Paper
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="./search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="#">Home</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#table-of-contents">Table of Contents</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#getting-started">Getting Started</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#paper-list">Paper List</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#year">year</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#2025">2025</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#2024">2024</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#2023">2023</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#2022">2022</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#2021">2021</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#2020">2020</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#2019">2019</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#2018">2018</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#2017">2017</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#2016">2016</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#1993">1993</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#1989">1989</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#references">References</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#hot">Hot</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cold">Cold</a>
    </li>
        </ul>
    </li>
    </ul>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Paper List</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="cls_year/">By Year</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="cls_keyword/">By Keyword</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="cls_publication/">By Publication</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="cls_institution/">By Institution</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="cls_author/">By Author</a>
                  </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="weekly_paper/latest/">Weekly Paper</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href=".">Efficient Paper</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Home</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="efficientpaper">EfficientPaper</h1>
<p>Pruning, Quantization and efficient-inference/training paper list.</p>
<h2 id="table-of-contents">Table of Contents</h2>
<ul>
<li><a href="#efficientpaper">EfficientPaper</a></li>
<li><a href="#getting-started">Getting Started</a></li>
<li><a href="#paper-list">Paper List</a><ul>
<li><a href="cls_keyword/">keyword</a></li>
<li><a href="cls_year/">year</a></li>
<li><a href="cls_publication/">publication</a></li>
<li><a href="cls_institution/">institution</a></li>
<li><a href="cls_author/">author</a></li>
</ul>
</li>
<li><a href="#references">References</a></li>
</ul>
<h2 id="getting-started">Getting Started</h2>
<pre><code class="language-bash">git clone https://github.com/hustzxd/EfficientPaper
pip install protobuf==5.27.2 pandas arxiv 
</code></pre>
<ol>
<li>Add paper information by <code>./add_paper_info.sh</code></li>
<li>Run <code>./refresh_readme.sh</code></li>
</ol>
<details><summary><b>efficient_paper.prototxt</b></summary> 
<p>


<pre><code>paper {
  title: &quot;EfficientPaper: manage your research papers in an efficient way.&quot;
  abbr: &quot;EfficientPaper&quot;
  url: &quot;https://github.com/hustzxd/EfficientPaper&quot;
  authors: &quot;hustzxd&quot;
}
pub {
  where: &quot;GitHub&quot;
  year: 2023
}
code {
  type: &quot;Pytorch&quot;
  url: &quot;https://github.com/hustzxd/EfficientPaper&quot;
}
note {
  url: &quot;EfficientPaper.md&quot;
}
keyword {
  words: efficient_paper
}
</code></pre>


</p>
</details>

<p align="center">
<img src="notes//conference_timeline.png" width="800" title="blank">
</p>

<h2 id="paper-list">Paper List</h2>
<h3 id="year">year</h3>
<h4 id="2025">2025</h4>
<ol>
<li><a href="http://arxiv.org/abs/2501.02336v1">AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-AAAI-FF4500" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ASISys/AdaSkip" /> </li>
<li><a href="http://arxiv.org/abs/2407.20584v3">Pruning Large Language Models with Semi-Structural Adaptive Sparse Training</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-AAAI-FF4500" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/thu-ml/Adaptive-Sparse-Trainer" /> </li>
<li><a href="http://arxiv.org/abs/2502.11089v1">Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-ACL-4169E1" />]  </li>
<li><a href="http://arxiv.org/abs/2410.12168v1">COMET: Towards Partical W4A4KV4 LLMs Serving</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-ASPLOS-9370DB" />]  </li>
<li><a href="http://arxiv.org/abs/2410.18038v2">POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-ASPLOS-9370DB" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/vattention" /> </li>
<li><a href="http://arxiv.org/abs/2405.04437v3">vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-ASPLOS-9370DB" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/vattention" /> </li>
<li><a href="http://arxiv.org/abs/2507.08771v1">BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-COLM-6495ED" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/thunlp/BlockFFN" /> </li>
<li><a href="http://arxiv.org/abs/2508.04257v1">KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-COLM-6495ED" />]  </li>
<li><a href="http://arxiv.org/abs/2408.10473v1">Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-Coling-1E90FF" />]  </li>
<li><a href="http://arxiv.org/abs/2502.20766v1">FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-ICLR-FF6B6B" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/bytedance/FlexPrefill" /> </li>
<li><a href="http://arxiv.org/abs/2503.02130v2">Forgetting Transformer: Softmax Attention with a Forget Gate</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-ICLR-FF6B6B" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zhixuan-lin/forgetting-transformer" /> </li>
<li><a href="http://arxiv.org/abs/2504.19449v1">R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-ICLR-FF6B6B" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/VITA-Group/R-Sparse" /> </li>
<li><a href="http://arxiv.org/abs/2407.15176v3">ReAttention: Training-Free Infinite Context with Finite Attention Scope</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-ICLR-FF6B6B" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/OpenMOSS/ReAttention" /> </li>
<li><a href="http://arxiv.org/abs/2410.20672v3">Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-ICLR-FF6B6B" />]  </li>
<li><a href="http://arxiv.org/abs/2408.14690v1">Training-Free Activation Sparsity in Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-ICLR-FF6B6B" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/FasterDecoding/TEAL" /> </li>
<li><a href="http://arxiv.org/abs/2502.12082">AdaSplash: Adaptive Sparse Flash Attention</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/deep-spin/adasplash" /> </li>
<li><a href="https://openreview.net/forum?id=YrCvW1Hx7g">BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" />]  </li>
<li><a href="https://openreview.net/forum?id=u7dlwgKstN">CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" />]  </li>
<li><a href="http://arxiv.org/abs/2412.02252v1">Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" />]  </li>
<li><a href="https://openreview.net/forum?id=Em2oaXd8Dc">HashAttention: Semantic Sparsity for Faster Inference</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/xAlg-ai/HashAttention-1.0" /> </li>
<li><a href="http://arxiv.org/abs/2507.01299v1">La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" />]  </li>
<li><a href="https://openreview.net/forum?id=me6PfbATWM">MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" />]  </li>
<li><a href="https://openreview.net/forum?id=oa7MYAO6h6">ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/bytedance/ShadowKV" /> </li>
<li><a href="http://arxiv.org/abs/2505.22689v1">SlimLLM: Accurate Structured Pruning for Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" />]  </li>
<li><a href="https://openreview.net/forum?id=74c3Wwk8Tc">SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/thu-ml/SpargeAttn" /> </li>
<li><a href="https://openreview.net/forum?id=SBUc5wirM8">Sparsing Law: Towards Large Language Models with Greater Activation Sparsity</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/thunlp/SparsingLaw" /> </li>
<li><a href="https://openreview.net/forum?id=QY7Au9nZwp">Star Attention: Efficient LLM Inference over Long Sequences</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/NVIDIA/Star-Attention" /> </li>
<li><a href="http://arxiv.org/abs/2503.16428v1">XAttention: Block Sparse Attention with Antidiagonal Scoring</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mit-han-lab/x-attention" /> </li>
<li><a href="http://arxiv.org/abs/2507.16099v1">TorchAO: PyTorch-Native Training-to-Serving Model Optimization</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-ICML_Workshop-green" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/pytorch/ao" /> </li>
<li><a href="https://dl.acm.org/doi/10.1145/3695053.3731064">AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-ISCA-9932CC" />]  </li>
<li><a href="http://arxiv.org/abs/2504.08850v1">SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-ISCA-9932CC" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/infinigence/SpecEE" /> </li>
<li><a href="http://arxiv.org/abs/2508.01261v1">Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-KDD_Workshop-green" />]  </li>
<li><a href="http://arxiv.org/abs/2505.24680v1">A Simple Linear Patch Revives Layer-Pruned Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2501.09251v1">Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2504.06319v1">Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2505.19578v1">Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2504.06949v1">Adaptive Computation Pruning for the Forgetting Transformer</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zhixuan-lin/arctic-fox" /> </li>
<li><a href="http://arxiv.org/abs/2503.23798v1">Adaptive Layer-skipping in Pre-trained LLMs</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2506.03762v1">AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2508.02128v1">Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2502.04077v1">AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2507.07145v1">CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2507.14392v1">Characterizing Communication Patterns in Distributed Large Language Model Inference</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2507.03114v1">Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2502.00299v1">ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2502.19811v3">Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/bytedance/flux" /> </li>
<li><a href="http://arxiv.org/abs/2502.16886v1">DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2501.12948v1">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1" /> </li>
<li><a href="http://arxiv.org/abs/2505.11254v1">Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2507.19608v1">DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2507.14397v1">Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2502.11147v1">Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2502.06766v2">Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ryansynk/topk-decoding" /> </li>
<li><a href="http://arxiv.org/abs/2507.02754v1">Fast and Simplex: 2-Simplicial Attention in Triton</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2502.01068v1">FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/dongwonjo/FastKV" /> </li>
<li><a href="http://arxiv.org/abs/2501.01005v2">FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/flashinfer-ai/flashinfer" /> </li>
<li><a href="http://arxiv.org/abs/2504.19519v1">FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/infinigence/FlashOverlap" /> </li>
<li><a href="http://arxiv.org/abs/2505.00570v2">FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2506.02572v1">HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/gpzlx1/HATA" /> </li>
<li><a href="http://arxiv.org/abs/2507.19823v1">HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2505.21487v1">Hardware-Efficient Attention for Fast Decoding</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Dao-AILab/grouped-latent-attention" /> </li>
<li><a href="http://arxiv.org/abs/2507.07120v1">Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2503.20552v1">Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ASISys/Adrenaline" /> </li>
<li><a href="http://arxiv.org/abs/2501.02086v2">Instruction-Following Pruning for Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2508.06297v1">KV Cache Compression for Inference Efficiency in LLMs: A Review</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2502.16002v1">KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/UCSB-NLP-Chang/KVLink" /> </li>
<li><a href="http://arxiv.org/abs/2504.09936v1">KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2502.14866v1">LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mit-han-lab/omniserve" /> </li>
<li><a href="http://arxiv.org/abs/2508.02215v1">LeanK: Learnable K Cache Channel Pruning for Efficient Decoding</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/MInference" /> </li>
<li><a href="http://arxiv.org/abs/2507.11507v1">MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2505.11432v2">MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2506.07900v1">MiniCPM4: Ultra-Efficient LLMs on End Devices</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/openbmb/minicpm" /> </li>
<li><a href="http://arxiv.org/abs/2507.11181v1">Mixture of Experts in Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2505.00315v1">Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/piotrpiekos/MoSA" /> </li>
<li><a href="http://arxiv.org/abs/2507.10524v1">Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/raymin0223/mixture_of_recursions" /> </li>
<li><a href="http://arxiv.org/abs/2502.13189v1">MoBA: Mixture of Block Attention for Long-Context LLMs</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/MoonshotAI/MoBA" /> </li>
<li><a href="http://arxiv.org/abs/2504.06323v1">Mosaic: Composite Projection Pruning for Resource-efficient LLMs</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2504.07866v2">Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2503.03588v1">PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2503.00392v1">Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ASISys/PSAttention" /> </li>
<li><a href="http://arxiv.org/abs/2506.02561v1">Pruning General Large Language Models into Customized Expert Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zhaoyiran924/Custom-Prune" /> </li>
<li><a href="http://arxiv.org/abs/2506.22396v1">QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2505.09388v1">Qwen3 Technical Report</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/QwenLM/Qwen3" /> </li>
<li><a href="http://arxiv.org/abs/2505.24133v2">R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Zefan-Cai/R-KV" /> </li>
<li><a href="http://arxiv.org/abs/2506.19852v1">Radial Attention: $O(n\log n)$ Sparse Attention with Energy Decay for Long Video Generation</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2506.04108v2">Rectified Sparse Attention</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/unilm" /> </li>
<li><a href="http://arxiv.org/abs/2503.24000v1">Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/LLMkvsys/rethink-kv-compression" /> </li>
<li><a href="http://arxiv.org/abs/2505.24179v1">SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/BirdChristopher/SALE" /> </li>
<li><a href="http://arxiv.org/abs/2503.07605v1">SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/IAAR-Shanghai/SEAP" /> </li>
<li><a href="http://arxiv.org/abs/2506.08889v1">SeerAttention-R: Sparse Attention Adaptation for Long Reasoning</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/SeerAttention" /> </li>
<li><a href="http://arxiv.org/abs/2503.06433v1">Seesaw: High-throughput LLM Inference via Model Re-sharding</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2507.06517v1">SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2507.19427v1">Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2501.15113v1">Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2504.17768v1">The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/PiotrNawrot/sparse-frontier" /> </li>
<li><a href="http://arxiv.org/abs/2503.20313v3">TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ByteDance-Seed/Triton-distributed" /> </li>
<li><a href="http://arxiv.org/abs/2505.11329v1">TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/tokenweave" /> </li>
<li><a href="http://arxiv.org/abs/2504.19442v3">Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ByteDance-Seed/Triton-distributed" /> </li>
<li><a href="http://arxiv.org/abs/2507.23279v1">Unveiling Super Experts in Mixture-of-Experts Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ZunhaiSu/Super-Experts-Profilling" /> </li>
<li><a href="https://github.com/Zefan-Cai/KVCache-Factory">Unified KV Cache Compression Methods for Auto-Regressive Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2025-github-2F4F4F" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Zefan-Cai/KVCache-Factory" /> </li>
<li>kvpress [<img alt="Publish" src="https://img.shields.io/badge/2025-github-2F4F4F" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/NVIDIA/kvpress" /> </li>
</ol>
<h4 id="2024">2024</h4>
<ol>
<li><a href="https://arxiv.org/abs/2312.11983">Fluctuation-based Adaptive Structured Pruning for Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-AAAI-FF4500" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/CASIA-IVA-Lab/FLAP" /> </li>
<li><a href="http://arxiv.org/abs/2402.15220v4">ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-ACL-4169E1" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/chunk-attention" /> </li>
<li><a href="http://arxiv.org/abs/2401.16677v1">T3: Transparent Tracking &amp; Triggering for Fine-grained Overlap of Compute &amp; Collectives</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-ASPLOS-9370DB" />]  </li>
<li><a href="http://arxiv.org/abs/2403.19708v3">Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-ATC-DC143C" />]  </li>
<li><a href="https://blog.shi-labs.com/distributed-gemm-88be6a481e2b">A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-Blog-696969" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/NVIDIA/cutlass" /> </li>
<li><a href="http://arxiv.org/abs/2404.08763v4">CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-COLM-6495ED" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ScalingIntelligence/CATS" /> </li>
<li><a href="http://arxiv.org/abs/2407.18003v3">Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-COLM-6495ED" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zcli-charlie/Awesome-KV-Cache" /> </li>
<li><a href="http://arxiv.org/abs/2411.12692v1">SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-DATE-B22222" />]  </li>
<li><a href="http://arxiv.org/abs/2412.07174v1">Post-Training Statistical Calibration for Higher Activation Sparsity</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-ENLSP-00BFFF" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/IntelLabs/SCAP" /> </li>
<li><a href="http://arxiv.org/abs/2306.11695">A Simple and Effective Pruning Approach for Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-ICLR-FF6B6B" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/locuslab/wanda" /> </li>
<li><a href="http://arxiv.org/abs/2310.01382v2">Compressing LLMs: The Truth is Rarely Pure and Never Simple</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-ICLR-FF6B6B" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/VITA-Group/llm-kick" /> </li>
<li><a href="http://arxiv.org/abs/2310.08915v3">Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-ICLR-FF6B6B" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zyxxmu/DSnoT" /> </li>
<li><a href="http://arxiv.org/abs/2309.17453v4">Efficient Streaming Language Models with Attention Sinks</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-ICLR-FF6B6B" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mit-han-lab/streaming-llm" /> </li>
<li><a href="https://openreview.net/forum?id=Tr0lPx9woF">Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-ICLR-FF6B6B" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/biomedical-cybernetics/Relative-importance-and-activation-pruning" /> </li>
<li><a href="https://arxiv.org/abs/2309.14717">QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-ICLR-FF6B6B" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/yuhuixu1993/qa-lora" /> </li>
<li><a href="https://openreview.net/forum?id=vZfi5to2Xl">SAS: Structured Activation Spasification</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-ICLR-FF6B6B" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/DensoITLab/sas_" /> </li>
<li><a href="http://arxiv.org/abs/2401.15024v2">SliceGPT: Compress Large Language Models by Deleting Rows and Columns</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-ICLR-FF6B6B" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/TransformerCompression" /> </li>
<li><a href="https://arxiv.org/abs/2310.04564">ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-ICLR_oral-green" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/sjtu-ipads/powerinfer" /> </li>
<li><a href="http://arxiv.org/abs/2404.01847v2">Accelerating Transformer Pre-training with 2:4 Sparsity</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/huyz2023/2by4-pretrain" /> </li>
<li><a href="http://arxiv.org/abs/2401.15077v2">EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/SafeAILab/EAGLE" /> </li>
<li><a href="http://arxiv.org/abs/2403.06082v1">FrameQuant: Flexible Low-Bit Quantization for Transformers</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/vsingh-group/FrameQuant" /> </li>
<li><a href="http://arxiv.org/abs/2402.12354v1">LoRA+: Efficient Low Rank Adaptation of Large Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/nikhil-ghosh-berkeley/loraplus" /> </li>
<li><a href="http://arxiv.org/abs/2403.12983v1">OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mazumder-lab/OSSCAR" /> </li>
<li><a href="https://arxiv.org/pdf/2310.05175.pdf">Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/luuyin/OWL" /> </li>
<li><a href="http://arxiv.org/abs/2406.10774">Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mit-han-lab/quest" /> </li>
<li><a href="http://arxiv.org/abs/2405.16057v1">SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Lucky-Lance/SPP" /> </li>
<li><a href="http://arxiv.org/abs/2312.04985v5">SparQ Attention: Bandwidth-Efficient LLM Inference</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" />]  </li>
<li><a href="http://arxiv.org/abs/2312.11875v3">Sparse is Enough in Fine-tuning Pre-trained Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/song-wx/SIFT" /> </li>
<li><a href="http://arxiv.org/abs/2303.11525v3">Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/CerebrasResearch/Sparse-IFT" /> </li>
<li><a href="http://arxiv.org/abs/2306.07629v4">SqueezeLLM: Dense-and-Sparse Quantization</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/SqueezeAILab/SqueezeLLM" /> </li>
<li><a href="http://arxiv.org/abs/2307.09988v2">TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/theyoungkwon/TinyTrain" /> </li>
<li><a href="http://arxiv.org/abs/2403.08477v3">Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/szc12153/sparse_interpolated_experts" /> </li>
<li><a href="https://arxiv.org/abs/2306.00978">AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-MLSys-DDA0DD" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mit-han-lab/llm-awq" /> </li>
<li><a href="http://arxiv.org/abs/2405.05465v2">Vidur: A Large-Scale Simulation Framework For LLM Inference</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-MLSys-DDA0DD" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/vidur" /> </li>
<li><a href="http://arxiv.org/abs/2407.02490v1">MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-NeurIPS-FF1493" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/MInference" /> </li>
<li><a href="http://arxiv.org/abs/2409.17481v1">MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-NeurIPS-FF1493" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/NVlabs/MaskLLM" /> </li>
<li><a href="http://arxiv.org/abs/2312.07104v2">SGLang: Efficient Execution of Structured Language Model Programs</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-NeurIPS-FF1493" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/sgl-project/sglang" /> </li>
<li><a href="http://arxiv.org/abs/2412.18110v1">SlimGPT: Layer-wise Structured Pruning for Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-NeurIPS-FF1493" />]  </li>
<li><a href="http://arxiv.org/abs/2402.17946v3">SparseLLM: Towards Global Pruning for Pre-trained Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-NeurIPS-FF1493" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/BaiTheBest/SparseLLM" /> </li>
<li><a href="http://arxiv.org/abs/2401.02938v2">Fast and Effective Weight Update for Pruned Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-TMLR-20B2AA" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/fmfi-compbio/admm-pruning" /> </li>
<li><a href="https://arxiv.org/abs/2309.10285">Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-VLDB-A52A2A" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/AlibabaResearch/flash-llm" /> </li>
<li><a href="http://arxiv.org/abs/2404.14294v2">A Survey on Efficient Inference for Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2412.14219v2">A Survey on Inference Optimization Techniques for Mixture of Experts Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/MoE-Inf/awesome-moe-inference" /> </li>
<li><a href="http://arxiv.org/abs/2412.19442v2">A Survey on Large Language Model Acceleration based on KV Cache Management</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/TreeAI-Lab/Awesome-KV-Cache-Management" /> </li>
<li><a href="http://arxiv.org/abs/2411.17651v2">APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/apex_plus" /> </li>
<li><a href="http://arxiv.org/abs/2411.02117v1">AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2407.11550v3">Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/FFY0/AdaKV" /> </li>
<li><a href="http://arxiv.org/abs/2410.16135v1">Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2407.12866v1">Beyond KV Caching: Shared Attention for Efficient LLMs</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/metacarbon/shareAtt" /> </li>
<li><a href="https://arxiv.org/abs/2408.11796v2">Compact Language Models via Pruning and Knowledge Distillation</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/NVlabs/Minitron" /> </li>
<li><a href="http://arxiv.org/abs/2410.18311v1">CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/wangqinsi1/CoreInfer" /> </li>
<li><a href="http://arxiv.org/abs/2405.04434v5">DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2" /> </li>
<li><a href="http://arxiv.org/abs/2412.19437v1">DeepSeek-V3 Technical Report</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3" /> </li>
<li><a href="http://arxiv.org/abs/2401.06066v1">DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/deepseek-ai/DeepSeek-MoE" /> </li>
<li><a href="http://arxiv.org/abs/2409.15241v1">Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/deepspeedai/DeepSpeedExamples" /> </li>
<li><a href="http://arxiv.org/abs/2410.10819v1">DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mit-han-lab/duo-attention" /> </li>
<li><a href="http://arxiv.org/abs/2405.03594v1">Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/neuralmagic/nm-vllm" /> </li>
<li><a href="https://arxiv.org/abs/2402.05406">Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ldery/Bonsai" /> </li>
<li><a href="http://arxiv.org/abs/2406.06858v5">FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2410.01359v1">FlashMask: Efficient and Rich Mask Extension of FlashAttention</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/PaddlePaddle/PaddleNLP" /> </li>
<li><a href="http://arxiv.org/abs/2401.02669v2">Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2401.18079v2">KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/SqueezeAILab/KVQuant" /> </li>
<li><a href="https://arxiv.org/abs/2402.04902">L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2403.17919v1">LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2407.12391v1">LLM Inference Serving: Survey of Recent Advances and Opportunities</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2402.17762v2">Massive Activations in Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/locuslab/massive-activations" /> </li>
<li><a href="http://arxiv.org/abs/2411.18077v2">MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2404.02258v1">Mixture-of-Depths: Dynamically allocating compute in transformer-based language models</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2406.14909v2">MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/thu-nics/MoA" /> </li>
<li><a href="http://arxiv.org/abs/2412.19255v2">Multi-matrix Factorization Attention</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2408.12757v2">NanoFlow: Towards Optimal Large Language Model Serving Throughput</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2409.01366v1">Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />] <a href="https://anonymous.4open.science/r/CHESS-BA40/README.md">Pytorch</a> </li>
<li><a href="http://arxiv.org/abs/2408.07092v2">Post-Training Sparse Attention with Double Sparsity</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/andy-yang-1/DoubleSparse" /> </li>
<li><a href="http://arxiv.org/abs/2406.06282v2">PowerInfer-2: Fast Large Language Model Inference on a Smartphone</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />] <a href="https://powerinfer.ai/v2/">Website</a> </li>
<li><a href="https://arxiv.org/abs/2402.13516">ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/raincleared-song/sparse_gpu_operator" /> </li>
<li><a href="http://arxiv.org/abs/2407.10969v1">Q-Sparse: All Large Language Models can be Fully Sparsely-Activated</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2405.04532v2">QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />] <a href="https://hanlab.mit.edu/projects/qserve">Pytorch</a> </li>
<li><a href="https://arxiv.org/abs/2402.03804">ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2412.14711v1">ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/thu-ml/ReMoE" /> </li>
<li><a href="http://arxiv.org/abs/2411.05787v1">Recycled Attention: Efficient inference for long-context language models</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/carriex/recycled-attention" /> </li>
<li><a href="http://arxiv.org/abs/2405.12981v1">Reducing Transformer Key-Value Cache Size with Cross-Layer Attention</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/JerryYin777/Cross-Layer-Attention" /> </li>
<li><a href="http://arxiv.org/abs/2402.11592v2">Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ZO-Bench/ZO-LLM" /> </li>
<li><a href="http://arxiv.org/abs/2412.10319v2">SCBench: A KV Cache-Centric Analysis of Long-Context Methods</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/MInference" /> </li>
<li><a href="http://arxiv.org/abs/2406.15486v2">SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2410.13276v2">SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/SeerAttention" /> </li>
<li><a href="http://arxiv.org/abs/2406.16635v1">ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/abdelfattah-lab/shadow_llm" /> </li>
<li><a href="http://arxiv.org/abs/2404.14469v2">SnapKV: LLM Knows What You are Looking for Before Generation</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/FasterDecoding/SnapKV" /> </li>
<li><a href="http://arxiv.org/abs/2401.06104v2">Transformers are Multi-State RNNs</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/schwartz-lab-NLP/TOVA" /> </li>
<li><a href="http://arxiv.org/abs/2406.05955v2">Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />] <a href="https://huggingface.co/PowerInfer">Pytorch</a> </li>
<li><a href="http://arxiv.org/abs/2411.15100v2">XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mlc-ai/xgrammar" /> </li>
<li><a href="http://arxiv.org/abs/2412.09036v1">ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />]  </li>
<li><a href="https://discuss.pytorch.org/t/distributed-w-torchtitan-introducing-async-tensor-parallelism-in-pytorch/209487/1">[Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch</a> [<img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/pytorch/torchtitan" /> </li>
</ol>
<h4 id="2023">2023</h4>
<ol>
<li><a href="https://arxiv.org/abs/2210.11794">Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-AAAI-FF4500" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/asFeng/Diffuser" /> </li>
<li><a href="https://arxiv.org/abs/2212.07634">Gradient-based Intra-attention Pruning on Pre-trained Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-ACL-4169E1" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/airaria/GRAIN" /> </li>
<li><a href="https://aclanthology.org/2023.acl-long.35.pdf">Pruning Pre-trained Language Models Without Fine-Tuning</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-ACL-4169E1" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/kongds/SMP" /> </li>
<li><a href="https://aclanthology.org/2023.findings-acl.573/">Pruning Pre-trained Language Models with Principled Importance and Self-regularization</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-ACL-4169E1" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/drsy/pins" /> </li>
<li><a href="https://aclanthology.org/2023.findings-acl.692.pdf">Structured Pruning for Efficient Generative Pre-trained Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-ACL-4169E1" />]  </li>
<li><a href="https://dl.acm.org/doi/abs/10.1145/3567955.3567959">Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-ASPLOS-9370DB" />]  </li>
<li><a href="https://openreview.net/forum?id=SHlZcInS6C">Structural Pruning of Large Language Models via Neural Architecture Search</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-AutoML_Workshop-A9A9A9" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/awslabs/syne-tune" /> </li>
<li><a href="https://arxiv.org/abs/2303.17605">SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-CVPR-2E8B57" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mit-han-lab/sparsevit" /> </li>
<li><a href="https://openaccess.thecvf.com/content/CVPR2023W/WAD/papers/Tang_TorchSparse_Efficient_Point_Cloud_Engine_CVPRW_2023_paper.pdf">TorchSparse++: Efficient Point Cloud Engine</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-CVPR_workshop-green" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mit-han-lab/torchsparse" /> </li>
<li><a href="https://arxiv.org/pdf/2303.10512.pdf">AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-ICLR-FF6B6B" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/QingruZhang/AdaLoRA" /> </li>
<li><a href="https://arxiv.org/pdf/2210.17323.pdf">GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-ICLR-FF6B6B" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/IST-DASLab/gptq" /> </li>
<li><a href="https://openreview.net/pdf?id=vuD2xEtxZcj">Minimum Variance Unbiased N:M Sparsity for the Neural Gradients</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-ICLR-FF6B6B" />]  </li>
<li><a href="https://openreview.net/forum?id=TJ2nxciYCk-">The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-ICLR-FF6B6B" />]  </li>
<li><a href="https://openreview.net/forum?id=wIPIhHd00i">Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-ICML-FF8C00" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/FMInference/DejaVu" /> </li>
<li><a href="https://arxiv.org/pdf/2301.00774.pdf">SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot.</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-ICML-FF8C00" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/IST-DASLab/sparsegpt" /> </li>
<li><a href="https://arxiv.org/abs/2306.11222">Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-ICML-FF8C00" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/yxli2123/LoSparse" /> </li>
<li><a href="https://proceedings.mlsys.org/paper_files/paper/2023/file/4552cedd396a308320209f75f56a5ad5-Paper-mlsys2023.pdf">Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-MLSys-DDA0DD" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/SparTA" /> </li>
<li><a href="https://openreview.net/pdf?id=bPFFPueAxm">ZipLM: Inference-Aware Structured Pruning of Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-NeurIPS-FF1493" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/IST-DASLab/ZipLM" /> </li>
<li><a href="http://arxiv.org/abs/2310.02065v1">VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-SC-CD5C5C" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/UDC-GAC/venom" /> </li>
<li><a href="http://arxiv.org/abs/2309.06180v1">Efficient Memory Management for Large Language Model Serving with PagedAttention</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-SOSP-8A2BE2" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/vllm-project/vllm" /> </li>
<li><a href="https://arxiv.org/abs/2209.00099">Efficient Methods for Natural Language Processing: A Survey</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-TACL-87CEEB" />]  </li>
<li><a href="https://arxiv.org/abs/2303.10464">SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-UAI-green" />]  </li>
<li><a href="https://arxiv.org/abs/2307.03109">A Survey on Evaluation of Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" />]  </li>
<li><a href="https://arxiv.org/abs/2308.07633">A Survey on Model Compression for Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2311.04902v2">Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/VILA-Lab/GBLM-Pruner" /> </li>
<li><a href="http://arxiv.org/abs/2303.17568v2">CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/THUDM/CodeGeeX" /> </li>
<li><a href="https://arxiv.org/abs/2310.05015">Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/Moonlit" /> </li>
<li><a href="https://arxiv.org/abs/2305.15805">Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2307.09702v4">Efficient Guided Generation for Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2305.17333v3">Fine-Tuning Language Models with Just Forward Passes</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/princeton-nlp/MeZO" /> </li>
<li><a href="https://crfm.stanford.edu/2023/10/12/flashdecoding.html">Flash-Decoding for long-context inference</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" />]  </li>
<li><a href="https://arxiv.org/abs/2307.08691">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Dao-AILab/flash-attention" /> </li>
<li><a href="https://arxiv.org/abs/2303.04185">Gradient-Free Structured Pruning with Unlabeled Data</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2306.14048">H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/FMInference/H2O" /> </li>
<li><a href="https://arxiv.org/abs/2308.03449">Knowledge-preserving Pruning for Pre-trained Language Models without Retraining</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2312.11514">LLM in a flash: Efficient Large Language Model Inference with Limited Memory</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2305.11627v3">LLM-Pruner: On the Structural Pruning of Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/horseee/LLM-Pruner" /> </li>
<li><a href="https://arxiv.org/abs/2310.18356">LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" />]  </li>
<li><a href="https://arxiv.org/abs/2310.08659">LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/yxli2123/LoftQ" /> </li>
<li><a href="https://arxiv.org/abs/2308.13137">OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/OpenGVLab/OmniQuant" /> </li>
<li><a href="https://arxiv.org/pdf/2201.11113.pdf">Post-training Quantization for Neural Networks with Provable Guarantees</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/YixuanSeanZhou/Quantized_Neural_Nets" /> </li>
<li><a href="http://arxiv.org/abs/2312.12456v1">PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/SJTU-IPADS/PowerInfer" /> </li>
<li><a href="https://arxiv.org/abs/2309.09507">Pruning Large Language Models via Accuracy Predictor</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" />]  </li>
<li><a href="https://arxiv.org/abs/2305.14314">QLoRA: Efficient Finetuning of Quantized LLMs</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/artidoro/qlora" /> </li>
<li><a href="https://arxiv.org/pdf/2307.13304.pdf">QuIP: Quantization with Incoherence Processing</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/jerry-chee/QuIP" /> </li>
<li><a href="https://arxiv.org/pdf/2304.01089.pdf">RPTQ: Reorder-based Post-training Quantization for Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/hahnyuan/RPTQ4LLM" /> </li>
<li><a href="https://xiamengzhou.github.io/sheared-llama/">Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/princeton-nlp/LLM-Shearing" /> </li>
<li><a href="https://arxiv.org/pdf/2306.03078.pdf">SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Vahe1994/SpQR" /> </li>
<li><a href="https://arxiv.org/pdf/2310.06927.pdf">Sparse Fine-tuning for Inference Acceleration of Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/IST-DASLab/SparseFinetuning" /> </li>
<li><a href="https://arxiv.org/abs/2303.11525">Sparse Iso-FLOP Transformations for Maximizing Training Efficiency</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/CerebrasResearch/Sparse-IFT" /> </li>
<li><a href="https://arxiv.org/abs/2306.16788">Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ZIB-IOL/SMS" /> </li>
<li><a href="https://arxiv.org/abs/2302.02596">Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" />]  </li>
<li><a href="https://arxiv.org/abs/2306.03805">The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/VITA-Group/essential_sparsity" /> </li>
<li><a href="https://arxiv.org/abs//2306.11987">Training Transformers with 4-bit Integers</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/xijiu9/Train_Transformers_with_INT4" /> </li>
<li><a href="https://arxiv.org/abs/2304.12102">Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/liyucheng09/Selective_Context" /> </li>
<li><a href="https://arxiv.org/abs/2303.08302">ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/DeepSpeed" /> </li>
<li><a href="https://github.com/NVIDIA/FasterTransformer">FasterTransformer</a> [<img alt="Publish" src="https://img.shields.io/badge/2023-github-2F4F4F" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/NVIDIA/FasterTransformer" /> </li>
</ol>
<h4 id="2022">2022</h4>
<ol>
<li><a href="https://aclanthology.org/2022.acl-long.16/">Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm</a> [<img alt="Publish" src="https://img.shields.io/badge/2022-ACL-4169E1" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/shaoyiHusky/SparseProgressiveDistillation" /> </li>
<li><a href="https://arxiv.org/abs/2203.15996">TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2022-ACL-4169E1" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/airaria/TextPruner" /> </li>
<li><a href="https://www.cerebras.net/blog/creating-sparse-gpt-3-models-with-iterative-pruning">Creating Sparse GPT-3 Models with Iterative Pruning</a> [<img alt="Publish" src="https://img.shields.io/badge/2022-Blog-696969" />]  </li>
<li><a href="https://arxiv.org/abs/2106.09685">LoRA: Low-rank adaptation of large language models</a> [<img alt="Publish" src="https://img.shields.io/badge/2022-ICLR-FF6B6B" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/LoRA" /> </li>
<li><a href="https://arxiv.org/abs/2201.13096">SPDY: Accurate Pruning with Speedup Guarantees</a> [<img alt="Publish" src="https://img.shields.io/badge/2022-ICML-FF8C00" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/IST-DASLab/spdy" /> </li>
<li><a href="https://arxiv.org/abs/2209.00606">Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation</a> [<img alt="Publish" src="https://img.shields.io/badge/2022-MICRO-BA55D3" />]  </li>
<li><a href="http://arxiv.org/abs/2204.09656v2">A Fast Post-Training Pruning Framework for Transformers</a> [<img alt="Publish" src="https://img.shields.io/badge/2022-NeurIPS-FF1493" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/WoosukKwon/retraining-free-pruning" /> </li>
<li><a href="https://arxiv.org/abs/2205.14135">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a> [<img alt="Publish" src="https://img.shields.io/badge/2022-NeurIPS-FF1493" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Dao-AILab/flash-attention" /> </li>
<li><a href="https://openreview.net/pdf?id=ksVGCOlOEba">Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning</a> [<img alt="Publish" src="https://img.shields.io/badge/2022-NeurIPS-FF1493" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/IST-DASLab/OBC" /> </li>
<li><a href="https://openreview.net/forum?id=f-fVCElZ-G1">ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers</a> [<img alt="Publish" src="https://img.shields.io/badge/2022-NeurIPS-FF1493" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/DeepSpeed" /> </li>
<li><a href="https://iopscience.iop.org/article/10.1088/2634-4386/ac7c8a">Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks</a> [<img alt="Publish" src="https://img.shields.io/badge/2022-Neuromorphic_Computing_and_Engineering-40E0D0" />]  </li>
<li><a href="http://arxiv.org/abs/2110.11299v1">Transformer Acceleration with Dynamic Sparse Attention</a> [<img alt="Publish" src="https://img.shields.io/badge/2022-TC-F08080" />]  </li>
<li><a href="https://arxiv.org/abs/2208.06118">An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers</a> [<img alt="Publish" src="https://img.shields.io/badge/2022-VLSI-8B0000" />]  </li>
<li><a href="https://arxiv.org/pdf/2203.07259.pdf">The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models</a> [<img alt="Publish" src="https://img.shields.io/badge/2022-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/neuralmagic/sparseml" /> </li>
</ol>
<h4 id="2021">2021</h4>
<ol>
<li><a href="https://openaccess.thecvf.com/content/ICCV2021W/LPCV/papers/Lazarevich_Post-Training_Deep_Neural_Network_Pruning_via_Layer-Wise_Calibration_ICCVW_2021_paper.pdf">Post-training deep neural network pruning via layer-wise calibration</a> [<img alt="Publish" src="https://img.shields.io/badge/2021-ICCV-green" />]  </li>
<li><a href="https://openreview.net/pdf?id=POWv6hDd9XH">BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction</a> [<img alt="Publish" src="https://img.shields.io/badge/2021-ICLR-FF6B6B" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/yhhhli/BRECQ" /> </li>
<li><a href="https://openreview.net/forum?id=K9bw7vqp_s">Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch</a> [<img alt="Publish" src="https://img.shields.io/badge/2021-ICLR-FF6B6B" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/aojunzz/NM-sparsity" /> </li>
<li><a href="https://jmlr.csail.mit.edu/papers/volume22/20-1233/20-1233.pdf">A Greedy Algorithm for Quantizing Neural Networks</a> [<img alt="Publish" src="https://img.shields.io/badge/2021-JMLR-008B8B" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/YixuanSeanZhou/Quantized_Neural_Nets" /> </li>
<li><a href="https://proceedings.neurips.cc/paper/2021/hash/6e8404c3b93a9527c8db241a1846599a-Abstract.html">Channel Permutations for N:M Sparsity</a> [<img alt="Publish" src="https://img.shields.io/badge/2021-NeurIPS-FF1493" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/NVIDIA/apex" /> </li>
<li><a href="https://arxiv.org/abs/2104.08378">Accelerating Sparse Deep Neural Networks</a> [<img alt="Publish" src="https://img.shields.io/badge/2021-arXiv-1E88E5" />]  </li>
<li><a href="http://arxiv.org/abs/2105.05720v5">Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads</a> [<img alt="Publish" src="https://img.shields.io/badge/2021-arXiv-1E88E5" />]  </li>
<li><a href="https://arxiv.org/abs/2102.00554">Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks</a> [<img alt="Publish" src="https://img.shields.io/badge/2021-arXiv-1E88E5" />]  </li>
</ol>
<h4 id="2020">2020</h4>
<ol>
<li><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Elsen_Fast_Sparse_ConvNets_CVPR_2020_paper.pdf">Fast Sparse ConvNets</a> [<img alt="Publish" src="https://img.shields.io/badge/2020-CVPR-2E8B57" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/fastconvnets/cvpr2020" /> </li>
<li><a href="http://proceedings.mlr.press/v119/kurtz20a/kurtz20a.pdf">Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference</a> [<img alt="Publish" src="https://img.shields.io/badge/2020-ICML-FF8C00" />]  </li>
<li><a href="https://arxiv.org/abs/2005.07683">Movement Pruning: Adaptive Sparsity by Fine-Tuning</a> [<img alt="Publish" src="https://img.shields.io/badge/2020-NeurIPS-FF1493" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/huggingface/block_movement_pruning" /> </li>
<li><a href="https://cdn.openai.com/blocksparse/blocksparsepaper.pdf">GPU Kernels for Block-Sparse Weights</a> [<img alt="Publish" src="https://img.shields.io/badge/2020-arXiv-1E88E5" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/openai/blocksparse" /> </li>
</ol>
<h4 id="2019">2019</h4>
<ol>
<li><a href="https://arxiv.org/abs/2104.14129">ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training</a> [<img alt="Publish" src="https://img.shields.io/badge/2019-ICML-FF8C00" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ucbrise/actnn" /> </li>
</ol>
<h4 id="2018">2018</h4>
<ol>
<li><a href="https://arxiv.org/abs/1804.03294">A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers</a> [<img alt="Publish" src="https://img.shields.io/badge/2018-ECCV-3CB371" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/bzantium/pytorch-admm-pruning" /> </li>
</ol>
<h4 id="2017">2017</h4>
<ol>
<li><a href="https://arxiv.org/pdf/1607.04381.pdf">DSD: Dense-Sparse-Dense Training for Deep Neural Networks</a> [<img alt="Publish" src="https://img.shields.io/badge/2017-ICLR-FF6B6B" />]  </li>
<li><a href="https://arxiv.org/pdf/1705.07565.pdf">Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon</a> [<img alt="Publish" src="https://img.shields.io/badge/2017-NeurIPS-FF1493" />] <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/csyhhu/L-OBS" /> </li>
</ol>
<h4 id="2016">2016</h4>
<ol>
<li><a href="https://arxiv.org/pdf/1510.00149.pdf">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding</a> [<img alt="Publish" src="https://img.shields.io/badge/2016-ICLR-FF6B6B" />]  </li>
</ol>
<h4 id="1993">1993</h4>
<ol>
<li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=298572&amp;tag=1">Optimal Brain Surgeon and general network pruning</a> [<img alt="Publish" src="https://img.shields.io/badge/1993--green" />]  </li>
</ol>
<h4 id="1989">1989</h4>
<ol>
<li><a href="https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf">Optimal Brain Damage</a> [<img alt="Publish" src="https://img.shields.io/badge/1989-NeurIPS-FF1493" />]  </li>
</ol>
<h2 id="references">References</h2>
<h3 id="hot">Hot</h3>
<ol>
<li>https://github.com/horseee/Awesome-Efficient-LLM</li>
<li>https://github.com/DefTruth/Awesome-Diffusion-Inference</li>
<li>https://github.com/DefTruth/Awesome-LLM-Inference</li>
<li>https://github.com/AmberLJC/LLMSys-PaperList</li>
<li>https://github.com/Hannibal046/Awesome-LLM</li>
<li>https://github.com/AmadeusChan/Awesome-LLM-System-Papers</li>
<li>https://github.com/KnowingNothing/compiler-and-arch</li>
<li>https://papercopilot.com/paper-list</li>
<li>https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management</li>
<li>https://github.com/October2001/Awesome-KV-Cache-Compression</li>
</ol>
<h3 id="cold">Cold</h3>
<ol>
<li>https://github.com/he-y/Awesome-Pruning</li>
<li>https://github.com/htqin/awesome-model-quantization</li>
<li>https://github.com/csyhhu/Awesome-Deep-Neural-Network-Compression</li>
<li>https://github.com/AojunZhou/Efficient-Deep-Learning</li>
<li>https://github.com/chester256/Model-Compression-Papers</li>
</ol>
<p><a href="#efficientpaper">:arrow_up: Back to top</a></p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="cls_year/" class="btn btn-neutral float-right" title="By Year">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
    
      <span><a href="cls_year/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="js/jquery-3.6.0.min.js"></script>
    <script>var base_url = ".";</script>
    <script src="js/theme_extra.js"></script>
    <script src="js/theme.js"></script>
      <script src="search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>

<!--
MkDocs version : 1.6.1
Build Date UTC : 2025-08-13 02:27:26.559905+00:00
-->
