<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="description" content="None" />
      <link rel="shortcut icon" href="img/favicon.ico" />
    <title>Efficient Paper</title>
    <link rel="stylesheet" href="css/theme.css" />
    <link rel="stylesheet" href="css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.css" rel="stylesheet" />
        <link href="css/extra.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Home";
        var mkdocs_page_input_path = "index.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
 <link href="assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }</style> <script src="assets/javascripts/glightbox.min.js"></script></head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="." class="icon icon-home"> Efficient Paper
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="./search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="#">Home</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#table-of-contents">Table of Contents</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#getting-started">Getting Started</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#recent-changes">Recent Changes</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#paper-list">Paper List</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#year">year</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#2026">2026</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#2025">2025</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#2024">2024</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#2023">2023</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#2022">2022</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#2021">2021</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#2020">2020</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#2019">2019</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#2018">2018</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#2017">2017</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#2016">2016</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#1993">1993</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="#1989">1989</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#references">References</a>
    </li>
    </ul>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Paper List</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="cls_year/">By Year</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="cls_keyword/">By Keyword</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="cls_publication/">By Publication</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="cls_institution/">By Institution</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="cls_author/">By Author</a>
                  </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="baseline_methods_graph/">Graph</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Weekly Paper</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="weekly_paper/2025-09-05/">2025-09-05</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="weekly_paper/2025-09-15/">2025-09-15</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="weekly_paper/2025-09-19/">2025-09-19</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="weekly_paper/2025-09-26/">2025-09-26</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="weekly_paper/2025-09-28/">2025-09-28</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Lagency</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="weekly_paper/lagency/2025-07-25/">2025-07-25</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="weekly_paper/lagency/2025-08-01/">2025-08-01</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="weekly_paper/lagency/2025-08-08/">2025-08-08</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="weekly_paper/lagency/2025-08-15/">2025-08-15</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="weekly_paper/lagency/2025-08-22/">2025-08-22</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="weekly_paper/lagency/2025-08-29/">2025-08-29</a>
                </li>
    </ul>
                  </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="contributors/">Contributors</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href=".">Efficient Paper</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Home</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
  
                <h1 id="efficientpaper">EfficientPaper</h1>
<p>Pruning, Quantization and efficient-inference/training paper list.</p>
<h2 id="table-of-contents">Table of Contents</h2>
<ul>
<li><a href="#efficientpaper">EfficientPaper</a></li>
<li><a href="#getting-started">Getting Started</a></li>
<li><a href="#paper-list">Paper List</a><ul>
<li><a href="cls_keyword/">keyword</a></li>
<li><a href="cls_year/">year</a></li>
<li><a href="cls_publication/">publication</a></li>
<li><a href="cls_institution/">institution</a></li>
<li><a href="cls_author/">author</a></li>
</ul>
</li>
<li><a href="#references">References</a></li>
</ul>
<h2 id="getting-started">Getting Started</h2>
<pre><code class="language-bash">git clone https://github.com/hustzxd/EfficientPaper
pip install protobuf==5.27.2 pandas arxiv 
</code></pre>
<ol>
<li>Add paper information by <code>./add_paper_info.sh</code></li>
<li>Run <code>./refresh_readme.sh</code></li>
</ol>
<details><summary><b>efficient_paper.prototxt</b></summary> 
<p>


<pre><code>paper {
  title: &quot;EfficientPaper: manage your research papers in an efficient way.&quot;
  abbr: &quot;EfficientPaper&quot;
  url: &quot;https://github.com/hustzxd/EfficientPaper&quot;
  authors: &quot;hustzxd&quot;
}
pub {
  where: &quot;GitHub&quot;
  year: 2023
}
code {
  type: &quot;Pytorch&quot;
  url: &quot;https://github.com/hustzxd/EfficientPaper&quot;
}
note {
  url: &quot;EfficientPaper.md&quot;
}
keyword {
  words: efficient_paper
}
</code></pre>


</p>
</details>

<p align="center">
<a class="glightbox" href="notes//conference_timeline.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img src="notes//conference_timeline.png" width="800" title="blank"></a>
</p>

<h2 id="recent-changes">Recent Changes</h2>
<table style="width: 100%; border-collapse: collapse; margin: 20px 0;">
    <thead>
        <tr style="background-color: #f5f5f5;">
            <th style="border: 1px solid #ddd; padding: 8px; text-align: left;">Commit</th>
            <th style="border: 1px solid #ddd; padding: 8px; text-align: left;">Date</th>
            <th style="border: 1px solid #ddd; padding: 8px; text-align: left;">Author</th>
            <th style="border: 1px solid #ddd; padding: 8px; text-align: left;">Message</th>
            <th style="border: 1px solid #ddd; padding: 8px; text-align: left;">Path</th>
        </tr>
    </thead>
    <tbody>
        <tr><td style="border: 1px solid #ddd; padding: 8px;"><a href="https://github.com/hustzxd/EfficientPaper/commit/8adb12c" target="_blank">8adb12c</a></td><td style="border: 1px solid #ddd; padding: 8px;">2025-09-28</td><td style="border: 1px solid #ddd; padding: 8px;">hustzxd</td><td style="border: 1px solid #ddd; padding: 8px;">weekly_update</td><td style="border: 1px solid #ddd; padding: 8px;"><code>docs/weekly_paper</code></td></tr><tr><td style="border: 1px solid #ddd; padding: 8px;"><a href="https://github.com/hustzxd/EfficientPaper/commit/8adb12c" target="_blank">8adb12c</a></td><td style="border: 1px solid #ddd; padding: 8px;">2025-09-28</td><td style="border: 1px solid #ddd; padding: 8px;">hustzxd</td><td style="border: 1px solid #ddd; padding: 8px;">weekly_update</td><td style="border: 1px solid #ddd; padding: 8px;"><code>meta</code></td></tr><tr><td style="border: 1px solid #ddd; padding: 8px;"><a href="https://github.com/hustzxd/EfficientPaper/commit/a20aef7" target="_blank">a20aef7</a></td><td style="border: 1px solid #ddd; padding: 8px;">2025-09-19</td><td style="border: 1px solid #ddd; padding: 8px;">hustzxd</td><td style="border: 1px solid #ddd; padding: 8px;">weekly-update</td><td style="border: 1px solid #ddd; padding: 8px;"><code>docs/weekly_paper</code></td></tr><tr><td style="border: 1px solid #ddd; padding: 8px;"><a href="https://github.com/hustzxd/EfficientPaper/commit/a20aef7" target="_blank">a20aef7</a></td><td style="border: 1px solid #ddd; padding: 8px;">2025-09-19</td><td style="border: 1px solid #ddd; padding: 8px;">hustzxd</td><td style="border: 1px solid #ddd; padding: 8px;">weekly-update</td><td style="border: 1px solid #ddd; padding: 8px;"><code>meta</code></td></tr><tr><td style="border: 1px solid #ddd; padding: 8px;"><a href="https://github.com/hustzxd/EfficientPaper/commit/6535071" target="_blank">6535071</a></td><td style="border: 1px solid #ddd; padding: 8px;">2025-09-16</td><td style="border: 1px solid #ddd; padding: 8px;">hustzxd</td><td style="border: 1px solid #ddd; padding: 8px;">5papers</td><td style="border: 1px solid #ddd; padding: 8px;"><code>docs/weekly_paper</code></td></tr><tr><td style="border: 1px solid #ddd; padding: 8px;"><a href="https://github.com/hustzxd/EfficientPaper/commit/6535071" target="_blank">6535071</a></td><td style="border: 1px solid #ddd; padding: 8px;">2025-09-16</td><td style="border: 1px solid #ddd; padding: 8px;">hustzxd</td><td style="border: 1px solid #ddd; padding: 8px;">5papers</td><td style="border: 1px solid #ddd; padding: 8px;"><code>meta</code></td></tr>
    </tbody>
</table>

<h2 id="paper-list">Paper List</h2>
<h3 id="year">year</h3>
<h4 id="2026">2026</h4>
<ol>
<li><a href="http://arxiv.org/abs/2504.19519v1">FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation</a> [<a class="glightbox" href="https://img.shields.io/badge/2026-EuroSys-green" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2026-EuroSys-green" /></a>] <a href="https://github.com/infinigence/FlashOverlap"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/infinigence/FlashOverlap" /></a> </li>
</ol>
<h4 id="2025">2025</h4>
<ol>
<li><a href="http://arxiv.org/abs/2501.02336v1">AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-AAAI-FF4500" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-AAAI-FF4500" /></a>] <a href="https://github.com/ASISys/AdaSkip"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ASISys/AdaSkip" /></a> </li>
<li><a href="http://arxiv.org/abs/2407.20584v3">Pruning Large Language Models with Semi-Structural Adaptive Sparse Training</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-AAAI-FF4500" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-AAAI-FF4500" /></a>] <a href="https://github.com/thu-ml/Adaptive-Sparse-Trainer"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/thu-ml/Adaptive-Sparse-Trainer" /></a> </li>
<li><a href="http://arxiv.org/abs/2406.03482v2">QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-AAAI-FF4500" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-AAAI-FF4500" /></a>] <a href="https://github.com/amirzandieh/QJL"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/amirzandieh/QJL" /></a> </li>
<li><a href="http://arxiv.org/abs/2502.11089v1">Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-ACL-4169E1" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-ACL-4169E1" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2502.20586v2">Training LLMs with MXFP4</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-AISTATS-green" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-AISTATS-green" /></a>] <a href="https://github.com/amazon-science/mxfp4-llm"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/amazon-science/mxfp4-llm" /></a> </li>
<li><a href="http://arxiv.org/abs/2410.12168v1">COMET: Towards Partical W4A4KV4 LLMs Serving</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-ASPLOS-9370DB" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-ASPLOS-9370DB" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2410.18038v2">POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-ASPLOS-9370DB" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-ASPLOS-9370DB" /></a>] <a href="https://github.com/microsoft/vattention"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/vattention" /></a> </li>
<li><a href="http://arxiv.org/abs/2405.04437v3">vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-ASPLOS-9370DB" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-ASPLOS-9370DB" /></a>] <a href="https://github.com/microsoft/vattention"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/vattention" /></a> </li>
<li><a href="http://arxiv.org/abs/2507.08771v1">BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-COLM-6495ED" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-COLM-6495ED" /></a>] <a href="https://github.com/thunlp/BlockFFN"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/thunlp/BlockFFN" /></a> </li>
<li><a href="http://arxiv.org/abs/2508.04257v1">KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-COLM-6495ED" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-COLM-6495ED" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2408.10473v1">Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-Coling-1E90FF" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-Coling-1E90FF" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2509.06436v1">Tree of Agents: Improving Long-Context Capabilities of Large Language Models through Multi-Perspective Reasoning</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-EMNLP_Findings-green" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-EMNLP_Findings-green" /></a>] <a href="https://github.com/Aireduce952/Tree-of-Agents"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Aireduce952/Tree-of-Agents" /></a> </li>
<li><a href="http://arxiv.org/abs/2405.16444v3">CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-EuroSys-green" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-EuroSys-green" /></a>] <a href="https://github.com/LMCache/LMCache"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/LMCache/LMCache" /></a> </li>
<li><a href="http://arxiv.org/abs/2502.20766v1">FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-ICLR-FF6B6B" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-ICLR-FF6B6B" /></a>] <a href="https://github.com/bytedance/FlexPrefill"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/bytedance/FlexPrefill" /></a> </li>
<li><a href="http://arxiv.org/abs/2503.02130v2">Forgetting Transformer: Softmax Attention with a Forget Gate</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-ICLR-FF6B6B" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-ICLR-FF6B6B" /></a>] <a href="https://github.com/zhixuan-lin/forgetting-transformer"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zhixuan-lin/forgetting-transformer" /></a> </li>
<li><a href="http://arxiv.org/abs/2504.19449v1">R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-ICLR-FF6B6B" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-ICLR-FF6B6B" /></a>] <a href="https://github.com/VITA-Group/R-Sparse"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/VITA-Group/R-Sparse" /></a> </li>
<li><a href="http://arxiv.org/abs/2407.15176v3">ReAttention: Training-Free Infinite Context with Finite Attention Scope</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-ICLR-FF6B6B" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-ICLR-FF6B6B" /></a>] <a href="https://github.com/OpenMOSS/ReAttention"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/OpenMOSS/ReAttention" /></a> </li>
<li><a href="http://arxiv.org/abs/2410.20672v3">Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-ICLR-FF6B6B" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-ICLR-FF6B6B" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2410.05076v1">TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-ICLR-FF6B6B" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-ICLR-FF6B6B" /></a>] <a href="https://github.com/DerrickYLJ/TidalDecode"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/DerrickYLJ/TidalDecode" /></a> </li>
<li><a href="http://arxiv.org/abs/2408.14690v1">Training-Free Activation Sparsity in Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-ICLR-FF6B6B" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-ICLR-FF6B6B" /></a>] <a href="https://github.com/FasterDecoding/TEAL"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/FasterDecoding/TEAL" /></a> </li>
<li><a href="http://arxiv.org/abs/2502.12082">AdaSplash: Adaptive Sparse Flash Attention</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-ICML-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></a>] <a href="https://github.com/deep-spin/adasplash"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/deep-spin/adasplash" /></a> </li>
<li><a href="https://openreview.net/forum?id=YrCvW1Hx7g">BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-ICML-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></a>]  </li>
<li><a href="https://openreview.net/forum?id=u7dlwgKstN">CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-ICML-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2412.02252v1">Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-ICML-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></a>]  </li>
<li><a href="https://openreview.net/forum?id=Em2oaXd8Dc">HashAttention: Semantic Sparsity for Faster Inference</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-ICML-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></a>] <a href="https://github.com/xAlg-ai/HashAttention-1.0"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/xAlg-ai/HashAttention-1.0" /></a> </li>
<li><a href="http://arxiv.org/abs/2507.01299v1">La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-ICML-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></a>]  </li>
<li><a href="https://openreview.net/forum?id=me6PfbATWM">MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-ICML-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></a>]  </li>
<li><a href="https://openreview.net/forum?id=oa7MYAO6h6">ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-ICML-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></a>] <a href="https://github.com/bytedance/ShadowKV"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/bytedance/ShadowKV" /></a> </li>
<li><a href="http://arxiv.org/abs/2505.22689v1">SlimLLM: Accurate Structured Pruning for Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-ICML-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></a>]  </li>
<li><a href="https://openreview.net/forum?id=74c3Wwk8Tc">SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-ICML-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></a>] <a href="https://github.com/thu-ml/SpargeAttn"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/thu-ml/SpargeAttn" /></a> </li>
<li><a href="https://openreview.net/forum?id=SBUc5wirM8">Sparsing Law: Towards Large Language Models with Greater Activation Sparsity</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-ICML-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></a>] <a href="https://github.com/thunlp/SparsingLaw"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/thunlp/SparsingLaw" /></a> </li>
<li><a href="https://openreview.net/forum?id=QY7Au9nZwp">Star Attention: Efficient LLM Inference over Long Sequences</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-ICML-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></a>] <a href="https://github.com/NVIDIA/Star-Attention"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/NVIDIA/Star-Attention" /></a> </li>
<li><a href="http://arxiv.org/abs/2503.16428v1">XAttention: Block Sparse Attention with Antidiagonal Scoring</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-ICML-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></a>] <a href="https://github.com/mit-han-lab/x-attention"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mit-han-lab/x-attention" /></a> </li>
<li><a href="http://arxiv.org/abs/2507.16099v1">TorchAO: PyTorch-Native Training-to-Serving Model Optimization</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-ICML_Workshop-green" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML_Workshop-green" /></a>] <a href="https://github.com/pytorch/ao"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/pytorch/ao" /></a> </li>
<li><a href="https://dl.acm.org/doi/10.1145/3695053.3731064">AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-ISCA-9932CC" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-ISCA-9932CC" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2504.08850v1">SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-ISCA-9932CC" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-ISCA-9932CC" /></a>] <a href="https://github.com/infinigence/SpecEE"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/infinigence/SpecEE" /></a> </li>
<li><a href="http://arxiv.org/abs/2508.01261v1">Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-KDD_Workshop-green" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-KDD_Workshop-green" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2503.24000v1">Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-MLSys-DDA0DD" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-MLSys-DDA0DD" /></a>] <a href="https://github.com/LLMkvsys/rethink-kv-compression"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/LLMkvsys/rethink-kv-compression" /></a> </li>
<li><a href="http://arxiv.org/abs/2505.11254v1">Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-NeurIPS-FF1493" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-NeurIPS-FF1493" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2502.13189v1">MoBA: Mixture of Block Attention for Long-Context LLMs</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-NeurIPS-FF1493" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-NeurIPS-FF1493" /></a>] <a href="https://github.com/MoonshotAI/MoBA"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/MoonshotAI/MoBA" /></a> </li>
<li><a href="http://arxiv.org/abs/2505.11594v1">SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-NeurIPS-FF1493" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-NeurIPS-FF1493" /></a>] <a href="https://github.com/thu-ml/SageAttention"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/thu-ml/SageAttention" /></a> </li>
<li><a href="http://arxiv.org/abs/2503.09657v2">Týr-the-Pruner: Unlocking Accurate 50% Structural Pruning for LLMs via Global Sparsity Distribution Optimization</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-NeurIPS-FF1493" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-NeurIPS-FF1493" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2408.12757v2">NanoFlow: Towards Optimal Large Language Model Serving Throughput</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-OSDI-green" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-OSDI-green" /></a>] <a href="https://github.com/efeslab/Nanoflow"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/efeslab/Nanoflow" /></a> </li>
<li><a href="http://arxiv.org/abs/2501.09251v1">Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-PPoPP-green" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-PPoPP-green" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2407.12820v2">PQCache: Product Quantization-based KVCache for Long Context LLM Inference</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-SIGMOD-green" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-SIGMOD-green" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2505.24680v1">A Simple Linear Patch Revives Layer-Pruned Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2504.06319v1">Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2505.19578v1">Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2504.06949v1">Adaptive Computation Pruning for the Forgetting Transformer</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/zhixuan-lin/arctic-fox"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zhixuan-lin/arctic-fox" /></a> </li>
<li><a href="http://arxiv.org/abs/2503.23798v1">Adaptive Layer-skipping in Pre-trained LLMs</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2506.03762v1">AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2508.02128v1">Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2502.04077v1">AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2509.03054v1">Binary Quantization For LLMs Through Dynamic Grouping</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/johnnyzheng0636/WGM_bi_quan"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/johnnyzheng0636/WGM_bi_quan" /></a> </li>
<li><a href="http://arxiv.org/abs/2507.07145v1">CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2507.14392v1">Characterizing Communication Patterns in Distributed Large Language Model Inference</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2507.03114v1">Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2502.00299v1">ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2502.19811v3">Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/bytedance/flux"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/bytedance/flux" /></a> </li>
<li><a href="http://arxiv.org/abs/2502.16886v1">DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2501.17905v3">DReSS: Data-driven Regularized Structured Streamlining for Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2501.12948v1">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/deepseek-ai/DeepSeek-R1"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1" /></a> </li>
<li><a href="http://arxiv.org/abs/2507.19608v1">DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2507.14397v1">Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2502.11147v1">Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2509.08315v1">EvolKV: Evolutionary KV Cache Compression for LLM Inference</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2502.06766v2">Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/ryansynk/topk-decoding"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ryansynk/topk-decoding" /></a> </li>
<li><a href="http://arxiv.org/abs/2507.02754v1">Fast and Simplex: 2-Simplicial Attention in Triton</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2502.01068v1">FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/dongwonjo/FastKV"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/dongwonjo/FastKV" /></a> </li>
<li><a href="http://arxiv.org/abs/2509.07120v1">Faster VGGT with Block-Sparse Global Attention</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2508.18224v1">Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernel</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/Relaxed-System-Lab/Flash-Sparse-Attention"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Relaxed-System-Lab/Flash-Sparse-Attention" /></a> </li>
<li><a href="http://arxiv.org/abs/2501.01005v2">FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/flashinfer-ai/flashinfer"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/flashinfer-ai/flashinfer" /></a> </li>
<li><a href="http://arxiv.org/abs/2505.00570v2">FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2508.06471v1">GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/zai-org/GLM-4.5"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zai-org/GLM-4.5" /></a> </li>
<li><a href="http://arxiv.org/abs/2506.02572v1">HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/gpzlx1/HATA"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/gpzlx1/HATA" /></a> </li>
<li><a href="http://arxiv.org/abs/2507.19823v1">HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2505.21487v1">Hardware-Efficient Attention for Fast Decoding</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/Dao-AILab/grouped-latent-attention"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Dao-AILab/grouped-latent-attention" /></a> </li>
<li><a href="http://arxiv.org/abs/2507.07120v1">Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2503.20552v1">Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/ASISys/Adrenaline"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ASISys/Adrenaline" /></a> </li>
<li><a href="http://arxiv.org/abs/2501.02086v2">Instruction-Following Pruning for Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2508.06297v1">KV Cache Compression for Inference Efficiency in LLMs: A Review</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2502.16002v1">KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/UCSB-NLP-Chang/KVLink"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/UCSB-NLP-Chang/KVLink" /></a> </li>
<li><a href="http://arxiv.org/abs/2506.08018v1">KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2504.09936v1">KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2509.09754v1">LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/MGDDestiny/Lava"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/MGDDestiny/Lava" /></a> </li>
<li><a href="http://arxiv.org/abs/2502.14866v1">LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/mit-han-lab/omniserve"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mit-han-lab/omniserve" /></a> </li>
<li><a href="http://arxiv.org/abs/2508.02215v1">LeanK: Learnable K Cache Channel Pruning for Efficient Decoding</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/microsoft/MInference"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/MInference" /></a> </li>
<li><a href="http://arxiv.org/abs/2507.11507v1">MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2505.11432v2">MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2506.07900v1">MiniCPM4: Ultra-Efficient LLMs on End Devices</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/openbmb/minicpm"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/openbmb/minicpm" /></a> </li>
<li><a href="http://arxiv.org/abs/2501.08313v1">MiniMax-01: Scaling Foundation Models with Lightning Attention</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/MiniMax-AI/MiniMax-01"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/MiniMax-AI/MiniMax-01" /></a> </li>
<li><a href="http://arxiv.org/abs/2506.13585v1">MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/MiniMax-AI/MiniMax-M1"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/MiniMax-AI/MiniMax-M1" /></a> </li>
<li><a href="http://arxiv.org/abs/2507.11181v1">Mixture of Experts in Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2505.00315v1">Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/piotrpiekos/MoSA"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/piotrpiekos/MoSA" /></a> </li>
<li><a href="http://arxiv.org/abs/2507.10524v1">Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/raymin0223/mixture_of_recursions"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/raymin0223/mixture_of_recursions" /></a> </li>
<li><a href="http://arxiv.org/abs/2509.02512v1">MoPEQ: Mixture of Mixed Precision Quantized Experts</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/krishnateja95/MoE-Mixed-Prec"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/krishnateja95/MoE-Mixed-Prec" /></a> </li>
<li><a href="http://arxiv.org/abs/2504.06323v1">Mosaic: Composite Projection Pruning for Resource-efficient LLMs</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2509.04377v1">PagedEviction: Structured Block-wise KV Cache Pruning for Efficient Large Language Model Inference</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2504.07866v2">Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2503.03588v1">PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2503.00392v1">Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/ASISys/PSAttention"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ASISys/PSAttention" /></a> </li>
<li><a href="http://arxiv.org/abs/2506.02561v1">Pruning General Large Language Models into Customized Expert Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/zhaoyiran924/Custom-Prune"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zhaoyiran924/Custom-Prune" /></a> </li>
<li><a href="http://arxiv.org/abs/2506.22396v1">QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2505.09388v1">Qwen3 Technical Report</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/QwenLM/Qwen3"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/QwenLM/Qwen3" /></a> </li>
<li><a href="http://arxiv.org/abs/2505.24133v2">R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/Zefan-Cai/R-KV"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Zefan-Cai/R-KV" /></a> </li>
<li><a href="http://arxiv.org/abs/2506.19852v1">Radial Attention: <script type="math/tex">O(n\log n)</script> Sparse Attention with Energy Decay for Long Video Generation</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2506.04108v2">Rectified Sparse Attention</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/microsoft/unilm"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/unilm" /></a> </li>
<li><a href="http://arxiv.org/abs/2508.09001v1">Retrospective Sparse Attention for Efficient Long-Context Generation</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2501.16383v2">RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via Outlier-Aware Adaptive Rotations</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2505.24179v1">SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/BirdChristopher/SALE"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/BirdChristopher/SALE" /></a> </li>
<li><a href="http://arxiv.org/abs/2503.07605v1">SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/IAAR-Shanghai/SEAP"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/IAAR-Shanghai/SEAP" /></a> </li>
<li><a href="http://arxiv.org/abs/2506.08889v1">SeerAttention-R: Sparse Attention Adaptation for Long Reasoning</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/microsoft/SeerAttention"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/SeerAttention" /></a> </li>
<li><a href="http://arxiv.org/abs/2503.06433v1">Seesaw: High-throughput LLM Inference via Model Re-sharding</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2508.06447v1">SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2508.09834v1">Speed Always Wins: A Survey on Efficient Architectures for Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/weigao266/Awesome-Efficient-Arch"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/weigao266/Awesome-Efficient-Arch" /></a> </li>
<li><a href="http://arxiv.org/abs/2507.06517v1">SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/tyxqc/SpindleKV"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/tyxqc/SpindleKV" /></a> </li>
<li><a href="http://arxiv.org/abs/2507.19427v1">Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2501.15113v1">Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2504.17768v1">The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/PiotrNawrot/sparse-frontier"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/PiotrNawrot/sparse-frontier" /></a> </li>
<li><a href="http://arxiv.org/abs/2503.20313v3">TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/ByteDance-Seed/Triton-distributed"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ByteDance-Seed/Triton-distributed" /></a> </li>
<li><a href="http://arxiv.org/abs/2505.11329v1">TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/microsoft/tokenweave"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/tokenweave" /></a> </li>
<li><a href="http://arxiv.org/abs/2504.19442v3">Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/ByteDance-Seed/Triton-distributed"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ByteDance-Seed/Triton-distributed" /></a> </li>
<li><a href="http://arxiv.org/abs/2507.23279v1">Unveiling Super Experts in Mixture-of-Experts Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></a>] <a href="https://github.com/ZunhaiSu/Super-Experts-Profilling"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ZunhaiSu/Super-Experts-Profilling" /></a> </li>
<li><a href="https://github.com/RiseAI-Sys/attention-gym">Attention-Gym: Triton-Based Sparse and Quantization Attention</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-github-2F4F4F" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-github-2F4F4F" /></a>] <a href="https://github.com/RiseAI-Sys/attention-gym"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/RiseAI-Sys/attention-gym" /></a> </li>
<li><a href="https://github.com/deepseek-ai/DeepEP">DeepEP: an efficient expert-parallel communication library</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-github-2F4F4F" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-github-2F4F4F" /></a>] <a href="https://github.com/deepseek-ai/DeepEP"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/deepseek-ai/DeepEP" /></a> </li>
<li><a href="https://github.com/Zefan-Cai/KVCache-Factory">Unified KV Cache Compression Methods for Auto-Regressive Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-github-2F4F4F" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-github-2F4F4F" /></a>] <a href="https://github.com/Zefan-Cai/KVCache-Factory"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Zefan-Cai/KVCache-Factory" /></a> </li>
<li><a href="https://github.com/NVIDIA/kvpress">kvpress: LLM KV cache compression made easy</a> [<a class="glightbox" href="https://img.shields.io/badge/2025-github-2F4F4F" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2025-github-2F4F4F" /></a>] <a href="https://github.com/NVIDIA/kvpress"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/NVIDIA/kvpress" /></a> </li>
</ol>
<h4 id="2024">2024</h4>
<ol>
<li><a href="https://arxiv.org/abs/2312.11983">Fluctuation-based Adaptive Structured Pruning for Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-AAAI-FF4500" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-AAAI-FF4500" /></a>] <a href="https://github.com/CASIA-IVA-Lab/FLAP"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/CASIA-IVA-Lab/FLAP" /></a> </li>
<li><a href="http://arxiv.org/abs/2402.13718v3"><script type="math/tex">\infty</script>Bench: Extending Long Context Evaluation Beyond 100K Tokens</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-ACL-4169E1" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-ACL-4169E1" /></a>] <a href="https://github.com/OpenBMB/InfiniteBench"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/OpenBMB/InfiniteBench" /></a> </li>
<li><a href="http://arxiv.org/abs/2402.15220v4">ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-ACL-4169E1" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-ACL-4169E1" /></a>] <a href="https://github.com/microsoft/chunk-attention"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/chunk-attention" /></a> </li>
<li><a href="http://arxiv.org/abs/2308.14508v2">LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-ACL-4169E1" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-ACL-4169E1" /></a>]  </li>
<li><a href="https://dl.acm.org/doi/10.1145/3620666.3651379">Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-ASPLOS-9370DB" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-ASPLOS-9370DB" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2401.16677v1">T3: Transparent Tracking &amp; Triggering for Fine-grained Overlap of Compute &amp; Collectives</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-ASPLOS-9370DB" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-ASPLOS-9370DB" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2403.19708v3">Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-ATC-DC143C" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-ATC-DC143C" /></a>]  </li>
<li><a href="https://blog.shi-labs.com/distributed-gemm-88be6a481e2b">A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-Blog-696969" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-Blog-696969" /></a>] <a href="https://github.com/NVIDIA/cutlass"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/NVIDIA/cutlass" /></a> </li>
<li><a href="https://discuss.pytorch.org/t/distributed-w-torchtitan-introducing-async-tensor-parallelism-in-pytorch/209487/1">[Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-Blog-696969" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-Blog-696969" /></a>] <a href="https://github.com/pytorch/torchtitan"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/pytorch/torchtitan" /></a> </li>
<li><a href="http://arxiv.org/abs/2404.08763v4">CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-COLM-6495ED" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-COLM-6495ED" /></a>] <a href="https://github.com/ScalingIntelligence/CATS"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ScalingIntelligence/CATS" /></a> </li>
<li><a href="http://arxiv.org/abs/2407.18003v3">Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-COLM-6495ED" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-COLM-6495ED" /></a>] <a href="https://github.com/zcli-charlie/Awesome-KV-Cache"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zcli-charlie/Awesome-KV-Cache" /></a> </li>
<li><a href="http://arxiv.org/abs/2411.12692v1">SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-DATE-B22222" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-DATE-B22222" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2412.07174v1">Post-Training Statistical Calibration for Higher Activation Sparsity</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-ENLSP-00BFFF" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-ENLSP-00BFFF" /></a>] <a href="https://github.com/IntelLabs/SCAP"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/IntelLabs/SCAP" /></a> </li>
<li><a href="http://arxiv.org/abs/2306.11695">A Simple and Effective Pruning Approach for Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-ICLR-FF6B6B" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-ICLR-FF6B6B" /></a>] <a href="https://github.com/locuslab/wanda"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/locuslab/wanda" /></a> </li>
<li><a href="http://arxiv.org/abs/2310.01382v2">Compressing LLMs: The Truth is Rarely Pure and Never Simple</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-ICLR-FF6B6B" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-ICLR-FF6B6B" /></a>] <a href="https://github.com/VITA-Group/llm-kick"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/VITA-Group/llm-kick" /></a> </li>
<li><a href="http://arxiv.org/abs/2310.08915v3">Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-ICLR-FF6B6B" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-ICLR-FF6B6B" /></a>] <a href="https://github.com/zyxxmu/DSnoT"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zyxxmu/DSnoT" /></a> </li>
<li><a href="http://arxiv.org/abs/2309.17453v4">Efficient Streaming Language Models with Attention Sinks</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-ICLR-FF6B6B" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-ICLR-FF6B6B" /></a>] <a href="https://github.com/mit-han-lab/streaming-llm"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mit-han-lab/streaming-llm" /></a> </li>
<li><a href="https://arxiv.org/abs/2307.08691">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-ICLR-FF6B6B" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-ICLR-FF6B6B" /></a>] <a href="https://github.com/Dao-AILab/flash-attention"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Dao-AILab/flash-attention" /></a> </li>
<li><a href="https://openreview.net/forum?id=Tr0lPx9woF">Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-ICLR-FF6B6B" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-ICLR-FF6B6B" /></a>] <a href="https://github.com/biomedical-cybernetics/Relative-importance-and-activation-pruning"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/biomedical-cybernetics/Relative-importance-and-activation-pruning" /></a> </li>
<li><a href="https://arxiv.org/abs/2309.14717">QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-ICLR-FF6B6B" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-ICLR-FF6B6B" /></a>] <a href="https://github.com/yuhuixu1993/qa-lora"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/yuhuixu1993/qa-lora" /></a> </li>
<li><a href="https://openreview.net/forum?id=vZfi5to2Xl">SAS: Structured Activation Spasification</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-ICLR-FF6B6B" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-ICLR-FF6B6B" /></a>] <a href="https://github.com/DensoITLab/sas_"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/DensoITLab/sas_" /></a> </li>
<li><a href="http://arxiv.org/abs/2401.15024v2">SliceGPT: Compress Large Language Models by Deleting Rows and Columns</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-ICLR-FF6B6B" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-ICLR-FF6B6B" /></a>] <a href="https://github.com/microsoft/TransformerCompression"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/TransformerCompression" /></a> </li>
<li><a href="https://arxiv.org/abs/2310.04564">ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-ICLR_oral-green" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-ICLR_oral-green" /></a>] <a href="https://github.com/sjtu-ipads/powerinfer"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/sjtu-ipads/powerinfer" /></a> </li>
<li><a href="http://arxiv.org/abs/2404.01847v2">Accelerating Transformer Pre-training with 2:4 Sparsity</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-ICML-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></a>] <a href="https://github.com/huyz2023/2by4-pretrain"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/huyz2023/2by4-pretrain" /></a> </li>
<li><a href="http://arxiv.org/abs/2401.15077v2">EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-ICML-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></a>] <a href="https://github.com/SafeAILab/EAGLE"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/SafeAILab/EAGLE" /></a> </li>
<li><a href="http://arxiv.org/abs/2403.06082v1">FrameQuant: Flexible Low-Bit Quantization for Transformers</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-ICML-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></a>] <a href="https://github.com/vsingh-group/FrameQuant"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/vsingh-group/FrameQuant" /></a> </li>
<li><a href="http://arxiv.org/abs/2402.02750v2">KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-ICML-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></a>] <a href="https://github.com/jy-yuan/KIVI"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/jy-yuan/KIVI" /></a> </li>
<li><a href="http://arxiv.org/abs/2402.12354v1">LoRA+: Efficient Low Rank Adaptation of Large Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-ICML-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></a>] <a href="https://github.com/nikhil-ghosh-berkeley/loraplus"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/nikhil-ghosh-berkeley/loraplus" /></a> </li>
<li><a href="http://arxiv.org/abs/2403.12983v1">OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-ICML-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></a>] <a href="https://github.com/mazumder-lab/OSSCAR"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mazumder-lab/OSSCAR" /></a> </li>
<li><a href="https://arxiv.org/pdf/2310.05175.pdf">Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-ICML-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></a>] <a href="https://github.com/luuyin/OWL"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/luuyin/OWL" /></a> </li>
<li><a href="http://arxiv.org/abs/2406.10774">Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-ICML-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></a>] <a href="https://github.com/mit-han-lab/quest"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mit-han-lab/quest" /></a> </li>
<li><a href="http://arxiv.org/abs/2405.16057v1">SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-ICML-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></a>] <a href="https://github.com/Lucky-Lance/SPP"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Lucky-Lance/SPP" /></a> </li>
<li><a href="http://arxiv.org/abs/2312.04985v5">SparQ Attention: Bandwidth-Efficient LLM Inference</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-ICML-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2312.11875v3">Sparse is Enough in Fine-tuning Pre-trained Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-ICML-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></a>] <a href="https://github.com/song-wx/SIFT"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/song-wx/SIFT" /></a> </li>
<li><a href="http://arxiv.org/abs/2303.11525v3">Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-ICML-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></a>] <a href="https://github.com/CerebrasResearch/Sparse-IFT"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/CerebrasResearch/Sparse-IFT" /></a> </li>
<li><a href="http://arxiv.org/abs/2306.07629">SqueezeLLM: Dense-and-Sparse Quantization</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-ICML-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></a>] <a href="https://github.com/SqueezeAILab/SqueezeLLM"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/SqueezeAILab/SqueezeLLM" /></a> </li>
<li><a href="http://arxiv.org/abs/2307.09988v2">TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-ICML-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></a>] <a href="https://github.com/theyoungkwon/TinyTrain"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/theyoungkwon/TinyTrain" /></a> </li>
<li><a href="http://arxiv.org/abs/2403.08477v3">Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-ICML-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></a>] <a href="https://github.com/szc12153/sparse_interpolated_experts"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/szc12153/sparse_interpolated_experts" /></a> </li>
<li><a href="http://arxiv.org/abs/2405.17381v2">Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-ICML-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></a>] <a href="https://github.com/OpenNLPLab/TransnormerLLM"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/OpenNLPLab/TransnormerLLM" /></a> </li>
<li><a href="http://arxiv.org/abs/2311.18677v2">Splitwise: Efficient generative LLM inference using phase splitting</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-ISCA-9932CC" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-ISCA-9932CC" /></a>] <a href="https://github.com/Mutinifni/splitwise-sim"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Mutinifni/splitwise-sim" /></a> </li>
<li><a href="https://arxiv.org/abs/2306.00978">AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-MLSys-DDA0DD" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-MLSys-DDA0DD" /></a>] <a href="https://github.com/mit-han-lab/llm-awq"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mit-han-lab/llm-awq" /></a> </li>
<li><a href="http://arxiv.org/abs/2405.05465v2">Vidur: A Large-Scale Simulation Framework For LLM Inference</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-MLSys-DDA0DD" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-MLSys-DDA0DD" /></a>] <a href="https://github.com/microsoft/vidur"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/vidur" /></a> </li>
<li><a href="http://arxiv.org/abs/2401.18079">KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-NeurIPS-FF1493" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-NeurIPS-FF1493" /></a>] <a href="https://github.com/SqueezeAILab/KVQuant"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/SqueezeAILab/KVQuant" /></a> </li>
<li><a href="http://arxiv.org/abs/2407.02490v1">MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-NeurIPS-FF1493" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-NeurIPS-FF1493" /></a>] <a href="https://github.com/microsoft/MInference"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/MInference" /></a> </li>
<li><a href="http://arxiv.org/abs/2409.17481v1">MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-NeurIPS-FF1493" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-NeurIPS-FF1493" /></a>] <a href="https://github.com/NVlabs/MaskLLM"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/NVlabs/MaskLLM" /></a> </li>
<li><a href="http://arxiv.org/abs/2312.07104v2">SGLang: Efficient Execution of Structured Language Model Programs</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-NeurIPS-FF1493" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-NeurIPS-FF1493" /></a>] <a href="https://github.com/sgl-project/sglang"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/sgl-project/sglang" /></a> </li>
<li><a href="http://arxiv.org/abs/2412.18110v1">SlimGPT: Layer-wise Structured Pruning for Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-NeurIPS-FF1493" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-NeurIPS-FF1493" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2402.17946v3">SparseLLM: Towards Global Pruning for Pre-trained Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-NeurIPS-FF1493" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-NeurIPS-FF1493" /></a>] <a href="https://github.com/BaiTheBest/SparseLLM"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/BaiTheBest/SparseLLM" /></a> </li>
<li><a href="http://arxiv.org/abs/2405.14256v1">ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-NeurIPS-FF1493" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-NeurIPS-FF1493" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2401.02938v2">Fast and Effective Weight Update for Pruned Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-TMLR-20B2AA" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-TMLR-20B2AA" /></a>] <a href="https://github.com/fmfi-compbio/admm-pruning"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/fmfi-compbio/admm-pruning" /></a> </li>
<li><a href="https://arxiv.org/abs/2309.10285">Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-VLDB-A52A2A" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-VLDB-A52A2A" /></a>] <a href="https://github.com/AlibabaResearch/flash-llm"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/AlibabaResearch/flash-llm" /></a> </li>
<li><a href="http://arxiv.org/abs/2404.14294v2">A Survey on Efficient Inference for Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2412.14219v2">A Survey on Inference Optimization Techniques for Mixture of Experts Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/MoE-Inf/awesome-moe-inference"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/MoE-Inf/awesome-moe-inference" /></a> </li>
<li><a href="http://arxiv.org/abs/2412.19442v2">A Survey on Large Language Model Acceleration based on KV Cache Management</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/TreeAI-Lab/Awesome-KV-Cache-Management" /></a> </li>
<li><a href="http://arxiv.org/abs/2411.17651v2">APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/microsoft/apex_plus"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/apex_plus" /></a> </li>
<li><a href="http://arxiv.org/abs/2411.02117v1">AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2407.11550v3">Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/FFY0/AdaKV"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/FFY0/AdaKV" /></a> </li>
<li><a href="http://arxiv.org/abs/2410.16135v1">Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2407.12866v1">Beyond KV Caching: Shared Attention for Efficient LLMs</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/metacarbon/shareAtt"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/metacarbon/shareAtt" /></a> </li>
<li><a href="https://arxiv.org/abs/2408.11796v2">Compact Language Models via Pruning and Knowledge Distillation</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/NVlabs/Minitron"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/NVlabs/Minitron" /></a> </li>
<li><a href="http://arxiv.org/abs/2410.18311v1">CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/wangqinsi1/CoreInfer"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/wangqinsi1/CoreInfer" /></a> </li>
<li><a href="http://arxiv.org/abs/2405.04434v5">DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/deepseek-ai/DeepSeek-V2"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2" /></a> </li>
<li><a href="http://arxiv.org/abs/2412.19437v1">DeepSeek-V3 Technical Report</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/deepseek-ai/DeepSeek-V3"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3" /></a> </li>
<li><a href="http://arxiv.org/abs/2401.06066v1">DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/deepseek-ai/DeepSeek-MoE"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/deepseek-ai/DeepSeek-MoE" /></a> </li>
<li><a href="http://arxiv.org/abs/2409.15241v1">Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/deepspeedai/DeepSpeedExamples"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/deepspeedai/DeepSpeedExamples" /></a> </li>
<li><a href="http://arxiv.org/abs/2410.10819v1">DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/mit-han-lab/duo-attention"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mit-han-lab/duo-attention" /></a> </li>
<li><a href="http://arxiv.org/abs/2405.03594v1">Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/neuralmagic/nm-vllm"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/neuralmagic/nm-vllm" /></a> </li>
<li><a href="https://arxiv.org/abs/2402.05406">Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/ldery/Bonsai"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ldery/Bonsai" /></a> </li>
<li><a href="http://arxiv.org/abs/2406.06858v5">FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/bytedance/flux"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/bytedance/flux" /></a> </li>
<li><a href="http://arxiv.org/abs/2410.01359v1">FlashMask: Efficient and Rich Mask Extension of FlashAttention</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/PaddlePaddle/PaddleNLP"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/PaddlePaddle/PaddleNLP" /></a> </li>
<li><a href="http://arxiv.org/abs/2403.05527v4">GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/HaoKang-Timmy/GEAR"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/HaoKang-Timmy/GEAR" /></a> </li>
<li><a href="http://arxiv.org/abs/2401.02669v2">Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>]  </li>
<li><a href="https://arxiv.org/abs/2402.04902">L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2403.17919v1">LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2407.12391v1">LLM Inference Serving: Survey of Recent Advances and Opportunities</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2407.14057v1">LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2401.04658v2">Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/OpenNLPLab/lightning-attention"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/OpenNLPLab/lightning-attention" /></a> </li>
<li><a href="http://arxiv.org/abs/2402.17762v2">Massive Activations in Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/locuslab/massive-activations"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/locuslab/massive-activations" /></a> </li>
<li><a href="http://arxiv.org/abs/2405.14366v2">MiniCache: KV Cache Compression in Depth Dimension for Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/AkideLiu/MiniCache"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/AkideLiu/MiniCache" /></a> </li>
<li><a href="http://arxiv.org/abs/2411.18077">MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/akshatsh49/MiniKV-Dev"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/akshatsh49/MiniKV-Dev" /></a> </li>
<li><a href="http://arxiv.org/abs/2404.02258v1">Mixture-of-Depths: Dynamically allocating compute in transformer-based language models</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2406.14909v2">MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/thu-nics/MoA"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/thu-nics/MoA" /></a> </li>
<li><a href="http://arxiv.org/abs/2412.19255v2">Multi-matrix Factorization Attention</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2402.18096v1">No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2409.01366v1">Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://anonymous.4open.science/r/CHESS-BA40/README.md">Pytorch</a> </li>
<li><a href="http://arxiv.org/abs/2408.07092v2">Post-Training Sparse Attention with Double Sparsity</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/andy-yang-1/DoubleSparse"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/andy-yang-1/DoubleSparse" /></a> </li>
<li><a href="http://arxiv.org/abs/2406.06282v2">PowerInfer-2: Fast Large Language Model Inference on a Smartphone</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://powerinfer.ai/v2/">Website</a> </li>
<li><a href="http://arxiv.org/abs/2410.05265v2">PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language Models Quantization</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/ChenMnZ/PrefixQuant"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ChenMnZ/PrefixQuant" /></a> </li>
<li><a href="https://arxiv.org/abs/2402.13516">ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/raincleared-song/sparse_gpu_operator"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/raincleared-song/sparse_gpu_operator" /></a> </li>
<li><a href="http://arxiv.org/abs/2407.10969v1">Q-Sparse: All Large Language Models can be Fully Sparsely-Activated</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2405.04532v2">QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://hanlab.mit.edu/projects/qserve">Pytorch</a> </li>
<li><a href="https://arxiv.org/abs/2402.03804">ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2412.14711v1">ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/thu-ml/ReMoE"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/thu-ml/ReMoE" /></a> </li>
<li><a href="http://arxiv.org/abs/2411.05787v1">Recycled Attention: Efficient inference for long-context language models</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/carriex/recycled-attention"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/carriex/recycled-attention" /></a> </li>
<li><a href="http://arxiv.org/abs/2405.12981v1">Reducing Transformer Key-Value Cache Size with Cross-Layer Attention</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/JerryYin777/Cross-Layer-Attention"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/JerryYin777/Cross-Layer-Attention" /></a> </li>
<li><a href="http://arxiv.org/abs/2402.11592v2">Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/ZO-Bench/ZO-LLM"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ZO-Bench/ZO-LLM" /></a> </li>
<li><a href="http://arxiv.org/abs/2412.10319v2">SCBench: A KV Cache-Centric Analysis of Long-Context Methods</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/microsoft/MInference"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/MInference" /></a> </li>
<li><a href="http://arxiv.org/abs/2411.10958v6">SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/thu-ml/SageAttention"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/thu-ml/SageAttention" /></a> </li>
<li><a href="http://arxiv.org/abs/2410.02367v8">SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/thu-ml/SageAttention"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/thu-ml/SageAttention" /></a> </li>
<li><a href="http://arxiv.org/abs/2406.15486v2">SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2410.13276v2">SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/microsoft/SeerAttention"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/SeerAttention" /></a> </li>
<li><a href="http://arxiv.org/abs/2406.16635v1">ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/abdelfattah-lab/shadow_llm"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/abdelfattah-lab/shadow_llm" /></a> </li>
<li><a href="http://arxiv.org/abs/2404.14469v2">SnapKV: LLM Knows What You are Looking for Before Generation</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/FasterDecoding/SnapKV"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/FasterDecoding/SnapKV" /></a> </li>
<li><a href="http://arxiv.org/abs/2401.06104v2">Transformers are Multi-State RNNs</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/schwartz-lab-NLP/TOVA"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/schwartz-lab-NLP/TOVA" /></a> </li>
<li><a href="http://arxiv.org/abs/2406.05955v2">Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://huggingface.co/PowerInfer">Pytorch</a> </li>
<li><a href="http://arxiv.org/abs/2411.15100v2">XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>] <a href="https://github.com/mlc-ai/xgrammar"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mlc-ai/xgrammar" /></a> </li>
<li><a href="http://arxiv.org/abs/2412.09036v1">ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2410.08584v2">ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification</a> [<a class="glightbox" href="https://img.shields.io/badge/2024-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></a>]  </li>
</ol>
<h4 id="2023">2023</h4>
<ol>
<li><a href="https://arxiv.org/abs/2210.11794">Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-AAAI-FF4500" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-AAAI-FF4500" /></a>] <a href="https://github.com/asFeng/Diffuser"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/asFeng/Diffuser" /></a> </li>
<li><a href="https://arxiv.org/abs/2212.07634">Gradient-based Intra-attention Pruning on Pre-trained Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-ACL-4169E1" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-ACL-4169E1" /></a>] <a href="https://github.com/airaria/GRAIN"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/airaria/GRAIN" /></a> </li>
<li><a href="https://aclanthology.org/2023.acl-long.35.pdf">Pruning Pre-trained Language Models Without Fine-Tuning</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-ACL-4169E1" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-ACL-4169E1" /></a>] <a href="https://github.com/kongds/SMP"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/kongds/SMP" /></a> </li>
<li><a href="https://aclanthology.org/2023.findings-acl.573/">Pruning Pre-trained Language Models with Principled Importance and Self-regularization</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-ACL-4169E1" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-ACL-4169E1" /></a>] <a href="https://github.com/drsy/pins"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/drsy/pins" /></a> </li>
<li><a href="https://aclanthology.org/2023.findings-acl.692.pdf">Structured Pruning for Efficient Generative Pre-trained Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-ACL-4169E1" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-ACL-4169E1" /></a>]  </li>
<li><a href="https://dl.acm.org/doi/abs/10.1145/3567955.3567959">Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-ASPLOS-9370DB" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-ASPLOS-9370DB" /></a>]  </li>
<li><a href="https://openreview.net/forum?id=SHlZcInS6C">Structural Pruning of Large Language Models via Neural Architecture Search</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-AutoML_Workshop-A9A9A9" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-AutoML_Workshop-A9A9A9" /></a>] <a href="https://github.com/awslabs/syne-tune"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/awslabs/syne-tune" /></a> </li>
<li><a href="https://arxiv.org/abs/2303.17605">SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-CVPR-2E8B57" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-CVPR-2E8B57" /></a>] <a href="https://github.com/mit-han-lab/sparsevit"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mit-han-lab/sparsevit" /></a> </li>
<li><a href="https://openaccess.thecvf.com/content/CVPR2023W/WAD/papers/Tang_TorchSparse_Efficient_Point_Cloud_Engine_CVPRW_2023_paper.pdf">TorchSparse++: Efficient Point Cloud Engine</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-CVPR_workshop-green" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-CVPR_workshop-green" /></a>] <a href="https://github.com/mit-han-lab/torchsparse"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mit-han-lab/torchsparse" /></a> </li>
<li><a href="https://arxiv.org/pdf/2303.10512.pdf">AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-ICLR-FF6B6B" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-ICLR-FF6B6B" /></a>] <a href="https://github.com/QingruZhang/AdaLoRA"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/QingruZhang/AdaLoRA" /></a> </li>
<li><a href="https://arxiv.org/pdf/2210.17323.pdf">GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-ICLR-FF6B6B" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-ICLR-FF6B6B" /></a>] <a href="https://github.com/IST-DASLab/gptq"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/IST-DASLab/gptq" /></a> </li>
<li><a href="https://openreview.net/pdf?id=vuD2xEtxZcj">Minimum Variance Unbiased N:M Sparsity for the Neural Gradients</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-ICLR-FF6B6B" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-ICLR-FF6B6B" /></a>]  </li>
<li><a href="https://openreview.net/forum?id=TJ2nxciYCk-">The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-ICLR-FF6B6B" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-ICLR-FF6B6B" /></a>]  </li>
<li><a href="https://openreview.net/forum?id=wIPIhHd00i">Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-ICML-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-ICML-FF8C00" /></a>] <a href="https://github.com/FMInference/DejaVu"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/FMInference/DejaVu" /></a> </li>
<li><a href="https://arxiv.org/pdf/2301.00774.pdf">SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot.</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-ICML-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-ICML-FF8C00" /></a>] <a href="https://github.com/IST-DASLab/sparsegpt"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/IST-DASLab/sparsegpt" /></a> </li>
<li><a href="https://arxiv.org/abs/2306.11222">Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-ICML-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-ICML-FF8C00" /></a>] <a href="https://github.com/yxli2123/LoSparse"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/yxli2123/LoSparse" /></a> </li>
<li><a href="https://proceedings.mlsys.org/paper_files/paper/2023/file/a10deb4d5227a8ea307ea8ff3cb712f4-Paper-mlsys2023.pdf">Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-MLSys-DDA0DD" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-MLSys-DDA0DD" /></a>] <a href="https://github.com/microsoft/SparTA"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/SparTA" /></a> </li>
<li><a href="https://openreview.net/pdf?id=bPFFPueAxm">ZipLM: Inference-Aware Structured Pruning of Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-NeurIPS-FF1493" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-NeurIPS-FF1493" /></a>] <a href="https://github.com/IST-DASLab/ZipLM"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/IST-DASLab/ZipLM" /></a> </li>
<li><a href="http://arxiv.org/abs/2310.02065v1">VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-SC-CD5C5C" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-SC-CD5C5C" /></a>] <a href="https://github.com/UDC-GAC/venom"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/UDC-GAC/venom" /></a> </li>
<li><a href="http://arxiv.org/abs/2309.06180v1">Efficient Memory Management for Large Language Model Serving with PagedAttention</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-SOSP-8A2BE2" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-SOSP-8A2BE2" /></a>] <a href="https://github.com/vllm-project/vllm"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/vllm-project/vllm" /></a> </li>
<li><a href="https://arxiv.org/abs/2209.00099">Efficient Methods for Natural Language Processing: A Survey</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-TACL-87CEEB" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-TACL-87CEEB" /></a>]  </li>
<li><a href="https://arxiv.org/abs/2303.10464">SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-UAI-green" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-UAI-green" /></a>]  </li>
<li><a href="https://arxiv.org/abs/2307.03109">A Survey on Evaluation of Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></a>]  </li>
<li><a href="https://arxiv.org/abs/2308.07633">A Survey on Model Compression for Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2311.04902v2">Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></a>] <a href="https://github.com/VILA-Lab/GBLM-Pruner"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/VILA-Lab/GBLM-Pruner" /></a> </li>
<li><a href="http://arxiv.org/abs/2303.17568v2">CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></a>] <a href="https://github.com/THUDM/CodeGeeX"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/THUDM/CodeGeeX" /></a> </li>
<li><a href="https://arxiv.org/abs/2310.05015">Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></a>] <a href="https://github.com/microsoft/Moonlit"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/Moonlit" /></a> </li>
<li><a href="https://arxiv.org/abs/2305.15805">Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></a>] <a href="https://github.com/sanagno/adaptively_sparse_attention"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/sanagno/adaptively_sparse_attention" /></a> </li>
<li><a href="http://arxiv.org/abs/2307.09702v4">Efficient Guided Generation for Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2305.17333v3">Fine-Tuning Language Models with Just Forward Passes</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></a>] <a href="https://github.com/princeton-nlp/MeZO"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/princeton-nlp/MeZO" /></a> </li>
<li><a href="https://crfm.stanford.edu/2023/10/12/flashdecoding.html">Flash-Decoding for long-context inference</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></a>]  </li>
<li><a href="https://arxiv.org/abs/2303.04185">Gradient-Free Structured Pruning with Unlabeled Data</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2306.14048">H<script type="math/tex">_2</script>O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></a>] <a href="https://github.com/FMInference/H2O"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/FMInference/H2O" /></a> </li>
<li><a href="https://arxiv.org/abs/2308.03449">Knowledge-preserving Pruning for Pre-trained Language Models without Retraining</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2312.11514">LLM in a flash: Efficient Large Language Model Inference with Limited Memory</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2305.11627v3">LLM-Pruner: On the Structural Pruning of Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></a>] <a href="https://github.com/horseee/LLM-Pruner"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/horseee/LLM-Pruner" /></a> </li>
<li><a href="https://arxiv.org/abs/2310.18356">LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></a>]  </li>
<li><a href="https://arxiv.org/abs/2310.08659">LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></a>] <a href="https://github.com/yxli2123/LoftQ"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/yxli2123/LoftQ" /></a> </li>
<li><a href="https://arxiv.org/abs/2308.13137">OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></a>] <a href="https://github.com/OpenGVLab/OmniQuant"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/OpenGVLab/OmniQuant" /></a> </li>
<li><a href="https://arxiv.org/pdf/2201.11113.pdf">Post-training Quantization for Neural Networks with Provable Guarantees</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></a>] <a href="https://github.com/YixuanSeanZhou/Quantized_Neural_Nets"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/YixuanSeanZhou/Quantized_Neural_Nets" /></a> </li>
<li><a href="http://arxiv.org/abs/2312.12456v1">PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></a>] <a href="https://github.com/SJTU-IPADS/PowerInfer"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/SJTU-IPADS/PowerInfer" /></a> </li>
<li><a href="https://arxiv.org/abs/2309.09507">Pruning Large Language Models via Accuracy Predictor</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></a>]  </li>
<li><a href="https://arxiv.org/abs/2305.14314">QLoRA: Efficient Finetuning of Quantized LLMs</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></a>] <a href="https://github.com/artidoro/qlora"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/artidoro/qlora" /></a> </li>
<li><a href="https://arxiv.org/pdf/2307.13304.pdf">QuIP: Quantization with Incoherence Processing</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></a>] <a href="https://github.com/jerry-chee/QuIP"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/jerry-chee/QuIP" /></a> </li>
<li><a href="https://arxiv.org/pdf/2304.01089.pdf">RPTQ: Reorder-based Post-training Quantization for Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></a>] <a href="https://github.com/hahnyuan/RPTQ4LLM"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/hahnyuan/RPTQ4LLM" /></a> </li>
<li><a href="https://xiamengzhou.github.io/sheared-llama/">Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></a>] <a href="https://github.com/princeton-nlp/LLM-Shearing"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/princeton-nlp/LLM-Shearing" /></a> </li>
<li><a href="https://arxiv.org/pdf/2306.03078.pdf">SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></a>] <a href="https://github.com/Vahe1994/SpQR"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Vahe1994/SpQR" /></a> </li>
<li><a href="https://arxiv.org/pdf/2310.06927.pdf">Sparse Fine-tuning for Inference Acceleration of Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></a>] <a href="https://github.com/IST-DASLab/SparseFinetuning"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/IST-DASLab/SparseFinetuning" /></a> </li>
<li><a href="https://arxiv.org/abs/2303.11525">Sparse Iso-FLOP Transformations for Maximizing Training Efficiency</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></a>] <a href="https://github.com/CerebrasResearch/Sparse-IFT"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/CerebrasResearch/Sparse-IFT" /></a> </li>
<li><a href="https://arxiv.org/abs/2306.16788">Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></a>] <a href="https://github.com/ZIB-IOL/SMS"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ZIB-IOL/SMS" /></a> </li>
<li><a href="https://arxiv.org/abs/2302.02596">Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></a>]  </li>
<li><a href="https://arxiv.org/abs/2306.03805">The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></a>] <a href="https://github.com/VITA-Group/essential_sparsity"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/VITA-Group/essential_sparsity" /></a> </li>
<li><a href="https://arxiv.org/abs//2306.11987">Training Transformers with 4-bit Integers</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></a>] <a href="https://github.com/xijiu9/Train_Transformers_with_INT4"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/xijiu9/Train_Transformers_with_INT4" /></a> </li>
<li><a href="https://arxiv.org/abs/2304.12102">Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></a>] <a href="https://github.com/liyucheng09/Selective_Context"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/liyucheng09/Selective_Context" /></a> </li>
<li><a href="https://arxiv.org/abs/2303.08302">ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></a>] <a href="https://github.com/microsoft/DeepSpeed"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/DeepSpeed" /></a> </li>
<li><a href="https://github.com/NVIDIA/FasterTransformer">FasterTransformer</a> [<a class="glightbox" href="https://img.shields.io/badge/2023-github-2F4F4F" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2023-github-2F4F4F" /></a>] <a href="https://github.com/NVIDIA/FasterTransformer"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/NVIDIA/FasterTransformer" /></a> </li>
</ol>
<h4 id="2022">2022</h4>
<ol>
<li><a href="https://aclanthology.org/2022.acl-long.16/">Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm</a> [<a class="glightbox" href="https://img.shields.io/badge/2022-ACL-4169E1" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2022-ACL-4169E1" /></a>] <a href="https://github.com/shaoyiHusky/SparseProgressiveDistillation"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/shaoyiHusky/SparseProgressiveDistillation" /></a> </li>
<li><a href="https://arxiv.org/abs/2203.15996">TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2022-ACL-4169E1" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2022-ACL-4169E1" /></a>] <a href="https://github.com/airaria/TextPruner"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/airaria/TextPruner" /></a> </li>
<li><a href="http://arxiv.org/abs/2105.05720v5">Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads</a> [<a class="glightbox" href="https://img.shields.io/badge/2022-ASPLOS-9370DB" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2022-ASPLOS-9370DB" /></a>] <a href="https://github.com/parasailteam/coconet"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/parasailteam/coconet" /></a> </li>
<li><a href="https://www.cerebras.net/blog/creating-sparse-gpt-3-models-with-iterative-pruning">Creating Sparse GPT-3 Models with Iterative Pruning</a> [<a class="glightbox" href="https://img.shields.io/badge/2022-Blog-696969" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2022-Blog-696969" /></a>]  </li>
<li><a href="https://arxiv.org/abs/2106.09685">LoRA: Low-rank adaptation of large language models</a> [<a class="glightbox" href="https://img.shields.io/badge/2022-ICLR-FF6B6B" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2022-ICLR-FF6B6B" /></a>] <a href="https://github.com/microsoft/LoRA"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/LoRA" /></a> </li>
<li><a href="https://arxiv.org/abs/2201.13096">SPDY: Accurate Pruning with Speedup Guarantees</a> [<a class="glightbox" href="https://img.shields.io/badge/2022-ICML-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2022-ICML-FF8C00" /></a>] <a href="https://github.com/IST-DASLab/spdy"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/IST-DASLab/spdy" /></a> </li>
<li><a href="https://arxiv.org/abs/2209.00606">Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation</a> [<a class="glightbox" href="https://img.shields.io/badge/2022-MICRO-BA55D3" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2022-MICRO-BA55D3" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2204.09656v2">A Fast Post-Training Pruning Framework for Transformers</a> [<a class="glightbox" href="https://img.shields.io/badge/2022-NeurIPS-FF1493" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2022-NeurIPS-FF1493" /></a>] <a href="https://github.com/WoosukKwon/retraining-free-pruning"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/WoosukKwon/retraining-free-pruning" /></a> </li>
<li><a href="https://arxiv.org/abs/2205.14135">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a> [<a class="glightbox" href="https://img.shields.io/badge/2022-NeurIPS-FF1493" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2022-NeurIPS-FF1493" /></a>] <a href="https://github.com/Dao-AILab/flash-attention"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Dao-AILab/flash-attention" /></a> </li>
<li><a href="https://openreview.net/pdf?id=ksVGCOlOEba">Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning</a> [<a class="glightbox" href="https://img.shields.io/badge/2022-NeurIPS-FF1493" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2022-NeurIPS-FF1493" /></a>] <a href="https://github.com/IST-DASLab/OBC"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/IST-DASLab/OBC" /></a> </li>
<li><a href="https://openreview.net/forum?id=f-fVCElZ-G1">ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers</a> [<a class="glightbox" href="https://img.shields.io/badge/2022-NeurIPS-FF1493" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2022-NeurIPS-FF1493" /></a>] <a href="https://github.com/microsoft/DeepSpeed"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/DeepSpeed" /></a> </li>
<li><a href="https://iopscience.iop.org/article/10.1088/2634-4386/ac7c8a">Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks</a> [<a class="glightbox" href="https://img.shields.io/badge/2022-Neuromorphic_Computing_and_Engineering-40E0D0" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2022-Neuromorphic_Computing_and_Engineering-40E0D0" /></a>]  </li>
<li><a href="http://arxiv.org/abs/2110.11299v1">Transformer Acceleration with Dynamic Sparse Attention</a> [<a class="glightbox" href="https://img.shields.io/badge/2022-TC-F08080" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2022-TC-F08080" /></a>]  </li>
<li><a href="https://arxiv.org/abs/2208.06118">An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers</a> [<a class="glightbox" href="https://img.shields.io/badge/2022-VLSI-8B0000" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2022-VLSI-8B0000" /></a>]  </li>
<li><a href="https://arxiv.org/pdf/2203.07259.pdf">The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models</a> [<a class="glightbox" href="https://img.shields.io/badge/2022-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2022-arXiv-1E88E5" /></a>] <a href="https://github.com/neuralmagic/sparseml"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/neuralmagic/sparseml" /></a> </li>
</ol>
<h4 id="2021">2021</h4>
<ol>
<li><a href="https://openaccess.thecvf.com/content/ICCV2021W/LPCV/papers/Lazarevich_Post-Training_Deep_Neural_Network_Pruning_via_Layer-Wise_Calibration_ICCVW_2021_paper.pdf">Post-training deep neural network pruning via layer-wise calibration</a> [<a class="glightbox" href="https://img.shields.io/badge/2021-ICCV-green" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2021-ICCV-green" /></a>]  </li>
<li><a href="https://openreview.net/pdf?id=POWv6hDd9XH">BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction</a> [<a class="glightbox" href="https://img.shields.io/badge/2021-ICLR-FF6B6B" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2021-ICLR-FF6B6B" /></a>] <a href="https://github.com/yhhhli/BRECQ"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/yhhhli/BRECQ" /></a> </li>
<li><a href="https://openreview.net/forum?id=K9bw7vqp_s">Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch</a> [<a class="glightbox" href="https://img.shields.io/badge/2021-ICLR-FF6B6B" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2021-ICLR-FF6B6B" /></a>] <a href="https://github.com/aojunzz/NM-sparsity"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/aojunzz/NM-sparsity" /></a> </li>
<li><a href="https://jmlr.csail.mit.edu/papers/volume22/20-1233/20-1233.pdf">A Greedy Algorithm for Quantizing Neural Networks</a> [<a class="glightbox" href="https://img.shields.io/badge/2021-JMLR-008B8B" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2021-JMLR-008B8B" /></a>] <a href="https://github.com/YixuanSeanZhou/Quantized_Neural_Nets"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/YixuanSeanZhou/Quantized_Neural_Nets" /></a> </li>
<li><a href="https://proceedings.neurips.cc/paper/2021/hash/6e8404c3b93a9527c8db241a1846599a-Abstract.html">Channel Permutations for N:M Sparsity</a> [<a class="glightbox" href="https://img.shields.io/badge/2021-NeurIPS-FF1493" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2021-NeurIPS-FF1493" /></a>] <a href="https://github.com/NVIDIA/apex"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/NVIDIA/apex" /></a> </li>
<li><a href="https://arxiv.org/abs/2104.08378">Accelerating Sparse Deep Neural Networks</a> [<a class="glightbox" href="https://img.shields.io/badge/2021-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2021-arXiv-1E88E5" /></a>]  </li>
<li><a href="https://arxiv.org/abs/2102.00554">Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks</a> [<a class="glightbox" href="https://img.shields.io/badge/2021-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2021-arXiv-1E88E5" /></a>]  </li>
</ol>
<h4 id="2020">2020</h4>
<ol>
<li><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Elsen_Fast_Sparse_ConvNets_CVPR_2020_paper.pdf">Fast Sparse ConvNets</a> [<a class="glightbox" href="https://img.shields.io/badge/2020-CVPR-2E8B57" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2020-CVPR-2E8B57" /></a>] <a href="https://github.com/fastconvnets/cvpr2020"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/fastconvnets/cvpr2020" /></a> </li>
<li><a href="http://proceedings.mlr.press/v119/kurtz20a/kurtz20a.pdf">Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference</a> [<a class="glightbox" href="https://img.shields.io/badge/2020-ICML-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2020-ICML-FF8C00" /></a>]  </li>
<li><a href="https://arxiv.org/abs/2005.07683">Movement Pruning: Adaptive Sparsity by Fine-Tuning</a> [<a class="glightbox" href="https://img.shields.io/badge/2020-NeurIPS-FF1493" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2020-NeurIPS-FF1493" /></a>] <a href="https://github.com/huggingface/block_movement_pruning"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/huggingface/block_movement_pruning" /></a> </li>
<li><a href="https://cdn.openai.com/blocksparse/blocksparsepaper.pdf">GPU Kernels for Block-Sparse Weights</a> [<a class="glightbox" href="https://img.shields.io/badge/2020-arXiv-1E88E5" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2020-arXiv-1E88E5" /></a>] <a href="https://github.com/openai/blocksparse"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/openai/blocksparse" /></a> </li>
</ol>
<h4 id="2019">2019</h4>
<ol>
<li><a href="https://arxiv.org/abs/2104.14129">ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training</a> [<a class="glightbox" href="https://img.shields.io/badge/2019-ICML-FF8C00" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2019-ICML-FF8C00" /></a>] <a href="https://github.com/ucbrise/actnn"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ucbrise/actnn" /></a> </li>
</ol>
<h4 id="2018">2018</h4>
<ol>
<li><a href="https://arxiv.org/abs/1804.03294">A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers</a> [<a class="glightbox" href="https://img.shields.io/badge/2018-ECCV-3CB371" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2018-ECCV-3CB371" /></a>] <a href="https://github.com/bzantium/pytorch-admm-pruning"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/bzantium/pytorch-admm-pruning" /></a> </li>
</ol>
<h4 id="2017">2017</h4>
<ol>
<li><a href="https://arxiv.org/pdf/1607.04381.pdf">DSD: Dense-Sparse-Dense Training for Deep Neural Networks</a> [<a class="glightbox" href="https://img.shields.io/badge/2017-ICLR-FF6B6B" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2017-ICLR-FF6B6B" /></a>]  </li>
<li><a href="http://arxiv.org/abs/1706.03762v7">Attention Is All You Need</a> [<a class="glightbox" href="https://img.shields.io/badge/2017-NeurIPS-FF1493" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2017-NeurIPS-FF1493" /></a>]  </li>
<li><a href="https://arxiv.org/pdf/1705.07565.pdf">Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon</a> [<a class="glightbox" href="https://img.shields.io/badge/2017-NeurIPS-FF1493" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2017-NeurIPS-FF1493" /></a>] <a href="https://github.com/csyhhu/L-OBS"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/csyhhu/L-OBS" /></a> </li>
</ol>
<h4 id="2016">2016</h4>
<ol>
<li><a href="https://arxiv.org/pdf/1510.00149.pdf">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding</a> [<a class="glightbox" href="https://img.shields.io/badge/2016-ICLR-FF6B6B" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/2016-ICLR-FF6B6B" /></a>]  </li>
</ol>
<h4 id="1993">1993</h4>
<ol>
<li><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=298572&amp;tag=1">Optimal Brain Surgeon and general network pruning</a> [<a class="glightbox" href="https://img.shields.io/badge/1993--green" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/1993--green" /></a>]  </li>
</ol>
<h4 id="1989">1989</h4>
<ol>
<li><a href="https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf">Optimal Brain Damage</a> [<a class="glightbox" href="https://img.shields.io/badge/1989-NeurIPS-FF1493" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="Publish" src="https://img.shields.io/badge/1989-NeurIPS-FF1493" /></a>]  </li>
</ol>
<h2 id="references">References</h2>
<ol>
<li>
<p><a href="https://github.com/Xnhyacinth/Awesome-LLM-Long-Context-Modeling">https://github.com/Xnhyacinth/Awesome-LLM-Long-Context-Modeling</a> [<a href="https://github.com/Xnhyacinth/Awesome-LLM-Long-Context-Modeling"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Xnhyacinth/Awesome-LLM-Long-Context-Modeling" /></a>]</p>
</li>
<li>
<p><a href="https://github.com/weigao266/Awesome-Efficient-Arch">https://github.com/weigao266/Awesome-Efficient-Arch</a> [<a href="https://github.com/weigao266/Awesome-Efficient-Arch"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/weigao266/Awesome-Efficient-Arch" /></a>]</p>
</li>
<li>
<p><a href="https://github.com/horseee/Awesome-Efficient-LLM">https://github.com/horseee/Awesome-Efficient-LLM</a> [<a href="https://github.com/horseee/Awesome-Efficient-LLM"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/horseee/Awesome-Efficient-LLM" /></a>]</p>
</li>
<li>
<p><a href="https://github.com/DefTruth/Awesome-Diffusion-Inference">https://github.com/DefTruth/Awesome-Diffusion-Inference</a> [<a href="https://github.com/DefTruth/Awesome-Diffusion-Inference"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/DefTruth/Awesome-Diffusion-Inference" /></a>]</p>
</li>
<li>
<p><a href="https://github.com/DefTruth/Awesome-LLM-Inference">https://github.com/DefTruth/Awesome-LLM-Inference</a> [<a href="https://github.com/DefTruth/Awesome-LLM-Inference"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/DefTruth/Awesome-LLM-Inference" /></a>]</p>
</li>
<li>
<p><a href="https://github.com/AmberLJC/LLMSys-PaperList">https://github.com/AmberLJC/LLMSys-PaperList</a> [<a href="https://github.com/AmberLJC/LLMSys-PaperList"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/AmberLJC/LLMSys-PaperList" /></a>]</p>
</li>
<li>
<p><a href="https://github.com/Hannibal046/Awesome-LLM">https://github.com/Hannibal046/Awesome-LLM</a> [<a href="https://github.com/Hannibal046/Awesome-LLM"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Hannibal046/Awesome-LLM" /></a>]</p>
</li>
<li>
<p><a href="https://github.com/AmadeusChan/Awesome-LLM-System-Papers">https://github.com/AmadeusChan/Awesome-LLM-System-Papers</a> [<a href="https://github.com/AmadeusChan/Awesome-LLM-System-Papers"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/AmadeusChan/Awesome-LLM-System-Papers" /></a>]</p>
</li>
<li>
<p><a href="https://github.com/KnowingNothing/compiler-and-arch">https://github.com/KnowingNothing/compiler-and-arch</a> [<a href="https://github.com/KnowingNothing/compiler-and-arch"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/KnowingNothing/compiler-and-arch" /></a>]</p>
</li>
<li>
<p><a href="https://papercopilot.com/paper-list">https://papercopilot.com/paper-list</a></p>
</li>
<li>
<p><a href="https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management">https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management</a> [<a href="https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/TreeAI-Lab/Awesome-KV-Cache-Management" /></a>]</p>
</li>
<li>
<p><a href="https://github.com/October2001/Awesome-KV-Cache-Compression">https://github.com/October2001/Awesome-KV-Cache-Compression</a> [<a href="https://github.com/October2001/Awesome-KV-Cache-Compression"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/October2001/Awesome-KV-Cache-Compression" /></a>]</p>
</li>
<li>
<p><a href="https://github.com/he-y/Awesome-Pruning">https://github.com/he-y/Awesome-Pruning</a> [<a href="https://github.com/he-y/Awesome-Pruning"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/he-y/Awesome-Pruning" /></a>]</p>
</li>
<li>
<p><a href="https://github.com/htqin/awesome-model-quantization">https://github.com/htqin/awesome-model-quantization</a> [<a href="https://github.com/htqin/awesome-model-quantization"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/htqin/awesome-model-quantization" /></a>]</p>
</li>
<li>
<p><a href="https://github.com/csyhhu/Awesome-Deep-Neural-Network-Compression">https://github.com/csyhhu/Awesome-Deep-Neural-Network-Compression</a> [<a href="https://github.com/csyhhu/Awesome-Deep-Neural-Network-Compression"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/csyhhu/Awesome-Deep-Neural-Network-Compression" /></a>]</p>
</li>
<li>
<p><a href="https://github.com/AojunZhou/Efficient-Deep-Learning">https://github.com/AojunZhou/Efficient-Deep-Learning</a> [<a href="https://github.com/AojunZhou/Efficient-Deep-Learning"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/AojunZhou/Efficient-Deep-Learning" /></a>]</p>
</li>
<li>
<p><a href="https://github.com/chester256/Model-Compression-Papers">https://github.com/chester256/Model-Compression-Papers</a> [<a href="https://github.com/chester256/Model-Compression-Papers"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/chester256/Model-Compression-Papers" /></a>]</p>
</li>
</ol>
              
  <!-- Giscus 评论系统 - 只在 notes 文件夹下显示 -->
<script>
  // 使用 JavaScript 来判断 URL 路径
  if (window.location.pathname.includes('/notes/')) {
    document.write(`
      <div id="giscus-container" style="margin-top: 2rem; padding-top: 1rem; border-top: 1px solid #e1e4e8;">
        <script src="https://giscus.app/client.js"
                data-repo="hustzxd/EfficientPaper"
                data-repo-id="R_kgDOKVYtOg"
                data-category="General"
                data-category-id="DIC_kwDOKVYtOs4CukCv"
                data-mapping="pathname"
                data-strict="0"
                data-reactions-enabled="1"
                data-emit-metadata="0"
                data-input-position="bottom"
                data-theme="light"
                data-lang="zh-CN"
                crossorigin="anonymous"
                async>
        <\/script>
      </div>
    `);
  }
</script>

            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="cls_year/" class="btn btn-neutral float-right" title="By Year">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
    
      <span><a href="cls_year/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="js/jquery-3.6.0.min.js"></script>
    <script>var base_url = ".";</script>
    <script src="js/theme_extra.js"></script>
    <script src="js/theme.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-clike.min.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
      <script src="js/prism-prototxt.js"></script>
      <script src="js/preview.js"></script>
      <script src="js/back-to-top.js"></script>
      <script src="search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
</script></body>
</html>

<!--
MkDocs version : 1.6.1
Build Date UTC : 2025-09-28 02:00:00.195272+00:00
-->
