{
  "papers": [
    {
      "id": "KVzip",
      "title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
      "abbr": "KVzip",
      "url": "http://arxiv.org/abs/2505.23416v2",
      "authors": [
        "Jang-Hyun Kim",
        "Jinuk Kim",
        "Sangwoo Kwon",
        "Jae W. Lee",
        "Sangdoo Yun",
        "Hyun Oh Song"
      ],
      "institutions": [
        "Seoul National University",
        "Neural Processing Research Center",
        "NAVER AI Lab"
      ],
      "year": 2025,
      "venue": "NeurIPS",
      "cover": "notes/2025/KVzip/cover.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": "https://github.com/snu-mllab/KVzip",
      "note_url": "notes/2025/KVzip/note/",
      "prototxt_path": "meta/2025/KVzip.prototxt",
      "update_time": 1768991701
    },
    {
      "id": "KVzap",
      "title": "KVzap: Fast, Adaptive, and Faithful KV Cache Pruning",
      "abbr": "KVzap",
      "url": "http://arxiv.org/abs/2601.07891v1",
      "authors": [
        "Simon Jegou",
        "Maximilian Jeblick"
      ],
      "institutions": [
        "NVIDIA"
      ],
      "year": 2026,
      "venue": "arXiv",
      "cover": "notes/2026/KVzap/cover.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": "https://github.com/NVIDIA/kvpress",
      "note_url": "notes/2026/KVzap/note/",
      "prototxt_path": "meta/2026/KVzap.prototxt",
      "update_time": 1768991644
    },
    {
      "id": "Mooncake",
      "title": "Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving",
      "abbr": "Mooncake",
      "url": "http://arxiv.org/abs/2407.00079v4",
      "authors": [
        "Ruoyu Qin",
        "Zheming Li",
        "Weiran He",
        "Mingxing Zhang",
        "Yongwei Wu",
        "Weimin Zheng",
        "Xinran Xu"
      ],
      "institutions": [
        "Moonshot AI",
        "Tsinghua University"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/Mooncake/cover.png",
      "keywords": [
        "21-KV Cache Management",
        "46-Tool"
      ],
      "code_url": "https://github.com/kvcache-ai/Mooncake",
      "note_url": "notes/2024/Mooncake/note/",
      "prototxt_path": "meta/2024/Mooncake.prototxt",
      "update_time": 1768881836
    },
    {
      "id": "eLLM",
      "title": "eLLM: Elastic Memory Management Framework for Efficient LLM Serving",
      "abbr": "eLLM",
      "url": "http://arxiv.org/abs/2506.15155v1",
      "authors": [
        "Jiale Xu",
        "Rui Zhang",
        "Yi Xiong",
        "Cong Guo",
        "Zihan Liu",
        "Yangjie Zhou",
        "Weiming Hu",
        "Hao Wu",
        "Changxu Shao",
        "Ziqing Wang",
        "Yongjie Yuan",
        "Junping Zhao",
        "Minyi Guo",
        "Jingwen Leng"
      ],
      "institutions": [
        "Shanghai Jiao Tong University",
        "Shanghai Qi Zhi Institute",
        "Ant Group",
        "National University of Singapore"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/eLLM/cover.png",
      "keywords": [
        "21-KV Cache Management",
        "46-Tool"
      ],
      "code_url": null,
      "note_url": "notes/2025/eLLM/note/",
      "prototxt_path": "meta/2025/eLLM.prototxt",
      "update_time": 1768879358
    },
    {
      "id": "MemServe",
      "title": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic Memory Pool",
      "abbr": "MemServe",
      "url": "http://arxiv.org/abs/2406.17565v3",
      "authors": [
        "Cunchen Hu",
        "Heyang Huang",
        "Junhao Hu",
        "Jiang Xu",
        "Xusheng Chen",
        "Tao Xie",
        "Chenxi Wang",
        "Sa Wang",
        "Yungang Bao",
        "Ninghui Sun",
        "Yizhou Shan"
      ],
      "institutions": [
        "Huawei Cloud",
        "Institute of Computing Technology",
        "Peking University"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/MemServe/cover.png",
      "keywords": [
        "21-KV Cache Management"
      ],
      "code_url": null,
      "note_url": "notes/2024/MemServe/note/",
      "prototxt_path": "meta/2024/MemServe.prototxt",
      "update_time": 1768879031
    },
    {
      "id": "vTensor",
      "title": "vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving",
      "abbr": "vTensor",
      "url": "http://arxiv.org/abs/2407.15309v1",
      "authors": [
        "Jiale Xu",
        "Rui Zhang",
        "Cong Guo",
        "Weiming Hu",
        "Zihan Liu",
        "Feiyang Wu",
        "Yu Feng",
        "Shixuan Sun",
        "Changxu Shao",
        "Yuhong Guo",
        "Junping Zhao",
        "Ke Zhang",
        "Minyi Guo",
        "Jingwen Leng"
      ],
      "institutions": [
        "Shanghai Jiao Tong University",
        "Shanghai Qi Zhi Institute",
        "Ant Group"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/vTensor/cover.png",
      "keywords": [
        "21-KV Cache Management"
      ],
      "code_url": null,
      "note_url": "notes/2024/vTensor/note/",
      "prototxt_path": "meta/2024/vTensor.prototxt",
      "update_time": 1768878373
    },
    {
      "id": "HATA",
      "title": "HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference",
      "abbr": "HATA",
      "url": "http://arxiv.org/abs/2506.02572v1",
      "authors": [
        "Ping Gong",
        "Jiawei Yi",
        "Shengnan Wang",
        "Juncheng Zhang",
        "Zewen Jin",
        "Ouxiang Zhou",
        "Ruibo Liu",
        "Guanbin Xu",
        "Youhui Bai",
        "Bowen Ye",
        "Kun Yuan",
        "Tong Yang",
        "Gong Zhang",
        "Renhai Chen",
        "Feng Wu",
        "Cheng Li"
      ],
      "institutions": [
        "University of Science and Technology of China",
        "Huawei",
        "Peking University"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/HATA/fig2.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": "https://github.com/gpzlx1/HATA",
      "note_url": "notes/2025/HATA/note/",
      "prototxt_path": "meta/2025/HATA.prototxt",
      "update_time": 1768877641
    },
    {
      "id": "UNComp",
      "title": "UNComp: Can Matrix Entropy Uncover Sparsity? -- A Compressor Design from an Uncertainty-Aware Perspective",
      "abbr": "UNComp",
      "url": "http://arxiv.org/abs/2410.03090v2",
      "authors": [
        "Jing Xiong",
        "Jianghan Shen",
        "Fanghua Ye",
        "Chaofan Tao",
        "Zhongwei Wan",
        "Jianqiao Lu",
        "Xun Wu",
        "Chuanyang Zheng",
        "Zhijiang Guo",
        "Min Yang",
        "Lingpeng Kong",
        "Ngai Wong"
      ],
      "institutions": [
        "The University of Hong Kong",
        "Nanjing University",
        "University College London",
        "The Ohio State University",
        "Microsoft Research Asia",
        "The Chinese University of Hong Kong",
        "Hong Kong University of Science and Technology"
      ],
      "year": 2025,
      "venue": "EMNLP",
      "cover": "notes/2025/UNComp/fig4.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": "https://github.com/menik1126/UNComp",
      "note_url": "notes/2025/UNComp/note/",
      "prototxt_path": "meta/2025/UNComp.prototxt",
      "update_time": 1768874671
    },
    {
      "id": "ReAttention",
      "title": "ReAttention: Training-Free Infinite Context with Finite Attention Scope",
      "abbr": "ReAttention",
      "url": "http://arxiv.org/abs/2407.15176v3",
      "authors": [
        "Xiaoran Liu",
        "Ruixiao Li",
        "Qipeng Guo",
        "Zhigeng Liu",
        "Yuerong Song",
        "Kai Lv",
        "Hang Yan",
        "Linlin Li",
        "Qun Liu",
        "Xipeng Qiu"
      ],
      "institutions": [
        "Fudan University",
        "Huawei"
      ],
      "year": 2025,
      "venue": "ICLR",
      "cover": "notes/2025/ReAttention/fig1.png",
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)",
        "21-KV Cache Management"
      ],
      "code_url": "https://github.com/OpenMOSS/ReAttention",
      "note_url": "notes/2025/ReAttention/note/",
      "prototxt_path": "meta/2025/ReAttention.prototxt",
      "update_time": 1768874199
    },
    {
      "id": "RetrievalAttention",
      "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval",
      "abbr": "RetrievalAttention",
      "url": "http://arxiv.org/abs/2409.10516v3",
      "authors": [
        "Di Liu",
        "Meng Chen",
        "Baotong Lu",
        "Huiqiang Jiang",
        "Zhenhua Han",
        "Qianxi Zhang",
        "Qi Chen",
        "Chengruidong Zhang",
        "Bailu Ding",
        "Kai Zhang",
        "Chen Chen",
        "Fan Yang",
        "Yuqing Yang",
        "Lili Qiu"
      ],
      "institutions": [
        "Microsoft Research",
        "Shanghai Jiao Tong University",
        "Fudan University"
      ],
      "year": 2025,
      "venue": "NeurIPS",
      "cover": "notes/2025/RetrievalAttention/cover.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)",
        "21-KV Cache Management"
      ],
      "code_url": "https://github.com/microsoft/RetrievalAttention",
      "note_url": "notes/2025/RetrievalAttention/note/",
      "prototxt_path": "meta/2025/RetrievalAttention.prototxt",
      "update_time": 1768874005
    },
    {
      "id": "MoA",
      "title": "MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression",
      "abbr": "MoA",
      "url": "http://arxiv.org/abs/2406.14909v2",
      "authors": [
        "Tianyu Fu",
        "Haofeng Huang",
        "Xuefei Ning",
        "Genghan Zhang",
        "Boju Chen",
        "Tianqi Wu",
        "Hongyi Wang",
        "Zixiao Huang",
        "Shiyao Li",
        "Shengen Yan",
        "Guohao Dai",
        "Huazhong Yang",
        "Yu Wang"
      ],
      "institutions": [
        "Tsinghua University",
        "Infinigence-AI",
        "Stanford University",
        "Shanghai Jiao Tong University"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/MoA/moa.png",
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)"
      ],
      "code_url": "https://github.com/thu-nics/MoA",
      "note_url": "notes/2024/MoA/note/",
      "prototxt_path": "meta/2024/MoA.prototxt",
      "update_time": 1768873775
    },
    {
      "id": "PrefixQuant",
      "title": "PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language Models Quantization",
      "abbr": "PrefixQuant",
      "url": "http://arxiv.org/abs/2410.05265v2",
      "authors": [
        "Mengzhao Chen",
        "Yi Liu",
        "Jiahao Wang",
        "Yi Bin",
        "Wenqi Shao",
        "Ping Luo"
      ],
      "institutions": [
        "The University of Hong Kong",
        "Tongji University",
        "Shanghai AI Laboratory"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/PrefixQuant/fig2.png",
      "keywords": [
        "09-Quantization",
        "19-Quantization (KV Cache)"
      ],
      "code_url": "https://github.com/ChenMnZ/PrefixQuant",
      "note_url": "notes/2024/PrefixQuant/note/",
      "prototxt_path": "meta/2024/PrefixQuant.prototxt",
      "update_time": 1768873723
    },
    {
      "id": "InfLLM",
      "title": "InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory",
      "abbr": "InfLLM",
      "url": "http://arxiv.org/abs/2402.04617v2",
      "authors": [
        "Chaojun Xiao",
        "Pengle Zhang",
        "Xu Han",
        "Guangxuan Xiao",
        "Yankai Lin",
        "Zhengyan Zhang",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "institutions": [
        "Tsinghua University",
        "Massachusetts Institution of Technology",
        "Renmin University of China"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/InfLLM/cover.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": "https://github.com/thunlp/InfLLM",
      "note_url": "notes/2024/InfLLM/note/",
      "prototxt_path": "meta/2024/InfLLM.prototxt",
      "update_time": 1768873667
    },
    {
      "id": "TidalDecode",
      "title": "TidalDecode: Fast and Accurate LLM Decoding with Position Persistent Sparse Attention",
      "abbr": "TidalDecode",
      "url": "http://arxiv.org/abs/2410.05076v1",
      "authors": [
        "Lijie Yang",
        "Zhihao Zhang",
        "Zhuofu Chen",
        "Zikun Li",
        "Zhihao Jia"
      ],
      "institutions": [
        "Carnegie Mellon University"
      ],
      "year": 2025,
      "venue": "ICLR",
      "cover": "notes/2025/TidalDecode/fig2.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": "https://github.com/DerrickYLJ/TidalDecode",
      "note_url": "notes/2025/TidalDecode/note/",
      "prototxt_path": "meta/2025/TidalDecode.prototxt",
      "update_time": 1768819197
    },
    {
      "id": "SpargeAttn",
      "title": "SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference",
      "abbr": "SpargeAttn",
      "url": "https://arxiv.org/pdf/2502.18137",
      "authors": [
        "Jintao Zhang",
        "Chendong Xiang",
        "Haofeng Huang",
        "Jia Wei",
        "Haocheng Xi",
        "Jun Zhu",
        "Jianfei Chen"
      ],
      "institutions": [
        "Tsinghua University",
        "University of California, Berkeley"
      ],
      "year": 2025,
      "venue": "ICML",
      "cover": "notes/2025/SpargeAttn/fig3.png",
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)"
      ],
      "code_url": "https://github.com/thu-ml/SpargeAttn",
      "note_url": "notes/2025/SpargeAttn/note/",
      "prototxt_path": "meta/2025/SpargeAttn.prototxt",
      "update_time": 1768818735
    },
    {
      "id": "ShadowKV",
      "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference",
      "abbr": "ShadowKV",
      "url": "https://arxiv.org/abs/2410.21465",
      "authors": [
        "Hanshi Sun",
        "Li-Wen Chang",
        "Wenlei Bao",
        "Size Zheng",
        "Ningxin Zheng",
        "Xin Liu",
        "Harry Dong",
        "Yuejie Chi",
        "Beidi Chen"
      ],
      "institutions": [
        "Carnegie Mellon University",
        "ByteDance Seed"
      ],
      "year": 2025,
      "venue": "ICML",
      "cover": "notes/2025/ShadowKV/shadowkv.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)",
        "43-Low Rank Decomposition"
      ],
      "code_url": "https://github.com/bytedance/ShadowKV",
      "note_url": "notes/2025/ShadowKV/note/",
      "prototxt_path": "meta/2025/ShadowKV.prototxt",
      "update_time": 1768817764
    },
    {
      "id": "RadialAttention",
      "title": "Radial Attention: $O(n\\log n)$ Sparse Attention with Energy Decay for Long Video Generation",
      "abbr": "RadialAttention",
      "url": "http://arxiv.org/abs/2506.19852v1",
      "authors": [
        "Xingyang Li",
        "Muyang Li",
        "Tianle Cai",
        "Haocheng Xi",
        "Shuo Yang",
        "Yujun Lin",
        "Lvmin Zhang",
        "Songlin Yang",
        "Jinbo Hu",
        "Kelly Peng",
        "Maneesh Agrawala",
        "Ion Stoica",
        "Kurt Keutzer",
        "Song Han"
      ],
      "institutions": [
        "Massachusetts Institute of Technology",
        "NVIDIA",
        "Princeton",
        "UC Berkeley",
        "Stanford",
        "First Intelligence"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/RadialAttention/fig3.png",
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)"
      ],
      "code_url": null,
      "note_url": "notes/2025/RadialAttention/note/",
      "prototxt_path": "meta/2025/RadialAttention.prototxt",
      "update_time": 1768817588
    },
    {
      "id": "R-KV",
      "title": "R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration",
      "abbr": "R-KV",
      "url": "http://arxiv.org/abs/2505.24133v2",
      "authors": [
        "Zefan Cai",
        "Wen Xiao",
        "Hanshi Sun",
        "Cheng Luo",
        "Yikai Zhang",
        "Ke Wan",
        "Yucheng Li",
        "Yeyang Zhou",
        "Li-Wen Chang",
        "Jiuxiang Gu",
        "Zhen Dong",
        "Anima Anandkumar",
        "Abedelkadir Asi",
        "Junjie Hu"
      ],
      "institutions": [
        "University of Wisconsin",
        "Microsoft",
        "Carnegie Mellon University",
        "University of Surrey"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/R-KV/fig1.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": "https://github.com/Zefan-Cai/R-KV",
      "note_url": "notes/2025/R-KV/note/",
      "prototxt_path": "meta/2025/R-KV.prototxt",
      "update_time": 1768817562
    },
    {
      "id": "QuickSilver",
      "title": "QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization",
      "abbr": "QuickSilver",
      "url": "http://arxiv.org/abs/2506.22396v1",
      "authors": [
        "Danush Khanna",
        "Aditya Kumar Guru",
        "Srivarshinee Sridhar",
        "Zidan Ahmed",
        "Rubhav Bahirwani",
        "Meetu Malhotra",
        "Vinija Jain",
        "Aman Chadha",
        "Amitava Das",
        "Kripabandhu Ghosh"
      ],
      "institutions": [
        "Manipal University Jaipur",
        "Vellore Institute of Technology",
        "NIT Silchar",
        "Harrisburg University of Science and Technology",
        "Meta",
        "Amazon",
        "IISER Kolkata",
        "BITS Pilani"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/QuickSilver/cover.png",
      "keywords": [
        "19-Quantization (KV Cache)",
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": "https://anonymous.4open.science/r/Quicksilver/codes/kvc.py",
      "note_url": "notes/2025/QuickSilver/note/",
      "prototxt_path": "meta/2025/QuickSilver.prototxt",
      "update_time": 1768817372
    },
    {
      "id": "PureKV",
      "title": "PureKV: Plug-and-Play KV Cache Optimization with Spatial-Temporal Sparse Attention for Vision-Language Large Models",
      "abbr": "PureKV",
      "url": "http://arxiv.org/abs/2510.25600v2",
      "authors": [
        "Zhonghua Jiang",
        "Kunxi Li",
        "Yiyun Zhou",
        "Sihao Liu",
        "Zhaode Wang",
        "Chengfei lv",
        "Shengyu Zhang"
      ],
      "institutions": [
        "Zhejiang University",
        "Alibaba Group"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/PureKV/fig1.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": null,
      "note_url": "notes/2025/PureKV/note/",
      "prototxt_path": "meta/2025/PureKV.prototxt",
      "update_time": 1768816533
    },
    {
      "id": "QJL",
      "title": "QJL: 1-Bit Quantized JL Transform for KV Cache Quantization with Zero Overhead",
      "abbr": "QJL",
      "url": "http://arxiv.org/abs/2406.03482v2",
      "authors": [
        "Amir Zandieh",
        "Majid Daliri",
        "Insu Han"
      ],
      "institutions": [
        "New York University",
        "Adobe Research"
      ],
      "year": 2025,
      "venue": "AAAI",
      "cover": "notes/2025/QJL/fig1.png",
      "keywords": [
        "09-Quantization",
        "19-Quantization (KV Cache)"
      ],
      "code_url": "https://github.com/amirzandieh/QJL",
      "note_url": "notes/2025/QJL/note/",
      "prototxt_path": "meta/2025/QJL.prototxt",
      "update_time": 1768816211
    },
    {
      "id": "PSA",
      "title": "Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving",
      "abbr": "PSA",
      "url": "http://arxiv.org/abs/2503.00392v1",
      "authors": [
        "Qihui Zhou",
        "Peiqi Yin",
        "Pengfei Zuo",
        "James Cheng"
      ],
      "institutions": [
        "The Chinese University of Hong Kong",
        "Huawei"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/PSA/fig4.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": "https://github.com/ASISys/PSAttention",
      "note_url": "notes/2025/PSA/note/",
      "prototxt_path": "meta/2025/PSA.prototxt",
      "update_time": 1768816157
    },
    {
      "id": "ComplementarySparsity",
      "title": "Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks",
      "abbr": "Complementary Sparsity",
      "url": "https://iopscience.iop.org/article/10.1088/2634-4386/ac7c8a",
      "authors": [
        "Kevin Hunter",
        "Lawrence Spracklen",
        "Subutai Ahmad"
      ],
      "institutions": [
        "Numenta"
      ],
      "year": 2022,
      "venue": "NCE",
      "cover": "notes/2022/ComplementarySparsity/cover.jpg",
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": null,
      "note_url": "notes/2022/ComplementarySparsity/note/",
      "prototxt_path": "meta/2022/ComplementarySparsity.prototxt",
      "update_time": 1768799470
    },
    {
      "id": "MixServe",
      "title": "MixServe: An Automatic Distributed Serving System for MoE Models with Hybrid Parallelism Based on Fused Communication Algorithm",
      "abbr": "MixServe",
      "url": "http://arxiv.org/abs/2601.08800v1",
      "authors": [
        "Bowen Zhou",
        "Jinrui Jia",
        "Wenhao He",
        "Yong Zhang",
        "Fang Dong"
      ],
      "institutions": [
        "inst1",
        "inst2"
      ],
      "year": 2026,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "00-None"
      ],
      "code_url": null,
      "note_url": "notes/2026/MixServe/note/",
      "prototxt_path": "meta/2026/MixServe.prototxt",
      "update_time": 1768533614
    },
    {
      "id": "CAOTE",
      "title": "CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based Token Eviction",
      "abbr": "CAOTE",
      "url": "http://arxiv.org/abs/2504.14051v6",
      "authors": [
        "Raghavv Goel",
        "Junyoung Park",
        "Mukul Gagrani",
        "Dalton Jones",
        "Matthew Morse",
        "Harper Langston",
        "Mingu Lee",
        "Chris Lott"
      ],
      "institutions": [
        "Qualcomm"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/CAOTE/cover.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": null,
      "note_url": "notes/2025/CAOTE/note/",
      "prototxt_path": "meta/2025/CAOTE.prototxt",
      "update_time": 1768446513
    },
    {
      "id": "POD-Attention",
      "title": "POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference",
      "abbr": "POD-Attention",
      "url": "http://arxiv.org/abs/2410.18038v2",
      "authors": [
        "Aditya K Kamath",
        "Ramya Prabhu",
        "Jayashree Mohan",
        "Simon Peter",
        "Ramachandran Ramjee",
        "Ashish Panwar"
      ],
      "institutions": [
        "University of Washington",
        "Microsoft Research"
      ],
      "year": 2025,
      "venue": "ASPLOS",
      "cover": "notes/2025/POD-Attention/fig2.png",
      "keywords": [
        "29-Comm-Comp Overlap"
      ],
      "code_url": "https://github.com/microsoft/vattention/tree/main/pod_attn",
      "note_url": "notes/2025/POD-Attention/note/",
      "prototxt_path": "meta/2025/POD-Attention.prototxt",
      "update_time": 1768295194
    },
    {
      "id": "Oneiros",
      "title": "Oneiros: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving",
      "abbr": "Oneiros",
      "url": "http://arxiv.org/abs/2507.11507v2",
      "authors": [
        "Ruihao Li",
        "Shagnik Pal",
        "Vineeth Narayan Pullu",
        "Prasoon Sinha",
        "Jeeho Ryoo",
        "Lizy K. John",
        "Neeraja J. Yadwadkar"
      ],
      "institutions": [
        "The University of Texas at Austin",
        "Fairleigh Dickinson University"
      ],
      "year": 2025,
      "venue": "SoCC",
      "cover": "notes/2025/Oneiros/cover.png",
      "keywords": [
        "21-KV Cache Management",
        "40-LLM Deployment"
      ],
      "code_url": "https://github.com/UT-SysML/Oneiros",
      "note_url": "notes/2025/Oneiros/note/",
      "prototxt_path": "meta/2025/Oneiros.prototxt",
      "update_time": 1768292955
    },
    {
      "id": "SageAttention2",
      "title": "SageAttention2: Efficient Attention with Thorough Outlier Smoothing and Per-thread INT4 Quantization",
      "abbr": "SageAttention2",
      "url": "http://arxiv.org/abs/2411.10958v6",
      "authors": [
        "Jintao Zhang",
        "Haofeng Huang",
        "Pengle Zhang",
        "Jia Wei",
        "Jun Zhu",
        "Jianfei Chen"
      ],
      "institutions": [
        "Tsinghua University"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "09-Quantization",
        "19-Quantization (KV Cache)"
      ],
      "code_url": "https://github.com/thu-ml/SageAttention",
      "note_url": "notes/2024/SageAttention2/note/",
      "prototxt_path": "meta/2024/SageAttention2.prototxt",
      "update_time": 1768291553
    },
    {
      "id": "SageAttention",
      "title": "SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration",
      "abbr": "SageAttention",
      "url": "http://arxiv.org/abs/2410.02367v8",
      "authors": [
        "Jintao Zhang",
        "Jia Wei",
        "Haofeng Huang",
        "Pengle Zhang",
        "Jun Zhu",
        "Jianfei Chen"
      ],
      "institutions": [
        "Tsinghua University"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "09-Quantization",
        "19-Quantization (KV Cache)"
      ],
      "code_url": "https://github.com/thu-ml/SageAttention",
      "note_url": "notes/2024/SageAttention/note/",
      "prototxt_path": "meta/2024/SageAttention.prototxt",
      "update_time": 1768291541
    },
    {
      "id": "MiniKV",
      "title": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache",
      "abbr": "MiniKV",
      "url": "http://arxiv.org/abs/2411.18077",
      "authors": [
        "Akshat Sharma",
        "Hangliang Ding",
        "Jianping Li",
        "Neel Dani",
        "Minjia Zhang"
      ],
      "institutions": [
        "University of Illinois at Urbana-Champaign"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/MiniKV/minikv.png",
      "keywords": [
        "19-Quantization (KV Cache)",
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": "https://github.com/akshatsh49/MiniKV-Dev",
      "note_url": "notes/2024/MiniKV/note/",
      "prototxt_path": "meta/2024/MiniKV.prototxt",
      "update_time": 1768291509
    },
    {
      "id": "MFA",
      "title": "Multi-matrix Factorization Attention",
      "abbr": "MFA",
      "url": "http://arxiv.org/abs/2412.19255v2",
      "authors": [
        "Jingcheng Hu",
        "Houyi Li",
        "Yinmin Zhang",
        "Zili Wang",
        "Shuigeng Zhou",
        "Xiangyu Zhang",
        "Heung-Yeung Shum",
        "Daxin Jiang"
      ],
      "institutions": [
        "StepFun",
        "Tsinghua University",
        "Fudan University",
        "Megvii Technology"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "42-Network Structure Design"
      ],
      "code_url": null,
      "note_url": "notes/2024/MFA/note/",
      "prototxt_path": "meta/2024/MFA.prototxt",
      "update_time": 1768291308
    },
    {
      "id": "LazyLLM",
      "title": "LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference",
      "abbr": "LazyLLM",
      "url": "http://arxiv.org/abs/2407.14057v1",
      "authors": [
        "Qichen Fu",
        "Minsik Cho",
        "Thomas Merth",
        "Sachin Mehta",
        "Mohammad Rastegari",
        "Mahyar Najibi"
      ],
      "institutions": [
        "Apple",
        "Meta AI"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/LazyLLM/fig3.png",
      "keywords": [
        "02-Sparsity (Attention)",
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": null,
      "note_url": "notes/2024/LazyLLM/note/",
      "prototxt_path": "meta/2024/LazyLLM.prototxt",
      "update_time": 1768291289
    },
    {
      "id": "GEAR",
      "title": "GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM",
      "abbr": "GEAR",
      "url": "http://arxiv.org/abs/2403.05527v4",
      "authors": [
        "Hao Kang",
        "Qingru Zhang",
        "Souvik Kundu",
        "Geonhwa Jeong",
        "Zaoxing Liu",
        "Tushar Krishna",
        "Tuo Zhao"
      ],
      "institutions": [
        "Georgia Tech",
        "Intel",
        "University of Maryland"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/GEAR/overview.png",
      "keywords": [
        "09-Quantization",
        "19-Quantization (KV Cache)"
      ],
      "code_url": "https://github.com/HaoKang-Timmy/GEAR",
      "note_url": "notes/2024/GEAR/note/",
      "prototxt_path": "meta/2024/GEAR.prototxt",
      "update_time": 1768291205
    },
    {
      "id": "MInference",
      "title": "MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention",
      "abbr": "MInference",
      "url": "http://arxiv.org/abs/2407.02490v1",
      "authors": [
        "Huiqiang Jiang",
        "Yucheng Li",
        "Chengruidong Zhang",
        "Qianhui Wu",
        "Xufang Luo",
        "Surin Ahn",
        "Zhenhua Han",
        "Amir H. Abdi",
        "Dongsheng Li",
        "Chin-Yew Lin",
        "Yuqing Yang",
        "Lili Qiu"
      ],
      "institutions": [
        "Microsoft",
        "University of Surrey"
      ],
      "year": 2024,
      "venue": "NeurIPS",
      "cover": "notes/2024/MInference/MInference_3shape.PNG",
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)"
      ],
      "code_url": "https://github.com/microsoft/MInference",
      "note_url": "notes/2024/MInference/note/",
      "prototxt_path": "meta/2024/MInference.prototxt",
      "update_time": 1768291167
    },
    {
      "id": "DoubleSparsity",
      "title": "Post-Training Sparse Attention with Double Sparsity",
      "abbr": "DoubleSparsity",
      "url": "http://arxiv.org/abs/2408.07092v2",
      "authors": [
        "Shuo Yang",
        "Ying Sheng",
        "Joseph E. Gonzalez",
        "Ion Stoica",
        "Lianmin Zheng"
      ],
      "institutions": [
        "UC Berkeley",
        "Stanford",
        "Shanghai Jiao Tong University"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/DoubleSparsity/cover.png",
      "keywords": [
        "01-Sparse/Pruning",
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": "https://github.com/andy-yang-1/DoubleSparse",
      "note_url": "notes/2024/DoubleSparsity/note/",
      "prototxt_path": "meta/2024/DoubleSparsity.prototxt",
      "update_time": 1768291146
    },
    {
      "id": "RecycledAttention",
      "title": "Recycled Attention: Efficient inference for long-context language models",
      "abbr": "Recycled Attention",
      "url": "http://arxiv.org/abs/2411.05787v1",
      "authors": [
        "Fangyuan Xu",
        "Tanya Goyal",
        "Eunsol Choi"
      ],
      "institutions": [
        "New York University",
        "Cornell University",
        "The University of Texas at Austin"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/RecycledAttention/fig1.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": "https://github.com/carriex/recycled-attention",
      "note_url": "notes/2024/RecycledAttention/note/",
      "prototxt_path": "meta/2024/RecycledAttention.prototxt",
      "update_time": 1768290661
    },
    {
      "id": "Quest",
      "title": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference",
      "abbr": "Quest",
      "url": "http://arxiv.org/abs/2406.10774",
      "authors": [
        "Jiaming Tang",
        "Yilong Zhao",
        "Kan Zhu",
        "Guangxuan Xiao",
        "Baris Kasikci",
        "Song Han"
      ],
      "institutions": [
        "Shanghai Jiao Tong University",
        "Massachusetts Institute of Technology"
      ],
      "year": 2024,
      "venue": "ICML",
      "cover": "notes/2024/Quest/quest.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": "https://github.com/mit-han-lab/quest",
      "note_url": "notes/2024/Quest/note/",
      "prototxt_path": "meta/2024/Quest.prototxt",
      "update_time": 1768290633
    },
    {
      "id": "SampleAttention",
      "title": "SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention",
      "abbr": "SampleAttention",
      "url": "http://arxiv.org/abs/2406.15486v2",
      "authors": [
        "Qianchao Zhu",
        "Jiangfei Duan",
        "Chang Chen",
        "Siran Liu",
        "Xiuhong Li",
        "Guanyu Feng",
        "Xin Lv",
        "Huanqi Cao",
        "Xiao Chuanfu",
        "Xingcheng Zhang",
        "Dahua Lin",
        "Chao Yang"
      ],
      "institutions": [
        "Peking University",
        "The Chinese University of Hong Kong",
        "Zhipu AI",
        "Tsinghua University"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/SampleAttention/cover.png",
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)"
      ],
      "code_url": null,
      "note_url": "notes/2024/SampleAttention/note/",
      "prototxt_path": "meta/2024/SampleAttention.prototxt",
      "update_time": 1768290587
    },
    {
      "id": "MiniCache",
      "title": "MiniCache: KV Cache Compression in Depth Dimension for Large Language Models",
      "abbr": "MiniCache",
      "url": "http://arxiv.org/abs/2405.14366v2",
      "authors": [
        "Akide Liu",
        "Jing Liu",
        "Zizheng Pan",
        "Yefei He",
        "Gholamreza Haffari",
        "Bohan Zhuang"
      ],
      "institutions": [
        "Monash University",
        "Zhejiang University"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/MiniCache/cover.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": "https://github.com/AkideLiu/MiniCache",
      "note_url": "notes/2024/MiniCache/note/",
      "prototxt_path": "meta/2024/MiniCache.prototxt",
      "update_time": 1768290522
    },
    {
      "id": "SparQ",
      "title": "SparQ Attention: Bandwidth-Efficient LLM Inference",
      "abbr": "SparQ",
      "url": "http://arxiv.org/abs/2312.04985v5",
      "authors": [
        "Luka Ribar",
        "Ivan Chelombiev",
        "Luke Hudlass-Galley",
        "Charlie Blake",
        "Carlo Luschi",
        "Douglas Orr"
      ],
      "institutions": [
        "Graphcore Research",
        "Synthesia"
      ],
      "year": 2024,
      "venue": "ICML",
      "cover": "notes/2024/SparQ/cover.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": null,
      "note_url": "notes/2024/SparQ/note/",
      "prototxt_path": "meta/2024/SparQ.prototxt",
      "update_time": 1768290225
    },
    {
      "id": "SeerAttention",
      "title": "SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs",
      "abbr": "SeerAttention",
      "url": "http://arxiv.org/abs/2410.13276v2",
      "authors": [
        "Yizhao Gao",
        "Zhichen Zeng",
        "Dayou Du",
        "Shijie Cao",
        "Hayden Kwok-Hay So",
        "Ting Cao",
        "Fan Yang",
        "Mao Yang"
      ],
      "institutions": [
        "University of Hong Kong",
        "University of Washington",
        "Microsoft Research"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/SeerAttention/seerattention.png",
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)"
      ],
      "code_url": "https://github.com/microsoft/SeerAttention",
      "note_url": "notes/2024/SeerAttention/note/",
      "prototxt_path": "meta/2024/SeerAttention.prototxt",
      "update_time": 1768284057
    },
    {
      "id": "SnapKV",
      "title": "SnapKV: LLM Knows What You are Looking for Before Generation",
      "abbr": "SnapKV",
      "url": "http://arxiv.org/abs/2404.14469v2",
      "authors": [
        "Yuhong Li",
        "Yingbing Huang",
        "Bowen Yang",
        "Bharat Venkitesh",
        "Acyr Locatelli",
        "Hanchen Ye",
        "Tianle Cai",
        "Patrick Lewis",
        "Deming Chen"
      ],
      "institutions": [
        "University of Illinois Urbana-Champaign",
        "Cohere",
        "Princeton University"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/SnapKV/fig1.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": "https://github.com/FasterDecoding/SnapKV",
      "note_url": "notes/2024/SnapKV/note/",
      "prototxt_path": "meta/2024/SnapKV.prototxt",
      "update_time": 1768283987
    },
    {
      "id": "ReSA",
      "title": "Rectified Sparse Attention",
      "abbr": "ReSA",
      "url": "http://arxiv.org/abs/2506.04108v2",
      "authors": [
        "Yutao Sun",
        "Tianzhu Ye",
        "Li Dong",
        "Yuqing Xia",
        "Jian Chen",
        "Yizhao Gao",
        "Shijie Cao",
        "Jianyong Wang",
        "Furu Wei"
      ],
      "institutions": [
        "Microsoft Research",
        "Tsinghua University",
        "The University of Hong Kong"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/ReSA/fig2.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": "https://github.com/microsoft/unilm/tree/master/ReSA",
      "note_url": "notes/2025/ReSA/note/",
      "prototxt_path": "meta/2025/ReSA.prototxt",
      "update_time": 1768283900
    },
    {
      "id": "Engram",
      "title": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
      "abbr": "Engram",
      "url": "https://github.com/deepseek-ai/Engram/blob/main/Engram_paper.pdf",
      "authors": [
        "Xin Cheng",
        "Wangding Zeng",
        "Damai Dai",
        "Qinyu Chen",
        "Bingxuan Wang",
        "Zhenda Xie",
        "Kezhao Huang",
        "Xingkai Yu",
        "Zhewen Hao",
        "Yukun Li",
        "Han Zhang",
        "Huishuai Zhang",
        "Dongyan Zhao",
        "Wenfeng Liang"
      ],
      "institutions": [
        "Peking University",
        "DeepSeek-AI"
      ],
      "year": 2026,
      "venue": "Github",
      "cover": "notes/2026/Engram/cover.png",
      "keywords": [
        "01-Sparse/Pruning",
        "42-Network Structure Design"
      ],
      "code_url": "https://github.com/deepseek-ai/Engram",
      "note_url": "notes/2026/Engram/note/",
      "prototxt_path": "meta/2026/Engram.prototxt",
      "update_time": 1768275148
    },
    {
      "id": "MILLION",
      "title": "MILLION: Mastering Long-Context LLM Inference Via Outlier-Immunized KV Product Quantization",
      "abbr": "MILLION",
      "url": "http://arxiv.org/abs/2504.03661v2",
      "authors": [
        "Zongwu Wang",
        "Peng Xu",
        "Fangxin Liu",
        "Yiwei Hu",
        "Qingxiao Sun",
        "Gezi Li",
        "Cheng Li",
        "Xuan Wang",
        "Li Jiang",
        "Haibing Guan"
      ],
      "institutions": [
        "Shanghai Jiao Tong University",
        "Shanghai Qi Zhi Institute",
        "Huawei",
        "China University of Petroleum-Beijing"
      ],
      "year": 2025,
      "venue": "DAC",
      "cover": "notes/2025/MILLION/fig4.png",
      "keywords": [
        "09-Quantization",
        "19-Quantization (KV Cache)"
      ],
      "code_url": "https://github.com/ZongwuWang/MILLION",
      "note_url": "notes/2025/MILLION/note/",
      "prototxt_path": "meta/2025/MILLION.prototxt",
      "update_time": 1768272143
    },
    {
      "id": "MoBA",
      "title": "MoBA: Mixture of Block Attention for Long-Context LLMs",
      "abbr": "MoBA",
      "url": "http://arxiv.org/abs/2502.13189v1",
      "authors": [
        "Enzhe Lu",
        "Zhejun Jiang",
        "Jingyuan Liu",
        "Yulun Du",
        "Tao Jiang",
        "Chao Hong",
        "Shaowei Liu",
        "Weiran He",
        "Enming Yuan",
        "Yuzhi Wang",
        "Zhiqi Huang",
        "Huan Yuan",
        "Suting Xu",
        "Xinran Xu",
        "Guokun Lai",
        "Yanru Chen",
        "Huabin Zheng",
        "Junjie Yan",
        "Jianlin Su",
        "Yuxin Wu",
        "Neo Y. Zhang",
        "Zhilin Yang",
        "Xinyu Zhou",
        "Mingxing Zhang",
        "Jiezhong Qiu"
      ],
      "institutions": [
        "Moonshot AI",
        "Tsinghua University",
        "Zhejiang University"
      ],
      "year": 2025,
      "venue": "NeurIPS",
      "cover": "notes/2025/MoBA/fig1.png",
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)",
        "20-Sparse/Eviction (KV Cache)",
        "42-Network Structure Design"
      ],
      "code_url": "https://github.com/MoonshotAI/MoBA",
      "note_url": "notes/2025/MoBA/note/",
      "prototxt_path": "meta/2025/MoBA.prototxt",
      "update_time": 1768272072
    },
    {
      "id": "MMInference",
      "title": "MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention",
      "abbr": "MMInference",
      "url": "https://openreview.net/forum?id=me6PfbATWM",
      "authors": [
        "Yucheng Li",
        "Huiqiang Jiang",
        "Chengruidong Zhang",
        "Qianhui Wu",
        "Xufang Luo",
        "Surin Ahn",
        "Amir H. Abdi",
        "Dongsheng Li",
        "Jianfeng Gao",
        "Yuqing Yang",
        "Lili Qiu"
      ],
      "institutions": [
        "Microsoft",
        "University of Surrey"
      ],
      "year": 2025,
      "venue": "ICML",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)"
      ],
      "code_url": null,
      "note_url": "notes/2025/MMInference/note/",
      "prototxt_path": "meta/2025/MMInference.prototxt",
      "update_time": 1768271352
    },
    {
      "id": "MiniMax-01",
      "title": "MiniMax-01: Scaling Foundation Models with Lightning Attention",
      "abbr": "MiniMax-01",
      "url": "http://arxiv.org/abs/2501.08313v1",
      "authors": [
        "MiniMax",
        "Aonian Li",
        "Bangwei Gong",
        "Bo Yang",
        "Boji Shan",
        "Chang Liu",
        "Cheng Zhu",
        "Chunhao Zhang",
        "Congchao Guo",
        "Da Chen",
        "Dong Li",
        "Enwei Jiao",
        "Gengxin Li",
        "Guojun Zhang",
        "Haohai Sun",
        "Houze Dong",
        "Jiadai Zhu",
        "Jiaqi Zhuang",
        "Jiayuan Song",
        "Jin Zhu",
        "Jingtao Han",
        "Jingyang Li",
        "Junbin Xie",
        "Junhao Xu",
        "Junjie Yan",
        "Kaishun Zhang",
        "Kecheng Xiao",
        "Kexi Kang",
        "Le Han",
        "Leyang Wang",
        "Lianfei Yu",
        "Liheng Feng",
        "Lin Zheng",
        "Linbo Chai",
        "Long Xing",
        "Meizhi Ju",
        "Mingyuan Chi",
        "Mozhi Zhang",
        "Peikai Huang",
        "Pengcheng Niu",
        "Pengfei Li",
        "Pengyu Zhao",
        "Qi Yang",
        "Qidi Xu",
        "Qiexiang Wang",
        "Qin Wang",
        "Qiuhui Li",
        "Ruitao Leng",
        "Shengmin Shi",
        "Shuqi Yu",
        "Sichen Li",
        "Songquan Zhu",
        "Tao Huang",
        "Tianrun Liang",
        "Weigao Sun",
        "Weixuan Sun",
        "Weiyu Cheng",
        "Wenkai Li",
        "Xiangjun Song",
        "Xiao Su",
        "Xiaodong Han",
        "Xinjie Zhang",
        "Xinzhu Hou",
        "Xu Min",
        "Xun Zou",
        "Xuyang Shen",
        "Yan Gong",
        "Yingjie Zhu",
        "Yipeng Zhou",
        "Yiran Zhong",
        "Yongyi Hu",
        "Yuanxiang Fan",
        "Yue Yu",
        "Yufeng Yang",
        "Yuhao Li",
        "Yunan Huang",
        "Yunji Li",
        "Yunpeng Huang",
        "Yunzhi Xu",
        "Yuxin Mao",
        "Zehan Li",
        "Zekang Li",
        "Zewei Tao",
        "Zewen Ying",
        "Zhaoyang Cong",
        "Zhen Qin",
        "Zhenhua Fan",
        "Zhihang Yu",
        "Zhuo Jiang",
        "Zijia Wu"
      ],
      "institutions": [
        "MiniMax"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "42-Network Structure Design"
      ],
      "code_url": "https://github.com/MiniMax-AI/MiniMax-01",
      "note_url": "notes/2025/MiniMax-01/note/",
      "prototxt_path": "meta/2025/MiniMax-01.prototxt",
      "update_time": 1768271332
    },
    {
      "id": "MiniCPM4",
      "title": "MiniCPM4: Ultra-Efficient LLMs on End Devices",
      "abbr": "MiniCPM4",
      "url": "http://arxiv.org/abs/2506.07900v1",
      "authors": [
        "MiniCPM Team",
        "Chaojun Xiao",
        "Yuxuan Li",
        "Xu Han",
        "Yuzhuo Bai",
        "Jie Cai",
        "Haotian Chen",
        "Wentong Chen",
        "Xin Cong",
        "Ganqu Cui",
        "Ning Ding",
        "Shengdan Fan",
        "Yewei Fang",
        "Zixuan Fu",
        "Wenyu Guan",
        "Yitong Guan",
        "Junshao Guo",
        "Yufeng Han",
        "Bingxiang He",
        "Yuxiang Huang",
        "Cunliang Kong",
        "Qiuzuo Li",
        "Siyuan Li",
        "Wenhao Li",
        "Yanghao Li",
        "Yishan Li",
        "Zhen Li",
        "Dan Liu",
        "Biyuan Lin",
        "Yankai Lin",
        "Xiang Long",
        "Quanyu Lu",
        "Yaxi Lu",
        "Peiyan Luo",
        "Hongya Lyu",
        "Litu Ou",
        "Yinxu Pan",
        "Zekai Qu",
        "Qundong Shi",
        "Zijun Song",
        "Jiayuan Su",
        "Zhou Su",
        "Ao Sun",
        "Xianghui Sun",
        "Peijun Tang",
        "Fangzheng Wang",
        "Feng Wang",
        "Shuo Wang",
        "Yudong Wang",
        "Yesai Wu",
        "Zhenyu Xiao",
        "Jie Xie",
        "Zihao Xie",
        "Yukun Yan",
        "Jiarui Yuan",
        "Kaihuo Zhang",
        "Lei Zhang",
        "Linyue Zhang",
        "Xueren Zhang",
        "Yudi Zhang",
        "Hengyu Zhao",
        "Weilin Zhao",
        "Weilun Zhao",
        "Yuanqian Zhao",
        "Zhi Zheng",
        "Ge Zhou",
        "Jie Zhou",
        "Wei Zhou",
        "Zihan Zhou",
        "Zixuan Zhou",
        "Zhiyuan Liu",
        "Guoyang Zeng",
        "Chao Jia",
        "Dahai Li",
        "Maosong Sun"
      ],
      "institutions": [
        "MiniCPM"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/MiniCPM4/fig2.png",
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)",
        "42-Network Structure Design"
      ],
      "code_url": "https://github.com/openbmb/minicpm",
      "note_url": "notes/2025/MiniCPM4/note/",
      "prototxt_path": "meta/2025/MiniCPM4.prototxt",
      "update_time": 1768271275
    },
    {
      "id": "MiniMax-M1",
      "title": "MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention",
      "abbr": "MiniMax-M1",
      "url": "http://arxiv.org/abs/2506.13585v1",
      "authors": [
        "MiniMax",
        ":",
        "Aili Chen",
        "Aonian Li",
        "Bangwei Gong",
        "Binyang Jiang",
        "Bo Fei",
        "Bo Yang",
        "Boji Shan",
        "Changqing Yu",
        "Chao Wang",
        "Cheng Zhu",
        "Chengjun Xiao",
        "Chengyu Du",
        "Chi Zhang",
        "Chu Qiao",
        "Chunhao Zhang",
        "Chunhui Du",
        "Congchao Guo",
        "Da Chen",
        "Deming Ding",
        "Dianjun Sun",
        "Dong Li",
        "Enwei Jiao",
        "Haigang Zhou",
        "Haimo Zhang",
        "Han Ding",
        "Haohai Sun",
        "Haoyu Feng",
        "Huaiguang Cai",
        "Haichao Zhu",
        "Jian Sun",
        "Jiaqi Zhuang",
        "Jiaren Cai",
        "Jiayuan Song",
        "Jin Zhu",
        "Jingyang Li",
        "Jinhao Tian",
        "Jinli Liu",
        "Junhao Xu",
        "Junjie Yan",
        "Junteng Liu",
        "Junxian He",
        "Kaiyi Feng",
        "Ke Yang",
        "Kecheng Xiao",
        "Le Han",
        "Leyang Wang",
        "Lianfei Yu",
        "Liheng Feng",
        "Lin Li",
        "Lin Zheng",
        "Linge Du",
        "Lingyu Yang",
        "Lunbin Zeng",
        "Minghui Yu",
        "Mingliang Tao",
        "Mingyuan Chi",
        "Mozhi Zhang",
        "Mujie Lin",
        "Nan Hu",
        "Nongyu Di",
        "Peng Gao",
        "Pengfei Li",
        "Pengyu Zhao",
        "Qibing Ren",
        "Qidi Xu",
        "Qile Li",
        "Qin Wang",
        "Rong Tian",
        "Ruitao Leng",
        "Shaoxiang Chen",
        "Shaoyu Chen",
        "Shengmin Shi",
        "Shitong Weng",
        "Shuchang Guan",
        "Shuqi Yu",
        "Sichen Li",
        "Songquan Zhu",
        "Tengfei Li",
        "Tianchi Cai",
        "Tianrun Liang",
        "Weiyu Cheng",
        "Weize Kong",
        "Wenkai Li",
        "Xiancai Chen",
        "Xiangjun Song",
        "Xiao Luo",
        "Xiao Su",
        "Xiaobo Li",
        "Xiaodong Han",
        "Xinzhu Hou",
        "Xuan Lu",
        "Xun Zou",
        "Xuyang Shen",
        "Yan Gong",
        "Yan Ma",
        "Yang Wang",
        "Yiqi Shi",
        "Yiran Zhong",
        "Yonghong Duan",
        "Yongxiang Fu",
        "Yongyi Hu",
        "Yu Gao",
        "Yuanxiang Fan",
        "Yufeng Yang",
        "Yuhao Li",
        "Yulin Hu",
        "Yunan Huang",
        "Yunji Li",
        "Yunzhi Xu",
        "Yuxin Mao",
        "Yuxuan Shi",
        "Yuze Wenren",
        "Zehan Li",
        "Zelin Li",
        "Zhanxu Tian",
        "Zhengmao Zhu",
        "Zhenhua Fan",
        "Zhenzhen Wu",
        "Zhichao Xu",
        "Zhihang Yu",
        "Zhiheng Lyu",
        "Zhuo Jiang",
        "Zibo Gao",
        "Zijia Wu",
        "Zijian Song",
        "Zijun Sun"
      ],
      "institutions": [
        "MiniMax"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "42-Network Structure Design"
      ],
      "code_url": "https://github.com/MiniMax-AI/MiniMax-M1",
      "note_url": "notes/2025/MiniMax-M1/note/",
      "prototxt_path": "meta/2025/MiniMax-M1.prototxt",
      "update_time": 1768271188
    },
    {
      "id": "MoSA",
      "title": "Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing",
      "abbr": "MoSA",
      "url": "http://arxiv.org/abs/2505.00315v1",
      "authors": [
        "Piotr Pikos",
        "Rbert Csords",
        "Jrgen Schmidhuber"
      ],
      "institutions": [
        "KAUST",
        "Stanford University"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/MoSA/fig1.png",
      "keywords": [
        "42-Network Structure Design"
      ],
      "code_url": "https://github.com/piotrpiekos/MoSA",
      "note_url": "notes/2025/MoSA/note/",
      "prototxt_path": "meta/2025/MoSA.prototxt",
      "update_time": 1768271153
    },
    {
      "id": "PQCache",
      "title": "PQCache: Product Quantization-based KVCache for Long Context LLM Inference",
      "abbr": "PQCache",
      "url": "http://arxiv.org/abs/2407.12820v2",
      "authors": [
        "Hailin Zhang",
        "Xiaodong Ji",
        "Yilin Chen",
        "Fangcheng Fu",
        "Xupeng Miao",
        "Xiaonan Nie",
        "Weipeng Chen",
        "Bin Cui"
      ],
      "institutions": [
        "Peking University",
        "Purdue University",
        "Baichuan Inc"
      ],
      "year": 2025,
      "venue": "SIGMOD",
      "cover": null,
      "keywords": [
        "09-Quantization",
        "19-Quantization (KV Cache)"
      ],
      "code_url": null,
      "note_url": "notes/2025/PQCache/note/",
      "prototxt_path": "meta/2025/PQCache.prototxt",
      "update_time": 1768271105
    },
    {
      "id": "PowerAttention",
      "title": "PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention",
      "abbr": "PowerAttention",
      "url": "http://arxiv.org/abs/2503.03588v1",
      "authors": [
        "Lida Chen",
        "Dong Xu",
        "Chenxin An",
        "Xintao Wang",
        "Yikai Zhang",
        "Jiangjie Chen",
        "Zujie Liang",
        "Feng Wei",
        "Jiaqing Liang",
        "Yanghua Xiao",
        "Wei Wang"
      ],
      "institutions": [
        "Fudan University",
        "The University of Hong Kong",
        "ByteDance"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/PowerAttention/fig1.png",
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)"
      ],
      "code_url": null,
      "note_url": "notes/2025/PowerAttention/note/",
      "prototxt_path": "meta/2025/PowerAttention.prototxt",
      "update_time": 1768271075
    },
    {
      "id": "MixKVQ",
      "title": "MixKVQ: Query-Aware Mixed-Precision KV Cache Quantization for Long-Context Reasoning",
      "abbr": "MixKVQ",
      "url": "http://arxiv.org/abs/2512.19206v1",
      "authors": [
        "Tao Zhang",
        "Ziqian Zeng",
        "Hao Peng",
        "Huiping Zhuang",
        "Cen Chen"
      ],
      "institutions": [
        "South China University of Technology",
        "Beihang University",
        "Pazhou Laboratory"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/MixKVQ/cover.png",
      "keywords": [
        "09-Quantization",
        "19-Quantization (KV Cache)"
      ],
      "code_url": null,
      "note_url": "notes/2025/MixKVQ/note/",
      "prototxt_path": "meta/2025/MixKVQ.prototxt",
      "update_time": 1768270692
    },
    {
      "id": "PagedEviction",
      "title": "PagedEviction: Structured Block-wise KV Cache Pruning for Efficient Large Language Model Inference",
      "abbr": "PagedEviction",
      "url": "http://arxiv.org/abs/2509.04377v1",
      "authors": [
        "Krishna Teja Chitty-Venkata",
        "Jie Ye",
        "Xian-He Sun",
        "Anthony Kougkas",
        "Murali Emani",
        "Venkatram Vishwanath",
        "Bogdan Nicolae"
      ],
      "institutions": [
        "Argonne National Laboratory",
        "Illinois Institute of Technology"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/PagedEviction/fig1.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": null,
      "note_url": "notes/2025/PagedEviction/note/",
      "prototxt_path": "meta/2025/PagedEviction.prototxt",
      "update_time": 1768270619
    },
    {
      "id": "NSA",
      "title": "Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention",
      "abbr": "NSA",
      "url": "http://arxiv.org/abs/2502.11089v1",
      "authors": [
        "Jingyang Yuan",
        "Huazuo Gao",
        "Damai Dai",
        "Junyu Luo",
        "Liang Zhao",
        "Zhengyan Zhang",
        "Zhenda Xie",
        "Y. X. Wei",
        "Lean Wang",
        "Zhiping Xiao",
        "Yuqing Wang",
        "Chong Ruan",
        "Ming Zhang",
        "Wenfeng Liang",
        "Wangding Zeng"
      ],
      "institutions": [
        "DeepSeek-AI",
        "Peking University",
        "University of Washington"
      ],
      "year": 2025,
      "venue": "ACL",
      "cover": "notes/2025/NSA/fig2.png",
      "keywords": [
        "02-Sparsity (Attention)",
        "20-Sparse/Eviction (KV Cache)",
        "42-Network Structure Design"
      ],
      "code_url": null,
      "note_url": "notes/2025/NSA/note/",
      "prototxt_path": "meta/2025/NSA.prototxt",
      "update_time": 1768270494
    },
    {
      "id": "QServer",
      "title": "QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving",
      "abbr": "QServe",
      "url": "http://arxiv.org/abs/2405.04532v2",
      "authors": [
        "Yujun Lin",
        "Haotian Tang",
        "Shang Yang",
        "Zhekai Zhang",
        "Guangxuan Xiao",
        "Chuang Gan",
        "Song Han"
      ],
      "institutions": [
        "Massachusetts Institute of Technology",
        "NVIDIA"
      ],
      "year": 2024,
      "venue": "MLSys",
      "cover": null,
      "keywords": [
        "09-Quantization",
        "19-Quantization (KV Cache)"
      ],
      "code_url": "https://hanlab.mit.edu/projects/qserve",
      "note_url": "notes/2024/QServer/note/",
      "prototxt_path": "meta/2024/QServer.prototxt",
      "update_time": 1768270440
    },
    {
      "id": "LServer",
      "title": "LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention",
      "abbr": "LServer",
      "url": "http://arxiv.org/abs/2502.14866v1",
      "authors": [
        "Shang Yang",
        "Junxian Guo",
        "Haotian Tang",
        "Qinghao Hu",
        "Guangxuan Xiao",
        "Jiaming Tang",
        "Yujun Lin",
        "Zhijian Liu",
        "Yao Lu",
        "Song Han"
      ],
      "institutions": [
        "NVIDIA",
        "Massachusetts Institute of Technology",
        "Shanghai Jiao Tong University"
      ],
      "year": 2025,
      "venue": "MLSys",
      "cover": "notes/2025/LServer/fig5.png",
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)",
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": "https://github.com/mit-han-lab/omniserve",
      "note_url": "notes/2025/LServer/note/",
      "prototxt_path": "meta/2025/LServer.prototxt",
      "update_time": 1768270395
    },
    {
      "id": "LeanK",
      "title": "LeanK: Learnable K Cache Channel Pruning for Efficient Decoding",
      "abbr": "LeanK",
      "url": "http://arxiv.org/abs/2508.02215v1",
      "authors": [
        "Yike Zhang",
        "Zhiyuan He",
        "Huiqiang Jiang",
        "Chengruidong Zhang",
        "Yuqing Yang",
        "Jianyong Wang",
        "Lili Qiu"
      ],
      "institutions": [
        "Tsinghua University",
        "Microsoft Research"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/LeanK/fig2.png",
      "keywords": [
        "01-Sparse/Pruning",
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": "https://github.com/microsoft/MInference/tree/main/LeanK",
      "note_url": "notes/2025/LeanK/note/",
      "prototxt_path": "meta/2025/LeanK.prototxt",
      "update_time": 1768269827
    },
    {
      "id": "FusedKV",
      "title": "Reconstructing KV Caches with Cross-layer Fusion For Enhanced Transformers",
      "abbr": "FusedKV",
      "url": "http://arxiv.org/abs/2512.03870",
      "authors": [
        "Hongzhan Lin",
        "Zhiqi Bai",
        "Xinmiao Zhang",
        "Sen Yang",
        "Xiang Li",
        "Siran Yang",
        "Yunlong Xu",
        "Jiaheng Liu",
        "Yongchi Zhao",
        "Jiamang Wang",
        "Yuchi Xu",
        "Wenbo Su",
        "Bo Zheng"
      ],
      "institutions": [
        "Alibaba",
        "Renmin University of China",
        "ICT",
        "Nanjing University"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/FusedKV/cover.png",
      "keywords": [
        "42-Network Structure Design"
      ],
      "code_url": "https://anonymous.4open.science/r/FusedKV/",
      "note_url": "notes/2025/FusedKV/note/",
      "prototxt_path": "meta/2025/FusedKV.prototxt",
      "update_time": 1768269423
    },
    {
      "id": "LAVa",
      "title": "LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation",
      "abbr": "LAVa",
      "url": "http://arxiv.org/abs/2509.09754v1",
      "authors": [
        "Yiqun Shen",
        "Song Yuan",
        "Zhengze Zhang",
        "Xiaoliang Wang",
        "Daxin Jiang",
        "Nguyen Cam-Tu"
      ],
      "institutions": [
        "Nanjing University",
        "Stepfun"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": "https://github.com/MGDDestiny/Lava",
      "note_url": "notes/2025/LAVa/note/",
      "prototxt_path": "meta/2025/LAVa.prototxt",
      "update_time": 1768208511
    },
    {
      "id": "KVSink",
      "title": "KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs",
      "abbr": "KVSink",
      "url": "http://arxiv.org/abs/2508.04257v1",
      "authors": [
        "Zunhai Su",
        "Kehong Yuan"
      ],
      "institutions": [
        "Tsinghua University"
      ],
      "year": 2025,
      "venue": "COLM",
      "cover": "notes/2025/KVSink/fig8.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": null,
      "note_url": "notes/2025/KVSink/note/",
      "prototxt_path": "meta/2025/KVSink.prototxt",
      "update_time": 1768208382
    },
    {
      "id": "kvpress",
      "title": "kvpress: LLM KV cache compression made easy",
      "abbr": "kvpress",
      "url": "https://github.com/NVIDIA/kvpress",
      "authors": [
        "NVIDIA"
      ],
      "institutions": [
        "NVIDIA"
      ],
      "year": 2025,
      "venue": "github",
      "cover": "notes/2025/kvpress/cover.jpg",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)",
        "46-Tool"
      ],
      "code_url": "https://github.com/NVIDIA/kvpress",
      "note_url": "notes/2025/kvpress/note/",
      "prototxt_path": "meta/2025/kvpress.prototxt",
      "update_time": 1768200165
    },
    {
      "id": "KVmix",
      "title": "KVmix: Gradient-Based Layer Importance-Aware Mixed-Precision Quantization for KV Cache",
      "abbr": "KVmix",
      "url": "http://arxiv.org/abs/2506.08018v1",
      "authors": [
        "Fei Li",
        "Song Liu",
        "Weiguo Wu",
        "Shiqiang Nie",
        "Jinyu Wang"
      ],
      "institutions": [
        "Xi'an Jiaotong University"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/KVmix/fig3.png",
      "keywords": [
        "09-Quantization",
        "19-Quantization (KV Cache)"
      ],
      "code_url": "https://github.com/LfLab-AI/KVmix",
      "note_url": "notes/2025/KVmix/note/",
      "prototxt_path": "meta/2025/KVmix.prototxt",
      "update_time": 1768200055
    },
    {
      "id": "KVLink",
      "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
      "abbr": "KVLink",
      "url": "http://arxiv.org/abs/2502.16002v1",
      "authors": [
        "Jingbo Yang",
        "Bairu Hou",
        "Wei Wei",
        "Yujia Bao",
        "Shiyu Chang"
      ],
      "institutions": [
        "UC Santa Barbara",
        "Center for Advanced AI"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/KVLink/fig1.png",
      "keywords": [
        "21-KV Cache Management"
      ],
      "code_url": "https://github.com/UCSB-NLP-Chang/KVLink",
      "note_url": "notes/2025/KVLink/note/",
      "prototxt_path": "meta/2025/KVLink.prototxt",
      "update_time": 1768199912
    },
    {
      "id": "KVCache-Factory",
      "title": "Unified KV Cache Compression Methods for Auto-Regressive Models",
      "abbr": "KVCache-Factory",
      "url": "https://github.com/Zefan-Cai/KVCache-Factory",
      "authors": [
        "Zefan Cai"
      ],
      "institutions": [],
      "year": 2025,
      "venue": "github",
      "cover": "notes/2025/KVCache-Factory/cover.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)",
        "46-Tool"
      ],
      "code_url": "https://github.com/Zefan-Cai/KVCache-Factory",
      "note_url": "notes/2025/KVCache-Factory/note/",
      "prototxt_path": "meta/2025/KVCache-Factory.prototxt",
      "update_time": 1768199825
    },
    {
      "id": "KeepKV",
      "title": "KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference",
      "abbr": "KeepKV",
      "url": "http://arxiv.org/abs/2504.09936v1",
      "authors": [
        "Yuxuan Tian",
        "Zihan Wang",
        "Yebo Peng",
        "Aomufei Yuan",
        "Zhiming Wang",
        "Bairen Yi",
        "Xin Liu",
        "Yong Cui",
        "Tong Yang"
      ],
      "institutions": [
        "Peking University",
        "ByteDance",
        "Tsinghua University"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/KeepKV/fig1.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": null,
      "note_url": "notes/2025/KeepKV/note/",
      "prototxt_path": "meta/2025/KeepKV.prototxt",
      "update_time": 1768199741
    },
    {
      "id": "Kascade",
      "title": "Kascade: A Practical Sparse Attention Method for Long-Context LLM Inference",
      "abbr": "Kascade",
      "url": "http://arxiv.org/abs/2512.16391v1",
      "authors": [
        "Dhruv Deshmukh",
        "Saurabh Goyal",
        "Nipun Kwatra",
        "Ramachandran Ramjee"
      ],
      "institutions": [
        "Microsoft Research India"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/Kascade/cover.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": "https://github.com/microsoft/kascade",
      "note_url": "notes/2025/Kascade/note/",
      "prototxt_path": "meta/2025/Kascade.prototxt",
      "update_time": 1768199139
    },
    {
      "id": "HCAttention",
      "title": "HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs",
      "abbr": "HCAttention",
      "url": "http://arxiv.org/abs/2507.19823v1",
      "authors": [
        "Dongquan Yang",
        "Yifan Yang",
        "Xiaotian Yu",
        "Xianbiao Qi",
        "Rong Xiao"
      ],
      "institutions": [
        "Intellifusion Inc."
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/HCAttention/fig1.png",
      "keywords": [
        "19-Quantization (KV Cache)",
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": null,
      "note_url": "notes/2025/HCAttention/note/",
      "prototxt_path": "meta/2025/HCAttention.prototxt",
      "update_time": 1768199096
    },
    {
      "id": "HashAttention",
      "title": "HashAttention: Semantic Sparsity for Faster Inference",
      "abbr": "HashAttention",
      "url": "https://openreview.net/forum?id=Em2oaXd8Dc",
      "authors": [
        "Aditya Desai",
        "Shuo Yang",
        "Alejandro Cuadron",
        "Matei Zaharia",
        "Joseph E. Gonzalez",
        "Ion Stoica"
      ],
      "institutions": [
        "UC Berkeley"
      ],
      "year": 2025,
      "venue": "ICML",
      "cover": "notes/2025/HashAttention/fig2.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": "https://github.com/xAlg-ai/HashAttention-1.0",
      "note_url": "notes/2025/HashAttention/note/",
      "prototxt_path": "meta/2025/HashAttention.prototxt",
      "update_time": 1768198661
    },
    {
      "id": "D2BMKWKF",
      "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache at a Large Cloud Provider",
      "abbr": "D2BMKWKF",
      "url": "http://arxiv.org/abs/2506.02634v4",
      "authors": [
        "Jiahao Wang",
        "Jinbo Han",
        "Xingda Wei",
        "Sijie Shen",
        "Dingyan Zhang",
        "Chenguang Fang",
        "Rong Chen",
        "Wenyuan Yu",
        "Haibo Chen"
      ],
      "institutions": [
        "Shanghai Jiao Tong University",
        "Alibaba Group"
      ],
      "year": 2025,
      "venue": "ATC",
      "cover": "notes/2025/D2BMKWKF/cover.png",
      "keywords": [
        "21-KV Cache Management",
        "40-LLM Deployment"
      ],
      "code_url": null,
      "note_url": "notes/2025/D2BMKWKF/note/",
      "prototxt_path": "meta/2025/D2BMKWKF.prototxt",
      "update_time": 1768190496
    },
    {
      "id": "FreqKV",
      "title": "FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension",
      "abbr": "FreqKV",
      "url": "http://arxiv.org/abs/2505.00570v2",
      "authors": [
        "Jushi Kai",
        "Boyi Zeng",
        "Yixuan Wang",
        "Haoli Bai",
        "Ziwei He",
        "Bo Jiang",
        "Zhouhan Lin"
      ],
      "institutions": [
        "Shanghai Jiao Tong University",
        "Huawei"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/FreqKV/fig2.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": null,
      "note_url": "notes/2025/FreqKV/note/",
      "prototxt_path": "meta/2025/FreqKV.prototxt",
      "update_time": 1768188252
    },
    {
      "id": "FreeKV",
      "title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference",
      "abbr": "FreeKV",
      "url": "http://arxiv.org/abs/2505.13109",
      "authors": [
        "Guangda Liu",
        "Chengwei Li",
        "Zhenyu Ning",
        "Jing Lin",
        "Yiwu Yao",
        "Danning Ke",
        "Minyi Guo",
        "Jieru Zhao"
      ],
      "institutions": [
        "Shanghai Jiao Tong University",
        "Huawei"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/FreeKV/cover.png",
      "keywords": [
        "21-KV Cache Management",
        "40-LLM Deployment"
      ],
      "code_url": null,
      "note_url": "notes/2025/FreeKV/note/",
      "prototxt_path": "meta/2025/FreeKV.prototxt",
      "update_time": 1768187837
    },
    {
      "id": "GLA",
      "title": "Hardware-Efficient Attention for Fast Decoding",
      "abbr": "GLA",
      "url": "http://arxiv.org/abs/2505.21487v1",
      "authors": [
        "Ted Zadouri",
        "Hubert Strauss",
        "Tri Dao"
      ],
      "institutions": [
        "Princeton University"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/GLA/gla.png",
      "keywords": [
        "42-Network Structure Design"
      ],
      "code_url": "https://github.com/Dao-AILab/grouped-latent-attention",
      "note_url": "notes/2025/GLA/note/",
      "prototxt_path": "meta/2025/GLA.prototxt",
      "update_time": 1768187112
    },
    {
      "id": "CacheBlend",
      "title": "CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion",
      "abbr": "CacheBlend",
      "url": "http://arxiv.org/abs/2405.16444",
      "authors": [
        "Jiayi Yao",
        "Hanchen Li",
        "Yuhan Liu",
        "Siddhant Ray",
        "Yihua Cheng",
        "Qizheng Zhang",
        "Kuntai Du",
        "Shan Lu",
        "Junchen Jiang"
      ],
      "institutions": [
        "University of Chicago",
        "Stanford University",
        "Microsoft Research"
      ],
      "year": 2025,
      "venue": "EuroSys",
      "cover": "notes/2025/CacheBlend/fig1.png",
      "keywords": [
        "21-KV Cache Management"
      ],
      "code_url": "https://github.com/LMCache/LMCache",
      "note_url": "notes/2025/CacheBlend/note/",
      "prototxt_path": "meta/2025/CacheBlend.prototxt",
      "update_time": 1768186162
    },
    {
      "id": "RoundAttention",
      "title": "Round Attention: A Novel Round-Level Attention Mechanism to Accelerate LLM Inference",
      "abbr": "RoundAttention",
      "url": "http://arxiv.org/abs/2502.15294v3",
      "authors": [
        "Yaohua Tang",
        "Zhicheng Hu",
        "Kun Cheng",
        "Fan Mo",
        "Qiheng Lv",
        "Hua Wang",
        "Zhi Chen"
      ],
      "institutions": [
        "Moore Threads AI"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/RoundAttention/cover.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)",
        "21-KV Cache Management"
      ],
      "code_url": null,
      "note_url": "notes/2025/RoundAttention/note/",
      "prototxt_path": "meta/2025/RoundAttention.prototxt",
      "update_time": 1768185911
    },
    {
      "id": "FlowKV",
      "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via Isolated Key-Value Cache Management",
      "abbr": "FlowKV",
      "url": "http://arxiv.org/abs/2505.15347v1",
      "authors": [
        "Xiang Liu",
        "Hong Chen",
        "Xuming Hu",
        "Xiaowen Chu"
      ],
      "institutions": [
        "Hong Kong University of Science and Technology"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/FlowKV/cover.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": null,
      "note_url": "notes/2025/FlowKV/note/",
      "prototxt_path": "meta/2025/FlowKV.prototxt",
      "update_time": 1768185132
    },
    {
      "id": "Task-KV",
      "title": "Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads",
      "abbr": "Task-KV",
      "url": "http://arxiv.org/abs/2501.15113v1",
      "authors": [
        "Xingyang He",
        "Jie Liu",
        "Shaowei Chen"
      ],
      "institutions": [
        "NanKai University"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/Task-KV/fig6.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": null,
      "note_url": "notes/2025/Task-KV/note/",
      "prototxt_path": "meta/2025/Task-KV.prototxt",
      "update_time": 1768130371
    },
    {
      "id": "DefensiveKV",
      "title": "Taming the Fragility of KV Cache Eviction in LLM Inference",
      "abbr": "DefensiveKV",
      "url": "http://arxiv.org/abs/2510.13334v1",
      "authors": [
        "Yuan Feng",
        "Haoyu Guo",
        "JunLin Lv",
        "S. Kevin Zhou",
        "Xike Xie"
      ],
      "institutions": [
        "University of Science and Technology of China",
        "Suzhou Institute for Advanced Research",
        "USTC"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": "https://github.com/FFY0/DefensiveKV",
      "note_url": "notes/2025/DefensiveKV/note/",
      "prototxt_path": "meta/2025/DefensiveKV.prototxt",
      "update_time": 1768129560
    },
    {
      "id": "SqueezeLLM",
      "title": "SqueezeLLM: Dense-and-Sparse Quantization",
      "abbr": "SqueezeLLM",
      "url": "http://arxiv.org/abs/2306.07629",
      "authors": [
        "Sehoon Kim",
        "Coleman Hooper",
        "Amir Gholami",
        "Zhen Dong",
        "Xiuyu Li",
        "Sheng Shen",
        "Michael W. Mahoney",
        "Kurt Keutzer"
      ],
      "institutions": [
        "UC Berkeley"
      ],
      "year": 2024,
      "venue": "ICML",
      "cover": "notes/2024/SqueezeLLM/squeezeLLM.png",
      "keywords": [
        "09-Quantization",
        "19-Quantization (KV Cache)"
      ],
      "code_url": "https://github.com/SqueezeAILab/SqueezeLLM",
      "note_url": "notes/2024/SqueezeLLM/note/",
      "prototxt_path": "meta/2024/SqueezeLLM.prototxt",
      "update_time": 1768129526
    },
    {
      "id": "LightningAttention",
      "title": "Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention",
      "abbr": "LightningAttention",
      "url": "http://arxiv.org/abs/2405.17381v2",
      "authors": [
        "Zhen Qin",
        "Weigao Sun",
        "Dong Li",
        "Xuyang Shen",
        "Weixuan Sun",
        "Yiran Zhong"
      ],
      "institutions": [
        "OpenNLPLab"
      ],
      "year": 2024,
      "venue": "ICML",
      "cover": null,
      "keywords": [
        "42-Network Structure Design"
      ],
      "code_url": "https://github.com/OpenNLPLab/TransnormerLLM",
      "note_url": "notes/2024/LightningAttention/note/",
      "prototxt_path": "meta/2024/LightningAttention.prototxt",
      "update_time": 1768129490
    },
    {
      "id": "ZipCache",
      "title": "ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification",
      "abbr": "ZipCache",
      "url": "http://arxiv.org/abs/2405.14256v1",
      "authors": [
        "Yefei He",
        "Luoming Zhang",
        "Weijia Wu",
        "Jing Liu",
        "Hong Zhou",
        "Bohan Zhuang"
      ],
      "institutions": [
        "Zhejiang University",
        "National University of Singapore",
        "Monash University"
      ],
      "year": 2024,
      "venue": "NeurIPS",
      "cover": null,
      "keywords": [
        "09-Quantization",
        "19-Quantization (KV Cache)"
      ],
      "code_url": null,
      "note_url": "notes/2024/ZipCache/note/",
      "prototxt_path": "meta/2024/ZipCache.prototxt",
      "update_time": 1768129395
    },
    {
      "id": "H1B-KV",
      "title": "H1B-KV: Hybrid One-Bit Caches for Memory-Efficient Large Language Model Inference",
      "abbr": "H1B-KV",
      "url": "http://arxiv.org/abs/2510.05529v1",
      "authors": [
        "Harshil Vejendla"
      ],
      "institutions": [
        "Rutgers University"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "09-Quantization",
        "19-Quantization (KV Cache)"
      ],
      "code_url": null,
      "note_url": "notes/2025/H1B-KV/note/",
      "prototxt_path": "meta/2025/H1B-KV.prototxt",
      "update_time": 1768129295
    },
    {
      "id": "FlexPrefill",
      "title": "FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference",
      "abbr": "FlexPrefill",
      "url": "http://arxiv.org/abs/2502.20766v1",
      "authors": [
        "Xunhao Lai",
        "Jianqiao Lu",
        "Yao Luo",
        "Yiyuan Ma",
        "Xun Zhou"
      ],
      "institutions": [
        "Peking University",
        "The University of Hong Kong",
        "ByteDance"
      ],
      "year": 2025,
      "venue": "ICLR",
      "cover": "notes/2025/FlexPrefill/fig1.png",
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)"
      ],
      "code_url": "https://github.com/bytedance/FlexPrefill",
      "note_url": "notes/2025/FlexPrefill/note/",
      "prototxt_path": "meta/2025/FlexPrefill.prototxt",
      "update_time": 1768128941
    },
    {
      "id": "StarAttention",
      "title": "Star Attention: Efficient LLM Inference over Long Sequences",
      "abbr": "StarAttention",
      "url": "https://openreview.net/forum?id=QY7Au9nZwp",
      "authors": [
        "Shantanu Acharya",
        "Fei Jia",
        "Boris Ginsburg"
      ],
      "institutions": [
        "NVIDIA"
      ],
      "year": 2025,
      "venue": "ICML",
      "cover": "notes/2025/StarAttention/fig1.png",
      "keywords": [
        "21-KV Cache Management"
      ],
      "code_url": "https://github.com/NVIDIA/Star-Attention",
      "note_url": "notes/2025/StarAttention/note/",
      "prototxt_path": "meta/2025/StarAttention.prototxt",
      "update_time": 1768127733
    },
    {
      "id": "SALS",
      "title": "SALS: Sparse Attention in Latent Space for KV cache Compression",
      "abbr": "SALS",
      "url": "http://arxiv.org/abs/2510.24273v1",
      "authors": [
        "Junlin Mu",
        "Hantao Huang",
        "Jihang Zhang",
        "Minghui Yu",
        "Tao Wang",
        "Yidong Li"
      ],
      "institutions": [
        "Beijing Jiaotong University",
        "ByteDance Seed"
      ],
      "year": 2025,
      "venue": "NeurIPS",
      "cover": "notes/2025/SALS/fig3.png",
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)",
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": null,
      "note_url": "notes/2025/SALS/note/",
      "prototxt_path": "meta/2025/SALS.prototxt",
      "update_time": 1768127645
    },
    {
      "id": "SageAttention3",
      "title": "SageAttention3: Microscaling FP4 Attention for Inference and An Exploration of 8-Bit Training",
      "abbr": "SageAttention3",
      "url": "http://arxiv.org/abs/2505.11594v1",
      "authors": [
        "Jintao Zhang",
        "Jia Wei",
        "Pengle Zhang",
        "Xiaoming Xu",
        "Haofeng Huang",
        "Haoxu Wang",
        "Kai Jiang",
        "Jun Zhu",
        "Jianfei Chen"
      ],
      "institutions": [
        "Tsinghua University"
      ],
      "year": 2025,
      "venue": "NeurIPS",
      "cover": "notes/2025/SageAttention3/cover.png",
      "keywords": [
        "09-Quantization",
        "19-Quantization (KV Cache)"
      ],
      "code_url": "https://github.com/thu-ml/SageAttention",
      "note_url": "notes/2025/SageAttention3/note/",
      "prototxt_path": "meta/2025/SageAttention3.prototxt",
      "update_time": 1768127223
    },
    {
      "id": "RetroAttention",
      "title": "Retrospective Sparse Attention for Efficient Long-Context Generation",
      "abbr": "RetroAttention",
      "url": "http://arxiv.org/abs/2508.09001v1",
      "authors": [
        "Seonghwan Choi",
        "Beomseok Kang",
        "Dongwon Jo",
        "Jae-Joon Kim"
      ],
      "institutions": [
        "Seoul National University"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/RetroAttention/fig3.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)",
        "21-KV Cache Management"
      ],
      "code_url": null,
      "note_url": "notes/2025/RetroAttention/note/",
      "prototxt_path": "meta/2025/RetroAttention.prototxt",
      "update_time": 1768127043
    },
    {
      "id": "SALE",
      "title": "SALE: Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling",
      "abbr": "SALE",
      "url": "http://arxiv.org/abs/2505.24179v1",
      "authors": [
        "Xiaodong Ji",
        "Hailin Zhang",
        "Fangcheng Fu",
        "Bin Cui"
      ],
      "institutions": [
        "Peking University"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/SALE/fig1.png",
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)",
        "09-Quantization"
      ],
      "code_url": "https://github.com/BirdChristopher/SALE",
      "note_url": "notes/2025/SALE/note/",
      "prototxt_path": "meta/2025/SALE.prototxt",
      "update_time": 1768126244
    },
    {
      "id": "SpindleKV",
      "title": "SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers",
      "abbr": "SpindleKV",
      "url": "http://arxiv.org/abs/2507.06517v1",
      "authors": [
        "Zicong Tang",
        "Shi Luohe",
        "Zuchao Li",
        "Baoyuan Qi",
        "Guoming Liu",
        "Lefei Zhang",
        "Ping Wang"
      ],
      "institutions": [
        "Wuhan University",
        "Xiaomi"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/SpindleKV/fig2.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": "https://github.com/tyxqc/SpindleKV",
      "note_url": "notes/2025/SpindleKV/note/",
      "prototxt_path": "meta/2025/SpindleKV.prototxt",
      "update_time": 1768126170
    },
    {
      "id": "SlimInfer",
      "title": "SlimInfer: Accelerating Long-Context LLM Inference via Dynamic Token Pruning",
      "abbr": "SlimInfer",
      "url": "http://arxiv.org/abs/2508.06447v1",
      "authors": [
        "Lingkun Long",
        "Rubing Yang",
        "Yushi Huang",
        "Desheng Hui",
        "Ao Zhou",
        "Jianlei Yang"
      ],
      "institutions": [
        "Beihang University",
        "Hong Kong University of Science and Technology"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/SlimInfer/fig4.png",
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)",
        "20-Sparse/Eviction (KV Cache)",
        "21-KV Cache Management"
      ],
      "code_url": null,
      "note_url": "notes/2025/SlimInfer/note/",
      "prototxt_path": "meta/2025/SlimInfer.prototxt",
      "update_time": 1768125886
    },
    {
      "id": "H2O",
      "title": "H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models",
      "abbr": "H2O",
      "url": "http://arxiv.org/abs/2306.14048",
      "authors": [
        "Zhenyu Zhang",
        "Ying Sheng",
        "Tianyi Zhou",
        "Tianlong Chen",
        "Lianmin Zheng",
        "Ruisi Cai",
        "Zhao Song",
        "Yuandong Tian",
        "Christopher R",
        "Clark Barrett",
        "Zhangyang Wang",
        "Beidi Chen"
      ],
      "institutions": [
        "University of Texas at Austin",
        "Stanford University",
        "University of California, San Diego",
        "University of California, Berkeley",
        "Meta AI (FAIR)",
        "Carnegie Mellon University"
      ],
      "year": 2023,
      "venue": "arXiv",
      "cover": "notes/2023/H2O/h2o.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": "https://github.com/FMInference/H2O",
      "note_url": "notes/2023/H2O/note/",
      "prototxt_path": "meta/2023/H2O.prototxt",
      "update_time": 1768012703
    },
    {
      "id": "FlashMask",
      "title": "FlashMask: Efficient and Rich Mask Extension of FlashAttention",
      "abbr": "FlashMask",
      "url": "http://arxiv.org/abs/2410.01359v1",
      "authors": [
        "Guoxia Wang",
        "Jinle Zeng",
        "Xiyuan Xiao",
        "Siming Wu",
        "Jiabin Yang",
        "Lujing Zheng",
        "Zeyu Chen",
        "Jiang Bian",
        "Dianhai Yu",
        "Haifeng Wang"
      ],
      "institutions": [
        "Baidu"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/FlashMask/flashmask.png",
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)"
      ],
      "code_url": "https://github.com/PaddlePaddle/PaddleNLP/tree/develop",
      "note_url": "notes/2024/FlashMask/note/",
      "prototxt_path": "meta/2024/FlashMask.prototxt",
      "update_time": 1768012626
    },
    {
      "id": "XAttention",
      "title": "XAttention: Block Sparse Attention with Antidiagonal Scoring",
      "abbr": "XAttention",
      "url": "http://arxiv.org/abs/2503.16428v1",
      "authors": [
        "Ruyi Xu",
        "Guangxuan Xiao",
        "Haofeng Huang",
        "Junxian Guo",
        "Song Han"
      ],
      "institutions": [
        "Tsinghua University",
        "Massachusetts Institute of Technology",
        "Shanghai Jiao Tong University",
        "NVIDIA"
      ],
      "year": 2025,
      "venue": "ICML",
      "cover": "notes/2025/XAttention/fig1.png",
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)"
      ],
      "code_url": "https://github.com/mit-han-lab/x-attention",
      "note_url": "notes/2025/XAttention/note/",
      "prototxt_path": "meta/2025/XAttention.prototxt",
      "update_time": 1768012233
    },
    {
      "id": "ZipVL",
      "title": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification",
      "abbr": "ZipVL",
      "url": "http://arxiv.org/abs/2410.08584v2",
      "authors": [
        "Yefei He",
        "Feng Chen",
        "Jing Liu",
        "Wenqi Shao",
        "Hong Zhou",
        "Kaipeng Zhang",
        "Bohan Zhuang"
      ],
      "institutions": [
        "Zhejiang University",
        "Shanghai AI Laboratory",
        "The University of Adelaide",
        "Monash University"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/ZipVL/fig2.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": null,
      "note_url": "notes/2024/ZipVL/note/",
      "prototxt_path": "meta/2024/ZipVL.prototxt",
      "update_time": 1768012220
    },
    {
      "id": "TOVA",
      "title": "Transformers are Multi-State RNNs",
      "abbr": "TOVA",
      "url": "http://arxiv.org/abs/2401.06104v2",
      "authors": [
        "Matanel Oren",
        "Michael Hassid",
        "Nir Yarden",
        "Yossi Adi",
        "Roy Schwartz"
      ],
      "institutions": [
        "The Hebrew University of Jerusalem",
        "FAIR"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/TOVA/tova.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": "https://github.com/schwartz-lab-NLP/TOVA",
      "note_url": "notes/2024/TOVA/note/",
      "prototxt_path": "meta/2024/TOVA.prototxt",
      "update_time": 1768010716
    },
    {
      "id": "DSA",
      "title": "Transformer Acceleration with Dynamic Sparse Attention",
      "abbr": "DSA",
      "url": "http://arxiv.org/abs/2110.11299v1",
      "authors": [
        "Liu Liu",
        "Zheng Qu",
        "Zhaodong Chen",
        "Yufei Ding",
        "Yuan Xie"
      ],
      "institutions": [
        "University of California"
      ],
      "year": 2022,
      "venue": "TC",
      "cover": "notes/2022/DSA/fig2.png",
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)"
      ],
      "code_url": null,
      "note_url": "notes/2022/DSA/note/",
      "prototxt_path": "meta/2022/DSA.prototxt",
      "update_time": 1768010701
    },
    {
      "id": "FlashInfer",
      "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving",
      "abbr": "FlashInfer",
      "url": "http://arxiv.org/abs/2501.01005v2",
      "authors": [
        "Zihao Ye",
        "Lequn Chen",
        "Ruihang Lai",
        "Wuwei Lin",
        "Yineng Zhang",
        "Stephanie Wang",
        "Tianqi Chen",
        "Baris Kasikci",
        "Vinod Grover",
        "Arvind Krishnamurthy",
        "Luis Ceze"
      ],
      "institutions": [
        "NVIDIA",
        "University of Washington",
        "Perplexity AI",
        "Carnegie Mellon University"
      ],
      "year": 2025,
      "venue": "MLSys",
      "cover": "notes/2025/FlashInfer/cover.png",
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)",
        "21-KV Cache Management",
        "46-Tool"
      ],
      "code_url": "https://github.com/flashinfer-ai/flashinfer",
      "note_url": "notes/2025/FlashInfer/note/",
      "prototxt_path": "meta/2025/FlashInfer.prototxt",
      "update_time": 1768010645
    },
    {
      "id": "topk-decoding",
      "title": "Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs",
      "abbr": "topk-decoding",
      "url": "http://arxiv.org/abs/2502.06766v2",
      "authors": [
        "Ryan Synk",
        "Monte Hoover",
        "John Kirchenbauer",
        "Neel Jain",
        "Alex Stein",
        "Manli Shu",
        "Josue Melendez Sanchez",
        "Ramani Duraiswami",
        "Tom Goldstein"
      ],
      "institutions": [
        "University of Maryland",
        "Salesforce Research"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/topk-decoding/alg1.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": "https://github.com/ryansynk/topk-decoding",
      "note_url": "notes/2025/topk-decoding/note/",
      "prototxt_path": "meta/2025/topk-decoding.prototxt",
      "update_time": 1768010484
    },
    {
      "id": "FlashDecoding",
      "title": "Flash-Decoding for long-context inference",
      "abbr": "Flash-Decoding",
      "url": "https://crfm.stanford.edu/2023/10/12/flashdecoding.html",
      "authors": [
        "Tri Dao"
      ],
      "institutions": [
        "Stanford University"
      ],
      "year": 2023,
      "venue": "blog",
      "cover": "notes/2023/FlashDecoding/cover.gif",
      "keywords": [
        "46-Tool"
      ],
      "code_url": null,
      "note_url": "notes/2023/FlashDecoding/note/",
      "prototxt_path": "meta/2023/FlashDecoding.prototxt",
      "update_time": 1768010480
    },
    {
      "id": "FSA",
      "title": "Flash Sparse Attention: An Alternative Efficient Implementation of Native Sparse Attention Kernel",
      "abbr": "FSA",
      "url": "http://arxiv.org/abs/2508.18224v1",
      "authors": [
        "Ran Yan",
        "Youhe Jiang",
        "Binhang Yuan"
      ],
      "institutions": [
        "Hong Kong University of Science and Technology"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/FSA/fig1.png",
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)"
      ],
      "code_url": "https://github.com/Relaxed-System-Lab/Flash-Sparse-Attention",
      "note_url": "notes/2025/FSA/note/",
      "prototxt_path": "meta/2025/FSA.prototxt",
      "update_time": 1768010112
    },
    {
      "id": "FastKV",
      "title": "FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation",
      "abbr": "FastKV",
      "url": "http://arxiv.org/abs/2502.01068v1",
      "authors": [
        "Dongwon Jo",
        "Jiwon Song",
        "Yulhwa Kim",
        "Jae-Joon Kim"
      ],
      "institutions": [
        "Seoul National University"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/FastKV/fig2.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": "https://github.com/dongwonjo/FastKV",
      "note_url": "notes/2025/FastKV/note/",
      "prototxt_path": "meta/2025/FastKV.prototxt",
      "update_time": 1768010010
    },
    {
      "id": "2ZU1IWL6",
      "title": "Fast and Simplex: 2-Simplicial Attention in Triton",
      "abbr": "2ZU1IWL6",
      "url": "http://arxiv.org/abs/2507.02754v1",
      "authors": [
        "Aurko Roy",
        "Timothy Chou",
        "Sai Surya Duvvuri",
        "Sijia Chen",
        "Jiecao Yu",
        "Xiaodong Wang",
        "Manzil Zaheer",
        "Rohan Anil"
      ],
      "institutions": [
        "Meta"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "42-Network Structure Design"
      ],
      "code_url": null,
      "note_url": "notes/2025/2ZU1IWL6/note/",
      "prototxt_path": "meta/2025/2ZU1IWL6.prototxt",
      "update_time": 1768009953
    },
    {
      "id": "Chelsea",
      "title": "Efficient Long-Context LLM Inference via KV Cache Clustering",
      "abbr": "Chelsea",
      "url": "http://arxiv.org/abs/2506.11418v1",
      "authors": [
        "Jie Hu",
        "Shengnan Wang",
        "Yutong He",
        "Ping Gong",
        "Jiawei Yi",
        "Juncheng Zhang",
        "Youhui Bai",
        "Renhai Chen",
        "Gong Zhang",
        "Cheng Li",
        "Kun Yuan"
      ],
      "institutions": [
        "Peking University",
        "Huawei",
        "University of Science and Technology of China"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": null,
      "note_url": "notes/2025/Chelsea/note/",
      "prototxt_path": "meta/2025/Chelsea.prototxt",
      "update_time": 1768009905
    },
    {
      "id": "EvolKV",
      "title": "EvolKV: Evolutionary KV Cache Compression for LLM Inference",
      "abbr": "EvolKV",
      "url": "http://arxiv.org/abs/2509.08315v1",
      "authors": [
        "Bohan Yu",
        "Yekun Chai"
      ],
      "institutions": [
        "University of Chinese Academy of Sciences",
        "Institute of Automation",
        "ETH Zurich"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/EvolKV/fig1.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": null,
      "note_url": "notes/2025/EvolKV/note/",
      "prototxt_path": "meta/2025/EvolKV.prototxt",
      "update_time": 1768007970
    },
    {
      "id": "PackCache",
      "title": "PackCache: A Training-Free Acceleration Method for Unified Autoregressive Video Generation via Compact KV-Cache",
      "abbr": "PackCache",
      "url": "http://arxiv.org/abs/2601.04359v1",
      "authors": [
        "Kunyang Li",
        "Mubarak Shah",
        "Yuzhang Shang"
      ],
      "institutions": [
        "University of Central Florida"
      ],
      "year": 2026,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": null,
      "note_url": "notes/2026/PackCache/note/",
      "prototxt_path": "meta/2026/PackCache.prototxt",
      "update_time": 1768007605
    },
    {
      "id": "X3NUE78O",
      "title": "GPU-Accelerated INT8 Quantization for KV Cache Compression in Large Language Models",
      "abbr": "X3NUE78O",
      "url": "http://arxiv.org/abs/2601.04719v1",
      "authors": [
        "Maanas Taneja",
        "Purab Shingvi"
      ],
      "institutions": [
        "inst1",
        "inst2"
      ],
      "year": 2026,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "09-Quantization",
        "19-Quantization (KV Cache)"
      ],
      "code_url": null,
      "note_url": "notes/2026/X3NUE78O/note/",
      "prototxt_path": "meta/2026/X3NUE78O.prototxt",
      "update_time": 1768007485
    },
    {
      "id": "ZigZagKV",
      "title": "ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty",
      "abbr": "ZigZagKV",
      "url": "http://arxiv.org/abs/2412.09036v1",
      "authors": [
        "Meizhi Zhong",
        "Xikai Liu",
        "Chen Zhang",
        "Yikun Lei",
        "Yan Gao",
        "Yao Hu",
        "Kehai Chen",
        "Min Zhang"
      ],
      "institutions": [
        "Harbin Institute of Technology",
        "Xiaohongshu"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/ZigZagKV/fig1.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": null,
      "note_url": "notes/2024/ZigZagKV/note/",
      "prototxt_path": "meta/2024/ZigZagKV.prototxt",
      "update_time": 1767954501
    },
    {
      "id": "streaming-llm",
      "title": "Efficient Streaming Language Models with Attention Sinks",
      "abbr": "StreamingLLM",
      "url": "http://arxiv.org/abs/2309.17453v4",
      "authors": [
        "Guangxuan Xiao",
        "Yuandong Tian",
        "Beidi Chen",
        "Song Han",
        "Mike Lewis"
      ],
      "institutions": [
        "Massachusetts Institute of Technology",
        "Meta AI",
        "Carnegie Mellon University",
        "NVIDIA"
      ],
      "year": 2024,
      "venue": "ICLR",
      "cover": "notes/2024/streaming-llm/cover.jpg",
      "keywords": [
        "02-Sparsity (Attention)",
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": "https://github.com/mit-han-lab/streaming-llm",
      "note_url": "notes/2024/streaming-llm/note/",
      "prototxt_path": "meta/2024/streaming-llm.prototxt",
      "update_time": 1767954444
    },
    {
      "id": "DuoAttention",
      "title": "DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads",
      "abbr": "DuoAttention",
      "url": "http://arxiv.org/abs/2410.10819v1",
      "authors": [
        "Guangxuan Xiao",
        "Jiaming Tang",
        "Jingwei Zuo",
        "Junxian Guo",
        "Shang Yang",
        "Haotian Tang",
        "Yao Fu",
        "Song Han"
      ],
      "institutions": [
        "Massachusetts Institute of Technology",
        "NVIDIA"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/DuoAttention/duoattention.png",
      "keywords": [
        "02-Sparsity (Attention)",
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": "https://github.com/mit-han-lab/duo-attention",
      "note_url": "notes/2024/DuoAttention/note/",
      "prototxt_path": "meta/2024/DuoAttention.prototxt",
      "update_time": 1767954412
    },
    {
      "id": "RaaS",
      "title": "Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity",
      "abbr": "RaaS",
      "url": "http://arxiv.org/abs/2502.11147v1",
      "authors": [
        "Junhao Hu",
        "Wenrui Huang",
        "Weidong Wang",
        "Zhenwen Li",
        "Tiancheng Hu",
        "Zhixia Liu",
        "Xusheng Chen",
        "Tao Xie",
        "Yizhou Shan"
      ],
      "institutions": [
        "Peking University",
        "Nanjing University",
        "Huawei"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/RaaS/fig5.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": null,
      "note_url": "notes/2025/RaaS/note/",
      "prototxt_path": "meta/2025/RaaS.prototxt",
      "update_time": 1767954056
    },
    {
      "id": "DeltaLLM",
      "title": "DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference",
      "abbr": "DeltaLLM",
      "url": "http://arxiv.org/abs/2507.19608v1",
      "authors": [
        "Jiawen Qi",
        "Chang Gao",
        "Zhaochun Ren",
        "Qinyu Chen"
      ],
      "institutions": [
        "Leiden University",
        "Delft University of Technology"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/DeltaLLM/fig5.png",
      "keywords": [
        "02-Sparsity (Attention)"
      ],
      "code_url": null,
      "note_url": "notes/2025/DeltaLLM/note/",
      "prototxt_path": "meta/2025/DeltaLLM.prototxt",
      "update_time": 1767953774
    },
    {
      "id": "DeltaAttention",
      "title": "Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction",
      "abbr": "DeltaAttention",
      "url": "http://arxiv.org/abs/2505.11254v1",
      "authors": [
        "Jeffrey Willette",
        "Heejun Lee",
        "Sung Ju Hwang"
      ],
      "institutions": [
        "KAIST",
        "DeepAuto.ai"
      ],
      "year": 2025,
      "venue": "NeurIPS",
      "cover": "notes/2025/DeltaAttention/fig4.png",
      "keywords": [
        "02-Sparsity (Attention)"
      ],
      "code_url": null,
      "note_url": "notes/2025/DeltaAttention/note/",
      "prototxt_path": "meta/2025/DeltaAttention.prototxt",
      "update_time": 1767953635
    },
    {
      "id": "CTkvr",
      "title": "CTkvr: KV Cache Retrieval for Long-Context LLMs via Centroid then Token Indexing",
      "abbr": "CTkvr",
      "url": "http://arxiv.org/abs/2512.15550v1",
      "authors": [
        "Kuan Lu",
        "Shuhang Lin",
        "Sai Wu",
        "Yichen Yao",
        "Junhan Yang",
        "Huan Li",
        "Wei Chu",
        "Xu Yinghui",
        "Yuan Qi",
        "Gang Chen"
      ],
      "institutions": [
        "Zhejiang University",
        "Rutgers University",
        "INFLY Tech"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/CTkvr/cover.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": null,
      "note_url": "notes/2025/CTkvr/note/",
      "prototxt_path": "meta/2025/CTkvr.prototxt",
      "update_time": 1767953388
    },
    {
      "id": "DBudgetKV",
      "title": "DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance",
      "abbr": "DBudgetKV",
      "url": "http://arxiv.org/abs/2502.16886v1",
      "authors": [
        "Xuanfan Ni",
        "Liyan Xu",
        "Chenyang Lyu",
        "Longyue Wang",
        "Mo Yu",
        "Lemao Liu",
        "Fandong Meng",
        "Jie Zhou",
        "Piji Li"
      ],
      "institutions": [
        "WeChat AI"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/DBudgetKV/fig1.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": null,
      "note_url": "notes/2025/DBudgetKV/note/",
      "prototxt_path": "meta/2025/DBudgetKV.prototxt",
      "update_time": 1767953233
    },
    {
      "id": "DSA",
      "title": "DeepSeek-V3.2-Exp: Boosting Long-Context Efficiency with DeepSeek Sparse Attention",
      "abbr": "DSA",
      "url": "https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf",
      "authors": [
        "DeepSeek-AI"
      ],
      "institutions": [
        "DeepSeek-AI"
      ],
      "year": 2025,
      "venue": "Github",
      "cover": "notes/2025/DSA/fig1.png",
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)",
        "20-Sparse/Eviction (KV Cache)",
        "42-Network Structure Design"
      ],
      "code_url": "https://github.com/deepseek-ai/DeepSeek-V3.2-Exp",
      "note_url": "notes/2025/DSA/note/",
      "prototxt_path": "meta/2025/DSA.prototxt",
      "update_time": 1767953224
    },
    {
      "id": "COMET",
      "title": "COMET: Towards Partical W4A4KV4 LLMs Serving",
      "abbr": "COMET",
      "url": "http://arxiv.org/abs/2410.12168v1",
      "authors": [
        "Lian Liu",
        "Haimeng Ren",
        "Long Cheng",
        "Zhaohui Xu",
        "Yudong Pan",
        "Mengdi Wang",
        "Xiaowei Li",
        "Yinhe Han",
        "Ying Wang"
      ],
      "institutions": [
        "Institute of Computing Technology",
        "University of Chinese Academy of Sciences",
        "ShanghaiTech University",
        "North China Electric Power University"
      ],
      "year": 2025,
      "venue": "ASPLOS",
      "cover": "notes/2025/COMET/fig5.png",
      "keywords": [
        "09-Quantization",
        "19-Quantization (KV Cache)"
      ],
      "code_url": null,
      "note_url": "notes/2025/COMET/note/",
      "prototxt_path": "meta/2025/COMET.prototxt",
      "update_time": 1767952262
    },
    {
      "id": "PoD",
      "title": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity",
      "abbr": "PoD",
      "url": "http://arxiv.org/abs/2412.02252v1",
      "authors": [
        "Da Ma",
        "Lu Chen",
        "Situo Zhang",
        "Yuxun Miao",
        "Su Zhu",
        "Zhi Chen",
        "Hongshen Xu",
        "Hanqi Li",
        "Shuai Fan",
        "Lei Pan",
        "Kai Yu"
      ],
      "institutions": [
        "Shanghai Jiao Tong University",
        "ByteDance"
      ],
      "year": 2025,
      "venue": "ICML",
      "cover": "notes/2025/PoD/fig2.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": null,
      "note_url": "notes/2025/PoD/note/",
      "prototxt_path": "meta/2025/PoD.prototxt",
      "update_time": 1767952234
    },
    {
      "id": "ChunkKV",
      "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference",
      "abbr": "ChunkKV",
      "url": "http://arxiv.org/abs/2502.00299",
      "authors": [
        "Xiang Liu",
        "Zhenheng Tang",
        "Peijie Dong",
        "Zeyu Li",
        "Bo Li",
        "Xuming Hu",
        "Xiaowen Chu"
      ],
      "institutions": [
        "Hong Kong University of Science and Technology"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/ChunkKV/fig1.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": null,
      "note_url": "notes/2025/ChunkKV/note/",
      "prototxt_path": "meta/2025/ChunkKV.prototxt",
      "update_time": 1767950791
    },
    {
      "id": "CateKV",
      "title": "CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration",
      "abbr": "CateKV",
      "url": "https://openreview.net/forum?id=u7dlwgKstN",
      "authors": [
        "Haoyun Jiang, Haolin li, jianwei zhang, Fei Huang, Qiang Hu, Minmin Sun, Shuai Xiao, Yong Li, Junyang Lin, Jiangchao Yao"
      ],
      "institutions": [
        "Shanghai Jiao Tong University",
        "Alibaba Group",
        "Fudan University"
      ],
      "year": 2025,
      "venue": "ICML",
      "cover": "notes/2025/CateKV/fig1.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": null,
      "note_url": "notes/2025/CateKV/note/",
      "prototxt_path": "meta/2025/CateKV.prototxt",
      "update_time": 1767950542
    },
    {
      "id": "PruLong",
      "title": "Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context LMs?",
      "abbr": "PruLong",
      "url": "http://arxiv.org/abs/2506.17121v1",
      "authors": [
        "Adithya Bhaskar",
        "Alexander Wettig",
        "Tianyu Gao",
        "Yihe Dong",
        "Danqi Chen"
      ],
      "institutions": [
        "Princeton University"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": "https://github.com/princeton-pli/PruLong",
      "note_url": "notes/2025/PruLong/note/",
      "prototxt_path": "meta/2025/PruLong.prototxt",
      "update_time": 1767950451
    },
    {
      "id": "SharedAttention",
      "title": "Beyond KV Caching: Shared Attention for Efficient LLMs",
      "abbr": "SharedAttention",
      "url": "http://arxiv.org/abs/2407.12866",
      "authors": [
        "Bingli Liao",
        "Danilo Vasconcellos Vargas"
      ],
      "institutions": [
        "Kyushu University"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/SharedAttention/fig1.png",
      "keywords": [
        "42-Network Structure Design"
      ],
      "code_url": "https://github.com/metacarbon/shareAtt/tree/main",
      "note_url": "notes/2024/SharedAttention/note/",
      "prototxt_path": "meta/2024/SharedAttention.prototxt",
      "update_time": 1767950129
    },
    {
      "id": "AttentionPredictor",
      "title": "AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference",
      "abbr": "AttentionPredictor",
      "url": "http://arxiv.org/abs/2502.04077v1",
      "authors": [
        "Qingyue Yang",
        "Jie Wang",
        "Xing Li",
        "Zhihai Wang",
        "Chen Chen",
        "Lei Chen",
        "Xianzhi Yu",
        "Wulong Liu",
        "Jianye Hao",
        "Mingxuan Yuan",
        "Bin Li"
      ],
      "institutions": [
        "University of Science and Technology of China",
        "Huawei"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/AttentionPredictor/fig1.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": null,
      "note_url": "notes/2025/AttentionPredictor/note/",
      "prototxt_path": "meta/2025/AttentionPredictor.prototxt",
      "update_time": 1767950062
    },
    {
      "id": "AhaKV",
      "title": "AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models",
      "abbr": "AhaKV",
      "url": "http://arxiv.org/abs/2506.03762v1",
      "authors": [
        "Yifeng Gu",
        "Zicong Jiang",
        "Jianxiu Jin",
        "Kailing Guo",
        "Ziyang Zhang",
        "Xiangmin Xu"
      ],
      "institutions": [
        "South China University of Technology",
        "Pazhou Laboratory"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": null,
      "note_url": "notes/2025/AhaKV/note/",
      "prototxt_path": "meta/2025/AhaKV.prototxt",
      "update_time": 1767949994
    },
    {
      "id": "AdaSplash",
      "title": "AdaSplash: Adaptive Sparse Flash Attention",
      "abbr": "AdaSplash",
      "url": "http://arxiv.org/abs/2502.12082",
      "authors": [
        "Nuno Gonalves",
        "Marcos Treviso",
        "Andr F. T. Martins"
      ],
      "institutions": [
        "Universidade de Lisboa"
      ],
      "year": 2025,
      "venue": "ICML",
      "cover": null,
      "keywords": [
        "02-Sparsity (Attention)"
      ],
      "code_url": "https://github.com/deep-spin/adasplash",
      "note_url": "notes/2025/AdaSplash/note/",
      "prototxt_path": "meta/2025/AdaSplash.prototxt",
      "update_time": 1767949876
    },
    {
      "id": "AdaKV",
      "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference",
      "abbr": "AdaKV",
      "url": "http://arxiv.org/abs/2407.11550v3",
      "authors": [
        "Yuan Feng",
        "Junlin Lv",
        "Yukun Cao",
        "Xike Xie",
        "S. Kevin Zhou"
      ],
      "institutions": [
        "University of Science and Technology of China"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/AdaKV/fig2.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": "https://github.com/FFY0/AdaKV",
      "note_url": "notes/2024/AdaKV/note/",
      "prototxt_path": "meta/2024/AdaKV.prototxt",
      "update_time": 1767941887
    },
    {
      "id": "SharePrefill",
      "title": "Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing",
      "abbr": "SharePrefill",
      "url": "http://arxiv.org/abs/2505.19578v1",
      "authors": [
        "Dan Peng",
        "Zhihui Fu",
        "Zewen Ye",
        "Zhuoran Song",
        "Jun Wang"
      ],
      "institutions": [
        "OPPO Research Institute",
        "Zhejiang University",
        "Shanghai Jiao Tong University"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/SharePrefill/fig3.png",
      "keywords": [
        "02-Sparsity (Attention)"
      ],
      "code_url": null,
      "note_url": "notes/2025/SharePrefill/note/",
      "prototxt_path": "meta/2025/SharePrefill.prototxt",
      "update_time": 1767940570
    },
    {
      "id": "07NWF4VE",
      "title": "Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching",
      "abbr": "07NWF4VE",
      "url": "https://arxiv.org/abs/2504.06319v2",
      "authors": [
        "Yanhao Dong",
        "Yubo Miao",
        "Weinan Li",
        "Xiao Zheng",
        "Chao Wang",
        "Feng Lyu"
      ],
      "institutions": [
        "Alibaba Cloud",
        "Central South University"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/07NWF4VE/fig1.png",
      "keywords": [
        "21-KV Cache Management"
      ],
      "code_url": null,
      "note_url": "notes/2025/07NWF4VE/note/",
      "prototxt_path": "meta/2025/07NWF4VE.prototxt",
      "update_time": 1767939862
    },
    {
      "id": "SpecEE",
      "title": "SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting",
      "abbr": "SpecEE",
      "url": "http://arxiv.org/abs/2504.08850v1",
      "authors": [
        "Jiaming Xu",
        "Jiayi Pan",
        "Yongkang Zhou",
        "Siming Chen",
        "Jinhao Li",
        "Yaoxiu Lian",
        "Junyi Wu",
        "Guohao Dai"
      ],
      "institutions": [
        "Shanghai Jiao Tong University",
        "Infinigence-AI"
      ],
      "year": 2025,
      "venue": "ISCA",
      "cover": "notes/2025/SpecEE/fig9.png",
      "keywords": [
        "01-Sparse/Pruning",
        "05-Sparsity (Structured)",
        "30-Speculative Decoding",
        "42-Network Structure Design"
      ],
      "code_url": "https://github.com/infinigence/SpecEE",
      "note_url": "notes/2025/SpecEE/note/",
      "prototxt_path": "meta/2025/SpecEE.prototxt",
      "update_time": 1767869479
    },
    {
      "id": "Eagle",
      "title": "EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty",
      "abbr": "EAGLE",
      "url": "http://arxiv.org/abs/2401.15077v2",
      "authors": [
        "Yuhui Li",
        "Fangyun Wei",
        "Chao Zhang",
        "Hongyang Zhang"
      ],
      "institutions": [
        "Peking University",
        "Microsoft Research",
        "University of Waterloo",
        "Vector Institute"
      ],
      "year": 2024,
      "venue": "ICML",
      "cover": "notes/2024/Eagle/eagle.jpg",
      "keywords": [
        "30-Speculative Decoding",
        "40-LLM Deployment"
      ],
      "code_url": "https://github.com/SafeAILab/EAGLE",
      "note_url": "notes/2024/Eagle/note/",
      "prototxt_path": "meta/2024/Eagle.prototxt",
      "update_time": 1767869467
    },
    {
      "id": "VRPJM9OQ",
      "title": "Accelerate Speculative Decoding with Sparse Computation in Verification",
      "abbr": "VRPJM9OQ",
      "url": "http://arxiv.org/abs/2512.21911v1",
      "authors": [
        "Jikai Wang",
        "Jianchao Tan",
        "Yuxuan Hu",
        "Jiayu Qin",
        "Yerui Sun",
        "Yuchen Xie",
        "Xunliang Cai",
        "Juntao Li",
        "Min Zhang"
      ],
      "institutions": [
        "Meituan",
        "Soochow University"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/VRPJM9OQ/cover.png",
      "keywords": [
        "02-Sparsity (Attention)",
        "03-Sparsity (Activation)",
        "30-Speculative Decoding",
        "40-LLM Deployment"
      ],
      "code_url": null,
      "note_url": "notes/2025/VRPJM9OQ/note/",
      "prototxt_path": "meta/2025/VRPJM9OQ.prototxt",
      "update_time": 1767869436
    },
    {
      "id": "SparseSpec",
      "title": "Accelerating Large-Scale Reasoning Model Inference with Sparse Self-Speculative Decoding",
      "abbr": "SparseSpec",
      "url": "http://arxiv.org/abs/2512.01278v1",
      "authors": [
        "Yilong Zhao",
        "Jiaming Tang",
        "Kan Zhu",
        "Zihao Ye",
        "Chi-Chih Chang",
        "Chaofan Lin",
        "Jongseok Park",
        "Guangxuan Xiao",
        "Mohamed S. Abdelfattah",
        "Mingyu Gao",
        "Baris Kasikci",
        "Song Han",
        "Ion Stoica"
      ],
      "institutions": [
        "University of California, Berkeley",
        "Massachusetts Institute of Technology",
        "University of Washington",
        "NVIDIA",
        "Cornell University",
        "Tsinghua University"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/SparseSpec/cover.png",
      "keywords": [
        "02-Sparsity (Attention)",
        "21-KV Cache Management",
        "30-Speculative Decoding",
        "40-LLM Deployment"
      ],
      "code_url": "https://github.com/sspec-project/SparseSpec",
      "note_url": "notes/2025/SparseSpec/note/",
      "prototxt_path": "meta/2025/SparseSpec.prototxt",
      "update_time": 1767869383
    },
    {
      "id": "ESS",
      "title": "ESS: An Offload-Centric Latent-Cache Management Architecture for DeepSeek-V3.2-Exp",
      "abbr": "ESS",
      "url": "http://arxiv.org/abs/2512.10576",
      "authors": [
        "Xinhang Chen",
        "Chao Zhang",
        "Jiahuan He",
        "Wei Liu",
        "Jianming Zhang",
        "Wenlong Zhou",
        "Xiao Li",
        "Pai Zeng",
        "Shiyong Li",
        "Yuanpan Qian",
        "Dong Li",
        "Zhaogeng Li"
      ],
      "institutions": [
        "Baidu"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/ESS/cover.png",
      "keywords": [
        "02-Sparsity (Attention)",
        "20-Sparse/Eviction (KV Cache)",
        "21-KV Cache Management"
      ],
      "code_url": null,
      "note_url": "notes/2025/ESS/note/",
      "prototxt_path": "meta/2025/ESS.prototxt",
      "update_time": 1767868778
    },
    {
      "id": "PWGG5HBE",
      "title": "A Survey on Large Language Model Acceleration based on KV Cache Management",
      "abbr": "PWGG5HBE",
      "url": "http://arxiv.org/abs/2412.19442",
      "authors": [
        "Haoyang Li",
        "Yiming Li",
        "Anxin Tian",
        "Tianhao Tang",
        "Zhanchao Xu",
        "Xuejia Chen",
        "Nicole Hu",
        "Wei Dong",
        "Qing Li",
        "Lei Chen"
      ],
      "institutions": [
        "The Hong Kong Polytechnic University",
        "Huazhong University of Science and Technology",
        "Hong Kong University of Science and Technology",
        "The Chinese University of Hong Kong"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/PWGG5HBE/fig2.png",
      "keywords": [
        "19-Quantization (KV Cache)",
        "20-Sparse/Eviction (KV Cache)",
        "21-KV Cache Management",
        "41-Survey"
      ],
      "code_url": "https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management",
      "note_url": "notes/2024/PWGG5HBE/note/",
      "prototxt_path": "meta/2024/PWGG5HBE.prototxt",
      "update_time": 1767864609
    },
    {
      "id": "SignRoundV2",
      "title": "SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs",
      "abbr": "SignRoundV2",
      "url": "http://arxiv.org/abs/2512.04746v1",
      "authors": [
        "Wenhua Cheng",
        "Weiwei Zhang",
        "Heng Guo",
        "Haihao Shen"
      ],
      "institutions": [
        "Intel"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "09-Quantization"
      ],
      "code_url": "https://github.com/intel/auto-round",
      "note_url": "notes/2025/SignRoundV2/note/",
      "prototxt_path": "meta/2025/SignRoundV2.prototxt",
      "update_time": 1767864059
    },
    {
      "id": "HSA",
      "title": "Every Token Counts: Generalizing 16M Ultra-Long Context in Large Language Models",
      "abbr": "HSA",
      "url": "http://arxiv.org/abs/2511.23319v1",
      "authors": [
        "Xiang Hu",
        "Zhanchao Zhou",
        "Ruiqi Liang",
        "Zehuan Li",
        "Wei Wu",
        "Jianguo Li"
      ],
      "institutions": [
        "Ant Group",
        "Westlake University"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/HSA/cover.png",
      "keywords": [
        "02-Sparsity (Attention)"
      ],
      "code_url": null,
      "note_url": "notes/2025/HSA/note/",
      "prototxt_path": "meta/2025/HSA.prototxt",
      "update_time": 1767863899
    },
    {
      "id": "RazorAttention",
      "title": "RazorAttention: Efficient KV Cache Compression Through Retrieval Heads",
      "abbr": "RazorAttention",
      "url": "http://arxiv.org/abs/2407.15891v1",
      "authors": [
        "Hanlin Tang",
        "Yang Lin",
        "Jing Lin",
        "Qingsen Han",
        "Shikuan Hong",
        "Yiwu Yao",
        "Gongyi Wang"
      ],
      "institutions": [
        "Huawei"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/RazorAttention/cover.png",
      "keywords": [
        "20-Sparse/Eviction (KV Cache)"
      ],
      "code_url": null,
      "note_url": "notes/2024/RazorAttention/note/",
      "prototxt_path": "meta/2024/RazorAttention.prototxt",
      "update_time": 1767863141
    },
    {
      "id": "InfiniteBench",
      "title": "$\\infty$Bench: Extending Long Context Evaluation Beyond 100K Tokens",
      "abbr": "InfiniteBench",
      "url": "http://arxiv.org/abs/2402.13718v3",
      "authors": [
        "Xinrong Zhang",
        "Yingfa Chen",
        "Shengding Hu",
        "Zihang Xu",
        "Junhao Chen",
        "Moo Khai Hao",
        "Xu Han",
        "Zhen Leng Thai",
        "Shuo Wang",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "institutions": [
        "Tsinghua University"
      ],
      "year": 2024,
      "venue": "ACL",
      "cover": "notes/2024/InfiniteBench/cover.png",
      "keywords": [
        "47-Benchmark"
      ],
      "code_url": "https://github.com/OpenBMB/InfiniteBench",
      "note_url": "notes/2024/InfiniteBench/note/",
      "prototxt_path": "meta/2024/InfiniteBench.prototxt",
      "update_time": 0
    },
    {
      "id": "Async-TP",
      "title": "[Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch",
      "abbr": "Async-TP",
      "url": "https://discuss.pytorch.org/t/distributed-w-torchtitan-introducing-async-tensor-parallelism-in-pytorch/209487/1",
      "authors": [
        "Yifu Wang",
        "Horace He, Less Wright, Luca Wehrstedt, Tianyu Liu, Wanchao Liang"
      ],
      "institutions": [
        "PyTorch"
      ],
      "year": 2024,
      "venue": "Blog",
      "cover": "notes/2024/Async-TP/Figure_2.png",
      "keywords": [
        "29-Comm-Comp Overlap"
      ],
      "code_url": "https://github.com/pytorch/torchtitan",
      "note_url": "notes/2024/Async-TP/note/",
      "prototxt_path": "meta/2024/Async-TP.prototxt",
      "update_time": 0
    },
    {
      "id": "fisherpruning",
      "title": "A Fast Post-Training Pruning Framework for Transformers",
      "abbr": "FisherPruning",
      "url": "http://arxiv.org/abs/2204.09656v2",
      "authors": [
        "Woosuk Kwon",
        "Sehoon Kim",
        "Michael W. Mahoney",
        "Joseph Hassoun",
        "Kurt Keutzer",
        "Amir Gholami"
      ],
      "institutions": [
        "UC Berkeley",
        "Samsung"
      ],
      "year": 2022,
      "venue": "NeurIPS",
      "cover": "notes/2022/fisherpruning/cover.jpg",
      "keywords": [
        "01-Sparse/Pruning",
        "05-Sparsity (Structured)"
      ],
      "code_url": "https://github.com/WoosukKwon/retraining-free-pruning",
      "note_url": "notes/2022/fisherpruning/note/",
      "prototxt_path": "meta/2022/fisherpruning.prototxt",
      "update_time": 0
    },
    {
      "id": "gpfq",
      "title": "A Greedy Algorithm for Quantizing Neural Networks",
      "abbr": "GPFQ",
      "url": "https://jmlr.csail.mit.edu/papers/volume22/20-1233/20-1233.pdf",
      "authors": [
        "Eric Lybrand",
        "Rayan Saab"
      ],
      "institutions": [
        "UCSD"
      ],
      "year": 2021,
      "venue": "JMLR",
      "cover": null,
      "keywords": [
        "09-Quantization"
      ],
      "code_url": "https://github.com/YixuanSeanZhou/Quantized_Neural_Nets",
      "note_url": null,
      "prototxt_path": "meta/2021/gpfq.prototxt",
      "update_time": 0
    },
    {
      "id": "DistGEMM",
      "title": "A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems",
      "abbr": "DistGEMM",
      "url": "https://blog.shi-labs.com/distributed-gemm-88be6a481e2b",
      "authors": [
        "Ali Hassani",
        "Michael Isaev",
        "Nic McDonald",
        "Jie Ren",
        "Vijay Thakkar",
        "Haicheng Wu",
        "Humphrey Shi"
      ],
      "institutions": [
        "NVIDIA",
        "KAUST"
      ],
      "year": 2024,
      "venue": "Blog",
      "cover": "notes/2024/DistGEMM/fig1.gif",
      "keywords": [
        "29-Comm-Comp Overlap"
      ],
      "code_url": "https://github.com/NVIDIA/cutlass/blob/main/examples/65_distributed_gemm/README.md",
      "note_url": "notes/2024/DistGEMM/note/",
      "prototxt_path": "meta/2024/DistGEMM.prototxt",
      "update_time": 0
    },
    {
      "id": "Wanda",
      "title": "A Simple and Effective Pruning Approach for Large Language Models",
      "abbr": "Wanda",
      "url": "http://arxiv.org/abs/2306.11695",
      "authors": [
        "Mingjie Sun",
        "Zhuang Liu",
        "Anna Bair",
        "J. Zico Kolter"
      ],
      "institutions": [
        "Carnegie Mellon University",
        "Meta"
      ],
      "year": 2024,
      "venue": "ICLR",
      "cover": "notes/2024/Wanda/wanda.png",
      "keywords": [
        "04-Sparsity (Weight)",
        "01-Sparse/Pruning"
      ],
      "code_url": "https://github.com/locuslab/wanda",
      "note_url": "notes/2024/Wanda/note/",
      "prototxt_path": "meta/2024/Wanda.prototxt",
      "update_time": 0
    },
    {
      "id": "LinearPatch",
      "title": "A Simple Linear Patch Revives Layer-Pruned Large Language Models",
      "abbr": "LinearPatch",
      "url": "http://arxiv.org/abs/2505.24680v1",
      "authors": [
        "Xinrui Chen",
        "Haoli Bai",
        "Tao Yuan",
        "Ruikang Liu",
        "Kang Zhao",
        "Xianzhi Yu",
        "Lu Hou",
        "Tian Guan",
        "Yonghong He",
        "Chun Yuan"
      ],
      "institutions": [
        "Tsinghua University",
        "Huawei"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/LinearPatch/fig3.png",
      "keywords": [
        "01-Sparse/Pruning",
        "05-Sparsity (Structured)"
      ],
      "code_url": null,
      "note_url": "notes/2025/LinearPatch/note/",
      "prototxt_path": "meta/2025/LinearPatch.prototxt",
      "update_time": 0
    },
    {
      "id": "DHIB73MC",
      "title": "A Survey on Efficient Inference for Large Language Models",
      "abbr": "DHIB73MC",
      "url": "http://arxiv.org/abs/2404.14294v2",
      "authors": [
        "Zixuan Zhou",
        "Xuefei Ning",
        "Ke Hong",
        "Tianyu Fu",
        "Jiaming Xu",
        "Shiyao Li",
        "Yuming Lou",
        "Luning Wang",
        "Zhihang Yuan",
        "Xiuhong Li",
        "Shengen Yan",
        "Guohao Dai",
        "Xiao-Ping Zhang",
        "Yuhan Dong",
        "Yu Wang"
      ],
      "institutions": [
        "Tsinghua University",
        "Shanghai Jiao Tong University",
        "Peking University"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/DHIB73MC/efficientinference.png",
      "keywords": [
        "41-Survey"
      ],
      "code_url": null,
      "note_url": "notes/2024/DHIB73MC/note/",
      "prototxt_path": "meta/2024/DHIB73MC.prototxt",
      "update_time": 0
    },
    {
      "id": "ELILXDQG",
      "title": "A Survey on Evaluation of Large Language Models",
      "abbr": "",
      "url": "https://arxiv.org/abs/2307.03109",
      "authors": [
        "Yupeng Chang",
        "Xing Xie"
      ],
      "institutions": [
        "Microsoft Research"
      ],
      "year": 2023,
      "venue": "arXiv",
      "cover": "notes/del/survey/eval_LLM.jpg",
      "keywords": [
        "41-Survey"
      ],
      "code_url": null,
      "note_url": null,
      "prototxt_path": "meta/2023/ELILXDQG.prototxt",
      "update_time": 0
    },
    {
      "id": "068ZPAME",
      "title": "A Survey on Inference Optimization Techniques for Mixture of Experts Models",
      "abbr": "068ZPAME",
      "url": "http://arxiv.org/abs/2412.14219v2",
      "authors": [
        "Jiacheng Liu",
        "Peng Tang",
        "Wenfeng Wang",
        "Yuhang Ren",
        "Xiaofeng Hou",
        "Pheng-Ann Heng",
        "Minyi Guo",
        "Chao Li"
      ],
      "institutions": [
        "Chinese University of Hong Kong",
        "Shanghai Jiao Tong University"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/068ZPAME/tab1.png",
      "keywords": [
        "41-Survey",
        "42-Network Structure Design"
      ],
      "code_url": "https://github.com/MoE-Inf/awesome-moe-inference/",
      "note_url": "notes/2024/068ZPAME/note/",
      "prototxt_path": "meta/2024/068ZPAME.prototxt",
      "update_time": 0
    },
    {
      "id": "L5D7520E",
      "title": "A Survey on Model Compression for Large Language Models",
      "abbr": "",
      "url": "https://arxiv.org/abs/2308.07633",
      "authors": [
        "Xunyu Zhu",
        "Jian Li",
        "Yong Liu",
        "Can Ma",
        "Weiping Wang"
      ],
      "institutions": [
        "Institute of Information Engineering, Chinese Academy of Sciences",
        "School of Cyber Security, University of Chinese Academy of Sciences",
        "Gaoling School of Artificial Intelligence, Renmin University of China"
      ],
      "year": 2023,
      "venue": "arXiv",
      "cover": "notes/del/survey/compression_LLM.jpg",
      "keywords": [
        "41-Survey"
      ],
      "code_url": null,
      "note_url": null,
      "prototxt_path": "meta/2023/L5D7520E.prototxt",
      "update_time": 0
    },
    {
      "id": "Z9R72EAT",
      "title": "A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers",
      "abbr": "ADMM-pruning",
      "url": "https://arxiv.org/abs/1804.03294",
      "authors": [
        "Tianyun Zhang",
        "Yanzhi Wang"
      ],
      "institutions": [
        "Northeastern University"
      ],
      "year": 2018,
      "venue": "ECCV",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": "https://github.com/bzantium/pytorch-admm-pruning",
      "note_url": null,
      "prototxt_path": "meta/2018/Z9R72EAT.prototxt",
      "update_time": 0
    },
    {
      "id": "Acc-SpMM",
      "title": "Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores",
      "abbr": "Acc-SpMM",
      "url": "http://arxiv.org/abs/2501.09251v1",
      "authors": [
        "Haisha Zhao",
        "San Li",
        "Jiaheng Wang",
        "Chunbao Zhou",
        "Jue Wang",
        "Zhikuang Xin",
        "Shunde Li",
        "Zhiqiang Liang",
        "Zhijie Pan",
        "Fang Liu",
        "Yan Zeng",
        "Yangang Wang",
        "Xuebin Chi"
      ],
      "institutions": [
        "Computer Network Information Center, Chinese Academy of Sciences",
        "Renmin University of China"
      ],
      "year": 2025,
      "venue": "PPoPP",
      "cover": "notes/2025/Acc-SpMM/fig3.png",
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": null,
      "note_url": "notes/2025/Acc-SpMM/note/",
      "prototxt_path": "meta/2025/Acc-SpMM.prototxt",
      "update_time": 0
    },
    {
      "id": "PUHJMVCM",
      "title": "Accelerating Sparse Deep Neural Networks",
      "abbr": "NMSparse",
      "url": "https://arxiv.org/abs/2104.08378",
      "authors": [
        "Asit Mishra",
        "Paulius Micikevicius"
      ],
      "institutions": [
        "NVIDIA"
      ],
      "year": 2021,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": null,
      "note_url": null,
      "prototxt_path": "meta/2021/PUHJMVCM.prototxt",
      "update_time": 0
    },
    {
      "id": "HYPL7G37",
      "title": "Accelerating Transformer Pre-training with 2:4 Sparsity",
      "abbr": "",
      "url": "http://arxiv.org/abs/2404.01847v2",
      "authors": [
        "Yuezhou Hu",
        "Kang Zhao",
        "Weiyu Huang",
        "Jianfei Chen",
        "Jun Zhu"
      ],
      "institutions": [
        "Tsinghua University"
      ],
      "year": 2024,
      "venue": "ICML",
      "cover": null,
      "keywords": [
        "45-Efficient Training",
        "01-Sparse/Pruning"
      ],
      "code_url": "https://github.com/huyz2023/2by4-pretrain/tree/main",
      "note_url": "notes/2024/HYPL7G37/note/",
      "prototxt_path": "meta/2024/HYPL7G37.prototxt",
      "update_time": 0
    },
    {
      "id": "actnn",
      "title": "ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training",
      "abbr": "ActNN",
      "url": "https://arxiv.org/abs/2104.14129",
      "authors": [
        "Jianfei Chen, Lianmin Zheng, Zhewei Yao, Dequan Wang, Ion Stoica, Michael W. Mahoney, Joseph E. Gonzalez"
      ],
      "institutions": [
        "UC Berkeley"
      ],
      "year": 2019,
      "venue": "ICML",
      "cover": null,
      "keywords": [
        "09-Quantization"
      ],
      "code_url": "https://github.com/ucbrise/actnn",
      "note_url": null,
      "prototxt_path": "meta/2019/actnn.prototxt",
      "update_time": 0
    },
    {
      "id": "adalora",
      "title": "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning",
      "abbr": "AdaLoRA",
      "url": "https://arxiv.org/pdf/2303.10512.pdf",
      "authors": [
        "Qingru Zhang",
        "Minshuo Chen",
        "Alexander Bukharin",
        "Pengcheng He",
        "Yu Cheng",
        "Weizhu Chen",
        "Tuo Zhao"
      ],
      "institutions": [
        "Georgia Institute of Technology",
        "Princeton University",
        "Microsoft Azure AI"
      ],
      "year": 2023,
      "venue": "ICLR",
      "cover": "notes/2023/adalora/adalora.jpg",
      "keywords": [
        "43-Low Rank Decomposition",
        "45-Efficient Training"
      ],
      "code_url": "https://github.com/QingruZhang/AdaLoRA",
      "note_url": null,
      "prototxt_path": "meta/2023/adalora.prototxt",
      "update_time": 0
    },
    {
      "id": "FlexiDepth",
      "title": "Adaptive Layer-skipping in Pre-trained LLMs",
      "abbr": "FlexiDepth",
      "url": "http://arxiv.org/abs/2503.23798v1",
      "authors": [
        "Xuan Luo",
        "Weizhi Wang",
        "Xifeng Yan"
      ],
      "institutions": [
        "UC Santa Barbara"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/FlexiDepth/fig2.png",
      "keywords": [
        "01-Sparse/Pruning",
        "05-Sparsity (Structured)"
      ],
      "code_url": null,
      "note_url": "notes/2025/FlexiDepth/note/",
      "prototxt_path": "meta/2025/FlexiDepth.prototxt",
      "update_time": 0
    },
    {
      "id": "AdaSkip",
      "title": "AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference",
      "abbr": "AdaSkip",
      "url": "http://arxiv.org/abs/2501.02336v1",
      "authors": [
        "Zhuomin He",
        "Yizhen Yao",
        "Pengfei Zuo",
        "Bin Gao",
        "Qinya Li",
        "Zhenzhe Zheng",
        "Fan Wu"
      ],
      "institutions": [
        "Shanghai Jiao Tong University",
        "Huawei",
        "National University of Singapore"
      ],
      "year": 2025,
      "venue": "AAAI",
      "cover": "notes/2025/AdaSkip/fig1.png",
      "keywords": [
        "05-Sparsity (Structured)",
        "01-Sparse/Pruning",
        "42-Network Structure Design"
      ],
      "code_url": "https://github.com/ASISys/AdaSkip",
      "note_url": "notes/2025/AdaSkip/note/",
      "prototxt_path": "meta/2025/AdaSkip.prototxt",
      "update_time": 0
    },
    {
      "id": "AMALI",
      "title": "AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs",
      "abbr": "AMALI",
      "url": "https://dl.acm.org/doi/10.1145/3695053.3731064",
      "authors": [
        "Shiheng Cao",
        "Junmin Wu",
        "Junshi Chen",
        "Hong An",
        "Zhibin Yu"
      ],
      "institutions": [
        "University of Science and Technology",
        "Shenzhen Institutes of Advanced Technology(SIAT), Chinese Academy of Science(CAS)"
      ],
      "year": 2025,
      "venue": "ISCA",
      "cover": "notes/2025/AMALI/fig6.png",
      "keywords": [
        "39-Performance Modeling"
      ],
      "code_url": null,
      "note_url": "notes/2025/AMALI/note/",
      "prototxt_path": "meta/2025/AMALI.prototxt",
      "update_time": 0
    },
    {
      "id": "AmberPruner",
      "title": "Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models",
      "abbr": "AmberPruner",
      "url": "http://arxiv.org/abs/2508.02128v1",
      "authors": [
        "Tai An",
        "Ruwu Cai",
        "Yanzhe Zhang",
        "Yang Liu",
        "Hao Chen",
        "Pengcheng Xie",
        "Sheng Chang",
        "Yiwu Yao",
        "Gongyi Wang"
      ],
      "institutions": [
        "Huawei"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning",
        "03-Sparsity (Activation)"
      ],
      "code_url": null,
      "note_url": "notes/2025/AmberPruner/note/",
      "prototxt_path": "meta/2025/AmberPruner.prototxt",
      "update_time": 0
    },
    {
      "id": "44KWQAWO",
      "title": "An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers",
      "abbr": "STA",
      "url": "https://arxiv.org/abs/2208.06118",
      "authors": [
        "Chao Fang, Aojun Zhou, Zhongfeng Wang"
      ],
      "institutions": [
        "Nanjing University"
      ],
      "year": 2022,
      "venue": "VLSI",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": null,
      "note_url": null,
      "prototxt_path": "meta/2022/44KWQAWO.prototxt",
      "update_time": 0
    },
    {
      "id": "APEX",
      "title": "APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving",
      "abbr": "APEX",
      "url": "http://arxiv.org/abs/2411.17651v2",
      "authors": [
        "Yi-Chien Lin",
        "Woosuk Kwon",
        "Ronald Pineda",
        "Fanny Nina Paravecino"
      ],
      "institutions": [
        "University of Southern California",
        "University of California, Berkeley",
        "Microsoft"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "39-Performance Modeling"
      ],
      "code_url": "https://github.com/microsoft/apex_plus",
      "note_url": "notes/2024/APEX/note/",
      "prototxt_path": "meta/2024/APEX.prototxt",
      "update_time": 0
    },
    {
      "id": "Transformer",
      "title": "Attention Is All You Need",
      "abbr": "Transformer",
      "url": "http://arxiv.org/abs/1706.03762v7",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan N. Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "institutions": [
        "Google"
      ],
      "year": 2017,
      "venue": "NeurIPS",
      "cover": "notes/2017/Transformer/fig1.png",
      "keywords": [
        "42-Network Structure Design"
      ],
      "code_url": null,
      "note_url": "notes/2017/Transformer/note/",
      "prototxt_path": "meta/2017/Transformer.prototxt",
      "update_time": 0
    },
    {
      "id": "AttentionSinks",
      "title": "Attention Sinks: A 'Catch, Tag, Release' Mechanism for Embeddings",
      "abbr": "AttentionSinks",
      "url": "http://arxiv.org/abs/2502.00919v2",
      "authors": [
        "Stephen Zhang",
        "Mustafa Khan",
        "Vardan Papyan"
      ],
      "institutions": [
        "University of Toronto",
        "Vector Institute"
      ],
      "year": 2025,
      "venue": "NeurIPS",
      "cover": null,
      "keywords": [
        "42-Network Structure Design"
      ],
      "code_url": null,
      "note_url": "notes/2025/AttentionSinks/note/",
      "prototxt_path": "meta/2025/AttentionSinks.prototxt",
      "update_time": 0
    },
    {
      "id": "attention-gym",
      "title": "Attention-Gym: Triton-Based Sparse and Quantization Attention",
      "abbr": "attention-gym",
      "url": "https://github.com/RiseAI-Sys/attention-gym",
      "authors": [
        "RiseAI-Sys"
      ],
      "institutions": [
        "RiseAI-Sys"
      ],
      "year": 2025,
      "venue": "github",
      "cover": null,
      "keywords": [
        "02-Sparsity (Attention)",
        "09-Quantization",
        "01-Sparse/Pruning",
        "46-Tool"
      ],
      "code_url": "https://github.com/RiseAI-Sys/attention-gym",
      "note_url": "notes/2025/attention-gym/note/",
      "prototxt_path": "meta/2025/attention-gym.prototxt",
      "update_time": 0
    },
    {
      "id": "AVSS",
      "title": "AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis",
      "abbr": "AVSS",
      "url": "http://arxiv.org/abs/2411.02117v1",
      "authors": [
        "Zichen Song",
        "Yuxin Wu",
        "Sitan Huang",
        "Zhongfeng Kang"
      ],
      "institutions": [
        "Lanzhou University"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/AVSS/avss.png",
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": null,
      "note_url": "notes/2024/AVSS/note/",
      "prototxt_path": "meta/2024/AVSS.prototxt",
      "update_time": 0
    },
    {
      "id": "awq",
      "title": "AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration",
      "abbr": "AWQ",
      "url": "https://arxiv.org/abs/2306.00978",
      "authors": [
        "Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, Chuang Gan, Song Han"
      ],
      "institutions": [
        "Massachusetts Institute of Technology",
        "Tsinghua University"
      ],
      "year": 2024,
      "venue": "MLSys",
      "cover": null,
      "keywords": [
        "09-Quantization"
      ],
      "code_url": "https://github.com/mit-han-lab/llm-awq",
      "note_url": null,
      "prototxt_path": "meta/2024/awq.prototxt",
      "update_time": 0
    },
    {
      "id": "BaWA",
      "title": "BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation",
      "abbr": "BaWA",
      "url": "https://openreview.net/forum?id=YrCvW1Hx7g",
      "authors": [
        "Lian Liu, Xiandong Zhao, Guanchen Li, Dong Li, Mengdi Wang, Yinhe Han, Xiaowei Li, ying wang"
      ],
      "institutions": [
        "Advanced Micro Devices",
        "Institute of Computing Technology"
      ],
      "year": 2025,
      "venue": "ICML",
      "cover": "notes/2025/BaWA/fig2.png",
      "keywords": [
        "04-Sparsity (Weight)",
        "01-Sparse/Pruning"
      ],
      "code_url": null,
      "note_url": "notes/2025/BaWA/note/",
      "prototxt_path": "meta/2025/BaWA.prototxt",
      "update_time": 0
    },
    {
      "id": "0Y41U1N2",
      "title": "Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs",
      "abbr": "",
      "url": "http://arxiv.org/abs/2410.16135v1",
      "authors": [
        "Kang Zhao",
        "Tao Yuan",
        "Han Bao",
        "Zhenfeng Su",
        "Chang Gao",
        "Zhaofeng Sun",
        "Zichen Liang",
        "Liping Jing",
        "Jianfei Chen"
      ],
      "institutions": [
        "Tsinghua University",
        "Huawei"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/0Y41U1N2/cover.png",
      "keywords": [
        "01-Sparse/Pruning",
        "04-Sparsity (Weight)"
      ],
      "code_url": null,
      "note_url": "notes/2024/0Y41U1N2/note/",
      "prototxt_path": "meta/2024/0Y41U1N2.prototxt",
      "update_time": 0
    },
    {
      "id": "GBLM-Pruner",
      "title": "Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models",
      "abbr": "GBLM-Pruner",
      "url": "http://arxiv.org/abs/2311.04902v2",
      "authors": [
        "Rocktim Jyoti Das",
        "Mingjie Sun",
        "Liqun Ma",
        "Zhiqiang Shen"
      ],
      "institutions": [
        "Mohamed bin Zayed University of AI",
        "Carnegie Mellon University"
      ],
      "year": 2023,
      "venue": "arXiv",
      "cover": "notes/2023/GBLM-Pruner/gblm-pruner.jpg",
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": "https://github.com/VILA-Lab/GBLM-Pruner",
      "note_url": "notes/2023/GBLM-Pruner/note/",
      "prototxt_path": "meta/2023/GBLM-Pruner.prototxt",
      "update_time": 0
    },
    {
      "id": "WGM",
      "title": "Binary Quantization For LLMs Through Dynamic Grouping",
      "abbr": "WGM",
      "url": "http://arxiv.org/abs/2509.03054v1",
      "authors": [
        "Xinzhe Zheng",
        "Zhen-Qun Yang",
        "Haoran Xie",
        "S. Joe Qin",
        "Arlene Chen",
        "Fangzhen Lin"
      ],
      "institutions": [
        "Hong Kong University of Science and Technology",
        "The Hong Kong Polytechnic University",
        "Lingnan University"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/WGM/fig1.png",
      "keywords": [
        "09-Quantization"
      ],
      "code_url": "https://github.com/johnnyzheng0636/WGM_bi_quan",
      "note_url": "notes/2025/WGM/note/",
      "prototxt_path": "meta/2025/WGM.prototxt",
      "update_time": 0
    },
    {
      "id": "BLASST",
      "title": "BLASST: Dynamic BLocked Attention Sparsity via Softmax Thresholding",
      "abbr": "BLASST",
      "url": "http://arxiv.org/abs/2512.12087v1",
      "authors": [
        "Jiayi Yuan",
        "Cameron Shinn",
        "Kai Xu",
        "Jingze Cui",
        "George Klimiashvili",
        "Guangxuan Xiao",
        "Perkz Zheng",
        "Bo Li",
        "Yuxin Zhou",
        "Zhouhai Ye",
        "Weijie You",
        "Tian Zheng",
        "Dominic Brown",
        "Pengbo Wang",
        "Richard Cai",
        "Julien Demouth",
        "John D. Owens",
        "Xia Hu",
        "Song Han",
        "Timmy Liu",
        "Huizi Mao"
      ],
      "institutions": [
        "NVIDIA",
        "Rice University",
        "University of California"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/BLASST/cover.png",
      "keywords": [
        "02-Sparsity (Attention)",
        "01-Sparse/Pruning",
        "40-LLM Deployment"
      ],
      "code_url": null,
      "note_url": "notes/2025/BLASST/note/",
      "prototxt_path": "meta/2025/BLASST.prototxt",
      "update_time": 0
    },
    {
      "id": "BlockFFN",
      "title": "BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity",
      "abbr": "BlockFFN",
      "url": "http://arxiv.org/abs/2507.08771v1",
      "authors": [
        "Chenyang Song",
        "Weilin Zhao",
        "Xu Han",
        "Chaojun Xiao",
        "Yingfa Chen",
        "Yuxuan Li",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "institutions": [
        "Tsinghua University"
      ],
      "year": 2025,
      "venue": "COLM",
      "cover": "notes/2025/BlockFFN/fig2.png",
      "keywords": [
        "03-Sparsity (Activation)",
        "42-Network Structure Design"
      ],
      "code_url": "https://github.com/thunlp/BlockFFN",
      "note_url": "notes/2025/BlockFFN/note/",
      "prototxt_path": "meta/2025/BlockFFN.prototxt",
      "update_time": 0
    },
    {
      "id": "GPUSQ-ViT",
      "title": "Boost Vision Transformer with GPU-Friendly Sparsity and Quantization",
      "abbr": "GPUSQ-ViT",
      "url": "http://arxiv.org/abs/2305.10727v1",
      "authors": [
        "Chong Yu",
        "Tao Chen",
        "Zhongxue Gan",
        "Jiayuan Fan"
      ],
      "institutions": [
        "Fudan University",
        "NVIDIA"
      ],
      "year": 2023,
      "venue": "CVPR",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning",
        "04-Sparsity (Weight)",
        "09-Quantization"
      ],
      "code_url": null,
      "note_url": "notes/2023/GPUSQ-ViT/note/",
      "prototxt_path": "meta/2023/GPUSQ-ViT.prototxt",
      "update_time": 0
    },
    {
      "id": "CoCoNet",
      "title": "Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads",
      "abbr": "CoCoNet",
      "url": "http://arxiv.org/abs/2105.05720v5",
      "authors": [
        "Abhinav Jangda",
        "Jun Huang",
        "Guodong Liu",
        "Amir Hossein Nodehi Sabet",
        "Saeed Maleki",
        "Youshan Miao",
        "Madanlal Musuvathi",
        "Todd Mytkowicz",
        "Olli Sarikivi"
      ],
      "institutions": [
        "University of Massachusetts Amherst",
        "Ohio State University",
        "Chinese Academy of Sciences",
        "University of California, Riverside",
        "Microsoft Research"
      ],
      "year": 2022,
      "venue": "ASPLOS",
      "cover": "notes/2022/CoCoNet/fig9.png",
      "keywords": [
        "29-Comm-Comp Overlap"
      ],
      "code_url": "https://github.com/parasailteam/coconet",
      "note_url": "notes/2022/CoCoNet/note/",
      "prototxt_path": "meta/2022/CoCoNet.prototxt",
      "update_time": 0
    },
    {
      "id": "brecq",
      "title": "BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction",
      "abbr": "BRECQ",
      "url": "https://openreview.net/pdf?id=POWv6hDd9XH",
      "authors": [
        "Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, Shi Gu"
      ],
      "institutions": [
        "University of Electronic Science and Technology of China",
        "SenseTime Research"
      ],
      "year": 2021,
      "venue": "ICLR",
      "cover": null,
      "keywords": [
        "09-Quantization"
      ],
      "code_url": "https://github.com/yhhhli/BRECQ",
      "note_url": null,
      "prototxt_path": "meta/2021/brecq.prototxt",
      "update_time": 0
    },
    {
      "id": "CATS",
      "title": "CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models",
      "abbr": "CATS",
      "url": "http://arxiv.org/abs/2404.08763v4",
      "authors": [
        "Donghyun Lee",
        "Je-Yong Lee",
        "Genghan Zhang",
        "Mo Tiwari",
        "Azalia Mirhoseini"
      ],
      "institutions": [
        "University College London",
        "Oxford University",
        "Stanford University"
      ],
      "year": 2024,
      "venue": "COLM",
      "cover": "notes/2024/CATS/fig1.png",
      "keywords": [
        "01-Sparse/Pruning",
        "03-Sparsity (Activation)"
      ],
      "code_url": "https://github.com/ScalingIntelligence/CATS",
      "note_url": "notes/2024/CATS/note/",
      "prototxt_path": "meta/2024/CATS.prototxt",
      "update_time": 0
    },
    {
      "id": "CCQ",
      "title": "CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs",
      "abbr": "CCQ",
      "url": "http://arxiv.org/abs/2507.07145v1",
      "authors": [
        "Zhaojing Zhou",
        "Xunchao Li",
        "Minghao Li",
        "Handi Zhang",
        "Haoshuang Wang",
        "Wenbin Chang",
        "Yiqun Liu",
        "Qingqing Dang",
        "Dianhai Yu",
        "Yanjun Ma",
        "Haifeng Wang"
      ],
      "institutions": [
        "Baidu"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "09-Quantization"
      ],
      "code_url": null,
      "note_url": "notes/2025/CCQ/note/",
      "prototxt_path": "meta/2025/CCQ.prototxt",
      "update_time": 0
    },
    {
      "id": "Centauri",
      "title": "Centauri: Enabling Efficient Scheduling for Communication-Computation Overlap in Large Model Training via Communication Partitioning",
      "abbr": "Centauri",
      "url": "https://dl.acm.org/doi/10.1145/3620666.3651379",
      "authors": [
        "Chang Chen, Xiuhong Li, Qianchao Zhu, Jiangfei Duan, Peng Sun, Xingcheng Zhang, Chao Yang"
      ],
      "institutions": [
        "Peking University",
        "The Chinese University of Hong Kong",
        "Shanghai AI Lab"
      ],
      "year": 2024,
      "venue": "ASPLOS",
      "cover": "notes/2024/Centauri/fig3.png",
      "keywords": [
        "29-Comm-Comp Overlap"
      ],
      "code_url": null,
      "note_url": "notes/2024/Centauri/note/",
      "prototxt_path": "meta/2024/Centauri.prototxt",
      "update_time": 0
    },
    {
      "id": "K7GSWQIC",
      "title": "Channel Permutations for N:M Sparsity",
      "abbr": "",
      "url": "https://proceedings.neurips.cc/paper/2021/hash/6e8404c3b93a9527c8db241a1846599a-Abstract.html",
      "authors": [
        "Jeff Pool",
        "Chong Yu"
      ],
      "institutions": [
        "NVIDIA"
      ],
      "year": 2021,
      "venue": "NeurIPS",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": "https://github.com/NVIDIA/apex/tree/master/apex/contrib/sparsity",
      "note_url": null,
      "prototxt_path": "meta/2021/K7GSWQIC.prototxt",
      "update_time": 0
    },
    {
      "id": "UC0D8DJ6",
      "title": "Characterizing Communication Patterns in Distributed Large Language Model Inference",
      "abbr": "UC0D8DJ6",
      "url": "http://arxiv.org/abs/2507.14392v1",
      "authors": [
        "Lang Xu",
        "Kaushik Kandadi Suresh",
        "Quentin Anthony",
        "Nawras Alnaasan",
        "Dhabaleswar K. Panda"
      ],
      "institutions": [
        "The Ohio State University",
        "KAIST"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "29-Comm-Comp Overlap"
      ],
      "code_url": null,
      "note_url": "notes/2025/UC0D8DJ6/note/",
      "prototxt_path": "meta/2025/UC0D8DJ6.prototxt",
      "update_time": 0
    },
    {
      "id": "1DZIJVBI",
      "title": "Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications",
      "abbr": "1DZIJVBI",
      "url": "http://arxiv.org/abs/2507.03114v1",
      "authors": [
        "Seonho Lee",
        "Jihwan Oh",
        "Junkyum Kim",
        "Seokjin Go",
        "Jongse Park",
        "Divya Mahajan"
      ],
      "institutions": [
        "Georgia Institute of Technology",
        "KAIST"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/1DZIJVBI/eq1.png",
      "keywords": [
        "29-Comm-Comp Overlap"
      ],
      "code_url": null,
      "note_url": "notes/2025/1DZIJVBI/note/",
      "prototxt_path": "meta/2025/1DZIJVBI.prototxt",
      "update_time": 0
    },
    {
      "id": "ChunkAttention",
      "title": "ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition",
      "abbr": "ChunkAttention",
      "url": "http://arxiv.org/abs/2402.15220v4",
      "authors": [
        "Lu Ye",
        "Ze Tao",
        "Yong Huang",
        "Yang Li"
      ],
      "institutions": [
        "Microsoft"
      ],
      "year": 2024,
      "venue": "ACL",
      "cover": "notes/2024/ChunkAttention/chunkattn.png",
      "keywords": [
        "21-KV Cache Management",
        "40-LLM Deployment"
      ],
      "code_url": "https://github.com/microsoft/chunk-attention",
      "note_url": "notes/2024/ChunkAttention/note/",
      "prototxt_path": "meta/2024/ChunkAttention.prototxt",
      "update_time": 0
    },
    {
      "id": "CometSeed",
      "title": "Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts",
      "abbr": "CometSeed",
      "url": "http://arxiv.org/abs/2502.19811v3",
      "authors": [
        "Shulai Zhang",
        "Ningxin Zheng",
        "Haibin Lin",
        "Ziheng Jiang",
        "Wenlei Bao",
        "Chengquan Jiang",
        "Qi Hou",
        "Weihao Cui",
        "Size Zheng",
        "Li-Wen Chang",
        "Quan Chen",
        "Xin Liu"
      ],
      "institutions": [
        "ByteDance Seed",
        "Shanghai Jiao Tong University"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/CometSeed/fig3.png",
      "keywords": [
        "29-Comm-Comp Overlap"
      ],
      "code_url": "https://github.com/bytedance/flux",
      "note_url": "notes/2025/CometSeed/note/",
      "prototxt_path": "meta/2025/CometSeed.prototxt",
      "update_time": 0
    },
    {
      "id": "Minitron",
      "title": "Compact Language Models via Pruning and Knowledge Distillation",
      "abbr": "Minitron",
      "url": "https://arxiv.org/abs/2408.11796v2",
      "authors": [
        "Saurav Muralidharan",
        "Sharath Turuvekere Sreenivas",
        "Raviraj Joshi",
        "Marcin Chochowski",
        "Mostofa Patwary",
        "Mohammad Shoeybi",
        "Bryan Catanzaro",
        "Jan Kautz",
        "Pavlo Molchanov"
      ],
      "institutions": [
        "NVIDIA"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/Minitron/minitron.png",
      "keywords": [
        "01-Sparse/Pruning",
        "05-Sparsity (Structured)"
      ],
      "code_url": "https://github.com/NVlabs/Minitron",
      "note_url": "notes/2024/Minitron/note/",
      "prototxt_path": "meta/2024/Minitron.prototxt",
      "update_time": 0
    },
    {
      "id": "VB8C61V6",
      "title": "Compressing LLMs: The Truth is Rarely Pure and Never Simple",
      "abbr": "LLM-KICK",
      "url": "http://arxiv.org/abs/2310.01382v2",
      "authors": [
        "Ajay Jaiswal",
        "Zhe Gan",
        "Xianzhi Du",
        "Bowen Zhang",
        "Zhangyang Wang",
        "Yinfei Yang"
      ],
      "institutions": [
        "University of Texas at Austin",
        "Apple"
      ],
      "year": 2024,
      "venue": "ICLR",
      "cover": "notes/2024/VB8C61V6/llm-kick.jpg",
      "keywords": [
        "01-Sparse/Pruning",
        "41-Survey"
      ],
      "code_url": "https://github.com/VITA-Group/llm-kick",
      "note_url": "notes/2024/VB8C61V6/note/",
      "prototxt_path": "meta/2024/VB8C61V6.prototxt",
      "update_time": 0
    },
    {
      "id": "Compresso",
      "title": "Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models",
      "abbr": "Compresso",
      "url": "https://arxiv.org/abs/2310.05015",
      "authors": [
        "Song Guo",
        "Jiahang Xu",
        "Li Lyna Zhang",
        "Mao Yang"
      ],
      "institutions": [
        "Microsoft",
        "Xiamen University"
      ],
      "year": 2023,
      "venue": "arXiv",
      "cover": "notes/2023/Compresso/cover.jpg",
      "keywords": [
        "01-Sparse/Pruning",
        "05-Sparsity (Structured)"
      ],
      "code_url": "https://github.com/microsoft/Moonlit/blob/main/Compresso",
      "note_url": "notes/2023/Compresso/note/",
      "prototxt_path": "meta/2023/Compresso.prototxt",
      "update_time": 0
    },
    {
      "id": "CoreInfer",
      "title": "CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation",
      "abbr": "CoreInfer",
      "url": "http://arxiv.org/abs/2410.18311v1",
      "authors": [
        "Qinsi Wang",
        "Saeed Vahidian",
        "Hancheng Ye",
        "Jianyang Gu",
        "Jianyi Zhang",
        "Yiran Chen"
      ],
      "institutions": [
        "Duke University",
        "Ohio State University"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/CoreInfer/framework.png",
      "keywords": [
        "01-Sparse/Pruning",
        "03-Sparsity (Activation)"
      ],
      "code_url": "https://github.com/wangqinsi1/CoreInfer",
      "note_url": "notes/2024/CoreInfer/note/",
      "prototxt_path": "meta/2024/CoreInfer.prototxt",
      "update_time": 0
    },
    {
      "id": "CachedAttention",
      "title": "Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention",
      "abbr": "CachedAttention",
      "url": "http://arxiv.org/abs/2403.19708v3",
      "authors": [
        "Bin Gao",
        "Zhuomin He",
        "Puru Sharma",
        "Qingxuan Kang",
        "Djordje Jevdjic",
        "Junbo Deng",
        "Xingkun Yang",
        "Zhou Yu",
        "Pengfei Zuo"
      ],
      "institutions": [
        "National University of Singapore",
        "Shanghai Jiao Tong University",
        "Huawei"
      ],
      "year": 2024,
      "venue": "ATC",
      "cover": "notes/2024/CachedAttention/fig5.png",
      "keywords": [
        "21-KV Cache Management",
        "40-LLM Deployment"
      ],
      "code_url": null,
      "note_url": "notes/2024/CachedAttention/note/",
      "prototxt_path": "meta/2024/CachedAttention.prototxt",
      "update_time": 0
    },
    {
      "id": "2EQV34KV",
      "title": "Creating Sparse GPT-3 Models with Iterative Pruning",
      "abbr": "",
      "url": "https://www.cerebras.net/blog/creating-sparse-gpt-3-models-with-iterative-pruning",
      "authors": [
        "Anshul Samar"
      ],
      "institutions": [],
      "year": 2022,
      "venue": "Blog",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": null,
      "note_url": null,
      "prototxt_path": "meta/2022/2EQV34KV.prototxt",
      "update_time": 0
    },
    {
      "id": "deepcompression",
      "title": "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding",
      "abbr": "Deep Compression",
      "url": "https://arxiv.org/pdf/1510.00149.pdf",
      "authors": [
        "Song Han",
        "Huizi Mao",
        "William J. Dally"
      ],
      "institutions": [
        "Stanford University",
        "Tsinghua University"
      ],
      "year": 2016,
      "venue": "ICLR",
      "cover": null,
      "keywords": [
        "09-Quantization"
      ],
      "code_url": null,
      "note_url": null,
      "prototxt_path": "meta/2016/deepcompression.prototxt",
      "update_time": 0
    },
    {
      "id": "DeepEP",
      "title": "DeepEP: an efficient expert-parallel communication library",
      "abbr": "DeepEP",
      "url": "https://github.com/deepseek-ai/DeepEP",
      "authors": [
        "DeepSeek"
      ],
      "institutions": [
        "DeepSeek"
      ],
      "year": 2025,
      "venue": "github",
      "cover": "notes/2025/DeepEP/low-latency.png",
      "keywords": [
        "29-Comm-Comp Overlap",
        "46-Tool"
      ],
      "code_url": "https://github.com/deepseek-ai/DeepEP",
      "note_url": "notes/2025/DeepEP/note/",
      "prototxt_path": "meta/2025/DeepEP.prototxt",
      "update_time": 0
    },
    {
      "id": "DeepSeek-R1",
      "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
      "abbr": "DeepSeek-R1",
      "url": "http://arxiv.org/abs/2501.12948v1",
      "authors": [
        "DeepSeek-AI",
        "Daya Guo",
        "Dejian Yang",
        "Haowei Zhang",
        "Junxiao Song",
        "Ruoyu Zhang",
        "Runxin Xu",
        "Qihao Zhu",
        "Shirong Ma",
        "Peiyi Wang",
        "Xiao Bi",
        "Xiaokang Zhang",
        "Xingkai Yu",
        "Yu Wu",
        "Z. F. Wu",
        "Zhibin Gou",
        "Zhihong Shao",
        "Zhuoshu Li",
        "Ziyi Gao",
        "Aixin Liu",
        "Bing Xue",
        "Bingxuan Wang",
        "Bochao Wu",
        "Bei Feng",
        "Chengda Lu",
        "Chenggang Zhao",
        "Chengqi Deng",
        "Chenyu Zhang",
        "Chong Ruan",
        "Damai Dai",
        "Deli Chen",
        "Dongjie Ji",
        "Erhang Li",
        "Fangyun Lin",
        "Fucong Dai",
        "Fuli Luo",
        "Guangbo Hao",
        "Guanting Chen",
        "Guowei Li",
        "H. Zhang",
        "Han Bao",
        "Hanwei Xu",
        "Haocheng Wang",
        "Honghui Ding",
        "Huajian Xin",
        "Huazuo Gao",
        "Hui Qu",
        "Hui Li",
        "Jianzhong Guo",
        "Jiashi Li",
        "Jiawei Wang",
        "Jingchang Chen",
        "Jingyang Yuan",
        "Junjie Qiu",
        "Junlong Li",
        "J. L. Cai",
        "Jiaqi Ni",
        "Jian Liang",
        "Jin Chen",
        "Kai Dong",
        "Kai Hu",
        "Kaige Gao",
        "Kang Guan",
        "Kexin Huang",
        "Kuai Yu",
        "Lean Wang",
        "Lecong Zhang",
        "Liang Zhao",
        "Litong Wang",
        "Liyue Zhang",
        "Lei Xu",
        "Leyi Xia",
        "Mingchuan Zhang",
        "Minghua Zhang",
        "Minghui Tang",
        "Meng Li",
        "Miaojun Wang",
        "Mingming Li",
        "Ning Tian",
        "Panpan Huang",
        "Peng Zhang",
        "Qiancheng Wang",
        "Qinyu Chen",
        "Qiushi Du",
        "Ruiqi Ge",
        "Ruisong Zhang",
        "Ruizhe Pan",
        "Runji Wang",
        "R. J. Chen",
        "R. L. Jin",
        "Ruyi Chen",
        "Shanghao Lu",
        "Shangyan Zhou",
        "Shanhuang Chen",
        "Shengfeng Ye",
        "Shiyu Wang",
        "Shuiping Yu",
        "Shunfeng Zhou",
        "Shuting Pan",
        "S. S. Li",
        "Shuang Zhou",
        "Shaoqing Wu",
        "Shengfeng Ye",
        "Tao Yun",
        "Tian Pei",
        "Tianyu Sun",
        "T. Wang",
        "Wangding Zeng",
        "Wanjia Zhao",
        "Wen Liu",
        "Wenfeng Liang",
        "Wenjun Gao",
        "Wenqin Yu",
        "Wentao Zhang",
        "W. L. Xiao",
        "Wei An",
        "Xiaodong Liu",
        "Xiaohan Wang",
        "Xiaokang Chen",
        "Xiaotao Nie",
        "Xin Cheng",
        "Xin Liu",
        "Xin Xie",
        "Xingchao Liu",
        "Xinyu Yang",
        "Xinyuan Li",
        "Xuecheng Su",
        "Xuheng Lin",
        "X. Q. Li",
        "Xiangyue Jin",
        "Xiaojin Shen",
        "Xiaosha Chen",
        "Xiaowen Sun",
        "Xiaoxiang Wang",
        "Xinnan Song",
        "Xinyi Zhou",
        "Xianzu Wang",
        "Xinxia Shan",
        "Y. K. Li",
        "Y. Q. Wang",
        "Y. X. Wei",
        "Yang Zhang",
        "Yanhong Xu",
        "Yao Li",
        "Yao Zhao",
        "Yaofeng Sun",
        "Yaohui Wang",
        "Yi Yu",
        "Yichao Zhang",
        "Yifan Shi",
        "Yiliang Xiong",
        "Ying He",
        "Yishi Piao",
        "Yisong Wang",
        "Yixuan Tan",
        "Yiyang Ma",
        "Yiyuan Liu",
        "Yongqiang Guo",
        "Yuan Ou",
        "Yuduan Wang",
        "Yue Gong",
        "Yuheng Zou",
        "Yujia He",
        "Yunfan Xiong",
        "Yuxiang Luo",
        "Yuxiang You",
        "Yuxuan Liu",
        "Yuyang Zhou",
        "Y. X. Zhu",
        "Yanhong Xu",
        "Yanping Huang",
        "Yaohui Li",
        "Yi Zheng",
        "Yuchen Zhu",
        "Yunxian Ma",
        "Ying Tang",
        "Yukun Zha",
        "Yuting Yan",
        "Z. Z. Ren",
        "Zehui Ren",
        "Zhangli Sha",
        "Zhe Fu",
        "Zhean Xu",
        "Zhenda Xie",
        "Zhengyan Zhang",
        "Zhewen Hao",
        "Zhicheng Ma",
        "Zhigang Yan",
        "Zhiyu Wu",
        "Zihui Gu",
        "Zijia Zhu",
        "Zijun Liu",
        "Zilin Li",
        "Ziwei Xie",
        "Ziyang Song",
        "Zizheng Pan",
        "Zhen Huang",
        "Zhipeng Xu",
        "Zhongyu Zhang",
        "Zhen Zhang"
      ],
      "institutions": [
        "DeepSeek-AI"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/DeepSeek-R1/fig1.png",
      "keywords": [
        "42-Network Structure Design"
      ],
      "code_url": "https://github.com/deepseek-ai/DeepSeek-R1",
      "note_url": "notes/2025/DeepSeek-R1/note/",
      "prototxt_path": "meta/2025/DeepSeek-R1.prototxt",
      "update_time": 0
    },
    {
      "id": "DeepSeek-V2",
      "title": "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model",
      "abbr": "DeepSeek-V2",
      "url": "http://arxiv.org/abs/2405.04434v5",
      "authors": [
        "DeepSeek-AI",
        "Aixin Liu",
        "Bei Feng",
        "Bin Wang",
        "Bingxuan Wang",
        "Bo Liu",
        "Chenggang Zhao",
        "Chengqi Dengr",
        "Chong Ruan",
        "Damai Dai",
        "Daya Guo",
        "Dejian Yang",
        "Deli Chen",
        "Dongjie Ji",
        "Erhang Li",
        "Fangyun Lin",
        "Fuli Luo",
        "Guangbo Hao",
        "Guanting Chen",
        "Guowei Li",
        "H. Zhang",
        "Hanwei Xu",
        "Hao Yang",
        "Haowei Zhang",
        "Honghui Ding",
        "Huajian Xin",
        "Huazuo Gao",
        "Hui Li",
        "Hui Qu",
        "J. L. Cai",
        "Jian Liang",
        "Jianzhong Guo",
        "Jiaqi Ni",
        "Jiashi Li",
        "Jin Chen",
        "Jingyang Yuan",
        "Junjie Qiu",
        "Junxiao Song",
        "Kai Dong",
        "Kaige Gao",
        "Kang Guan",
        "Lean Wang",
        "Lecong Zhang",
        "Lei Xu",
        "Leyi Xia",
        "Liang Zhao",
        "Liyue Zhang",
        "Meng Li",
        "Miaojun Wang",
        "Mingchuan Zhang",
        "Minghua Zhang",
        "Minghui Tang",
        "Mingming Li",
        "Ning Tian",
        "Panpan Huang",
        "Peiyi Wang",
        "Peng Zhang",
        "Qihao Zhu",
        "Qinyu Chen",
        "Qiushi Du",
        "R. J. Chen",
        "R. L. Jin",
        "Ruiqi Ge",
        "Ruizhe Pan",
        "Runxin Xu",
        "Ruyi Chen",
        "S. S. Li",
        "Shanghao Lu",
        "Shangyan Zhou",
        "Shanhuang Chen",
        "Shaoqing Wu",
        "Shengfeng Ye",
        "Shirong Ma",
        "Shiyu Wang",
        "Shuang Zhou",
        "Shuiping Yu",
        "Shunfeng Zhou",
        "Size Zheng",
        "T. Wang",
        "Tian Pei",
        "Tian Yuan",
        "Tianyu Sun",
        "W. L. Xiao",
        "Wangding Zeng",
        "Wei An",
        "Wen Liu",
        "Wenfeng Liang",
        "Wenjun Gao",
        "Wentao Zhang",
        "X. Q. Li",
        "Xiangyue Jin",
        "Xianzu Wang",
        "Xiao Bi",
        "Xiaodong Liu",
        "Xiaohan Wang",
        "Xiaojin Shen",
        "Xiaokang Chen",
        "Xiaosha Chen",
        "Xiaotao Nie",
        "Xiaowen Sun",
        "Xiaoxiang Wang",
        "Xin Liu",
        "Xin Xie",
        "Xingkai Yu",
        "Xinnan Song",
        "Xinyi Zhou",
        "Xinyu Yang",
        "Xuan Lu",
        "Xuecheng Su",
        "Y. Wu",
        "Y. K. Li",
        "Y. X. Wei",
        "Y. X. Zhu",
        "Yanhong Xu",
        "Yanping Huang",
        "Yao Li",
        "Yao Zhao",
        "Yaofeng Sun",
        "Yaohui Li",
        "Yaohui Wang",
        "Yi Zheng",
        "Yichao Zhang",
        "Yiliang Xiong",
        "Yilong Zhao",
        "Ying He",
        "Ying Tang",
        "Yishi Piao",
        "Yixin Dong",
        "Yixuan Tan",
        "Yiyuan Liu",
        "Yongji Wang",
        "Yongqiang Guo",
        "Yuchen Zhu",
        "Yuduan Wang",
        "Yuheng Zou",
        "Yukun Zha",
        "Yunxian Ma",
        "Yuting Yan",
        "Yuxiang You",
        "Yuxuan Liu",
        "Z. Z. Ren",
        "Zehui Ren",
        "Zhangli Sha",
        "Zhe Fu",
        "Zhen Huang",
        "Zhen Zhang",
        "Zhenda Xie",
        "Zhewen Hao",
        "Zhihong Shao",
        "Zhiniu Wen",
        "Zhipeng Xu",
        "Zhongyu Zhang",
        "Zhuoshu Li",
        "Zihan Wang",
        "Zihui Gu",
        "Zilin Li",
        "Ziwei Xie"
      ],
      "institutions": [
        "DeepSeek-AI"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/DeepSeek-V2/fig2.png",
      "keywords": [
        "42-Network Structure Design"
      ],
      "code_url": "https://github.com/deepseek-ai/DeepSeek-V2",
      "note_url": "notes/2024/DeepSeek-V2/note/",
      "prototxt_path": "meta/2024/DeepSeek-V2.prototxt",
      "update_time": 0
    },
    {
      "id": "DeepSeek-V3",
      "title": "DeepSeek-V3 Technical Report",
      "abbr": "DeepSeek-V3",
      "url": "http://arxiv.org/abs/2412.19437v1",
      "authors": [
        "DeepSeek-AI",
        "Aixin Liu",
        "Bei Feng",
        "Bing Xue",
        "Bingxuan Wang",
        "Bochao Wu",
        "Chengda Lu",
        "Chenggang Zhao",
        "Chengqi Deng",
        "Chenyu Zhang",
        "Chong Ruan",
        "Damai Dai",
        "Daya Guo",
        "Dejian Yang",
        "Deli Chen",
        "Dongjie Ji",
        "Erhang Li",
        "Fangyun Lin",
        "Fucong Dai",
        "Fuli Luo",
        "Guangbo Hao",
        "Guanting Chen",
        "Guowei Li",
        "H. Zhang",
        "Han Bao",
        "Hanwei Xu",
        "Haocheng Wang",
        "Haowei Zhang",
        "Honghui Ding",
        "Huajian Xin",
        "Huazuo Gao",
        "Hui Li",
        "Hui Qu",
        "J. L. Cai",
        "Jian Liang",
        "Jianzhong Guo",
        "Jiaqi Ni",
        "Jiashi Li",
        "Jiawei Wang",
        "Jin Chen",
        "Jingchang Chen",
        "Jingyang Yuan",
        "Junjie Qiu",
        "Junlong Li",
        "Junxiao Song",
        "Kai Dong",
        "Kai Hu",
        "Kaige Gao",
        "Kang Guan",
        "Kexin Huang",
        "Kuai Yu",
        "Lean Wang",
        "Lecong Zhang",
        "Lei Xu",
        "Leyi Xia",
        "Liang Zhao",
        "Litong Wang",
        "Liyue Zhang",
        "Meng Li",
        "Miaojun Wang",
        "Mingchuan Zhang",
        "Minghua Zhang",
        "Minghui Tang",
        "Mingming Li",
        "Ning Tian",
        "Panpan Huang",
        "Peiyi Wang",
        "Peng Zhang",
        "Qiancheng Wang",
        "Qihao Zhu",
        "Qinyu Chen",
        "Qiushi Du",
        "R. J. Chen",
        "R. L. Jin",
        "Ruiqi Ge",
        "Ruisong Zhang",
        "Ruizhe Pan",
        "Runji Wang",
        "Runxin Xu",
        "Ruoyu Zhang",
        "Ruyi Chen",
        "S. S. Li",
        "Shanghao Lu",
        "Shangyan Zhou",
        "Shanhuang Chen",
        "Shaoqing Wu",
        "Shengfeng Ye",
        "Shengfeng Ye",
        "Shirong Ma",
        "Shiyu Wang",
        "Shuang Zhou",
        "Shuiping Yu",
        "Shunfeng Zhou",
        "Shuting Pan",
        "T. Wang",
        "Tao Yun",
        "Tian Pei",
        "Tianyu Sun",
        "W. L. Xiao",
        "Wangding Zeng",
        "Wanjia Zhao",
        "Wei An",
        "Wen Liu",
        "Wenfeng Liang",
        "Wenjun Gao",
        "Wenqin Yu",
        "Wentao Zhang",
        "X. Q. Li",
        "Xiangyue Jin",
        "Xianzu Wang",
        "Xiao Bi",
        "Xiaodong Liu",
        "Xiaohan Wang",
        "Xiaojin Shen",
        "Xiaokang Chen",
        "Xiaokang Zhang",
        "Xiaosha Chen",
        "Xiaotao Nie",
        "Xiaowen Sun",
        "Xiaoxiang Wang",
        "Xin Cheng",
        "Xin Liu",
        "Xin Xie",
        "Xingchao Liu",
        "Xingkai Yu",
        "Xinnan Song",
        "Xinxia Shan",
        "Xinyi Zhou",
        "Xinyu Yang",
        "Xinyuan Li",
        "Xuecheng Su",
        "Xuheng Lin",
        "Y. K. Li",
        "Y. Q. Wang",
        "Y. X. Wei",
        "Y. X. Zhu",
        "Yang Zhang",
        "Yanhong Xu",
        "Yanhong Xu",
        "Yanping Huang",
        "Yao Li",
        "Yao Zhao",
        "Yaofeng Sun",
        "Yaohui Li",
        "Yaohui Wang",
        "Yi Yu",
        "Yi Zheng",
        "Yichao Zhang",
        "Yifan Shi",
        "Yiliang Xiong",
        "Ying He",
        "Ying Tang",
        "Yishi Piao",
        "Yisong Wang",
        "Yixuan Tan",
        "Yiyang Ma",
        "Yiyuan Liu",
        "Yongqiang Guo",
        "Yu Wu",
        "Yuan Ou",
        "Yuchen Zhu",
        "Yuduan Wang",
        "Yue Gong",
        "Yuheng Zou",
        "Yujia He",
        "Yukun Zha",
        "Yunfan Xiong",
        "Yunxian Ma",
        "Yuting Yan",
        "Yuxiang Luo",
        "Yuxiang You",
        "Yuxuan Liu",
        "Yuyang Zhou",
        "Z. F. Wu",
        "Z. Z. Ren",
        "Zehui Ren",
        "Zhangli Sha",
        "Zhe Fu",
        "Zhean Xu",
        "Zhen Huang",
        "Zhen Zhang",
        "Zhenda Xie",
        "Zhengyan Zhang",
        "Zhewen Hao",
        "Zhibin Gou",
        "Zhicheng Ma",
        "Zhigang Yan",
        "Zhihong Shao",
        "Zhipeng Xu",
        "Zhiyu Wu",
        "Zhongyu Zhang",
        "Zhuoshu Li",
        "Zihui Gu",
        "Zijia Zhu",
        "Zijun Liu",
        "Zilin Li",
        "Ziwei Xie",
        "Ziyang Song",
        "Ziyi Gao",
        "Zizheng Pan"
      ],
      "institutions": [
        "DeepSeek-AI"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/DeepSeek-V3/fig5.png",
      "keywords": [
        "42-Network Structure Design"
      ],
      "code_url": "https://github.com/deepseek-ai/DeepSeek-V3",
      "note_url": "notes/2024/DeepSeek-V3/note/",
      "prototxt_path": "meta/2024/DeepSeek-V3.prototxt",
      "update_time": 0
    },
    {
      "id": "DeepSeekMoE",
      "title": "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models",
      "abbr": "DeepSeekMoE",
      "url": "http://arxiv.org/abs/2401.06066v1",
      "authors": [
        "Damai Dai",
        "Chengqi Deng",
        "Chenggang Zhao",
        "R. X. Xu",
        "Huazuo Gao",
        "Deli Chen",
        "Jiashi Li",
        "Wangding Zeng",
        "Xingkai Yu",
        "Y. Wu",
        "Zhenda Xie",
        "Y. K. Li",
        "Panpan Huang",
        "Fuli Luo",
        "Chong Ruan",
        "Zhifang Sui",
        "Wenfeng Liang"
      ],
      "institutions": [
        "DeepSeek-AI",
        "Tsinghua University",
        "Nanjing University"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/DeepSeekMoE/fig2.png",
      "keywords": [
        "42-Network Structure Design"
      ],
      "code_url": "https://github.com/deepseek-ai/DeepSeek-MoE",
      "note_url": "notes/2024/DeepSeekMoE/note/",
      "prototxt_path": "meta/2024/DeepSeekMoE.prototxt",
      "update_time": 0
    },
    {
      "id": "dejavu",
      "title": "Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time",
      "abbr": "Deja Vu",
      "url": "https://openreview.net/forum?id=wIPIhHd00i",
      "authors": [
        "Zichang Liu",
        "Beidi Chen"
      ],
      "institutions": [
        "Rice University",
        "Zhe Jiang University",
        "Stanford University"
      ],
      "year": 2023,
      "venue": "ICML",
      "cover": "notes/2023/dejavu/dejavu.jpg",
      "keywords": [
        "01-Sparse/Pruning",
        "05-Sparsity (Structured)"
      ],
      "code_url": "https://github.com/FMInference/DejaVu",
      "note_url": null,
      "prototxt_path": "meta/2023/dejavu.prototxt",
      "update_time": 0
    },
    {
      "id": "diffuser",
      "title": "Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences",
      "abbr": "Diffuser",
      "url": "https://arxiv.org/abs/2210.11794",
      "authors": [
        "Aosong Feng",
        "Rex Ying"
      ],
      "institutions": [
        "Yale University"
      ],
      "year": 2023,
      "venue": "AAAI",
      "cover": "notes/2023/diffuser/diffuser.jpg",
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": "https://github.com/asFeng/Diffuser",
      "note_url": null,
      "prototxt_path": "meta/2023/diffuser.prototxt",
      "update_time": 0
    },
    {
      "id": "Domino",
      "title": "Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping",
      "abbr": "Domino",
      "url": "http://arxiv.org/abs/2409.15241v1",
      "authors": [
        "Guanhua Wang",
        "Chengming Zhang",
        "Zheyu Shen",
        "Ang Li",
        "Olatunji Ruwase"
      ],
      "institutions": [
        "Microsoft",
        "DeepSpeed"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/Domino/fig5.png",
      "keywords": [
        "29-Comm-Comp Overlap"
      ],
      "code_url": "https://github.com/deepspeedai/DeepSpeedExamples/blob/master/training/DeepSpeed-Domino/README.md",
      "note_url": "notes/2024/Domino/note/",
      "prototxt_path": "meta/2024/Domino.prototxt",
      "update_time": 0
    },
    {
      "id": "DReSS",
      "title": "DReSS: Data-driven Regularized Structured Streamlining for Large Language Models",
      "abbr": "DReSS",
      "url": "http://arxiv.org/abs/2501.17905v3",
      "authors": [
        "Mingkuan Feng",
        "Jinyang Wu",
        "Shuai Zhang",
        "Pengpeng Shao",
        "Ruihan Jin",
        "Zhengqi Wen",
        "Jianhua Tao",
        "Feihu Che"
      ],
      "institutions": [
        "Tsinghua University"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/DReSS/fig1.png",
      "keywords": [
        "01-Sparse/Pruning",
        "05-Sparsity (Structured)"
      ],
      "code_url": null,
      "note_url": "notes/2025/DReSS/note/",
      "prototxt_path": "meta/2025/DReSS.prototxt",
      "update_time": 0
    },
    {
      "id": "dsd",
      "title": "DSD: Dense-Sparse-Dense Training for Deep Neural Networks",
      "abbr": "DSD",
      "url": "https://arxiv.org/pdf/1607.04381.pdf",
      "authors": [
        "Song Han"
      ],
      "institutions": [
        "Stanford University"
      ],
      "year": 2017,
      "venue": "ICLR",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": null,
      "note_url": null,
      "prototxt_path": "meta/2017/dsd.prototxt",
      "update_time": 0
    },
    {
      "id": "adaptively_sparse_attention",
      "title": "Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers",
      "abbr": "Adaptively Sparse Attention",
      "url": "https://arxiv.org/abs/2305.15805",
      "authors": [
        "Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aurelien Lucchi, Thomas Hofmann"
      ],
      "institutions": [
        "University of Basel"
      ],
      "year": 2023,
      "venue": "arXiv",
      "cover": "notes/2023/adaptively_sparse_attention/adaptively_sparse_attention.jpg",
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)"
      ],
      "code_url": "https://github.com/sanagno/adaptively_sparse_attention",
      "note_url": null,
      "prototxt_path": "meta/2023/adaptively_sparse_attention.prototxt",
      "update_time": 0
    },
    {
      "id": "DynaExq",
      "title": "Dynamic Expert Quantization for Scalable Mixture-of-Experts Inference",
      "abbr": "DynaExq",
      "url": "http://arxiv.org/abs/2511.15015v1",
      "authors": [
        "Kexin Chu",
        "Dawei Xiang",
        "Zixu Shen",
        "Yiwei Yang",
        "Zecheng Liu",
        "Wei Zhang"
      ],
      "institutions": [
        "University of Connecticut",
        "University of California"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/DynaExq/fig2.png",
      "keywords": [
        "09-Quantization"
      ],
      "code_url": null,
      "note_url": "notes/2025/DynaExq/note/",
      "prototxt_path": "meta/2025/DynaExq.prototxt",
      "update_time": 0
    },
    {
      "id": "DSnoT",
      "title": "Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs",
      "abbr": "DSnoT",
      "url": "http://arxiv.org/abs/2310.08915v3",
      "authors": [
        "Yuxin Zhang",
        "Lirui Zhao",
        "Mingbao Lin",
        "Yunyun Sun",
        "Yiwu Yao",
        "Xingjia Han",
        "Jared Tanner",
        "Shiwei Liu",
        "Rongrong Ji"
      ],
      "institutions": [
        "Key Laboratory of Multimedia Trusted Perception and Efficient Computing",
        "Ministry of Education of China, Xiamen University",
        "Tencent Youtu Lab",
        "Huawei"
      ],
      "year": 2024,
      "venue": "ICLR",
      "cover": "notes/2024/DSnoT/dsnot.png",
      "keywords": [
        "01-Sparse/Pruning",
        "04-Sparsity (Weight)"
      ],
      "code_url": "https://github.com/zyxxmu/DSnoT",
      "note_url": "notes/2024/DSnoT/note/",
      "prototxt_path": "meta/2024/DSnoT.prototxt",
      "update_time": 0
    },
    {
      "id": "PCYZ0DJX",
      "title": "Efficient Attention Mechanisms for Large Language Models: A Survey",
      "abbr": "PCYZ0DJX",
      "url": "http://arxiv.org/abs/2507.19595v2",
      "authors": [
        "Yutao Sun",
        "Zhenyu Li",
        "Yike Zhang",
        "Tengyu Pan",
        "Bowen Dong",
        "Yuyi Guo",
        "Jianyong Wang"
      ],
      "institutions": [
        "Tsinghua University"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/PCYZ0DJX/cover.png",
      "keywords": [
        "02-Sparsity (Attention)",
        "41-Survey",
        "42-Network Structure Design"
      ],
      "code_url": null,
      "note_url": "notes/2025/PCYZ0DJX/note/",
      "prototxt_path": "meta/2025/PCYZ0DJX.prototxt",
      "update_time": 0
    },
    {
      "id": "P0JBYHCN",
      "title": "Efficient CPU-GPU Collaborative Inference for MoE-based LLMs on Memory-Limited Systems",
      "abbr": "P0JBYHCN",
      "url": "http://arxiv.org/abs/2512.16473v1",
      "authors": [
        "En-Ming Huang",
        "Li-Shang Lin",
        "Chun-Yi Lee"
      ],
      "institutions": [
        "National Taiwan University"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/P0JBYHCN/cover.png",
      "keywords": [
        "40-LLM Deployment"
      ],
      "code_url": "https://github.com/elsa-lab/MoE-CPU-GPU-Collaborative-Inference",
      "note_url": "notes/2025/P0JBYHCN/note/",
      "prototxt_path": "meta/2025/P0JBYHCN.prototxt",
      "update_time": 0
    },
    {
      "id": "nmSPARSE",
      "title": "Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning",
      "abbr": "nmSPARSE",
      "url": "https://proceedings.mlsys.org/paper_files/paper/2023/file/a10deb4d5227a8ea307ea8ff3cb712f4-Paper-mlsys2023.pdf",
      "authors": [
        "Bin Lin",
        "Fan Yang"
      ],
      "institutions": [
        "Microsoft Research",
        "Tsinghua University"
      ],
      "year": 2023,
      "venue": "MLSys",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning",
        "04-Sparsity (Weight)"
      ],
      "code_url": "https://github.com/microsoft/SparTA/tree/nmsparse",
      "note_url": null,
      "prototxt_path": "meta/2023/nmSPARSE.prototxt",
      "update_time": 0
    },
    {
      "id": "IHOT8YP4",
      "title": "Efficient Guided Generation for Large Language Models",
      "abbr": "",
      "url": "http://arxiv.org/abs/2307.09702v4",
      "authors": [
        "Brandon T. Willard",
        "Rmi Louf"
      ],
      "institutions": [
        "Normal Computing"
      ],
      "year": 2023,
      "venue": "arXiv",
      "cover": "notes/2023/IHOT8YP4/fig1.png",
      "keywords": [
        "40-LLM Deployment"
      ],
      "code_url": null,
      "note_url": "notes/2023/IHOT8YP4/note/",
      "prototxt_path": "meta/2023/IHOT8YP4.prototxt",
      "update_time": 0
    },
    {
      "id": "LIMINAL",
      "title": "Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need",
      "abbr": "LIMINAL",
      "url": "http://arxiv.org/abs/2507.14397v1",
      "authors": [
        "Michael Davies",
        "Neal Crago",
        "Karthikeyan Sankaralingam",
        "Christos Kozyrakis"
      ],
      "institutions": [
        "NVIDIA"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "39-Performance Modeling"
      ],
      "code_url": null,
      "note_url": "notes/2025/LIMINAL/note/",
      "prototxt_path": "meta/2025/LIMINAL.prototxt",
      "update_time": 0
    },
    {
      "id": "PagedAttention",
      "title": "Efficient Memory Management for Large Language Model Serving with PagedAttention",
      "abbr": "PagedAttention",
      "url": "http://arxiv.org/abs/2309.06180v1",
      "authors": [
        "Woosuk Kwon",
        "Zhuohan Li",
        "Siyuan Zhuang",
        "Ying Sheng",
        "Lianmin Zheng",
        "Cody Hao Yu",
        "Joseph E. Gonzalez",
        "Hao Zhang",
        "Ion Stoica"
      ],
      "institutions": [
        "UC Berkeley",
        "Stanford University"
      ],
      "year": 2023,
      "venue": "SOSP",
      "cover": "notes/2023/PagedAttention/vllm.png",
      "keywords": [
        "21-KV Cache Management",
        "40-LLM Deployment",
        "46-Tool"
      ],
      "code_url": "https://github.com/vllm-project/vllm",
      "note_url": "notes/2023/PagedAttention/note/",
      "prototxt_path": "meta/2023/PagedAttention.prototxt",
      "update_time": 0
    },
    {
      "id": "68I8KKBV",
      "title": "Efficient Methods for Natural Language Processing: A Survey",
      "abbr": "",
      "url": "https://arxiv.org/abs/2209.00099",
      "authors": [
        "Marcos Treviso, Ji-Ung Lee, Tianchu Ji, Betty van Aken, Qingqing Cao, Manuel R. Ciosici, Michael Hassid, Kenneth Heafield, Sara Hooker, Colin Raffel, Pedro H. Martins, Andr F. T. Martins, Jessica Zosa Forde, Peter Milder, Edwin Simpson, Noam Slonim, Jesse Dodge, Emma Strubell, Niranjan Balasubramanian, Leon Derczynski, Iryna Gurevych, Roy Schwartz"
      ],
      "institutions": [
        "IST",
        "The Hebrew University of Jerusalem, Israel"
      ],
      "year": 2023,
      "venue": "TACL",
      "cover": "notes/del/survey/efficient_NLP.jpg",
      "keywords": [
        "41-Survey"
      ],
      "code_url": null,
      "note_url": null,
      "prototxt_path": "meta/2023/68I8KKBV.prototxt",
      "update_time": 0
    },
    {
      "id": "ULY1AZGY",
      "title": "Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment",
      "abbr": "",
      "url": "http://arxiv.org/abs/2405.03594v1",
      "authors": [
        "Abhinav Agarwalla",
        "Abhay Gupta",
        "Alexandre Marques",
        "Shubhra Pandit",
        "Michael Goin",
        "Eldar Kurtic",
        "Kevin Leong",
        "Tuan Nguyen",
        "Mahmoud Salem",
        "Dan Alistarh",
        "Sean Lie",
        "Mark Kurtz"
      ],
      "institutions": [
        "Neural Magic",
        "Cerebras Systems",
        "IST Austria"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": "https://github.com/neuralmagic/nm-vllm",
      "note_url": "notes/2024/ULY1AZGY/note/",
      "prototxt_path": "meta/2024/ULY1AZGY.prototxt",
      "update_time": 0
    },
    {
      "id": "SDS",
      "title": "Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism",
      "abbr": "SDS",
      "url": "http://arxiv.org/abs/2408.10473v1",
      "authors": [
        "Guanchen Li",
        "Xiandong Zhao",
        "Lian Liu",
        "Zeping Li",
        "Dong Li",
        "Lu Tian",
        "Jie He",
        "Ashish Sirasao",
        "Emad Barsoum"
      ],
      "institutions": [
        "Advanced Micro Devices"
      ],
      "year": 2025,
      "venue": "Coling",
      "cover": "notes/2025/SDS/sds.png",
      "keywords": [
        "01-Sparse/Pruning",
        "04-Sparsity (Weight)"
      ],
      "code_url": null,
      "note_url": "notes/2025/SDS/note/",
      "prototxt_path": "meta/2025/SDS.prototxt",
      "update_time": 0
    },
    {
      "id": "Bonsa",
      "title": "Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes",
      "abbr": "Bonsa",
      "url": "https://arxiv.org/abs/2402.05406",
      "authors": [
        "Lucio Dery, Steven Kolawole, Jean-Franois Kagy, Virginia Smith, Graham Neubig, Ameet Talwalkar"
      ],
      "institutions": [
        "Carnegie Mellon University",
        "Google Research"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning",
        "05-Sparsity (Structured)"
      ],
      "code_url": "https://github.com/ldery/Bonsai",
      "note_url": null,
      "prototxt_path": "meta/2024/Bonsa.prototxt",
      "update_time": 0
    },
    {
      "id": "ADMM-pruning",
      "title": "Fast and Effective Weight Update for Pruned Large Language Models",
      "abbr": "ADMM-pruning",
      "url": "http://arxiv.org/abs/2401.02938v2",
      "authors": [
        "Vladimr Boa"
      ],
      "institutions": [
        "Comenius University"
      ],
      "year": 2024,
      "venue": "TMLR",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning",
        "04-Sparsity (Weight)"
      ],
      "code_url": "https://github.com/fmfi-compbio/admm-pruning",
      "note_url": "notes/2024/ADMM-pruning/note/",
      "prototxt_path": "meta/2024/ADMM-pruning.prototxt",
      "update_time": 0
    },
    {
      "id": "2AL79IUH",
      "title": "Fast Sparse ConvNets",
      "abbr": "",
      "url": "https://openaccess.thecvf.com/content_CVPR_2020/papers/Elsen_Fast_Sparse_ConvNets_CVPR_2020_paper.pdf",
      "authors": [
        "Erich Elsen, Marat Dukhan, Trevor Gale, Karen Simonyan"
      ],
      "institutions": [
        "Google",
        "DeepMind"
      ],
      "year": 2020,
      "venue": "CVPR",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": "https://github.com/fastconvnets/cvpr2020",
      "note_url": null,
      "prototxt_path": "meta/2020/2AL79IUH.prototxt",
      "update_time": 0
    },
    {
      "id": "FasterVGGT",
      "title": "Faster VGGT with Block-Sparse Global Attention",
      "abbr": "FasterVGGT",
      "url": "http://arxiv.org/abs/2509.07120v1",
      "authors": [
        "Chung-Shien Brian Wang",
        "Christian Schmidt",
        "Jens Piekenbrinck",
        "Bastian Leibe"
      ],
      "institutions": [
        "RWTH Aachen University"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)"
      ],
      "code_url": null,
      "note_url": "notes/2025/FasterVGGT/note/",
      "prototxt_path": "meta/2025/FasterVGGT.prototxt",
      "update_time": 0
    },
    {
      "id": "fastertransfomer",
      "title": "FasterTransformer",
      "abbr": "FT",
      "url": "https://github.com/NVIDIA/FasterTransformer",
      "authors": [],
      "institutions": [
        "NVIDIA"
      ],
      "year": 2023,
      "venue": "github",
      "cover": null,
      "keywords": [
        "46-Tool"
      ],
      "code_url": "https://github.com/NVIDIA/FasterTransformer",
      "note_url": null,
      "prototxt_path": "meta/2023/fastertransfomer.prototxt",
      "update_time": 0
    },
    {
      "id": "MeZO",
      "title": "Fine-Tuning Language Models with Just Forward Passes",
      "abbr": "MeZO",
      "url": "http://arxiv.org/abs/2305.17333v3",
      "authors": [
        "Sadhika Malladi",
        "Tianyu Gao",
        "Eshaan Nichani",
        "Alex Damian",
        "Jason D. Lee",
        "Danqi Chen",
        "Sanjeev Arora"
      ],
      "institutions": [
        "Princeton University"
      ],
      "year": 2023,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "45-Efficient Training"
      ],
      "code_url": "https://github.com/princeton-nlp/MeZO",
      "note_url": "notes/2023/MeZO/note/",
      "prototxt_path": "meta/2023/MeZO.prototxt",
      "update_time": 0
    },
    {
      "id": "flash_llm",
      "title": "Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity",
      "abbr": "Flash-LLM",
      "url": "https://arxiv.org/abs/2309.10285",
      "authors": [
        "Haojun Xia",
        "Zhen Zheng",
        "Yuchao Li",
        "Donglin Zhuang",
        "Zhongzhu Zhou",
        "Xiafei Qiu",
        "Yong Li",
        "Wei Lin",
        "Shuaiwen Leon Song"
      ],
      "institutions": [
        "Univeristy of Sydney",
        "Alibaba Group"
      ],
      "year": 2024,
      "venue": "VLDB",
      "cover": "notes/2024/flash_llm/cover.jpg",
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": "https://github.com/AlibabaResearch/flash-llm/tree/main",
      "note_url": null,
      "prototxt_path": "meta/2024/flash_llm.prototxt",
      "update_time": 0
    },
    {
      "id": "flashattention2",
      "title": "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning",
      "abbr": "FlashAttention-2",
      "url": "https://arxiv.org/abs/2307.08691",
      "authors": [
        "Tri Dao"
      ],
      "institutions": [
        "Stanford University"
      ],
      "year": 2024,
      "venue": "ICLR",
      "cover": null,
      "keywords": [
        "44-Layer Fusion (Reduce IO)"
      ],
      "code_url": "https://github.com/Dao-AILab/flash-attention",
      "note_url": null,
      "prototxt_path": "meta/2024/flashattention2.prototxt",
      "update_time": 0
    },
    {
      "id": "flashattention",
      "title": "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness",
      "abbr": "FlashAttention",
      "url": "https://arxiv.org/abs/2205.14135",
      "authors": [
        "Tri Dao",
        "Christopher"
      ],
      "institutions": [
        "Stanford University"
      ],
      "year": 2022,
      "venue": "NeurIPS",
      "cover": "notes/2022/flashattention/cover.jpg",
      "keywords": [
        "44-Layer Fusion (Reduce IO)"
      ],
      "code_url": "https://github.com/Dao-AILab/flash-attention",
      "note_url": null,
      "prototxt_path": "meta/2022/flashattention.prototxt",
      "update_time": 0
    },
    {
      "id": "FlashOverlap",
      "title": "FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation",
      "abbr": "FlashOverlap",
      "url": "http://arxiv.org/abs/2504.19519v1",
      "authors": [
        "Ke Hong",
        "Xiuhong Li",
        "Minxu Liu",
        "Qiuli Mao",
        "Tianqi Wu",
        "Zixiao Huang",
        "Lufang Chen",
        "Zhong Wang",
        "Yichong Zhang",
        "Zhenhua Zhu",
        "Guohao Dai",
        "Yu Wang"
      ],
      "institutions": [
        "Tsinghua University",
        "Infinigence-AI",
        "Shanghai Jiao Tong University"
      ],
      "year": 2026,
      "venue": "EuroSys",
      "cover": "notes/2026/FlashOverlap/fig3.png",
      "keywords": [
        "29-Comm-Comp Overlap"
      ],
      "code_url": "https://github.com/infinigence/FlashOverlap",
      "note_url": "notes/2026/FlashOverlap/note/",
      "prototxt_path": "meta/2026/FlashOverlap.prototxt",
      "update_time": 0
    },
    {
      "id": "flap",
      "title": "Fluctuation-based Adaptive Structured Pruning for Large Language Models",
      "abbr": "FLAP",
      "url": "https://arxiv.org/abs/2312.11983",
      "authors": [
        "Yongqi An",
        "Xu Zhao",
        "Tao Yu",
        "Ming Tang",
        "Jinqiao Wang"
      ],
      "institutions": [
        "Institute of Automation, Chinese Academy of Sciences"
      ],
      "year": 2024,
      "venue": "AAAI",
      "cover": "notes/del/flap.jpg",
      "keywords": [
        "01-Sparse/Pruning",
        "05-Sparsity (Structured)"
      ],
      "code_url": "https://github.com/CASIA-IVA-Lab/FLAP",
      "note_url": null,
      "prototxt_path": "meta/2024/flap.prototxt",
      "update_time": 0
    },
    {
      "id": "FLUX",
      "title": "FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion",
      "abbr": "FLUX",
      "url": "http://arxiv.org/abs/2406.06858v5",
      "authors": [
        "Li-Wen Chang",
        "Wenlei Bao",
        "Qi Hou",
        "Chengquan Jiang",
        "Ningxin Zheng",
        "Yinmin Zhong",
        "Xuanrun Zhang",
        "Zuquan Song",
        "Chengji Yao",
        "Ziheng Jiang",
        "Haibin Lin",
        "Xin Jin",
        "Xin Liu"
      ],
      "institutions": [
        "ByteDance",
        "Peking University"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/FLUX/fig3.png",
      "keywords": [
        "29-Comm-Comp Overlap"
      ],
      "code_url": "https://github.com/bytedance/flux",
      "note_url": "notes/2024/FLUX/note/",
      "prototxt_path": "meta/2024/FLUX.prototxt",
      "update_time": 0
    },
    {
      "id": "FoX",
      "title": "Forgetting Transformer: Softmax Attention with a Forget Gate",
      "abbr": "FoX",
      "url": "http://arxiv.org/abs/2503.02130v2",
      "authors": [
        "Zhixuan Lin",
        "Evgenii Nikishin",
        "Xu Owen He",
        "Aaron Courville"
      ],
      "institutions": [
        "Mila & Universite de Montreal",
        "MakerMaker AI"
      ],
      "year": 2025,
      "venue": "ICLR",
      "cover": null,
      "keywords": [
        "42-Network Structure Design"
      ],
      "code_url": "https://github.com/zhixuan-lin/forgetting-transformer",
      "note_url": "notes/2025/FoX/note/",
      "prototxt_path": "meta/2025/FoX.prototxt",
      "update_time": 0
    },
    {
      "id": "FPSAttention",
      "title": "FPSAttention: Training-Aware FP8 and Sparsity Co-Design for Fast Video Diffusion",
      "abbr": "FPSAttention",
      "url": "http://arxiv.org/abs/2506.04648v2",
      "authors": [
        "Akide Liu",
        "Zeyu Zhang",
        "Zhexin Li",
        "Xuehai Bai",
        "Yizeng Han",
        "Jiasheng Tang",
        "Yuanjie Xing",
        "Jichao Wu",
        "Mingyang Yang",
        "Weihua Chen",
        "Jiahao He",
        "Yuanyu He",
        "Fan Wang",
        "Gholamreza Haffari",
        "Bohan Zhuang"
      ],
      "institutions": [
        "Monash University",
        "Alibaba",
        "Zhejiang University"
      ],
      "year": 2025,
      "venue": "NeurIPS",
      "cover": "notes/2025/FPSAttention/cover.png",
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)",
        "09-Quantization"
      ],
      "code_url": null,
      "note_url": "notes/2025/FPSAttention/note/",
      "prototxt_path": "meta/2025/FPSAttention.prototxt",
      "update_time": 0
    },
    {
      "id": "FrameQuant",
      "title": "FrameQuant: Flexible Low-Bit Quantization for Transformers",
      "abbr": "FrameQuant",
      "url": "http://arxiv.org/abs/2403.06082v1",
      "authors": [
        "Harshavardhan Adepu",
        "Zhanpeng Zeng",
        "Li Zhang",
        "Vikas Singh"
      ],
      "institutions": [
        "University of Wisconsin-Madison",
        "Google Research"
      ],
      "year": 2024,
      "venue": "ICML",
      "cover": "notes/2024/FrameQuant/framequant.png",
      "keywords": [
        "09-Quantization"
      ],
      "code_url": "https://github.com/vsingh-group/FrameQuant",
      "note_url": "notes/2024/FrameQuant/note/",
      "prototxt_path": "meta/2024/FrameQuant.prototxt",
      "update_time": 0
    },
    {
      "id": "GatedAttention",
      "title": "Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free",
      "abbr": "GatedAttention",
      "url": "http://arxiv.org/abs/2505.06708v1",
      "authors": [
        "Zihan Qiu",
        "Zekun Wang",
        "Bo Zheng",
        "Zeyu Huang",
        "Kaiyue Wen",
        "Songlin Yang",
        "Rui Men",
        "Le Yu",
        "Fei Huang",
        "Suozhi Huang",
        "Dayiheng Liu",
        "Jingren Zhou",
        "Junyang Lin"
      ],
      "institutions": [
        "Qwen Team",
        "Alibaba Group",
        "University of Edinburgh",
        "Stanford University",
        "Massachusetts Institute of Technology",
        "Tsinghua University"
      ],
      "year": 2025,
      "venue": "NeurIPS",
      "cover": "notes/2025/GatedAttention/fig1.png",
      "keywords": [
        "42-Network Structure Design"
      ],
      "code_url": "https://github.com/qiuzh20/gated_attention",
      "note_url": "notes/2025/GatedAttention/note/",
      "prototxt_path": "meta/2025/GatedAttention.prototxt",
      "update_time": 0
    },
    {
      "id": "GLM-4.5",
      "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models",
      "abbr": "GLM-4.5",
      "url": "http://arxiv.org/abs/2508.06471v1",
      "authors": [
        "GLM-4. 5 Team",
        "Aohan Zeng",
        "Xin Lv",
        "Qinkai Zheng",
        "Zhenyu Hou",
        "Bin Chen",
        "Chengxing Xie",
        "Cunxiang Wang",
        "Da Yin",
        "Hao Zeng",
        "Jiajie Zhang",
        "Kedong Wang",
        "Lucen Zhong",
        "Mingdao Liu",
        "Rui Lu",
        "Shulin Cao",
        "Xiaohan Zhang",
        "Xuancheng Huang",
        "Yao Wei",
        "Yean Cheng",
        "Yifan An",
        "Yilin Niu",
        "Yuanhao Wen",
        "Yushi Bai",
        "Zhengxiao Du",
        "Zihan Wang",
        "Zilin Zhu",
        "Bohan Zhang",
        "Bosi Wen",
        "Bowen Wu",
        "Bowen Xu",
        "Can Huang",
        "Casey Zhao",
        "Changpeng Cai",
        "Chao Yu",
        "Chen Li",
        "Chendi Ge",
        "Chenghua Huang",
        "Chenhui Zhang",
        "Chenxi Xu",
        "Chenzheng Zhu",
        "Chuang Li",
        "Congfeng Yin",
        "Daoyan Lin",
        "Dayong Yang",
        "Dazhi Jiang",
        "Ding Ai",
        "Erle Zhu",
        "Fei Wang",
        "Gengzheng Pan",
        "Guo Wang",
        "Hailong Sun",
        "Haitao Li",
        "Haiyang Li",
        "Haiyi Hu",
        "Hanyu Zhang",
        "Hao Peng",
        "Hao Tai",
        "Haoke Zhang",
        "Haoran Wang",
        "Haoyu Yang",
        "He Liu",
        "He Zhao",
        "Hongwei Liu",
        "Hongxi Yan",
        "Huan Liu",
        "Huilong Chen",
        "Ji Li",
        "Jiajing Zhao",
        "Jiamin Ren",
        "Jian Jiao",
        "Jiani Zhao",
        "Jianyang Yan",
        "Jiaqi Wang",
        "Jiayi Gui",
        "Jiayue Zhao",
        "Jie Liu",
        "Jijie Li",
        "Jing Li",
        "Jing Lu",
        "Jingsen Wang",
        "Jingwei Yuan",
        "Jingxuan Li",
        "Jingzhao Du",
        "Jinhua Du",
        "Jinxin Liu",
        "Junkai Zhi",
        "Junli Gao",
        "Ke Wang",
        "Lekang Yang",
        "Liang Xu",
        "Lin Fan",
        "Lindong Wu",
        "Lintao Ding",
        "Lu Wang",
        "Man Zhang",
        "Minghao Li",
        "Minghuan Xu",
        "Mingming Zhao",
        "Mingshu Zhai",
        "Pengfan Du",
        "Qian Dong",
        "Shangde Lei",
        "Shangqing Tu",
        "Shangtong Yang",
        "Shaoyou Lu",
        "Shijie Li",
        "Shuang Li",
        "Shuang-Li",
        "Shuxun Yang",
        "Sibo Yi",
        "Tianshu Yu",
        "Wei Tian",
        "Weihan Wang",
        "Wenbo Yu",
        "Weng Lam Tam",
        "Wenjie Liang",
        "Wentao Liu",
        "Xiao Wang",
        "Xiaohan Jia",
        "Xiaotao Gu",
        "Xiaoying Ling",
        "Xin Wang",
        "Xing Fan",
        "Xingru Pan",
        "Xinyuan Zhang",
        "Xinze Zhang",
        "Xiuqing Fu",
        "Xunkai Zhang",
        "Yabo Xu",
        "Yandong Wu",
        "Yida Lu",
        "Yidong Wang",
        "Yilin Zhou",
        "Yiming Pan",
        "Ying Zhang",
        "Yingli Wang",
        "Yingru Li",
        "Yinpei Su",
        "Yipeng Geng",
        "Yitong Zhu",
        "Yongkun Yang",
        "Yuhang Li",
        "Yuhao Wu",
        "Yujiang Li",
        "Yunan Liu",
        "Yunqing Wang",
        "Yuntao Li",
        "Yuxuan Zhang",
        "Zezhen Liu",
        "Zhen Yang",
        "Zhengda Zhou",
        "Zhongpei Qiao",
        "Zhuoer Feng",
        "Zhuorui Liu",
        "Zichen Zhang",
        "Zihan Wang",
        "Zijun Yao",
        "Zikang Wang",
        "Ziqiang Liu",
        "Ziwei Chai",
        "Zixuan Li",
        "Zuodong Zhao",
        "Wenguang Chen",
        "Jidong Zhai",
        "Bin Xu",
        "Minlie Huang",
        "Hongning Wang",
        "Juanzi Li",
        "Yuxiao Dong",
        "Jie Tang"
      ],
      "institutions": [
        "Zhipu AI",
        "Tsinghua University"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "42-Network Structure Design"
      ],
      "code_url": "https://github.com/zai-org/GLM-4.5",
      "note_url": "notes/2025/GLM-4.5/note/",
      "prototxt_path": "meta/2025/GLM-4.5.prototxt",
      "update_time": 0
    },
    {
      "id": "gptq",
      "title": "GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers",
      "abbr": "GPTQ",
      "url": "https://arxiv.org/pdf/2210.17323.pdf",
      "authors": [
        "Elias Frantar",
        "Dan Alistarh"
      ],
      "institutions": [
        "IST Austria",
        "Neural Magic"
      ],
      "year": 2023,
      "venue": "ICLR",
      "cover": null,
      "keywords": [
        "09-Quantization"
      ],
      "code_url": "https://github.com/IST-DASLab/gptq",
      "note_url": null,
      "prototxt_path": "meta/2023/gptq.prototxt",
      "update_time": 0
    },
    {
      "id": "blocksparse",
      "title": "GPU Kernels for Block-Sparse Weights",
      "abbr": "blocksparse",
      "url": "https://cdn.openai.com/blocksparse/blocksparsepaper.pdf",
      "authors": [
        "Scott Gray",
        "Diederik P. Kingma"
      ],
      "institutions": [
        "OpenAI"
      ],
      "year": 2020,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": "https://github.com/openai/blocksparse",
      "note_url": null,
      "prototxt_path": "meta/2020/blocksparse.prototxt",
      "update_time": 0
    },
    {
      "id": "grain",
      "title": "Gradient-based Intra-attention Pruning on Pre-trained Language Models",
      "abbr": "GRAIN",
      "url": "https://arxiv.org/abs/2212.07634",
      "authors": [
        "Ziqing Yang",
        "Yiming Cui",
        "Xin Yao",
        "Shijin Wang"
      ],
      "institutions": [
        "Harbin Institute of Technology",
        "iFLYTEK Research"
      ],
      "year": 2023,
      "venue": "ACL",
      "cover": "notes/2023/grain/grain.jpg",
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": "https://github.com/airaria/GRAIN",
      "note_url": null,
      "prototxt_path": "meta/2023/grain.prototxt",
      "update_time": 0
    },
    {
      "id": "kcm",
      "title": "Gradient-Free Structured Pruning with Unlabeled Data",
      "abbr": "KCM",
      "url": "https://arxiv.org/abs/2303.04185",
      "authors": [
        "Azade Nova",
        "Dale Schuurmans"
      ],
      "institutions": [
        "Google"
      ],
      "year": 2023,
      "venue": "arXiv",
      "cover": "notes/2023/kcm/kcm.jpg",
      "keywords": [
        "01-Sparse/Pruning",
        "05-Sparsity (Structured)"
      ],
      "code_url": null,
      "note_url": null,
      "prototxt_path": "meta/2023/kcm.prototxt",
      "update_time": 0
    },
    {
      "id": "H2VD7Q70",
      "title": "Hardware-Centric Analysis of DeepSeek's Multi-Head Latent Attention",
      "abbr": "H2VD7Q70",
      "url": "http://arxiv.org/abs/2506.02523v1",
      "authors": [
        "Robin Geens",
        "Marian Verhelst"
      ],
      "institutions": [
        "KU Leuven"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "39-Performance Modeling"
      ],
      "code_url": null,
      "note_url": "notes/2025/H2VD7Q70/note/",
      "prototxt_path": "meta/2025/H2VD7Q70.prototxt",
      "update_time": 0
    },
    {
      "id": "HelixParallelism",
      "title": "Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding",
      "abbr": "HelixParallelism",
      "url": "http://arxiv.org/abs/2507.07120v1",
      "authors": [
        "Nidhi Bhatia",
        "Ankit More",
        "Ritika Borkar",
        "Tiyasa Mitra",
        "Ramon Matas",
        "Ritchie Zhao",
        "Maximilian Golub",
        "Dheevatsa Mudigere",
        "Brian Pharris",
        "Bita Darvish Rouhani"
      ],
      "institutions": [
        "NVIDIA"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/HelixParallelism/fig4.png",
      "keywords": [
        "29-Comm-Comp Overlap",
        "40-LLM Deployment"
      ],
      "code_url": null,
      "note_url": "notes/2025/HelixParallelism/note/",
      "prototxt_path": "meta/2025/HelixParallelism.prototxt",
      "update_time": 0
    },
    {
      "id": "DHC",
      "title": "Hyper-Connections",
      "abbr": "DHC",
      "url": "http://arxiv.org/abs/2409.19606v3",
      "authors": [
        "Defa Zhu",
        "Hongzhi Huang",
        "Zihao Huang",
        "Yutao Zeng",
        "Yunyao Mao",
        "Banggu Wu",
        "Qiyang Min",
        "Xun Zhou"
      ],
      "institutions": [
        "ByteDance Seed"
      ],
      "year": 2025,
      "venue": "ICLR",
      "cover": "notes/2025/DHC/cover.png",
      "keywords": [
        "42-Network Structure Design"
      ],
      "code_url": "https://github.com/lucidrains/hyper-connections",
      "note_url": "notes/2025/DHC/note/",
      "prototxt_path": "meta/2025/DHC.prototxt",
      "update_time": 0
    },
    {
      "id": "V3MFIRLV",
      "title": "Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference",
      "abbr": "",
      "url": "http://proceedings.mlr.press/v119/kurtz20a/kurtz20a.pdf",
      "authors": [
        "Mark Kurtz",
        "Dan Alistarh"
      ],
      "institutions": [
        "IST Austria",
        "Neural Magic"
      ],
      "year": 2020,
      "venue": "ICML",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": null,
      "note_url": null,
      "prototxt_path": "meta/2020/V3MFIRLV.prototxt",
      "update_time": 0
    },
    {
      "id": "DistAttention",
      "title": "Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache",
      "abbr": "DistAttention",
      "url": "http://arxiv.org/abs/2401.02669v2",
      "authors": [
        "Bin Lin",
        "Chen Zhang",
        "Tao Peng",
        "Hanyu Zhao",
        "Wencong Xiao",
        "Minmin Sun",
        "Anmin Liu",
        "Zhipeng Zhang",
        "Lanbo Li",
        "Xiafei Qiu",
        "Shen Li",
        "Zhigang Ji",
        "Tao Xie",
        "Yong Li",
        "Wei Lin"
      ],
      "institutions": [
        "Alibaba Group",
        "Shanghai Jiao Tong University",
        "Peking University"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/DistAttention/fig1.png",
      "keywords": [
        "21-KV Cache Management",
        "40-LLM Deployment"
      ],
      "code_url": null,
      "note_url": "notes/2024/DistAttention/note/",
      "prototxt_path": "meta/2024/DistAttention.prototxt",
      "update_time": 0
    },
    {
      "id": "InfLLM-V2",
      "title": "InfLLM-V2: Dense-Sparse Switchable Attention for Seamless Short-to-Long Adaptation",
      "abbr": "InfLLM-V2",
      "url": "http://arxiv.org/abs/2509.24663v1",
      "authors": [
        "Weilin Zhao",
        "Zihan Zhou",
        "Zhou Su",
        "Chaojun Xiao",
        "Yuxuan Li",
        "Yanghao Li",
        "Yudi Zhang",
        "Weilun Zhao",
        "Zhen Li",
        "Yuxiang Huang",
        "Ao Sun",
        "Xu Han",
        "Zhiyuan Liu"
      ],
      "institutions": [
        "Tsinghua University",
        "OpenBMB",
        "Harbin Institute of Technology"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/InfLLM-V2/fig2.png",
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)"
      ],
      "code_url": "https://github.com/OpenBMB/infllmv2_cuda_impl",
      "note_url": "notes/2025/InfLLM-V2/note/",
      "prototxt_path": "meta/2025/InfLLM-V2.prototxt",
      "update_time": 0
    },
    {
      "id": "Adrenaline",
      "title": "Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation",
      "abbr": "Adrenaline",
      "url": "http://arxiv.org/abs/2503.20552v1",
      "authors": [
        "Yunkai Liang",
        "Zhangyu Chen",
        "Pengfei Zuo",
        "Zhi Zhou",
        "Xu Chen",
        "Zhou Yu"
      ],
      "institutions": [
        "Sun Yat-sen University",
        "Huawei"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/Adrenaline/fig4.png",
      "keywords": [
        "21-KV Cache Management",
        "40-LLM Deployment"
      ],
      "code_url": "https://github.com/ASISys/Adrenaline",
      "note_url": "notes/2025/Adrenaline/note/",
      "prototxt_path": "meta/2025/Adrenaline.prototxt",
      "update_time": 0
    },
    {
      "id": "IFPruning",
      "title": "Instruction-Following Pruning for Large Language Models",
      "abbr": "IFPruning",
      "url": "http://arxiv.org/abs/2501.02086v2",
      "authors": [
        "Bairu Hou",
        "Qibin Chen",
        "Jianyu Wang",
        "Guoli Yin",
        "Chong Wang",
        "Nan Du",
        "Ruoming Pang",
        "Shiyu Chang",
        "Tao Lei"
      ],
      "institutions": [
        "Apple",
        "UC Santa Barbara"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/IFPruning/fig1.png",
      "keywords": [
        "01-Sparse/Pruning",
        "03-Sparsity (Activation)"
      ],
      "code_url": null,
      "note_url": "notes/2025/IFPruning/note/",
      "prototxt_path": "meta/2025/IFPruning.prototxt",
      "update_time": 0
    },
    {
      "id": "KIVI",
      "title": "KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache",
      "abbr": "KIVI",
      "url": "http://arxiv.org/abs/2402.02750v2",
      "authors": [
        "Zirui Liu",
        "Jiayi Yuan",
        "Hongye Jin",
        "Shaochen Zhong",
        "Zhaozhuo Xu",
        "Vladimir Braverman",
        "Beidi Chen",
        "Xia Hu"
      ],
      "institutions": [
        "Rice University",
        "Texas A&M University",
        "Stevens Institute of Technology",
        "Carnegie Mellon University"
      ],
      "year": 2024,
      "venue": "ICML",
      "cover": "notes/2024/KIVI/fig3.png",
      "keywords": [
        "09-Quantization",
        "19-Quantization (KV Cache)"
      ],
      "code_url": "https://github.com/jy-yuan/KIVI",
      "note_url": "notes/2024/KIVI/note/",
      "prototxt_path": "meta/2024/KIVI.prototxt",
      "update_time": 0
    },
    {
      "id": "k_pruning",
      "title": "Knowledge-preserving Pruning for Pre-trained Language Models without Retraining",
      "abbr": "K-pruning",
      "url": "https://arxiv.org/abs/2308.03449",
      "authors": [
        "Seungcheol Park",
        "Hojun Choi",
        "U Kang"
      ],
      "institutions": [
        "Seoul National University"
      ],
      "year": 2023,
      "venue": "arXiv",
      "cover": "notes/2023/k_pruning/kp.jpg",
      "keywords": [
        "01-Sparse/Pruning",
        "05-Sparsity (Structured)"
      ],
      "code_url": null,
      "note_url": "notes/2023/k_pruning/note/",
      "prototxt_path": "meta/2023/k_pruning.prototxt",
      "update_time": 0
    },
    {
      "id": "KVQuant",
      "title": "KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization",
      "abbr": "KVQuant",
      "url": "http://arxiv.org/abs/2401.18079",
      "authors": [
        "Coleman Hooper",
        "Sehoon Kim",
        "Hiva Mohammadzadeh",
        "Michael W. Mahoney",
        "Yakun Sophia Shao",
        "Kurt Keutzer",
        "Amir Gholami"
      ],
      "institutions": [
        "UC Berkeley"
      ],
      "year": 2024,
      "venue": "NeurIPS",
      "cover": "notes/2024/KVQuant/fig1.png",
      "keywords": [
        "09-Quantization",
        "19-Quantization (KV Cache)"
      ],
      "code_url": "https://github.com/SqueezeAILab/KVQuant",
      "note_url": "notes/2024/KVQuant/note/",
      "prototxt_path": "meta/2024/KVQuant.prototxt",
      "update_time": 0
    },
    {
      "id": "L4Q",
      "title": "L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ",
      "abbr": "L4Q",
      "url": "https://arxiv.org/abs/2402.04902",
      "authors": [
        "Hyesung Jeon, Yulhwa Kim, Jae-joon Kim"
      ],
      "institutions": [
        "Seoul National University",
        "Sungkyunkwan University"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/L4Q/l4q.jpg",
      "keywords": [
        "09-Quantization",
        "43-Low Rank Decomposition"
      ],
      "code_url": null,
      "note_url": "notes/2024/L4Q/note/",
      "prototxt_path": "meta/2024/L4Q.prototxt",
      "update_time": 0
    },
    {
      "id": "LaRoSA",
      "title": "La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation",
      "abbr": "LaRoSA",
      "url": "http://arxiv.org/abs/2507.01299v1",
      "authors": [
        "Kai Liu",
        "Bowen Xu",
        "Shaoyu Wu",
        "Xin Chen",
        "Hao Zhou",
        "Yongliang Tao",
        "Lulu Hu"
      ],
      "institutions": [
        "Alibaba Group"
      ],
      "year": 2025,
      "venue": "ICML",
      "cover": "notes/2025/LaRoSA/fig1.png",
      "keywords": [
        "01-Sparse/Pruning",
        "03-Sparsity (Activation)"
      ],
      "code_url": null,
      "note_url": "notes/2025/LaRoSA/note/",
      "prototxt_path": "meta/2025/LaRoSA.prototxt",
      "update_time": 0
    },
    {
      "id": "sr-ste",
      "title": "Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch",
      "abbr": "SR-STE",
      "url": "https://openreview.net/forum?id=K9bw7vqp_s",
      "authors": [
        "Aojun Zhou",
        "Hongsheng Li"
      ],
      "institutions": [
        "SenseTime Research",
        "Northwestern University"
      ],
      "year": 2021,
      "venue": "ICLR",
      "cover": "notes/2021/sr-ste/sr-ste.jpg",
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": "https://github.com/aojunzz/NM-sparsity",
      "note_url": null,
      "prototxt_path": "meta/2021/sr-ste.prototxt",
      "update_time": 0
    },
    {
      "id": "lobs",
      "title": "Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon",
      "abbr": "L-OBS",
      "url": "https://arxiv.org/pdf/1705.07565.pdf",
      "authors": [
        "Xin Dong",
        "Sinno Jialin Pan"
      ],
      "institutions": [
        "Nanyang Technological University"
      ],
      "year": 2017,
      "venue": "NeurIPS",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": "https://github.com/csyhhu/L-OBS",
      "note_url": null,
      "prototxt_path": "meta/2017/lobs.prototxt",
      "update_time": 0
    },
    {
      "id": "LightningAttention-2",
      "title": "Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models",
      "abbr": "LightningAttention-2",
      "url": "http://arxiv.org/abs/2401.04658v2",
      "authors": [
        "Zhen Qin",
        "Weigao Sun",
        "Dong Li",
        "Xuyang Shen",
        "Weixuan Sun",
        "Yiran Zhong"
      ],
      "institutions": [
        "OpenNLPLab"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "42-Network Structure Design"
      ],
      "code_url": "https://github.com/OpenNLPLab/lightning-attention",
      "note_url": "notes/2024/LightningAttention-2/note/",
      "prototxt_path": "meta/2024/LightningAttention-2.prototxt",
      "update_time": 0
    },
    {
      "id": "LISA",
      "title": "LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning",
      "abbr": "LISA",
      "url": "http://arxiv.org/abs/2403.17919v1",
      "authors": [
        "Rui Pan",
        "Xiang Liu",
        "Shizhe Diao",
        "Renjie Pi",
        "Jipeng Zhang",
        "Chi Han",
        "Tong Zhang"
      ],
      "institutions": [
        "Hong Kong University of Science and Technology",
        "University of Illinois Urbana-Champaign"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "45-Efficient Training"
      ],
      "code_url": null,
      "note_url": "notes/2024/LISA/note/",
      "prototxt_path": "meta/2024/LISA.prototxt",
      "update_time": 0
    },
    {
      "id": "LiteAttention",
      "title": "LiteAttention: A Temporal Sparse Attention for Diffusion Transformers",
      "abbr": "LiteAttention",
      "url": "http://arxiv.org/abs/2511.11062v1",
      "authors": [
        "Dor Shmilovich",
        "Tony Wu",
        "Aviad Dahan",
        "Yuval Domb"
      ],
      "institutions": [
        "MoonMath.ai"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)"
      ],
      "code_url": "https://github.com/moonmath-ai/LiteAttention",
      "note_url": "notes/2025/LiteAttention/note/",
      "prototxt_path": "meta/2025/LiteAttention.prototxt",
      "update_time": 0
    },
    {
      "id": "LLM_in_a_flash",
      "title": "LLM in a flash: Efficient Large Language Model Inference with Limited Memory",
      "abbr": "LLM in a flash",
      "url": "http://arxiv.org/abs/2312.11514",
      "authors": [
        "Keivan Alizadeh",
        "Iman Mirzadeh",
        "Dmitry Belenko",
        "Karen Khatamifard",
        "Minsik Cho",
        "Carlo C Del Mundo",
        "Mohammad Rastegari",
        "Mehrdad Farajtabar"
      ],
      "institutions": [
        "Apple"
      ],
      "year": 2023,
      "venue": "arXiv",
      "cover": "notes/2023/LLM_in_a_flash/windows.png",
      "keywords": [
        "01-Sparse/Pruning",
        "03-Sparsity (Activation)"
      ],
      "code_url": null,
      "note_url": "notes/2023/LLM_in_a_flash/note/",
      "prototxt_path": "meta/2023/LLM_in_a_flash.prototxt",
      "update_time": 0
    },
    {
      "id": "YS9YTT55",
      "title": "LLM Inference Serving: Survey of Recent Advances and Opportunities",
      "abbr": "YS9YTT55",
      "url": "http://arxiv.org/abs/2407.12391v1",
      "authors": [
        "Baolin Li",
        "Yankai Jiang",
        "Vijay Gadepally",
        "Devesh Tiwari"
      ],
      "institutions": [
        "Northeastern University",
        "Massachusetts Institute of Technology"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "40-LLM Deployment",
        "41-Survey"
      ],
      "code_url": null,
      "note_url": "notes/2024/YS9YTT55/note/",
      "prototxt_path": "meta/2024/YS9YTT55.prototxt",
      "update_time": 0
    },
    {
      "id": "LLM-Pruner",
      "title": "LLM-Pruner: On the Structural Pruning of Large Language Models",
      "abbr": "LLM-Pruner",
      "url": "http://arxiv.org/abs/2305.11627v3",
      "authors": [
        "Xinyin Ma",
        "Gongfan Fang",
        "Xinchao Wang"
      ],
      "institutions": [
        "National University of Singapore"
      ],
      "year": 2023,
      "venue": "arXiv",
      "cover": "notes/2023/LLM-Pruner/cover.jpg",
      "keywords": [
        "01-Sparse/Pruning",
        "05-Sparsity (Structured)"
      ],
      "code_url": "https://github.com/horseee/LLM-Pruner",
      "note_url": "notes/2023/LLM-Pruner/note/",
      "prototxt_path": "meta/2023/LLM-Pruner.prototxt",
      "update_time": 0
    },
    {
      "id": "LMCache",
      "title": "LMCache: An Efficient KV Cache Layer for Enterprise-Scale LLM Inference",
      "abbr": "LMCache",
      "url": "http://arxiv.org/abs/2510.09665v2",
      "authors": [
        "Yuhan Liu",
        "Yihua Cheng",
        "Jiayi Yao",
        "Yuwei An",
        "Xiaokun Chen",
        "Shaoting Feng",
        "Yuyang Huang",
        "Samuel Shen",
        "Rui Zhang",
        "Kuntai Du",
        "Junchen Jiang"
      ],
      "institutions": [
        "Tensormesh",
        "University of Chicago"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/LMCache/cover.png",
      "keywords": [
        "21-KV Cache Management"
      ],
      "code_url": "https://github.com/LMCache/LMCache",
      "note_url": "notes/2025/LMCache/note/",
      "prototxt_path": "meta/2025/LMCache.prototxt",
      "update_time": 0
    },
    {
      "id": "loftq",
      "title": "LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models",
      "abbr": "LoftQ",
      "url": "https://arxiv.org/abs/2310.08659",
      "authors": [
        "Yixiao Li, Yifan Yu, Chen Liang, Pengcheng He, Nikos Karampatziakis, Weizhu Chen, Tuo Zhao"
      ],
      "institutions": [
        "Georgia Institute of Technology",
        "Microsoft Azure"
      ],
      "year": 2023,
      "venue": "arXiv",
      "cover": "notes/2023/loftq/loftq.jpg",
      "keywords": [
        "09-Quantization",
        "43-Low Rank Decomposition",
        "45-Efficient Training"
      ],
      "code_url": "https://github.com/yxli2123/LoftQ",
      "note_url": "notes/2023/loftq/note/",
      "prototxt_path": "meta/2023/loftq.prototxt",
      "update_time": 0
    },
    {
      "id": "LongBench",
      "title": "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding",
      "abbr": "LongBench",
      "url": "http://arxiv.org/abs/2308.14508v2",
      "authors": [
        "Yushi Bai",
        "Xin Lv",
        "Jiajie Zhang",
        "Hongchang Lyu",
        "Jiankai Tang",
        "Zhidian Huang",
        "Zhengxiao Du",
        "Xiao Liu",
        "Aohan Zeng",
        "Lei Hou",
        "Yuxiao Dong",
        "Jie Tang",
        "Juanzi Li"
      ],
      "institutions": [
        "Tsinghua University",
        "Zhipu AI",
        "Institute of Automation, Chinese Academy of Sciences"
      ],
      "year": 2024,
      "venue": "ACL",
      "cover": "notes/2024/LongBench/cover.png",
      "keywords": [
        "47-Benchmark"
      ],
      "code_url": "https://github.com/THUDM/LongBench",
      "note_url": "notes/2024/LongBench/note/",
      "prototxt_path": "meta/2024/LongBench.prototxt",
      "update_time": 0
    },
    {
      "id": "LoRA+",
      "title": "LoRA+: Efficient Low Rank Adaptation of Large Models",
      "abbr": "LoRA+",
      "url": "http://arxiv.org/abs/2402.12354v1",
      "authors": [
        "Soufiane Hayou",
        "Nikhil Ghosh",
        "Bin Yu"
      ],
      "institutions": [
        "UC Berkeley"
      ],
      "year": 2024,
      "venue": "ICML",
      "cover": null,
      "keywords": [
        "45-Efficient Training"
      ],
      "code_url": "https://github.com/nikhil-ghosh-berkeley/loraplus",
      "note_url": "notes/2024/LoRA+/note/",
      "prototxt_path": "meta/2024/LoRA+.prototxt",
      "update_time": 0
    },
    {
      "id": "lora",
      "title": "LoRA: Low-rank adaptation of large language models",
      "abbr": "LoRA",
      "url": "https://arxiv.org/abs/2106.09685",
      "authors": [
        "Edward Hu",
        "Yelong Shen",
        "Weizhu Chen"
      ],
      "institutions": [
        "Microsoft"
      ],
      "year": 2022,
      "venue": "ICLR",
      "cover": "notes/2022/lora/lora.jpg",
      "keywords": [
        "43-Low Rank Decomposition",
        "45-Efficient Training"
      ],
      "code_url": "https://github.com/microsoft/LoRA",
      "note_url": null,
      "prototxt_path": "meta/2022/lora.prototxt",
      "update_time": 0
    },
    {
      "id": "lorashear",
      "title": "LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery",
      "abbr": "LoRAShear",
      "url": "https://arxiv.org/abs/2310.18356",
      "authors": [
        "Tianyi Chen",
        "Tianyu Ding",
        "Luming Liang"
      ],
      "institutions": [
        "Microsoft"
      ],
      "year": 2023,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning",
        "43-Low Rank Decomposition",
        "05-Sparsity (Structured)"
      ],
      "code_url": null,
      "note_url": null,
      "prototxt_path": "meta/2023/lorashear.prototxt",
      "update_time": 0
    },
    {
      "id": "Marconi",
      "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
      "abbr": "Marconi",
      "url": "http://arxiv.org/abs/2411.19379v3",
      "authors": [
        "Rui Pan",
        "Zhuang Wang",
        "Zhen Jia",
        "Can Karakus",
        "Luca Zancato",
        "Tri Dao",
        "Yida Wang",
        "Ravi Netravali"
      ],
      "institutions": [
        "Princeton University",
        "AWS"
      ],
      "year": 2025,
      "venue": "MLSys",
      "cover": null,
      "keywords": [
        "40-LLM Deployment"
      ],
      "code_url": null,
      "note_url": "notes/2025/Marconi/note/",
      "prototxt_path": "meta/2025/Marconi.prototxt",
      "update_time": 0
    },
    {
      "id": "MaskLLM",
      "title": "MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models",
      "abbr": "MaskLLM",
      "url": "http://arxiv.org/abs/2409.17481v1",
      "authors": [
        "Gongfan Fang",
        "Hongxu Yin",
        "Saurav Muralidharan",
        "Greg Heinrich",
        "Jeff Pool",
        "Jan Kautz",
        "Pavlo Molchanov",
        "Xinchao Wang"
      ],
      "institutions": [
        "NVIDIA",
        "National University of Singapore"
      ],
      "year": 2024,
      "venue": "NeurIPS",
      "cover": "notes/2024/MaskLLM/maskllm.png",
      "keywords": [
        "01-Sparse/Pruning",
        "04-Sparsity (Weight)"
      ],
      "code_url": "https://github.com/NVlabs/MaskLLM",
      "note_url": "notes/2024/MaskLLM/note/",
      "prototxt_path": "meta/2024/MaskLLM.prototxt",
      "update_time": 0
    },
    {
      "id": "massive-activations",
      "title": "Massive Activations in Large Language Models",
      "abbr": "massive-activations",
      "url": "http://arxiv.org/abs/2402.17762v2",
      "authors": [
        "Mingjie Sun",
        "Xinlei Chen",
        "J. Zico Kolter",
        "Zhuang Liu"
      ],
      "institutions": [
        "CMU",
        "Meta"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/massive-activations/massive_act.jpg",
      "keywords": [
        "41-Survey"
      ],
      "code_url": "https://github.com/locuslab/massive-activations",
      "note_url": "notes/2024/massive-activations/note/",
      "prototxt_path": "meta/2024/massive-activations.prototxt",
      "update_time": 0
    },
    {
      "id": "MegaScale-MoE",
      "title": "MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production",
      "abbr": "MegaScale-MoE",
      "url": "http://arxiv.org/abs/2505.11432v2",
      "authors": [
        "Chao Jin",
        "Ziheng Jiang",
        "Zhihao Bai",
        "Zheng Zhong",
        "Juncai Liu",
        "Xiang Li",
        "Ningxin Zheng",
        "Xi Wang",
        "Cong Xie",
        "Qi Huang",
        "Wen Heng",
        "Yiyuan Ma",
        "Wenlei Bao",
        "Size Zheng",
        "Yanghua Peng",
        "Haibin Lin",
        "Xuanzhe Liu",
        "Xin Jin",
        "Xin Liu"
      ],
      "institutions": [
        "ByteDance Seed",
        "Peking University"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/MegaScale-MoE/fig8.png",
      "keywords": [
        "29-Comm-Comp Overlap"
      ],
      "code_url": null,
      "note_url": "notes/2025/MegaScale-MoE/note/",
      "prototxt_path": "meta/2025/MegaScale-MoE.prototxt",
      "update_time": 0
    },
    {
      "id": "mHC",
      "title": "mHC: Manifold-Constrained Hyper-Connections",
      "abbr": "mHC",
      "url": "http://arxiv.org/abs/2512.24880v1",
      "authors": [
        "Zhenda Xie",
        "Yixuan Wei",
        "Huanqi Cao",
        "Chenggang Zhao",
        "Chengqi Deng",
        "Jiashi Li",
        "Damai Dai",
        "Huazuo Gao",
        "Jiang Chang",
        "Liang Zhao",
        "Shangyan Zhou",
        "Zhean Xu",
        "Zhengyan Zhang",
        "Wangding Zeng",
        "Shengding Hu",
        "Yuqing Wang",
        "Jingyang Yuan",
        "Lean Wang",
        "Wenfeng Liang"
      ],
      "institutions": [
        "DeepSeek-AI"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/mHC/cover.png",
      "keywords": [
        "42-Network Structure Design"
      ],
      "code_url": null,
      "note_url": "notes/2025/mHC/note/",
      "prototxt_path": "meta/2025/mHC.prototxt",
      "update_time": 0
    },
    {
      "id": "MVUE",
      "title": "Minimum Variance Unbiased N:M Sparsity for the Neural Gradients",
      "abbr": "MVUE",
      "url": "https://openreview.net/pdf?id=vuD2xEtxZcj",
      "authors": [
        "Brian Chmiel",
        "Daniel Soudry"
      ],
      "institutions": [
        "Habana Labs"
      ],
      "year": 2023,
      "venue": "ICLR",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": null,
      "note_url": null,
      "prototxt_path": "meta/2023/MVUE.prototxt",
      "update_time": 0
    },
    {
      "id": "MPK",
      "title": "Mirage Persistent Kernel: A Compiler and Runtime for Mega-Kernelizing Tensor Programs",
      "abbr": "MPK",
      "url": "http://arxiv.org/abs/2512.22219v1",
      "authors": [
        "Xinhao Cheng",
        "Zhihao Zhang",
        "Yu Zhou",
        "Jianan Ji",
        "Jinchen Jiang",
        "Zepeng Zhao",
        "Ziruo Xiao",
        "Zihao Ye",
        "Yingyi Huang",
        "Ruihang Lai",
        "Hongyi Jin",
        "Bohan Hou",
        "Mengdi Wu",
        "Yixin Dong",
        "Anthony Yip",
        "Zihao Ye",
        "Songting Wang",
        "Wenqin Yang",
        "Xupeng Miao",
        "Tianqi Chen",
        "Zhihao Jia"
      ],
      "institutions": [
        "Carnegie Mellon University",
        "Tsinghua University",
        "NVIDIA",
        "University of Michigan",
        "Purdue University"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/MPK/cover.png",
      "keywords": [
        "29-Comm-Comp Overlap",
        "40-LLM Deployment",
        "46-Tool"
      ],
      "code_url": "https://github.com/mirage-project/mirage",
      "note_url": "notes/2025/MPK/note/",
      "prototxt_path": "meta/2025/MPK.prototxt",
      "update_time": 0
    },
    {
      "id": "MPK-Mirage",
      "title": "Mirage: A Multi-Level Superoptimizer for Tensor Programs",
      "abbr": "Mirage",
      "url": "http://arxiv.org/abs/2405.05751v3",
      "authors": [
        "Mengdi Wu",
        "Xinhao Cheng",
        "Shengyu Liu",
        "Chunan Shi",
        "Jianan Ji",
        "Kit Ao",
        "Praveen Velliengiri",
        "Xupeng Miao",
        "Oded Padon",
        "Zhihao Jia"
      ],
      "institutions": [
        "Carnegie Mellon University",
        "Peking University",
        "Pennsylvania State University",
        "Purdue University",
        "Weizmann Institute of Science"
      ],
      "year": 2025,
      "venue": "OSDI",
      "cover": "notes/2025/MPK-Mirage/cover.png",
      "keywords": [
        "29-Comm-Comp Overlap",
        "40-LLM Deployment",
        "46-Tool"
      ],
      "code_url": "https://github.com/mirage-project/mirage",
      "note_url": "notes/2025/MPK-Mirage/note/",
      "prototxt_path": "meta/2025/MPK-Mirage.prototxt",
      "update_time": 0
    },
    {
      "id": "52A7RO95",
      "title": "Mixture of Experts in Large Language Models",
      "abbr": "52A7RO95",
      "url": "http://arxiv.org/abs/2507.11181v1",
      "authors": [
        "Danyang Zhang",
        "Junhao Song",
        "Ziqian Bi",
        "Yingfang Yuan",
        "Tianyang Wang",
        "Joe Yeong",
        "Junfeng Hao"
      ],
      "institutions": [
        "ByteDance",
        "Imperial College London",
        "Purdue University",
        "Heriot-Watt University",
        "Vokram Group"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/52A7RO95/fig1.png",
      "keywords": [
        "42-Network Structure Design",
        "41-Survey"
      ],
      "code_url": null,
      "note_url": "notes/2025/52A7RO95/note/",
      "prototxt_path": "meta/2025/52A7RO95.prototxt",
      "update_time": 0
    },
    {
      "id": "MoD",
      "title": "Mixture-of-Depths: Dynamically allocating compute in transformer-based language models",
      "abbr": "MoD",
      "url": "http://arxiv.org/abs/2404.02258v1",
      "authors": [
        "David Raposo",
        "Sam Ritter",
        "Blake Richards",
        "Timothy Lillicrap",
        "Peter Conway Humphreys",
        "Adam Santoro"
      ],
      "institutions": [
        "Google DeepMind"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/MoD/mod.jpg",
      "keywords": [
        "42-Network Structure Design"
      ],
      "code_url": null,
      "note_url": "notes/2024/MoD/note/",
      "prototxt_path": "meta/2024/MoD.prototxt",
      "update_time": 0
    },
    {
      "id": "MoR",
      "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation",
      "abbr": "MoR",
      "url": "http://arxiv.org/abs/2507.10524v1",
      "authors": [
        "Sangmin Bae",
        "Yujin Kim",
        "Reza Bayat",
        "Sungnyun Kim",
        "Jiyoun Ha",
        "Tal Schuster",
        "Adam Fisch",
        "Hrayr Harutyunyan",
        "Ziwei Ji",
        "Aaron Courville",
        "Se-Young Yun"
      ],
      "institutions": [
        "Google DeepMind",
        "Google Research",
        "Google Cloud",
        "KAIST AI"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/MoR/fig1.png",
      "keywords": [
        "42-Network Structure Design",
        "02-Sparsity (Attention)"
      ],
      "code_url": "https://github.com/raymin0223/mixture_of_recursions",
      "note_url": "notes/2025/MoR/note/",
      "prototxt_path": "meta/2025/MoR.prototxt",
      "update_time": 0
    },
    {
      "id": "MoDES",
      "title": "MoDES: Accelerating Mixture-of-Experts Multimodal Large Language Models via Dynamic Expert Skipping",
      "abbr": "MoDES",
      "url": "http://arxiv.org/abs/2511.15690v1",
      "authors": [
        "Yushi Huang",
        "Zining Wang",
        "Zhihang Yuan",
        "Yifu Ding",
        "Ruihao Gong",
        "Jinyang Guo",
        "Xianglong Liu",
        "Jun Zhang"
      ],
      "institutions": [
        "Hong Kong University of Science and Technology",
        "Beihang University",
        "Peking University"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/MoDES/fig4.png",
      "keywords": [
        "01-Sparse/Pruning",
        "05-Sparsity (Structured)"
      ],
      "code_url": null,
      "note_url": "notes/2025/MoDES/note/",
      "prototxt_path": "meta/2025/MoDES.prototxt",
      "update_time": 0
    },
    {
      "id": "MoPEQ",
      "title": "MoPEQ: Mixture of Mixed Precision Quantized Experts",
      "abbr": "MoPEQ",
      "url": "http://arxiv.org/abs/2509.02512v1",
      "authors": [
        "Krishna Teja Chitty-Venkata",
        "Jie Ye",
        "Murali Emani"
      ],
      "institutions": [
        "Argonne National Laboratory",
        "Illinois Institute of Technology"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/MoPEQ/fig1.png",
      "keywords": [
        "09-Quantization"
      ],
      "code_url": "https://github.com/krishnateja95/MoE-Mixed-Prec",
      "note_url": "notes/2025/MoPEQ/note/",
      "prototxt_path": "meta/2025/MoPEQ.prototxt",
      "update_time": 0
    },
    {
      "id": "VFPWFOQA",
      "title": "MoR: Mixture Of Representations For Mixed-Precision Training",
      "abbr": "MoR",
      "url": "http://arxiv.org/abs/2512.22804v1",
      "authors": [
        "Bor-Yiing Su",
        "Peter Dykas",
        "Mike Chrzanowski",
        "Jatin Chhugani"
      ],
      "institutions": [
        "NVIDIA",
        "Meta"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "45-Efficient Training",
        "09-Quantization"
      ],
      "code_url": null,
      "note_url": "notes/2025/VFPWFOQA/note/",
      "prototxt_path": "meta/2025/VFPWFOQA.prototxt",
      "update_time": 0
    },
    {
      "id": "Mosaic",
      "title": "Mosaic: Composite Projection Pruning for Resource-efficient LLMs",
      "abbr": "Mosaic",
      "url": "http://arxiv.org/abs/2504.06323v1",
      "authors": [
        "Bailey J. Eccles",
        "Leon Wong",
        "Blesson Varghese"
      ],
      "institutions": [
        "University of St Andrews"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/Mosaic/fig6.png",
      "keywords": [
        "01-Sparse/Pruning",
        "05-Sparsity (Structured)"
      ],
      "code_url": null,
      "note_url": "notes/2025/Mosaic/note/",
      "prototxt_path": "meta/2025/Mosaic.prototxt",
      "update_time": 0
    },
    {
      "id": "movement_pruning",
      "title": "Movement Pruning: Adaptive Sparsity by Fine-Tuning",
      "abbr": "Movement Pruning",
      "url": "https://arxiv.org/abs/2005.07683",
      "authors": [
        "Victor Sanh",
        "Thomas Wolf",
        "Alexander M. Rush"
      ],
      "institutions": [
        "Hugging Face",
        "Cornell University"
      ],
      "year": 2020,
      "venue": "NeurIPS",
      "cover": "notes/2020/movement_pruning/mp.jpg",
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": "https://github.com/huggingface/block_movement_pruning",
      "note_url": null,
      "prototxt_path": "meta/2020/movement_pruning.prototxt",
      "update_time": 0
    },
    {
      "id": "NanoFlow",
      "title": "NanoFlow: Towards Optimal Large Language Model Serving Throughput",
      "abbr": "NanoFlow",
      "url": "http://arxiv.org/abs/2408.12757v2",
      "authors": [
        "Kan Zhu",
        "Yufei Gao",
        "Yilong Zhao",
        "Liangyu Zhao",
        "Gefei Zuo",
        "Yile Gu",
        "Dedong Xie",
        "Tian Tang",
        "Qinyu Xu",
        "Zihao Ye",
        "Keisuke Kamahori",
        "Chien-Yu Lin",
        "Ziren Wang",
        "Stephanie Wang",
        "Arvind Krishnamurthy",
        "Baris Kasikci"
      ],
      "institutions": [
        "University of Washington",
        "Tsinghua University"
      ],
      "year": 2025,
      "venue": "OSDI",
      "cover": "notes/2025/NanoFlow/pipeline.gif",
      "keywords": [
        "29-Comm-Comp Overlap"
      ],
      "code_url": "https://github.com/efeslab/Nanoflow",
      "note_url": "notes/2025/NanoFlow/note/",
      "prototxt_path": "meta/2025/NanoFlow.prototxt",
      "update_time": 0
    },
    {
      "id": "MiKV",
      "title": "No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization",
      "abbr": "MiKV",
      "url": "http://arxiv.org/abs/2402.18096v1",
      "authors": [
        "June Yong Yang",
        "Byeongwook Kim",
        "Jeongin Bae",
        "Beomseok Kwon",
        "Gunho Park",
        "Eunho Yang",
        "Se Jung Kwon",
        "Dongsoo Lee"
      ],
      "institutions": [
        "KAIST",
        "NAVER Cloud"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "19-Quantization (KV Cache)",
        "09-Quantization"
      ],
      "code_url": null,
      "note_url": "notes/2024/MiKV/note/",
      "prototxt_path": "meta/2024/MiKV.prototxt",
      "update_time": 0
    },
    {
      "id": "NOSA",
      "title": "NOSA: Native and Offloadable Sparse Attention",
      "abbr": "NOSA",
      "url": "http://arxiv.org/abs/2510.13602v1",
      "authors": [
        "Yuxiang Huang",
        "Chaojun Xiao",
        "Xu Han",
        "Zhiyuan Liu"
      ],
      "institutions": [
        "Tsinghua University"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "02-Sparsity (Attention)"
      ],
      "code_url": null,
      "note_url": "notes/2025/NOSA/note/",
      "prototxt_path": "meta/2025/NOSA.prototxt",
      "update_time": 0
    },
    {
      "id": "omniquant",
      "title": "OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models",
      "abbr": "OmniQuant",
      "url": "https://arxiv.org/abs/2308.13137",
      "authors": [
        "Wenqi Shao",
        "Ping Luo"
      ],
      "institutions": [
        "OpenGVLab",
        "The University of Hong Kong"
      ],
      "year": 2023,
      "venue": "arXiv",
      "cover": "notes/2023/omniquant/omniquant.png",
      "keywords": [
        "09-Quantization"
      ],
      "code_url": "https://github.com/OpenGVLab/OmniQuant",
      "note_url": null,
      "prototxt_path": "meta/2023/omniquant.prototxt",
      "update_time": 0
    },
    {
      "id": "obc",
      "title": "Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning",
      "abbr": "OBC",
      "url": "https://openreview.net/pdf?id=ksVGCOlOEba",
      "authors": [
        "Elias Frantar",
        "Dan Alistarh"
      ],
      "institutions": [
        "IST Austria",
        "Neural Magic"
      ],
      "year": 2022,
      "venue": "NeurIPS",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning",
        "09-Quantization"
      ],
      "code_url": "https://github.com/IST-DASLab/OBC",
      "note_url": null,
      "prototxt_path": "meta/2022/obc.prototxt",
      "update_time": 0
    },
    {
      "id": "obd",
      "title": "Optimal Brain Damage",
      "abbr": "OBD",
      "url": "https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf",
      "authors": [
        "Yann Le Cun"
      ],
      "institutions": [],
      "year": 1989,
      "venue": "NeurIPS",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": null,
      "note_url": null,
      "prototxt_path": "meta/1989/obd.prototxt",
      "update_time": 0
    },
    {
      "id": "obs",
      "title": "Optimal Brain Surgeon and general network pruning",
      "abbr": "OBS",
      "url": "https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=298572&tag=1",
      "authors": [],
      "institutions": [],
      "year": 1993,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": null,
      "note_url": null,
      "prototxt_path": "meta/1993/obs.prototxt",
      "update_time": 0
    },
    {
      "id": "CHESS",
      "title": "Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification",
      "abbr": "CHESS",
      "url": "http://arxiv.org/abs/2409.01366v1",
      "authors": [
        "Junhui He",
        "Shangyu Wu",
        "Weidong Wen",
        "Chun Jason Xue",
        "Qingan Li"
      ],
      "institutions": [
        "Wuhan University",
        "MBZUAI",
        "City University of Hong Kong"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": "https://anonymous.4open.science/r/CHESS-BA40/README.md",
      "note_url": "notes/2024/CHESS/note/",
      "prototxt_path": "meta/2024/CHESS.prototxt",
      "update_time": 0
    },
    {
      "id": "OSSCAR",
      "title": "OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization",
      "abbr": "OSSCAR",
      "url": "http://arxiv.org/abs/2403.12983v1",
      "authors": [
        "Xiang Meng",
        "Shibal Ibrahim",
        "Kayhan Behdin",
        "Hussein Hazimeh",
        "Natalia Ponomareva",
        "Rahul Mazumder"
      ],
      "institutions": [
        "Massachusetts Institute of Technology",
        "Google Research"
      ],
      "year": 2024,
      "venue": "ICML",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning",
        "05-Sparsity (Structured)"
      ],
      "code_url": "https://github.com/mazumder-lab/OSSCAR",
      "note_url": "notes/2024/OSSCAR/note/",
      "prototxt_path": "meta/2024/OSSCAR.prototxt",
      "update_time": 0
    },
    {
      "id": "owl",
      "title": "Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity",
      "abbr": "OWL",
      "url": "https://arxiv.org/pdf/2310.05175.pdf",
      "authors": [
        "Lu Yin",
        "You Wu",
        "Zhenyu Zhang",
        "Cheng-Yu Hsieh",
        "Yaqing Wang",
        "Yiling Jia",
        "Mykola Pechenizkiy",
        "Yi Liang",
        "Zhangyang Wang",
        "Shiwei Liu"
      ],
      "institutions": [
        "Eindhoven University of Technology",
        "University of Texas at Austin",
        "Google Research",
        "University of Washington"
      ],
      "year": 2024,
      "venue": "ICML",
      "cover": "notes/2024/owl/cover.jpg",
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": "https://github.com/luuyin/OWL",
      "note_url": null,
      "prototxt_path": "meta/2024/owl.prototxt",
      "update_time": 0
    },
    {
      "id": "Dist-Einsum",
      "title": "Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models",
      "abbr": "Dist-Einsum",
      "url": "https://dl.acm.org/doi/abs/10.1145/3567955.3567959",
      "authors": [
        "Shibo Wang, Jinliang Wei, Amit Sabne, Andy Davis, Berkin Ilbeyi, Blake Hechtman, Dehao Chen, Karthik Srinivasa Murthy, Marcello Maggioni, Qiao Zhang, Sameer Kumar, Tongfei Guo, Yuanzhong Xu, Zongwei Zhou"
      ],
      "institutions": [
        "Google"
      ],
      "year": 2023,
      "venue": "ASPLOS",
      "cover": "notes/2023/Dist-Einsum/fig5.png",
      "keywords": [
        "29-Comm-Comp Overlap"
      ],
      "code_url": null,
      "note_url": "notes/2023/Dist-Einsum/note/",
      "prototxt_path": "meta/2023/Dist-Einsum.prototxt",
      "update_time": 0
    },
    {
      "id": "PanguUltra",
      "title": "Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs",
      "abbr": "PanguUltra",
      "url": "http://arxiv.org/abs/2504.07866v2",
      "authors": [
        "Yichun Yin",
        "Wenyong Huang",
        "Kaikai Song",
        "Yehui Tang",
        "Xueyu Wu",
        "Wei Guo",
        "Peng Guo",
        "Yaoyuan Wang",
        "Xiaojun Meng",
        "Yasheng Wang",
        "Dong Li",
        "Can Chen",
        "Dandan Tu",
        "Yin Li",
        "Fisher Yu",
        "Ruiming Tang",
        "Yunhe Wang",
        "Baojun Wang",
        "Bin Wang",
        "Bo Wang",
        "Boxiao Liu",
        "Changzheng Zhang",
        "Duyu Tang",
        "Fei Mi",
        "Hui Jin",
        "Jiansheng Wei",
        "Jiarui Qin",
        "Jinpeng Li",
        "Jun Zhao",
        "Liqun Deng",
        "Lin Li",
        "Minghui Xu",
        "Naifu Zhang",
        "Nianzu Zheng",
        "Qiang Li",
        "Rongju Ruan",
        "Shengjun Cheng",
        "Tianyu Guo",
        "Wei He",
        "Wei Li",
        "Weiwen Liu",
        "Wulong Liu",
        "Xinyi Dai",
        "Yonghan Dong",
        "Yu Pan",
        "Yue Li",
        "Yufei Wang",
        "Yujun Li",
        "Yunsheng Ni",
        "Zhe Liu",
        "Zhenhe Zhang",
        "Zhicheng Liu"
      ],
      "institutions": [
        "Huawei"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "42-Network Structure Design"
      ],
      "code_url": null,
      "note_url": "notes/2025/PanguUltra/note/",
      "prototxt_path": "meta/2025/PanguUltra.prototxt",
      "update_time": 0
    },
    {
      "id": "PAROAttention",
      "title": "PAROAttention: Pattern-Aware ReOrdering for Efficient Sparse and Quantized Attention in Visual Generation Models",
      "abbr": "PAROAttention",
      "url": "http://arxiv.org/abs/2506.16054v1",
      "authors": [
        "Tianchen Zhao",
        "Ke Hong",
        "Xinhao Yang",
        "Xuefeng Xiao",
        "Huixia Li",
        "Feng Ling",
        "Ruiqi Xie",
        "Siqi Chen",
        "Hongyu Zhu",
        "Yichong Zhang",
        "Yu Wang"
      ],
      "institutions": [
        "Tsinghua University",
        "ByteDance Seed"
      ],
      "year": 2025,
      "venue": "NeurIPS",
      "cover": "notes/2025/PAROAttention/fig3.png",
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)"
      ],
      "code_url": null,
      "note_url": "notes/2025/PAROAttention/note/",
      "prototxt_path": "meta/2025/PAROAttention.prototxt",
      "update_time": 0
    },
    {
      "id": "PAT",
      "title": "PAT: Accelerating LLM Decoding via Prefix-Aware Attention with Resource Efficient Multi-Tile Kernel",
      "abbr": "PAT",
      "url": "http://arxiv.org/abs/2511.22333v2",
      "authors": [
        "Jinjun Yi",
        "Zhixin Zhao",
        "Yitao Hu",
        "Ke Yan",
        "Weiwei Sun",
        "Hao Wang",
        "Laiping Zhao",
        "Yuhao Zhang",
        "Wenxin Li",
        "Keqiu Li"
      ],
      "institutions": [
        "Tianjin University",
        "Stevens Institute of Technology"
      ],
      "year": 2026,
      "venue": "ASPLOS",
      "cover": null,
      "keywords": [
        "40-LLM Deployment"
      ],
      "code_url": "https://github.com/flashserve/PAT",
      "note_url": "notes/2026/PAT/note/",
      "prototxt_path": "meta/2026/PAT.prototxt",
      "update_time": 0
    },
    {
      "id": "IA8CS3VH",
      "title": "Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models",
      "abbr": "RIA",
      "url": "https://openreview.net/forum?id=Tr0lPx9woF",
      "authors": [
        "Yingtao Zhang, Haoli Bai, Haokun Lin, Jialin Zhao, Lu Hou, Carlo Vittorio Cannistraci"
      ],
      "institutions": [
        "Tsinghua University",
        "Institute of Automation, Chinese Academy of Sciences",
        "Huawei"
      ],
      "year": 2024,
      "venue": "ICLR",
      "cover": "notes/del/Plug-and-Play.jpg",
      "keywords": [
        "01-Sparse/Pruning",
        "04-Sparsity (Weight)"
      ],
      "code_url": "https://github.com/biomedical-cybernetics/Relative-importance-and-activation-pruning",
      "note_url": null,
      "prototxt_path": "meta/2024/IA8CS3VH.prototxt",
      "update_time": 0
    },
    {
      "id": "PPCL",
      "title": "Pluggable Pruning with Contiguous Layer Distillation for Diffusion Transformers",
      "abbr": "PPCL",
      "url": "http://arxiv.org/abs/2511.16156v1",
      "authors": [
        "Jian Ma",
        "Qirong Peng",
        "Xujie Zhu",
        "Peixing Xie",
        "Chen Chen",
        "Haonan Lu"
      ],
      "institutions": [
        "OPPO AI Center",
        "Sun Yat-sen University",
        "The Chinese University of Hong Kong"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/PPCL/fig3.png",
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": "https://github.com/OPPO-Mente-Lab/Qwen-Image-Pruning",
      "note_url": "notes/2025/PPCL/note/",
      "prototxt_path": "meta/2025/PPCL.prototxt",
      "update_time": 0
    },
    {
      "id": "OpenVINO",
      "title": "Post-training deep neural network pruning via layer-wise calibration",
      "abbr": "OpenVINO",
      "url": "https://openaccess.thecvf.com/content/ICCV2021W/LPCV/papers/Lazarevich_Post-Training_Deep_Neural_Network_Pruning_via_Layer-Wise_Calibration_ICCVW_2021_paper.pdf",
      "authors": [
        "Ivan Lazarevich",
        "Nikita Malinin"
      ],
      "institutions": [
        "Intel Corporation"
      ],
      "year": 2021,
      "venue": "ICCV",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": null,
      "note_url": null,
      "prototxt_path": "meta/2021/OpenVINO.prototxt",
      "update_time": 0
    },
    {
      "id": "gpfqv2",
      "title": "Post-training Quantization for Neural Networks with Provable Guarantees",
      "abbr": "GPFQv2",
      "url": "https://arxiv.org/pdf/2201.11113.pdf",
      "authors": [
        "Jinjie Zhang",
        "Rayan Saab"
      ],
      "institutions": [
        "UCSD"
      ],
      "year": 2023,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "09-Quantization"
      ],
      "code_url": "https://github.com/YixuanSeanZhou/Quantized_Neural_Nets",
      "note_url": null,
      "prototxt_path": "meta/2023/gpfqv2.prototxt",
      "update_time": 0
    },
    {
      "id": "SCAP",
      "title": "Post-Training Statistical Calibration for Higher Activation Sparsity",
      "abbr": "SCAP",
      "url": "http://arxiv.org/abs/2412.07174v1",
      "authors": [
        "Vui Seng Chua",
        "Yujie Pan",
        "Nilesh Jain"
      ],
      "institutions": [
        "Intel"
      ],
      "year": 2024,
      "venue": "ENLSP",
      "cover": "notes/2024/SCAP/fig2.png",
      "keywords": [
        "01-Sparse/Pruning",
        "03-Sparsity (Activation)"
      ],
      "code_url": "https://github.com/IntelLabs/SCAP",
      "note_url": "notes/2024/SCAP/note/",
      "prototxt_path": "meta/2024/SCAP.prototxt",
      "update_time": 0
    },
    {
      "id": "PowerInfer-2",
      "title": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone",
      "abbr": "PowerInfer-2",
      "url": "http://arxiv.org/abs/2406.06282v2",
      "authors": [
        "Zhenliang Xue",
        "Yixin Song",
        "Zeyu Mi",
        "Le Chen",
        "Yubin Xia",
        "Haibo Chen"
      ],
      "institutions": [
        "Shanghai Jiao Tong University"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning",
        "03-Sparsity (Activation)"
      ],
      "code_url": "https://powerinfer.ai/v2/",
      "note_url": "notes/2024/PowerInfer-2/note/",
      "prototxt_path": "meta/2024/PowerInfer-2.prototxt",
      "update_time": 0
    },
    {
      "id": "PowerInfer",
      "title": "PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU",
      "abbr": "PowerInfer",
      "url": "http://arxiv.org/abs/2312.12456v1",
      "authors": [
        "Yixin Song",
        "Zeyu Mi",
        "Haotong Xie",
        "Haibo Chen"
      ],
      "institutions": [
        "Shanghai Jiao Tong University"
      ],
      "year": 2023,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning",
        "03-Sparsity (Activation)"
      ],
      "code_url": "https://github.com/SJTU-IPADS/PowerInfer",
      "note_url": "notes/2023/PowerInfer/note/",
      "prototxt_path": "meta/2023/PowerInfer.prototxt",
      "update_time": 0
    },
    {
      "id": "ProSparse",
      "title": "ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models",
      "abbr": "ProSparse",
      "url": "https://arxiv.org/abs/2402.13516",
      "authors": [
        "Chenyang Song, Xu Han, Zhengyan Zhang, Shengding Hu, Xiyu Shi, Kuai Li, Chen Chen, Zhiyuan Liu, Guangli Li, Tao Yang, Maosong Sun"
      ],
      "institutions": [
        "Tsinghua University",
        "Institute of Computing Technology, Chinese Academy of Sciences",
        "Tencent Machine Learning Platform"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/ProSparse/prosparse.jpg",
      "keywords": [
        "01-Sparse/Pruning",
        "03-Sparsity (Activation)"
      ],
      "code_url": "https://github.com/raincleared-song/sparse_gpu_operator",
      "note_url": "notes/2024/ProSparse/note/",
      "prototxt_path": "meta/2024/ProSparse.prototxt",
      "update_time": 0
    },
    {
      "id": "Pruner-Zero",
      "title": "Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models",
      "abbr": "Pruner-Zero",
      "url": "http://arxiv.org/abs/2406.02924v1",
      "authors": [
        "Peijie Dong",
        "Lujun Li",
        "Zhenheng Tang",
        "Xiang Liu",
        "Xinglin Pan",
        "Qiang Wang",
        "Xiaowen Chu"
      ],
      "institutions": [
        "Hong Kong University of Science and Technology",
        "Hong Kong Baptist Univeristy",
        "Harbin Institude of Technology"
      ],
      "year": 2024,
      "venue": "ICML",
      "cover": "notes/2024/Pruner-Zero/fig1.png",
      "keywords": [
        "04-Sparsity (Weight)",
        "01-Sparse/Pruning"
      ],
      "code_url": "https://github.com/pprp/Pruner-Zero",
      "note_url": "notes/2024/Pruner-Zero/note/",
      "prototxt_path": "meta/2024/Pruner-Zero.prototxt",
      "update_time": 0
    },
    {
      "id": "Cus-Prun",
      "title": "Pruning General Large Language Models into Customized Expert Models",
      "abbr": "Cus-Prun",
      "url": "http://arxiv.org/abs/2506.02561v1",
      "authors": [
        "Yirao Zhao",
        "Guizhen Chen",
        "Kenji Kawaguchi",
        "Lidong Bing",
        "Wenxuan Zhang"
      ],
      "institutions": [
        "National University of Singapore",
        "Nanyang Technological University",
        "Alibaba Group",
        "Singapore University of Technology and Design"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/Cus-Prun/fig1.png",
      "keywords": [
        "01-Sparse/Pruning",
        "05-Sparsity (Structured)"
      ],
      "code_url": "https://github.com/zhaoyiran924/Custom-Prune",
      "note_url": "notes/2025/Cus-Prun/note/",
      "prototxt_path": "meta/2025/Cus-Prun.prototxt",
      "update_time": 0
    },
    {
      "id": "gbdt",
      "title": "Pruning Large Language Models via Accuracy Predictor",
      "abbr": "GBDT",
      "url": "https://arxiv.org/abs/2309.09507",
      "authors": [
        "Yupeng Ji",
        "Yibo Cao",
        "Jiucai Liu"
      ],
      "institutions": [
        "Tongji University",
        "Chongqing University"
      ],
      "year": 2023,
      "venue": "arXiv",
      "cover": "notes/2023/gbdt/gbdt.jpg",
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": null,
      "note_url": null,
      "prototxt_path": "meta/2023/gbdt.prototxt",
      "update_time": 0
    },
    {
      "id": "AdaptiveSparseTrainer",
      "title": "Pruning Large Language Models with Semi-Structural Adaptive Sparse Training",
      "abbr": "AdaptiveSparseTrainer",
      "url": "http://arxiv.org/abs/2407.20584v3",
      "authors": [
        "Weiyu Huang",
        "Yuezhou Hu",
        "Guohao Jian",
        "Jun Zhu",
        "Jianfei Chen"
      ],
      "institutions": [
        "Tsinghua University"
      ],
      "year": 2025,
      "venue": "AAAI",
      "cover": "notes/2025/AdaptiveSparseTrainer/fig1.png",
      "keywords": [
        "01-Sparse/Pruning",
        "04-Sparsity (Weight)"
      ],
      "code_url": "https://github.com/thu-ml/Adaptive-Sparse-Trainer",
      "note_url": "notes/2025/AdaptiveSparseTrainer/note/",
      "prototxt_path": "meta/2025/AdaptiveSparseTrainer.prototxt",
      "update_time": 0
    },
    {
      "id": "PINS",
      "title": "Pruning Pre-trained Language Models with Principled Importance and Self-regularization",
      "abbr": "PINS",
      "url": "https://aclanthology.org/2023.findings-acl.573/",
      "authors": [
        "Siyu Ren",
        "Kenny Q. Zhu"
      ],
      "institutions": [
        "Shanghai Jiao Tong University"
      ],
      "year": 2023,
      "venue": "ACL",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": "https://github.com/drsy/pins",
      "note_url": null,
      "prototxt_path": "meta/2023/PINS.prototxt",
      "update_time": 0
    },
    {
      "id": "smp",
      "title": "Pruning Pre-trained Language Models Without Fine-Tuning",
      "abbr": "SMP",
      "url": "https://aclanthology.org/2023.acl-long.35.pdf",
      "authors": [
        "Ting Jiang",
        "Deqing Wang",
        "Fuzhen Zhuang",
        "Ruobing Xie",
        "Feng Xia"
      ],
      "institutions": [
        "Beihang University",
        "Zhongguancun Laboratory"
      ],
      "year": 2023,
      "venue": "ACL",
      "cover": "notes/2023/smp/smp.jpg",
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": "https://github.com/kongds/SMP",
      "note_url": null,
      "prototxt_path": "meta/2023/smp.prototxt",
      "update_time": 0
    },
    {
      "id": "Q-Sparse",
      "title": "Q-Sparse: All Large Language Models can be Fully Sparsely-Activated",
      "abbr": "Q-Sparse",
      "url": "http://arxiv.org/abs/2407.10969v1",
      "authors": [
        "Hongyu Wang",
        "Shuming Ma",
        "Ruiping Wang",
        "Furu Wei"
      ],
      "institutions": [
        "Microsoft Research",
        "University of Chinese Academy of Sciences"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/Q-Sparse/q-sparse.png",
      "keywords": [
        "09-Quantization",
        "01-Sparse/Pruning",
        "03-Sparsity (Activation)"
      ],
      "code_url": null,
      "note_url": "notes/2024/Q-Sparse/note/",
      "prototxt_path": "meta/2024/Q-Sparse.prototxt",
      "update_time": 0
    },
    {
      "id": "QA-LoRA",
      "title": "QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models",
      "abbr": "QA-LoRA",
      "url": "https://arxiv.org/abs/2309.14717",
      "authors": [
        "Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang, Zhengsu Chen, Xiaopeng Zhang, Qi Tian"
      ],
      "institutions": [
        "Huawei"
      ],
      "year": 2024,
      "venue": "ICLR",
      "cover": "notes/2024/QA-LoRA/qalora.jpg",
      "keywords": [
        "09-Quantization",
        "43-Low Rank Decomposition"
      ],
      "code_url": "https://github.com/yuhuixu1993/qa-lora",
      "note_url": "notes/2024/QA-LoRA/note/",
      "prototxt_path": "meta/2024/QA-LoRA.prototxt",
      "update_time": 0
    },
    {
      "id": "qlora",
      "title": "QLoRA: Efficient Finetuning of Quantized LLMs",
      "abbr": "QLoRA",
      "url": "https://arxiv.org/abs/2305.14314",
      "authors": [
        "Tim Dettmers",
        "Artidoro Pagnoni"
      ],
      "institutions": [
        "University of Washington"
      ],
      "year": 2023,
      "venue": "arXiv",
      "cover": "notes/2023/qlora/qlora.jpg",
      "keywords": [
        "43-Low Rank Decomposition",
        "09-Quantization"
      ],
      "code_url": "https://github.com/artidoro/qlora",
      "note_url": null,
      "prototxt_path": "meta/2023/qlora.prototxt",
      "update_time": 0
    },
    {
      "id": "QuIP",
      "title": "QuIP: Quantization with Incoherence Processing",
      "abbr": "QuIP",
      "url": "https://arxiv.org/pdf/2307.13304.pdf",
      "authors": [
        "Jerry Chee",
        "Christopher De Sa"
      ],
      "institutions": [
        "Cornell University"
      ],
      "year": 2023,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "09-Quantization"
      ],
      "code_url": "https://github.com/jerry-chee/QuIP",
      "note_url": null,
      "prototxt_path": "meta/2023/QuIP.prototxt",
      "update_time": 0
    },
    {
      "id": "Qwen3",
      "title": "Qwen3 Technical Report",
      "abbr": "Qwen3",
      "url": "http://arxiv.org/abs/2505.09388v1",
      "authors": [
        "An Yang",
        "Anfeng Li",
        "Baosong Yang",
        "Beichen Zhang",
        "Binyuan Hui",
        "Bo Zheng",
        "Bowen Yu",
        "Chang Gao",
        "Chengen Huang",
        "Chenxu Lv",
        "Chujie Zheng",
        "Dayiheng Liu",
        "Fan Zhou",
        "Fei Huang",
        "Feng Hu",
        "Hao Ge",
        "Haoran Wei",
        "Huan Lin",
        "Jialong Tang",
        "Jian Yang",
        "Jianhong Tu",
        "Jianwei Zhang",
        "Jianxin Yang",
        "Jiaxi Yang",
        "Jing Zhou",
        "Jingren Zhou",
        "Junyang Lin",
        "Kai Dang",
        "Keqin Bao",
        "Kexin Yang",
        "Le Yu",
        "Lianghao Deng",
        "Mei Li",
        "Mingfeng Xue",
        "Mingze Li",
        "Pei Zhang",
        "Peng Wang",
        "Qin Zhu",
        "Rui Men",
        "Ruize Gao",
        "Shixuan Liu",
        "Shuang Luo",
        "Tianhao Li",
        "Tianyi Tang",
        "Wenbiao Yin",
        "Xingzhang Ren",
        "Xinyu Wang",
        "Xinyu Zhang",
        "Xuancheng Ren",
        "Yang Fan",
        "Yang Su",
        "Yichang Zhang",
        "Yinger Zhang",
        "Yu Wan",
        "Yuqiong Liu",
        "Zekun Wang",
        "Zeyu Cui",
        "Zhenru Zhang",
        "Zhipeng Zhou",
        "Zihan Qiu"
      ],
      "institutions": [
        "Qwen Team"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/Qwen3/fig1.png",
      "keywords": [
        "42-Network Structure Design"
      ],
      "code_url": "https://github.com/QwenLM/Qwen3",
      "note_url": "notes/2025/Qwen3/note/",
      "prototxt_path": "meta/2025/Qwen3.prototxt",
      "update_time": 0
    },
    {
      "id": "R-Sparse",
      "title": "R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference",
      "abbr": "R-Sparse",
      "url": "http://arxiv.org/abs/2504.19449v1",
      "authors": [
        "Zhenyu Zhang",
        "Zechun Liu",
        "Yuandong Tian",
        "Harshit Khaitan",
        "Zhangyang Wang",
        "Steven Li"
      ],
      "institutions": [
        "The University of Texas at Austin",
        "Meta AI"
      ],
      "year": 2025,
      "venue": "ICLR",
      "cover": "notes/2025/R-Sparse/fig4.png",
      "keywords": [
        "01-Sparse/Pruning",
        "03-Sparsity (Activation)"
      ],
      "code_url": "https://github.com/VITA-Group/R-Sparse",
      "note_url": "notes/2025/R-Sparse/note/",
      "prototxt_path": "meta/2025/R-Sparse.prototxt",
      "update_time": 0
    },
    {
      "id": "CLA",
      "title": "Reducing Transformer Key-Value Cache Size with Cross-Layer Attention",
      "abbr": "CLA",
      "url": "http://arxiv.org/abs/2405.12981v1",
      "authors": [
        "William Brandon",
        "Mayank Mishra",
        "Aniruddha Nrusimha",
        "Rameswar Panda",
        "Jonathan Ragan Kelly"
      ],
      "institutions": [
        "Massachusetts Institute of Technology"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/CLA/fig1.png",
      "keywords": [
        "42-Network Structure Design"
      ],
      "code_url": "https://github.com/JerryYin777/Cross-Layer-Attention",
      "note_url": "notes/2024/CLA/note/",
      "prototxt_path": "meta/2024/CLA.prototxt",
      "update_time": 0
    },
    {
      "id": "RecursiveTransformers",
      "title": "Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA",
      "abbr": "RecursiveTransformers",
      "url": "http://arxiv.org/abs/2410.20672v3",
      "authors": [
        "Sangmin Bae",
        "Adam Fisch",
        "Hrayr Harutyunyan",
        "Ziwei Ji",
        "Seungyeon Kim",
        "Tal Schuster"
      ],
      "institutions": [
        "KAIST AI",
        "Google DeepMind",
        "Google Research"
      ],
      "year": 2025,
      "venue": "ICLR",
      "cover": "notes/2025/RecursiveTransformers/fig1.png",
      "keywords": [
        "42-Network Structure Design"
      ],
      "code_url": null,
      "note_url": "notes/2025/RecursiveTransformers/note/",
      "prototxt_path": "meta/2025/RecursiveTransformers.prototxt",
      "update_time": 0
    },
    {
      "id": "HMR7HKFV",
      "title": "ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models",
      "abbr": "ReLU Strikes Back",
      "url": "https://arxiv.org/abs/2310.04564",
      "authors": [
        "Iman Mirzadeh",
        "Mehrdad Farajtabar"
      ],
      "institutions": [
        "Apple"
      ],
      "year": 2024,
      "venue": "ICLR oral",
      "cover": "notes/2024/HMR7HKFV/ReLU_Strikes_Back.jpg",
      "keywords": [
        "01-Sparse/Pruning",
        "03-Sparsity (Activation)"
      ],
      "code_url": "https://github.com/sjtu-ipads/powerinfer",
      "note_url": null,
      "prototxt_path": "meta/2024/HMR7HKFV.prototxt",
      "update_time": 0
    },
    {
      "id": "ReLU2",
      "title": "ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs",
      "abbr": "ReLU2",
      "url": "https://arxiv.org/abs/2402.03804",
      "authors": [
        "Zhengyan Zhang, Yixin Song, Guanghui Yu, Xu Han, Yankai Lin, Chaojun Xiao, Chenyang Song, Zhiyuan Liu, Zeyu Mi, Maosong Sun"
      ],
      "institutions": [
        "Tsinghua University",
        "Shanghai Jiao Tong University"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/ReLU2/activation.png",
      "keywords": [
        "01-Sparse/Pruning",
        "03-Sparsity (Activation)"
      ],
      "code_url": null,
      "note_url": "notes/2024/ReLU2/note/",
      "prototxt_path": "meta/2024/ReLU2.prototxt",
      "update_time": 0
    },
    {
      "id": "ReMoE",
      "title": "ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing",
      "abbr": "ReMoE",
      "url": "http://arxiv.org/abs/2412.14711v1",
      "authors": [
        "Ziteng Wang",
        "Jianfei Chen",
        "Jun Zhu"
      ],
      "institutions": [
        "Tsinghua University"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/ReMoE/fig1.png",
      "keywords": [
        "42-Network Structure Design"
      ],
      "code_url": "https://github.com/thu-ml/ReMoE",
      "note_url": "notes/2024/ReMoE/note/",
      "prototxt_path": "meta/2024/ReMoE.prototxt",
      "update_time": 0
    },
    {
      "id": "0VRXJQ3F",
      "title": "Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving",
      "abbr": "0VRXJQ3F",
      "url": "http://arxiv.org/abs/2503.24000v1",
      "authors": [
        "Wei Gao",
        "Xinyu Zhou",
        "Peng Sun",
        "Tianwei Zhang",
        "Yonggang Wen"
      ],
      "institutions": [
        "Nanyang Technological University",
        "Shanghai AI Laboratory",
        "SenseTime"
      ],
      "year": 2025,
      "venue": "MLSys",
      "cover": "notes/2025/0VRXJQ3F/tab1.png",
      "keywords": [
        "21-KV Cache Management",
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)",
        "41-Survey"
      ],
      "code_url": "https://github.com/LLMkvsys/rethink-kv-compression",
      "note_url": "notes/2025/0VRXJQ3F/note/",
      "prototxt_path": "meta/2025/0VRXJQ3F.prototxt",
      "update_time": 0
    },
    {
      "id": "SN1PK7EK",
      "title": "Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark",
      "abbr": "",
      "url": "http://arxiv.org/abs/2402.11592v2",
      "authors": [
        "Yihua Zhang",
        "Pingzhi Li",
        "Junyuan Hong",
        "Jiaxiang Li",
        "Yimeng Zhang",
        "Wenqing Zheng",
        "Pin-Yu Chen",
        "Jason D. Lee",
        "Wotao Yin",
        "Mingyi Hong",
        "Zhangyang Wang",
        "Sijia Liu",
        "Tianlong Chen"
      ],
      "institutions": [
        "Michigan State University",
        "The University of North Carolina"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "45-Efficient Training"
      ],
      "code_url": " https://github.com/ZO-Bench/ZO-LLM",
      "note_url": "notes/2024/SN1PK7EK/note/",
      "prototxt_path": "meta/2024/SN1PK7EK.prototxt",
      "update_time": 0
    },
    {
      "id": "RotateKV",
      "title": "RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via Outlier-Aware Adaptive Rotations",
      "abbr": "RotateKV",
      "url": "http://arxiv.org/abs/2501.16383v2",
      "authors": [
        "Zunhai Su",
        "Zhe Chen",
        "Wang Shen",
        "Hanyu Wei",
        "Linge Li",
        "Huangqi Yu",
        "Kehong Yuan"
      ],
      "institutions": [
        "Tsinghua University",
        "Huawei"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/RotateKV/fig3.png",
      "keywords": [
        "09-Quantization",
        "19-Quantization (KV Cache)"
      ],
      "code_url": null,
      "note_url": "notes/2025/RotateKV/note/",
      "prototxt_path": "meta/2025/RotateKV.prototxt",
      "update_time": 0
    },
    {
      "id": "RPTQ",
      "title": "RPTQ: Reorder-based Post-training Quantization for Large Language Models",
      "abbr": "RPTQ",
      "url": "https://arxiv.org/pdf/2304.01089.pdf",
      "authors": [
        "Zhihang Yuan",
        "Bingzhe Wu"
      ],
      "institutions": [
        "Houmo AI",
        "Tencent AI Lab"
      ],
      "year": 2023,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "09-Quantization"
      ],
      "code_url": "https://github.com/hahnyuan/RPTQ4LLM",
      "note_url": null,
      "prototxt_path": "meta/2023/RPTQ.prototxt",
      "update_time": 0
    },
    {
      "id": "SAS",
      "title": "SAS: Structured Activation Spasification",
      "abbr": "SAS",
      "url": "https://openreview.net/forum?id=vZfi5to2Xl",
      "authors": [
        "Yusuke Sekikawa, Shingo Yashima"
      ],
      "institutions": [
        "DENSO IT Lab"
      ],
      "year": 2024,
      "venue": "ICLR",
      "cover": "notes/2024/SAS/sas.jpg",
      "keywords": [
        "01-Sparse/Pruning",
        "03-Sparsity (Activation)"
      ],
      "code_url": "https://github.com/DensoITLab/sas_",
      "note_url": "notes/2024/SAS/note/",
      "prototxt_path": "meta/2024/SAS.prototxt",
      "update_time": 0
    },
    {
      "id": "SCBench",
      "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
      "abbr": "SCBench",
      "url": "http://arxiv.org/abs/2412.10319v2",
      "authors": [
        "Yucheng Li",
        "Huiqiang Jiang",
        "Qianhui Wu",
        "Xufang Luo",
        "Surin Ahn",
        "Chengruidong Zhang",
        "Amir H. Abdi",
        "Dongsheng Li",
        "Jianfeng Gao",
        "Yuqing Yang",
        "Lili Qiu"
      ],
      "institutions": [
        "Microsoft",
        "University of Surrey"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "46-Tool"
      ],
      "code_url": "https://github.com/microsoft/MInference/tree/main/scbench",
      "note_url": "notes/2024/SCBench/note/",
      "prototxt_path": "meta/2024/SCBench.prototxt",
      "update_time": 0
    },
    {
      "id": "SEA",
      "title": "SEA: Sparse Linear Attention with Estimated Attention Mask",
      "abbr": "SEA",
      "url": "http://arxiv.org/abs/2310.01777v2",
      "authors": [
        "Heejun Lee",
        "Jina Kim",
        "Jeffrey Willette",
        "Sung Ju Hwang"
      ],
      "institutions": [
        "Korea Advanced Institute of Science and Technology",
        "DeepAuto.ai"
      ],
      "year": 2024,
      "venue": "ICLR",
      "cover": "notes/2024/SEA/figure1.png",
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)"
      ],
      "code_url": "https://github.com/gmlwns2000/sea-attention",
      "note_url": "notes/2024/SEA/note/",
      "prototxt_path": "meta/2024/SEA.prototxt",
      "update_time": 0
    },
    {
      "id": "SEAP",
      "title": "SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models",
      "abbr": "SEAP",
      "url": "http://arxiv.org/abs/2503.07605v1",
      "authors": [
        "Xun Liang",
        "Hanyu Wang",
        "Huayi Lai",
        "Simin Niu",
        "Shichao Song",
        "Jiawei Yang",
        "Jihao Zhao",
        "Feiyu Xiong",
        "Bo Tang",
        "Zhiyu Li"
      ],
      "institutions": [
        "Renmin University of China",
        "Institute for Advanced Algorithms Research"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/SEAP/fig2.png",
      "keywords": [
        "01-Sparse/Pruning",
        "05-Sparsity (Structured)"
      ],
      "code_url": "https://github.com/IAAR-Shanghai/SEAP/tree/main",
      "note_url": "notes/2025/SEAP/note/",
      "prototxt_path": "meta/2025/SEAP.prototxt",
      "update_time": 0
    },
    {
      "id": "SeerAttention-R",
      "title": "SeerAttention-R: Sparse Attention Adaptation for Long Reasoning",
      "abbr": "SeerAttention-R",
      "url": "http://arxiv.org/abs/2506.08889v1",
      "authors": [
        "Yizhao Gao",
        "Shuming Guo",
        "Shijie Cao",
        "Yuqing Xia",
        "Yu Cheng",
        "Lei Wang",
        "Lingxiao Ma",
        "Yutao Sun",
        "Tianzhu Ye",
        "Li Dong",
        "Hayden Kwok-Hay So",
        "Yu Hua",
        "Ting Cao",
        "Fan Yang",
        "Mao Yang"
      ],
      "institutions": [
        "Microsoft Research",
        "The University of Hong Kong",
        "Huazhong University of Science and Technology",
        "Peking University",
        "Tsinghua University"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/SeerAttention-R/fig1.png",
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)"
      ],
      "code_url": "https://github.com/microsoft/SeerAttention",
      "note_url": "notes/2025/SeerAttention-R/note/",
      "prototxt_path": "meta/2025/SeerAttention-R.prototxt",
      "update_time": 0
    },
    {
      "id": "Seesaw",
      "title": "Seesaw: High-throughput LLM Inference via Model Re-sharding",
      "abbr": "Seesaw",
      "url": "http://arxiv.org/abs/2503.06433v1",
      "authors": [
        "Qidong Su",
        "Wei Zhao",
        "Xin Li",
        "Muralidhar Andoorveedu",
        "Chenhao Jiang",
        "Zhanda Zhu",
        "Kevin Song",
        "Christina Giannoula",
        "Gennady Pekhimenko"
      ],
      "institutions": [
        "University of Toronto",
        "Vector Institute",
        "CentML",
        "Stanford University"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/Seesaw/fig8.png",
      "keywords": [
        "29-Comm-Comp Overlap"
      ],
      "code_url": null,
      "note_url": "notes/2025/Seesaw/note/",
      "prototxt_path": "meta/2025/Seesaw.prototxt",
      "update_time": 0
    },
    {
      "id": "CloudMatrix384",
      "title": "Serving Large Language Models on HuaweiMatrix384",
      "abbr": "CloudMatrix384",
      "url": "http://arxiv.org/abs/2506.12708v3",
      "authors": [
        "Pengfei Zuo",
        "Huimin Lin",
        "Junbo Deng",
        "Nan Zou",
        "Xingkun Yang",
        "Yingyu Diao",
        "Weifeng Gao",
        "Ke Xu",
        "Zhangyu Chen",
        "Shirui Lu",
        "Zhao Qiu",
        "Peiyang Li",
        "Xianyu Chang",
        "Zhengzhong Yu",
        "Fangzheng Miao",
        "Jia Zheng",
        "Ying Li",
        "Yuan Feng",
        "Bei Wang",
        "Zaijian Zong",
        "Mosong Zhou",
        "Wenli Zhou",
        "Houjiang Chen",
        "Xingyu Liao",
        "Yipeng Li",
        "Wenxiao Zhang",
        "Ping Zhu",
        "Yinggang Wang",
        "Chuanjie Xiao",
        "Depeng Liang",
        "Dong Cao",
        "Juncheng Liu",
        "Yongqiang Yang",
        "Xiaolong Bai",
        "Yi Li",
        "Huaguo Xie",
        "Huatao Wu",
        "Zhibin Yu",
        "Lv Chen",
        "Hu Liu",
        "Yujun Ding",
        "Haipei Zhu",
        "Jing Xia",
        "Yi Xiong",
        "Zhou Yu",
        "Heng Liao"
      ],
      "institutions": [
        "Huawei",
        "SiliconFlow"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "40-LLM Deployment"
      ],
      "code_url": null,
      "note_url": "notes/2025/CloudMatrix384/note/",
      "prototxt_path": "meta/2025/CloudMatrix384.prototxt",
      "update_time": 0
    },
    {
      "id": "vLLM",
      "title": "SGLang",
      "abbr": "SGLang-Code",
      "url": "",
      "authors": [
        "Name1",
        "Name2"
      ],
      "institutions": [
        "inst1",
        "inst2"
      ],
      "year": 2023,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "40-LLM Deployment",
        "46-Tool"
      ],
      "code_url": "https://github.com/sgl-project/sglang",
      "note_url": "notes/2023/vLLM/note/",
      "prototxt_path": "meta/2023/vLLM.prototxt",
      "update_time": 0
    },
    {
      "id": "SGLang",
      "title": "SGLang: Efficient Execution of Structured Language Model Programs",
      "abbr": "SGLang",
      "url": "http://arxiv.org/abs/2312.07104v2",
      "authors": [
        "Lianmin Zheng",
        "Liangsheng Yin",
        "Zhiqiang Xie",
        "Chuyue Sun",
        "Jeff Huang",
        "Cody Hao Yu",
        "Shiyi Cao",
        "Christos Kozyrakis",
        "Ion Stoica",
        "Joseph E. Gonzalez",
        "Clark Barrett",
        "Ying Sheng"
      ],
      "institutions": [
        "Stanford University",
        "UC Berkeley",
        "Shanghai Jiao Tong University"
      ],
      "year": 2024,
      "venue": "NeurIPS",
      "cover": "notes/2024/SGLang/fig9.png",
      "keywords": [
        "21-KV Cache Management",
        "40-LLM Deployment"
      ],
      "code_url": "https://github.com/sgl-project/sglang",
      "note_url": "notes/2024/SGLang/note/",
      "prototxt_path": "meta/2024/SGLang.prototxt",
      "update_time": 0
    },
    {
      "id": "ShadowLLM",
      "title": "ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models",
      "abbr": "ShadowLLM",
      "url": "http://arxiv.org/abs/2406.16635v1",
      "authors": [
        "Yash Akhauri",
        "Ahmed F AbouElhamayed",
        "Jordan Dotzel",
        "Zhiru Zhang",
        "Alexander M Rush",
        "Safeen Huda",
        "Mohamed S Abdelfattah"
      ],
      "institutions": [
        "Cornell University",
        "Google"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/ShadowLLM/shadowLLM.png",
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": "https://github.com/abdelfattah-lab/shadow_llm",
      "note_url": "notes/2024/ShadowLLM/note/",
      "prototxt_path": "meta/2024/ShadowLLM.prototxt",
      "update_time": 0
    },
    {
      "id": "LLM_shearing",
      "title": "Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning",
      "abbr": "LLM-shearing",
      "url": "https://xiamengzhou.github.io/sheared-llama/",
      "authors": [
        "Mengzhou Xia",
        "Tianyu Gao"
      ],
      "institutions": [
        "Princeton University"
      ],
      "year": 2023,
      "venue": "arXiv",
      "cover": "notes/2023/LLM_shearing/cover.jpg",
      "keywords": [
        "01-Sparse/Pruning",
        "05-Sparsity (Structured)"
      ],
      "code_url": "https://github.com/princeton-nlp/LLM-Shearing",
      "note_url": "notes/2023/LLM_shearing/note/",
      "prototxt_path": "meta/2023/LLM_shearing.prototxt",
      "update_time": 0
    },
    {
      "id": "SLA",
      "title": "SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention",
      "abbr": "SLA",
      "url": "http://arxiv.org/abs/2509.24006v1",
      "authors": [
        "Jintao Zhang",
        "Haoxu Wang",
        "Kai Jiang",
        "Shuo Yang",
        "Kaiwen Zheng",
        "Haocheng Xi",
        "Ziteng Wang",
        "Hongzhou Zhu",
        "Min Zhao",
        "Ion Stoica",
        "Joseph E. Gonzalez",
        "Jun Zhu",
        "Jianfei Chen"
      ],
      "institutions": [
        "Tsinghua University",
        "UC Berkeley"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/SLA/fig4.png",
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)",
        "42-Network Structure Design"
      ],
      "code_url": "https://github.com/thu-ml/SLA",
      "note_url": "notes/2025/SLA/note/",
      "prototxt_path": "meta/2025/SLA.prototxt",
      "update_time": 0
    },
    {
      "id": "SliceGPT",
      "title": "SliceGPT: Compress Large Language Models by Deleting Rows and Columns",
      "abbr": "SliceGPT",
      "url": "http://arxiv.org/abs/2401.15024v2",
      "authors": [
        "Saleh Ashkboos",
        "Maximilian L. Croci",
        "Marcelo Gennari do Nascimento",
        "Torsten Hoefler",
        "James Hensman"
      ],
      "institutions": [
        "Microsoft",
        "ETH Zurich"
      ],
      "year": 2024,
      "venue": "ICLR",
      "cover": "notes/2024/SliceGPT/sliceGPT.jpg",
      "keywords": [
        "01-Sparse/Pruning",
        "05-Sparsity (Structured)"
      ],
      "code_url": "https://github.com/microsoft/TransformerCompression",
      "note_url": "notes/2024/SliceGPT/note/",
      "prototxt_path": "meta/2024/SliceGPT.prototxt",
      "update_time": 0
    },
    {
      "id": "SlimGPT",
      "title": "SlimGPT: Layer-wise Structured Pruning for Large Language Models",
      "abbr": "SlimGPT",
      "url": "http://arxiv.org/abs/2412.18110v1",
      "authors": [
        "Gui Ling",
        "Ziyang Wang",
        "Yuliang Yan",
        "Qingwen Liu"
      ],
      "institutions": [
        "Alibaba Group"
      ],
      "year": 2024,
      "venue": "NeurIPS",
      "cover": "notes/2024/SlimGPT/fig1.png",
      "keywords": [
        "01-Sparse/Pruning",
        "05-Sparsity (Structured)"
      ],
      "code_url": null,
      "note_url": "notes/2024/SlimGPT/note/",
      "prototxt_path": "meta/2024/SlimGPT.prototxt",
      "update_time": 0
    },
    {
      "id": "SlimLLM",
      "title": "SlimLLM: Accurate Structured Pruning for Large Language Models",
      "abbr": "SlimLLM",
      "url": "http://arxiv.org/abs/2505.22689v1",
      "authors": [
        "Jialong Guo",
        "Xinghao Chen",
        "Yehui Tang",
        "Yunhe Wang"
      ],
      "institutions": [
        "Huawei"
      ],
      "year": 2025,
      "venue": "ICML",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning",
        "05-Sparsity (Structured)"
      ],
      "code_url": null,
      "note_url": "notes/2025/SlimLLM/note/",
      "prototxt_path": "meta/2025/SlimLLM.prototxt",
      "update_time": 0
    },
    {
      "id": "SparkTransformer",
      "title": "Spark Transformer: Reactivating Sparsity in FFN and Attention",
      "abbr": "SparkTransformer",
      "url": "http://arxiv.org/abs/2506.06644v2",
      "authors": [
        "Chong You",
        "Kan Wu",
        "Zhipeng Jia",
        "Lin Chen",
        "Srinadh Bhojanapalli",
        "Jiaxian Guo",
        "Utku Evci",
        "Jan Wassenberg",
        "Praneeth Netrapalli",
        "Jeremiah J. Willcock",
        "Suvinay Subramanian",
        "Felix Chern",
        "Alek Andreev",
        "Shreya Pathak",
        "Felix Yu",
        "Prateek Jain",
        "David E. Culler",
        "Henry M. Levy",
        "Sanjiv Kumar"
      ],
      "institutions": [
        "Google",
        "xAI",
        "Anthropic"
      ],
      "year": 2025,
      "venue": "NeurIPS",
      "cover": "notes/2025/SparkTransformer/fig2.png",
      "keywords": [
        "01-Sparse/Pruning",
        "03-Sparsity (Activation)"
      ],
      "code_url": null,
      "note_url": "notes/2025/SparkTransformer/note/",
      "prototxt_path": "meta/2025/SparkTransformer.prototxt",
      "update_time": 0
    },
    {
      "id": "Sprint",
      "title": "Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation",
      "abbr": "Sprint",
      "url": "https://arxiv.org/abs/2209.00606",
      "authors": [
        "Amir Yazdanbakhsh",
        "Mingu Kang"
      ],
      "institutions": [
        "Google"
      ],
      "year": 2022,
      "venue": "MICRO",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": null,
      "note_url": null,
      "prototxt_path": "meta/2022/Sprint.prototxt",
      "update_time": 0
    },
    {
      "id": "SquareHead",
      "title": "Sparse Fine-tuning for Inference Acceleration of Large Language Models",
      "abbr": "SquareHead",
      "url": "https://arxiv.org/pdf/2310.06927.pdf",
      "authors": [
        "Eldar Kurtic",
        "Denis Kuznedelev",
        "Elias Frantar",
        "Michael Goin",
        "Dan Alistarh"
      ],
      "institutions": [
        "IST Austria",
        "Neural Magic"
      ],
      "year": 2023,
      "venue": "arXiv",
      "cover": "notes/2023/SquareHead/cover.png",
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": "https://github.com/IST-DASLab/SparseFinetuning",
      "note_url": null,
      "prototxt_path": "meta/2023/SquareHead.prototxt",
      "update_time": 0
    },
    {
      "id": "SIFT",
      "title": "Sparse is Enough in Fine-tuning Pre-trained Large Language Models",
      "abbr": "SIFT",
      "url": "http://arxiv.org/abs/2312.11875v3",
      "authors": [
        "Weixi Song",
        "Zuchao Li",
        "Lefei Zhang",
        "Hai Zhao",
        "Bo Du"
      ],
      "institutions": [
        "Wuhan University",
        "Shanghai Jiao Tong University"
      ],
      "year": 2024,
      "venue": "ICML",
      "cover": null,
      "keywords": [
        "45-Efficient Training"
      ],
      "code_url": "https://github.com/song-wx/SIFT",
      "note_url": "notes/2024/SIFT/note/",
      "prototxt_path": "meta/2024/SIFT.prototxt",
      "update_time": 0
    },
    {
      "id": "Sparse-IFT",
      "title": "Sparse Iso-FLOP Transformations for Maximizing Training Efficiency",
      "abbr": "Sparse-IFT",
      "url": "https://arxiv.org/abs/2303.11525",
      "authors": [
        "Shreyas Saxena",
        "Vithursan Thangarasa",
        "Sean Lie"
      ],
      "institutions": [
        "Cerebras Systems"
      ],
      "year": 2023,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": "https://github.com/CerebrasResearch/Sparse-IFT",
      "note_url": null,
      "prototxt_path": "meta/2023/Sparse-IFT.prototxt",
      "update_time": 0
    },
    {
      "id": "sms",
      "title": "Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging",
      "abbr": "SMS",
      "url": "https://arxiv.org/abs/2306.16788",
      "authors": [
        "Max Zimmer",
        "Sebastian Pokutta"
      ],
      "institutions": [],
      "year": 2023,
      "venue": "arXiv",
      "cover": "notes/2023/sms/sms.jpg",
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": "https://github.com/ZIB-IOL/SMS",
      "note_url": null,
      "prototxt_path": "meta/2023/sms.prototxt",
      "update_time": 0
    },
    {
      "id": "XZBX1Z9G",
      "title": "Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm",
      "abbr": "",
      "url": "https://aclanthology.org/2022.acl-long.16/",
      "authors": [
        "Shaoyi Huang",
        "Dongkuan Xu",
        "Caiwen Ding"
      ],
      "institutions": [
        "University of Connecticut"
      ],
      "year": 2022,
      "venue": "ACL",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": "https://github.com/shaoyiHusky/SparseProgressiveDistillation",
      "note_url": null,
      "prototxt_path": "meta/2022/XZBX1Z9G.prototxt",
      "update_time": 0
    },
    {
      "id": "SVG2",
      "title": "Sparse VideoGen2: Accelerate Video Generation with Sparse Attention via Semantic-Aware Permutation",
      "abbr": "SVG2",
      "url": "http://arxiv.org/abs/2505.18875v3",
      "authors": [
        "Shuo Yang",
        "Haocheng Xi",
        "Yilong Zhao",
        "Muyang Li",
        "Jintao Zhang",
        "Han Cai",
        "Yujun Lin",
        "Xiuyu Li",
        "Chenfeng Xu",
        "Kelly Peng",
        "Jianfei Chen",
        "Song Han",
        "Kurt Keutzer",
        "Ion Stoica"
      ],
      "institutions": [
        "University of California, Berkeley",
        "Massachusetts Institute of Technology",
        "Stanford University"
      ],
      "year": 2025,
      "venue": "NeurIPS",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)"
      ],
      "code_url": "https://github.com/svg-project/Sparse-VideoGen",
      "note_url": "notes/2025/SVG2/note/",
      "prototxt_path": "meta/2025/SVG2.prototxt",
      "update_time": 0
    },
    {
      "id": "SVG",
      "title": "Sparse VideoGen: Accelerating Video Diffusion Transformers with Spatial-Temporal Sparsity",
      "abbr": "SVG",
      "url": "http://arxiv.org/abs/2502.01776v2",
      "authors": [
        "Haocheng Xi",
        "Shuo Yang",
        "Yilong Zhao",
        "Chenfeng Xu",
        "Muyang Li",
        "Xiuyu Li",
        "Yujun Lin",
        "Han Cai",
        "Jintao Zhang",
        "Dacheng Li",
        "Jianfei Chen",
        "Ion Stoica",
        "Kurt Keutzer",
        "Song Han"
      ],
      "institutions": [
        "University of California, Berkeley",
        "Massachusetts Institute of Technology",
        "NVIDIA",
        "Tsinghua University"
      ],
      "year": 2025,
      "venue": "ICML",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)"
      ],
      "code_url": "https://github.com/svg-project/Sparse-VideoGen",
      "note_url": "notes/2025/SVG/note/",
      "prototxt_path": "meta/2025/SVG.prototxt",
      "update_time": 0
    },
    {
      "id": "Sparse-IFT",
      "title": "Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency",
      "abbr": "Sparse-IFT",
      "url": "http://arxiv.org/abs/2303.11525v3",
      "authors": [
        "Vithursan Thangarasa",
        "Shreyas Saxena",
        "Abhay Gupta",
        "Sean Lie"
      ],
      "institutions": [
        "Cerebras"
      ],
      "year": 2024,
      "venue": "ICML",
      "cover": "notes/2024/Sparse-IFT/sparseIFT.png",
      "keywords": [
        "45-Efficient Training",
        "01-Sparse/Pruning"
      ],
      "code_url": "https://github.com/CerebrasResearch/Sparse-IFT",
      "note_url": "notes/2024/Sparse-IFT/note/",
      "prototxt_path": "meta/2024/Sparse-IFT.prototxt",
      "update_time": 0
    },
    {
      "id": "sparsegpt",
      "title": "SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot.",
      "abbr": "SparseGPT",
      "url": "https://arxiv.org/pdf/2301.00774.pdf",
      "authors": [
        "Elias Frantar",
        "Dan Alistarh"
      ],
      "institutions": [
        "IST Austria",
        "Neural Magic"
      ],
      "year": 2023,
      "venue": "ICML",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning",
        "04-Sparsity (Weight)"
      ],
      "code_url": "https://github.com/IST-DASLab/sparsegpt",
      "note_url": null,
      "prototxt_path": "meta/2023/sparsegpt.prototxt",
      "update_time": 0
    },
    {
      "id": "SparseInfer",
      "title": "SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference",
      "abbr": "SparseInfer",
      "url": "http://arxiv.org/abs/2411.12692v1",
      "authors": [
        "Jiho Shin",
        "Hoeseok Yang",
        "Youngmin Yi"
      ],
      "institutions": [
        "University of Seoul",
        "Santa Clara University",
        "Sogang University"
      ],
      "year": 2024,
      "venue": "DATE",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning",
        "03-Sparsity (Activation)"
      ],
      "code_url": null,
      "note_url": "notes/2024/SparseInfer/note/",
      "prototxt_path": "meta/2024/SparseInfer.prototxt",
      "update_time": 0
    },
    {
      "id": "SparseLLM",
      "title": "SparseLLM: Towards Global Pruning for Pre-trained Language Models",
      "abbr": "SparseLLM",
      "url": "http://arxiv.org/abs/2402.17946v3",
      "authors": [
        "Guangji Bai",
        "Yijiang Li",
        "Chen Ling",
        "Kibaek Kim",
        "Liang Zhao"
      ],
      "institutions": [
        "Emory University"
      ],
      "year": 2024,
      "venue": "NeurIPS",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning",
        "04-Sparsity (Weight)"
      ],
      "code_url": "https://github.com/BaiTheBest/SparseLLM",
      "note_url": "notes/2024/SparseLLM/note/",
      "prototxt_path": "meta/2024/SparseLLM.prototxt",
      "update_time": 0
    },
    {
      "id": "SparseServe",
      "title": "SparseServe: Unlocking Parallelism for Dynamic Sparse Attention in Long-Context LLM Serving",
      "abbr": "SparseServe",
      "url": "http://arxiv.org/abs/2509.24626v1",
      "authors": [
        "Qihui Zhou",
        "Peiqi Yin",
        "Pengfei Zuo",
        "James Cheng"
      ],
      "institutions": [
        "The Chinese University of Hong Kong",
        "Huawei"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/SparseServe/cover.png",
      "keywords": [
        "02-Sparsity (Attention)",
        "40-LLM Deployment",
        "21-KV Cache Management"
      ],
      "code_url": null,
      "note_url": "notes/2025/SparseServe/note/",
      "prototxt_path": "meta/2025/SparseServe.prototxt",
      "update_time": 0
    },
    {
      "id": "SparseViT",
      "title": "SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer",
      "abbr": "SparseViT",
      "url": "https://arxiv.org/abs/2303.17605",
      "authors": [
        "Xuanyao Chen",
        "Song Han"
      ],
      "institutions": [
        "Massachusetts Institute of Technology"
      ],
      "year": 2023,
      "venue": "CVPR",
      "cover": "notes/2023/SparseViT/sparsevit.jpg",
      "keywords": [
        "01-Sparse/Pruning",
        "03-Sparsity (Activation)"
      ],
      "code_url": "https://github.com/mit-han-lab/sparsevit",
      "note_url": null,
      "prototxt_path": "meta/2023/SparseViT.prototxt",
      "update_time": 0
    },
    {
      "id": "SparsingLaw",
      "title": "Sparsing Law: Towards Large Language Models with Greater Activation Sparsity",
      "abbr": "SparsingLaw",
      "url": "https://openreview.net/forum?id=SBUc5wirM8",
      "authors": [
        "Yuqi Luo",
        "Chenyang Song",
        "Xu Han",
        "Yingfa Chen",
        "Chaojun Xiao",
        "Zhiyuan Liu",
        "Maosong Sun"
      ],
      "institutions": [
        "Tsinghua University"
      ],
      "year": 2025,
      "venue": "ICML",
      "cover": "notes/2025/SparsingLaw/fig4.png",
      "keywords": [
        "01-Sparse/Pruning",
        "03-Sparsity (Activation)"
      ],
      "code_url": "https://github.com/thunlp/SparsingLaw",
      "note_url": "notes/2025/SparsingLaw/note/",
      "prototxt_path": "meta/2025/SparsingLaw.prototxt",
      "update_time": 0
    },
    {
      "id": "ITZS3TU3",
      "title": "Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks",
      "abbr": "",
      "url": "https://arxiv.org/abs/2102.00554",
      "authors": [
        "Torsten Hoefler",
        "Dan Alistarh"
      ],
      "institutions": [
        "IST Austria"
      ],
      "year": 2021,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "41-Survey",
        "01-Sparse/Pruning"
      ],
      "code_url": null,
      "note_url": null,
      "prototxt_path": "meta/2021/ITZS3TU3.prototxt",
      "update_time": 0
    },
    {
      "id": "spdf",
      "title": "SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models",
      "abbr": "SPDF",
      "url": "https://arxiv.org/abs/2303.10464",
      "authors": [
        "Vithursan Thangarasa",
        "Shreyas Saxena"
      ],
      "institutions": [
        "Cerebras Systems"
      ],
      "year": 2023,
      "venue": "UAI",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": null,
      "note_url": null,
      "prototxt_path": "meta/2023/spdf.prototxt",
      "update_time": 0
    },
    {
      "id": "spdy",
      "title": "SPDY: Accurate Pruning with Speedup Guarantees",
      "abbr": "SPDY",
      "url": "https://arxiv.org/abs/2201.13096",
      "authors": [
        "Elias Frantar",
        "Dan Alistarh"
      ],
      "institutions": [
        "IST Austria",
        "Neural Magic"
      ],
      "year": 2022,
      "venue": "ICML",
      "cover": "notes/2022/spdy/cover.jpg",
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": "https://github.com/IST-DASLab/spdy",
      "note_url": null,
      "prototxt_path": "meta/2022/spdy.prototxt",
      "update_time": 0
    },
    {
      "id": "Awesome-Efficient-Arch",
      "title": "Speed Always Wins: A Survey on Efficient Architectures for Large Language Models",
      "abbr": "Awesome-Efficient-Arch",
      "url": "http://arxiv.org/abs/2508.09834v1",
      "authors": [
        "Weigao Sun",
        "Jiaxi Hu",
        "Yucheng Zhou",
        "Jusen Du",
        "Disen Lan",
        "Kexin Wang",
        "Tong Zhu",
        "Xiaoye Qu",
        "Yu Zhang",
        "Xiaoyu Mo",
        "Daizong Liu",
        "Yuxuan Liang",
        "Wenliang Chen",
        "Guoqi Li",
        "Yu Cheng"
      ],
      "institutions": [
        "Shanghai AI Laboratory",
        "Hong Kong University of Science and Technology",
        "University of Macau",
        "Institute of Automation, Chinese Academy of Sciences",
        "Soochow University",
        "KTH Royal Institute of Technology",
        "Peking University",
        "The Chinese University of Hong Kong"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/Awesome-Efficient-Arch/fig1.png",
      "keywords": [
        "41-Survey",
        "02-Sparsity (Attention)",
        "42-Network Structure Design",
        "01-Sparse/Pruning"
      ],
      "code_url": "https://github.com/weigao266/Awesome-Efficient-Arch",
      "note_url": "notes/2025/Awesome-Efficient-Arch/note/",
      "prototxt_path": "meta/2025/Awesome-Efficient-Arch.prototxt",
      "update_time": 0
    },
    {
      "id": "SpInfer",
      "title": "SpInfer: Leveraging Low-Level Sparsity for Efficient Large Language Model Inference on GPUs",
      "abbr": "SpInfer",
      "url": "https://dl.acm.org/doi/10.1145/3689031.3717481",
      "authors": [
        "Ruibo Fan, Xiangrui Yu, Peijie Dong, Zeyu Li, Gu Gong, Qiang Wang, Wei Wang, Xiaowen Chu"
      ],
      "institutions": [
        "Hong Kong University of Science and Technology",
        "Harbin Institute of Technology"
      ],
      "year": 2025,
      "venue": "EuroSys",
      "cover": "notes/2025/SpInfer/fig5.png",
      "keywords": [
        "01-Sparse/Pruning",
        "04-Sparsity (Weight)"
      ],
      "code_url": "https://github.com/xxyux/SpInfer",
      "note_url": "notes/2025/SpInfer/note/",
      "prototxt_path": "meta/2025/SpInfer.prototxt",
      "update_time": 0
    },
    {
      "id": "Splitwise",
      "title": "Splitwise: Efficient generative LLM inference using phase splitting",
      "abbr": "Splitwise",
      "url": "http://arxiv.org/abs/2311.18677v2",
      "authors": [
        "Pratyush Patel",
        "Esha Choukse",
        "Chaojie Zhang",
        "Aashaka Shah",
        "igo Goiri",
        "Saeed Maleki",
        "Ricardo Bianchini"
      ],
      "institutions": [
        "University of Washington",
        "Microsoft"
      ],
      "year": 2024,
      "venue": "ISCA",
      "cover": "notes/2024/Splitwise/fig10.png",
      "keywords": [
        "40-LLM Deployment"
      ],
      "code_url": "https://github.com/Mutinifni/splitwise-sim",
      "note_url": "notes/2024/Splitwise/note/",
      "prototxt_path": "meta/2024/Splitwise.prototxt",
      "update_time": 0
    },
    {
      "id": "SPP",
      "title": "SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models",
      "abbr": "SPP",
      "url": "http://arxiv.org/abs/2405.16057v1",
      "authors": [
        "Xudong Lu",
        "Aojun Zhou",
        "Yuhui Xu",
        "Renrui Zhang",
        "Peng Gao",
        "Hongsheng Li"
      ],
      "institutions": [
        "Multimedia Laboratory (MMLab)",
        "The Chinese University of Hong Kong",
        "Salesforce AI Research",
        "Shanghai Artificial Intelligence Laboratorys",
        "CPII under InnoHK"
      ],
      "year": 2024,
      "venue": "ICML",
      "cover": "notes/2024/SPP/spp.png",
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": "https://github.com/Lucky-Lance/SPP",
      "note_url": "notes/2024/SPP/note/",
      "prototxt_path": "meta/2024/SPP.prototxt",
      "update_time": 0
    },
    {
      "id": "spqr",
      "title": "SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression",
      "abbr": "SpQR",
      "url": "https://arxiv.org/pdf/2306.03078.pdf",
      "authors": [
        "Tim Dettmers",
        "Dan Alistarh"
      ],
      "institutions": [
        "IST Austria",
        "Neural Magic"
      ],
      "year": 2023,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "09-Quantization"
      ],
      "code_url": "https://github.com/Vahe1994/SpQR",
      "note_url": null,
      "prototxt_path": "meta/2023/spqr.prototxt",
      "update_time": 0
    },
    {
      "id": "SQ-format",
      "title": "SQ-format: A Unified Sparse-Quantized Hardware-friendly Data Format for LLMs",
      "abbr": "SQ-format",
      "url": "http://arxiv.org/abs/2512.05409v1",
      "authors": [
        "Ruixuan Huang",
        "Hao Zeng",
        "Hantao Huang",
        "Jinyuan Shi",
        "Minghui Yu",
        "Ian En-Hsu Yen",
        "Shuai Wang"
      ],
      "institutions": [
        "Hong Kong University of Science and Technology",
        "Moffett AI",
        "ByteDance Seed"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/SQ-format/cover.png",
      "keywords": [
        "09-Quantization",
        "04-Sparsity (Weight)"
      ],
      "code_url": null,
      "note_url": "notes/2025/SQ-format/note/",
      "prototxt_path": "meta/2025/SQ-format.prototxt",
      "update_time": 0
    },
    {
      "id": "SSA",
      "title": "SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space",
      "abbr": "SSA",
      "url": "http://arxiv.org/abs/2511.20102v1",
      "authors": [
        "Zhenyi Shen",
        "Junru Lu",
        "Lin Gui",
        "Jiazheng Li",
        "Yulan He",
        "Di Yin",
        "Xing Sun"
      ],
      "institutions": [
        "Kings College London",
        "Tencent Youtu Lab"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/SSA/fig2.png",
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)"
      ],
      "code_url": null,
      "note_url": "notes/2025/SSA/note/",
      "prototxt_path": "meta/2025/SSA.prototxt",
      "update_time": 0
    },
    {
      "id": "Step-3",
      "title": "Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding",
      "abbr": "Step-3",
      "url": "http://arxiv.org/abs/2507.19427v1",
      "authors": [
        "Bin Wang",
        "Bojun Wang",
        "Changyi Wan",
        "Zili Wang",
        "Ziqi Wang",
        "Zixin Zhang",
        "Binxing Jiao",
        "Daxin Jiang",
        "Heung-Yeung Shum",
        "Xiangyu Zhang"
      ],
      "institutions": [
        "StepFun"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "42-Network Structure Design"
      ],
      "code_url": null,
      "note_url": "notes/2025/Step-3/note/",
      "prototxt_path": "meta/2025/Step-3.prototxt",
      "update_time": 0
    },
    {
      "id": "WDCO13S6",
      "title": "Structural Pruning of Large Language Models via Neural Architecture Search",
      "abbr": "",
      "url": "https://openreview.net/forum?id=SHlZcInS6C",
      "authors": [
        "Aaron Klein",
        "Jacek Golebiowski",
        "Xingchen Ma",
        "Valerio Perrone",
        "Cedric Archambeau"
      ],
      "institutions": [
        "AWS AI Labs"
      ],
      "year": 2023,
      "venue": "AutoML Workshop",
      "cover": "notes/2023/nas_pruning/nas_pruning.jpg",
      "keywords": [
        "01-Sparse/Pruning",
        "05-Sparsity (Structured)"
      ],
      "code_url": "https://github.com/awslabs/syne-tune/tree/nas/benchmarking/nursery/nas_llm",
      "note_url": null,
      "prototxt_path": "meta/2023/WDCO13S6.prototxt",
      "update_time": 0
    },
    {
      "id": "LoSparse",
      "title": "Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation",
      "abbr": "LoSparse",
      "url": "https://arxiv.org/abs/2306.11222",
      "authors": [
        "Yixiao Li",
        "Tuo Zhao"
      ],
      "institutions": [
        "Microsoft"
      ],
      "year": 2023,
      "venue": "ICML",
      "cover": "notes/2023/LoSparse/losparse.jpg",
      "keywords": [
        "01-Sparse/Pruning",
        "43-Low Rank Decomposition",
        "05-Sparsity (Structured)"
      ],
      "code_url": "https://github.com/yxli2123/LoSparse",
      "note_url": null,
      "prototxt_path": "meta/2023/LoSparse.prototxt",
      "update_time": 0
    },
    {
      "id": "simple",
      "title": "Structured Pruning for Efficient Generative Pre-trained Language Models",
      "abbr": "SIMPLE",
      "url": "https://aclanthology.org/2023.findings-acl.692.pdf",
      "authors": [
        "Chaofan Tao",
        "Ngai Wong"
      ],
      "institutions": [
        "The University of Hong Kong",
        "Huawei"
      ],
      "year": 2023,
      "venue": "ACL",
      "cover": "notes/2023/simple/cover.jpg",
      "keywords": [
        "01-Sparse/Pruning",
        "05-Sparsity (Structured)"
      ],
      "code_url": null,
      "note_url": null,
      "prototxt_path": "meta/2023/simple.prototxt",
      "update_time": 0
    },
    {
      "id": "T3",
      "title": "T3: Transparent Tracking & Triggering for Fine-grained Overlap of Compute & Collectives",
      "abbr": "T3",
      "url": "http://arxiv.org/abs/2401.16677v1",
      "authors": [
        "Suchita Pati",
        "Shaizeen Aga",
        "Mahzabeen Islam",
        "Nuwan Jayasena",
        "Matthew D. Sinclair"
      ],
      "institutions": [
        "Advanced Micro Devices",
        "University of Wisconsin-Madison"
      ],
      "year": 2024,
      "venue": "ASPLOS",
      "cover": "notes/2024/T3/fig1.png",
      "keywords": [
        "29-Comm-Comp Overlap"
      ],
      "code_url": null,
      "note_url": "notes/2024/T3/note/",
      "prototxt_path": "meta/2024/T3.prototxt",
      "update_time": 0
    },
    {
      "id": "TextPruner",
      "title": "TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models",
      "abbr": "TextPruner",
      "url": "https://arxiv.org/abs/2203.15996",
      "authors": [
        "Ziqing Yang",
        "Zhigang Chen"
      ],
      "institutions": [
        "Harbin Institute of Technology"
      ],
      "year": 2022,
      "venue": "ACL",
      "cover": "notes/2022/TextPruner/textpruner.jpg",
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": "https://github.com/airaria/TextPruner",
      "note_url": null,
      "prototxt_path": "meta/2022/TextPruner.prototxt",
      "update_time": 0
    },
    {
      "id": "EssentialSparsity",
      "title": "The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter",
      "abbr": "Essential Sparsity",
      "url": "https://arxiv.org/abs/2306.03805",
      "authors": [
        "Ajay Jaiswal",
        "Shiwei Liu",
        "Zhangyang Wang"
      ],
      "institutions": [
        "VITA Group",
        "University of Texas at Austin"
      ],
      "year": 2023,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": "https://github.com/VITA-Group/essential_sparsity/tree/main",
      "note_url": null,
      "prototxt_path": "meta/2023/EssentialSparsity.prototxt",
      "update_time": 0
    },
    {
      "id": "WMMGA0AR",
      "title": "The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers",
      "abbr": "",
      "url": "https://openreview.net/forum?id=TJ2nxciYCk-",
      "authors": [
        "Zonglin Li",
        "Chong You",
        "Sanjiv Kumar"
      ],
      "institutions": [
        "Google"
      ],
      "year": 2023,
      "venue": "ICLR",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning",
        "03-Sparsity (Activation)"
      ],
      "code_url": null,
      "note_url": null,
      "prototxt_path": "meta/2023/WMMGA0AR.prototxt",
      "update_time": 0
    },
    {
      "id": "oBERT",
      "title": "The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models",
      "abbr": "oBERT",
      "url": "https://arxiv.org/pdf/2203.07259.pdf",
      "authors": [
        "Eldar Kurtic",
        "Dan Alistarh"
      ],
      "institutions": [
        "IST Austria",
        "Neural Magic"
      ],
      "year": 2022,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning",
        "04-Sparsity (Weight)"
      ],
      "code_url": "https://github.com/neuralmagic/sparseml/blob/main/research/optimal_BERT_surgeon_oBERT/README.md",
      "note_url": null,
      "prototxt_path": "meta/2022/oBERT.prototxt",
      "update_time": 0
    },
    {
      "id": "sparse-frontier",
      "title": "The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs",
      "abbr": "sparse-frontier",
      "url": "http://arxiv.org/abs/2504.17768v1",
      "authors": [
        "Piotr Nawrot",
        "Robert Li",
        "Renjie Huang",
        "Sebastian Ruder",
        "Kelly Marchisio",
        "Edoardo M. Ponti"
      ],
      "institutions": [
        "University of Edinburgh",
        "Cohere",
        "Meta"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/sparse-frontier/tb1.png",
      "keywords": [
        "41-Survey",
        "02-Sparsity (Attention)"
      ],
      "code_url": "https://github.com/PiotrNawrot/sparse-frontier",
      "note_url": "notes/2025/sparse-frontier/note/",
      "prototxt_path": "meta/2025/sparse-frontier.prototxt",
      "update_time": 0
    },
    {
      "id": "TileLink",
      "title": "TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives",
      "abbr": "TileLink",
      "url": "http://arxiv.org/abs/2503.20313v3",
      "authors": [
        "Size Zheng",
        "Jin Fang",
        "Xuegui Zheng",
        "Qi Hou",
        "Wenlei Bao",
        "Ningxin Zheng",
        "Ziheng Jiang",
        "Dongyang Wang",
        "Jianxi Ye",
        "Haibin Lin",
        "Li-Wen Chang",
        "Xin Liu"
      ],
      "institutions": [
        "ByteDance Seed"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/TileLink/fig7.png",
      "keywords": [
        "29-Comm-Comp Overlap"
      ],
      "code_url": "https://github.com/ByteDance-Seed/Triton-distributed",
      "note_url": "notes/2025/TileLink/note/",
      "prototxt_path": "meta/2025/TileLink.prototxt",
      "update_time": 0
    },
    {
      "id": "TinyTrain",
      "title": "TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge",
      "abbr": "TinyTrain",
      "url": "http://arxiv.org/abs/2307.09988v2",
      "authors": [
        "Young D. Kwon",
        "Rui Li",
        "Stylianos I. Venieris",
        "Jagmohan Chauhan",
        "Nicholas D. Lane",
        "Cecilia Mascolo"
      ],
      "institutions": [
        "University of Cambridge, United Kingdom",
        "Samsung AI Center"
      ],
      "year": 2024,
      "venue": "ICML",
      "cover": null,
      "keywords": [
        "45-Efficient Training"
      ],
      "code_url": "https://github.com/theyoungkwon/TinyTrain",
      "note_url": "notes/2024/TinyTrain/note/",
      "prototxt_path": "meta/2024/TinyTrain.prototxt",
      "update_time": 0
    },
    {
      "id": "TokenWeave",
      "title": "TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference",
      "abbr": "TokenWeave",
      "url": "http://arxiv.org/abs/2505.11329v1",
      "authors": [
        "Raja Gond",
        "Nipun Kwatra",
        "Ramachandran Ramjee"
      ],
      "institutions": [
        "Microsoft Research India"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/TokenWeave/fig8.png",
      "keywords": [
        "29-Comm-Comp Overlap"
      ],
      "code_url": "https://github.com/microsoft/tokenweave",
      "note_url": "notes/2025/TokenWeave/note/",
      "prototxt_path": "meta/2025/TokenWeave.prototxt",
      "update_time": 0
    },
    {
      "id": "TorchAO",
      "title": "TorchAO: PyTorch-Native Training-to-Serving Model Optimization",
      "abbr": "TorchAO",
      "url": "http://arxiv.org/abs/2507.16099v1",
      "authors": [
        "Andrew Or",
        "Apurva Jain",
        "Daniel Vega-Myhre",
        "Jesse Cai",
        "Charles David Hernandez",
        "Zhenrui Zheng",
        "Driss Guessous",
        "Vasiliy Kuznetsov",
        "Christian Puhrsch",
        "Mark Saroufim",
        "Supriya Rao",
        "Thien Tran",
        "Aleksandar Samardi"
      ],
      "institutions": [
        "Meta Platforms Inc",
        "OpenTeams Inc"
      ],
      "year": 2025,
      "venue": "ICML Workshop",
      "cover": "notes/2025/TorchAO/fig1.png",
      "keywords": [
        "09-Quantization",
        "04-Sparsity (Weight)",
        "46-Tool"
      ],
      "code_url": "https://github.com/pytorch/ao/",
      "note_url": "notes/2025/TorchAO/note/",
      "prototxt_path": "meta/2025/TorchAO.prototxt",
      "update_time": 0
    },
    {
      "id": "TorchSparse",
      "title": "TorchSparse++: Efficient Point Cloud Engine",
      "abbr": "TorchSparse++",
      "url": "https://openaccess.thecvf.com/content/CVPR2023W/WAD/papers/Tang_TorchSparse_Efficient_Point_Cloud_Engine_CVPRW_2023_paper.pdf",
      "authors": [
        "Haotian Tang",
        "Song Han"
      ],
      "institutions": [
        "Massachusetts Institute of Technology"
      ],
      "year": 2023,
      "venue": "CVPR workshop",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": "https://github.com/mit-han-lab/torchsparse",
      "note_url": null,
      "prototxt_path": "meta/2023/TorchSparse.prototxt",
      "update_time": 0
    },
    {
      "id": "MXFP4Train",
      "title": "Training LLMs with MXFP4",
      "abbr": "MXFP4Train",
      "url": "http://arxiv.org/abs/2502.20586v2",
      "authors": [
        "Albert Tseng",
        "Tao Yu",
        "Youngsuk Park"
      ],
      "institutions": [
        "Cornell University",
        "AWS AI"
      ],
      "year": 2025,
      "venue": "AISTATS",
      "cover": "notes/2025/MXFP4Train/fig1.png",
      "keywords": [
        "09-Quantization",
        "45-Efficient Training"
      ],
      "code_url": "https://github.com/amazon-science/mxfp4-llm",
      "note_url": "notes/2025/MXFP4Train/note/",
      "prototxt_path": "meta/2025/MXFP4Train.prototxt",
      "update_time": 0
    },
    {
      "id": "23LQ9SVH",
      "title": "Training Transformers with 4-bit Integers",
      "abbr": "",
      "url": "https://arxiv.org/abs//2306.11987",
      "authors": [
        "Haocheng Xi, Changhao Li, Jianfei Chen, Jun Zhu"
      ],
      "institutions": [
        "Tsinghua University"
      ],
      "year": 2023,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "09-Quantization"
      ],
      "code_url": "https://github.com/xijiu9/Train_Transformers_with_INT4",
      "note_url": null,
      "prototxt_path": "meta/2023/23LQ9SVH.prototxt",
      "update_time": 0
    },
    {
      "id": "TEAL",
      "title": "Training-Free Activation Sparsity in Large Language Models",
      "abbr": "TEAL",
      "url": "http://arxiv.org/abs/2408.14690v1",
      "authors": [
        "James Liu",
        "Pragaash Ponnusamy",
        "Tianle Cai",
        "Han Guo",
        "Yoon Kim",
        "Ben Athiwaratkun"
      ],
      "institutions": [
        "Massachusetts Institute of Technology",
        "Together AI",
        "Princeton University"
      ],
      "year": 2025,
      "venue": "ICLR",
      "cover": "notes/2025/TEAL/fig1.png",
      "keywords": [
        "01-Sparse/Pruning",
        "03-Sparsity (Activation)"
      ],
      "code_url": "https://github.com/FasterDecoding/TEAL",
      "note_url": "notes/2025/TEAL/note/",
      "prototxt_path": "meta/2025/TEAL.prototxt",
      "update_time": 0
    },
    {
      "id": "TCA-Attention",
      "title": "Training-free Context-adaptive Attention for Efficient Long Context Modeling",
      "abbr": "TCA-Attention",
      "url": "http://arxiv.org/abs/2512.09238v1",
      "authors": [
        "Zeng You",
        "Yaofo Chen",
        "Shuhai Zhang",
        "Zhijie Qiu",
        "Tingyu Wu",
        "Yingjian Li",
        "Yaowei Wang",
        "Mingkui Tan"
      ],
      "institutions": [
        "Pengcheng Laboratory",
        "South China University",
        "Harbin Institute of Technology"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)"
      ],
      "code_url": null,
      "note_url": "notes/2025/TCA-Attention/note/",
      "prototxt_path": "meta/2025/TCA-Attention.prototxt",
      "update_time": 0
    },
    {
      "id": "TOA",
      "title": "Tree of Agents: Improving Long-Context Capabilities of Large Language Models through Multi-Perspective Reasoning",
      "abbr": "TOA",
      "url": "http://arxiv.org/abs/2509.06436v1",
      "authors": [
        "Song Yu",
        "Xiaofei Xu",
        "Ke Deng",
        "Li Li",
        "Lin Tian"
      ],
      "institutions": [
        "Southwest University",
        "Murdoch University",
        "Massachusetts Institute of Technology University",
        "University of Technology Sydney"
      ],
      "year": 2025,
      "venue": "EMNLP Findings",
      "cover": "notes/2025/TOA/fig1.png",
      "keywords": [
        "42-Network Structure Design"
      ],
      "code_url": "https://github.com/Aireduce952/Tree-of-Agents",
      "note_url": "notes/2025/TOA/note/",
      "prototxt_path": "meta/2025/TOA.prototxt",
      "update_time": 0
    },
    {
      "id": "Triton-distributed",
      "title": "Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler",
      "abbr": "Triton-distributed",
      "url": "http://arxiv.org/abs/2504.19442v3",
      "authors": [
        "Size Zheng",
        "Wenlei Bao",
        "Qi Hou",
        "Xuegui Zheng",
        "Jin Fang",
        "Chenhui Huang",
        "Tianqi Li",
        "Haojie Duanmu",
        "Renze Chen",
        "Ruifan Xu",
        "Yifan Guo",
        "Ningxin Zheng",
        "Ziheng Jiang",
        "Xinyi Di",
        "Dongyang Wang",
        "Jianxi Ye",
        "Haibin Lin",
        "Li-Wen Chang",
        "Liqiang Lu",
        "Yun Liang",
        "Jidong Zhai",
        "Xin Liu"
      ],
      "institutions": [
        "ByteDance Seed"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/Triton-distributed/fig2.png",
      "keywords": [
        "29-Comm-Comp Overlap"
      ],
      "code_url": "https://github.com/ByteDance-Seed/Triton-distributed",
      "note_url": "notes/2025/Triton-distributed/note/",
      "prototxt_path": "meta/2025/Triton-distributed.prototxt",
      "update_time": 0
    },
    {
      "id": "TurboSparse",
      "title": "Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters",
      "abbr": "Turbo Sparse",
      "url": "http://arxiv.org/abs/2406.05955v2",
      "authors": [
        "Yixin Song",
        "Haotong Xie",
        "Zhengyan Zhang",
        "Bo Wen",
        "Li Ma",
        "Zeyu Mi",
        "Haibo Chen"
      ],
      "institutions": [
        "Shanghai Jiao Tong University",
        "Tsinghua University",
        "Shanghai Artificial Intelligence Laboratory"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "01-Sparse/Pruning",
        "03-Sparsity (Activation)"
      ],
      "code_url": "https://huggingface.co/PowerInfer",
      "note_url": "notes/2024/TurboSparse/note/",
      "prototxt_path": "meta/2024/TurboSparse.prototxt",
      "update_time": 0
    },
    {
      "id": "Tr-the-Pruner",
      "title": "Tr-the-Pruner: Unlocking Accurate 50% Structural Pruning for LLMs via Global Sparsity Distribution Optimization",
      "abbr": "Tr-the-Pruner",
      "url": "http://arxiv.org/abs/2503.09657v2",
      "authors": [
        "Guanchen Li",
        "Yixing Xu",
        "Zeping Li",
        "Ji Liu",
        "Xuanwu Yin",
        "Dong Li",
        "Emad Barsoum"
      ],
      "institutions": [
        "Advanced Micro Devices"
      ],
      "year": 2025,
      "venue": "NeurIPS",
      "cover": "notes/2025/Tr-the-Pruner/fig2.png",
      "keywords": [
        "01-Sparse/Pruning",
        "05-Sparsity (Structured)"
      ],
      "code_url": null,
      "note_url": "notes/2025/Tr-the-Pruner/note/",
      "prototxt_path": "meta/2025/Tr-the-Pruner.prototxt",
      "update_time": 0
    },
    {
      "id": "MoE-MLA-RoPE",
      "title": "Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models",
      "abbr": "MoE-MLA-RoPE",
      "url": "http://arxiv.org/abs/2508.01261v1",
      "authors": [
        "Sushant Mehta",
        "Raj Dandekar",
        "Rajat Dandekar",
        "Sreedath Panat"
      ],
      "institutions": [
        "Vizuara AI Labs"
      ],
      "year": 2025,
      "venue": "KDD Workshop",
      "cover": null,
      "keywords": [
        "42-Network Structure Design"
      ],
      "code_url": null,
      "note_url": "notes/2025/MoE-MLA-RoPE/note/",
      "prototxt_path": "meta/2025/MoE-MLA-RoPE.prototxt",
      "update_time": 0
    },
    {
      "id": "SMAT",
      "title": "Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts",
      "abbr": "SMAT",
      "url": "http://arxiv.org/abs/2403.08477v3",
      "authors": [
        "Shengzhuang Chen",
        "Jihoon Tack",
        "Yunqiao Yang",
        "Yee Whye Teh",
        "Jonathan Richard Schwarz",
        "Ying Wei"
      ],
      "institutions": [
        "City University of Hong Kong",
        "University of Oxford",
        "Harvard University",
        "Nanyang Technological University"
      ],
      "year": 2024,
      "venue": "ICML",
      "cover": "notes/2024/SMAT/fig2.png",
      "keywords": [
        "03-Sparsity (Activation)"
      ],
      "code_url": "https://github.com/szc12153/sparse_interpolated_experts",
      "note_url": "notes/2024/SMAT/note/",
      "prototxt_path": "meta/2024/SMAT.prototxt",
      "update_time": 0
    },
    {
      "id": "selective_context",
      "title": "Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering",
      "abbr": "Selective Context",
      "url": "https://arxiv.org/abs/2304.12102",
      "authors": [
        "Yucheng Li"
      ],
      "institutions": [
        "University of Surrey, UK"
      ],
      "year": 2023,
      "venue": "arXiv",
      "cover": "notes/2023/selective_context/selective_context.jpg",
      "keywords": [
        "01-Sparse/Pruning"
      ],
      "code_url": "https://github.com/liyucheng09/Selective_Context",
      "note_url": null,
      "prototxt_path": "meta/2023/selective_context.prototxt",
      "update_time": 0
    },
    {
      "id": "Super-Experts-Profilling",
      "title": "Unveiling Super Experts in Mixture-of-Experts Large Language Models",
      "abbr": "Super-Experts-Profilling",
      "url": "http://arxiv.org/abs/2507.23279v1",
      "authors": [
        "Zunhai Su",
        "Qingyuan Li",
        "Hao Zhang",
        "YuLei Qian",
        "Yuchen Xie",
        "Kehong Yuan"
      ],
      "institutions": [
        "Tsinghua University",
        "Meituan"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/Super-Experts-Profilling/fig3.png",
      "keywords": [
        "01-Sparse/Pruning",
        "42-Network Structure Design"
      ],
      "code_url": "https://github.com/ZunhaiSu/Super-Experts-Profilling",
      "note_url": "notes/2025/Super-Experts-Profilling/note/",
      "prototxt_path": "meta/2025/Super-Experts-Profilling.prototxt",
      "update_time": 0
    },
    {
      "id": "vAttention",
      "title": "vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention",
      "abbr": "vAttention",
      "url": "http://arxiv.org/abs/2405.04437v3",
      "authors": [
        "Ramya Prabhu",
        "Ajay Nayak",
        "Jayashree Mohan",
        "Ramachandran Ramjee",
        "Ashish Panwar"
      ],
      "institutions": [
        "Microsoft Research",
        "Indian Institute of Science"
      ],
      "year": 2025,
      "venue": "ASPLOS",
      "cover": "notes/2025/vAttention/fig5.png",
      "keywords": [
        "21-KV Cache Management",
        "40-LLM Deployment"
      ],
      "code_url": "https://github.com/microsoft/vattention",
      "note_url": "notes/2025/vAttention/note/",
      "prototxt_path": "meta/2025/vAttention.prototxt",
      "update_time": 0
    },
    {
      "id": "VecInfer",
      "title": "VecInfer: Efficient LLM Inference with Low-Bit KV Cache via Outlier-Suppressed Vector Quantization",
      "abbr": "VecInfer",
      "url": "http://arxiv.org/abs/2510.06175v1",
      "authors": [
        "Dingyu Yao",
        "Chenxu Yang",
        "Zhengyang Tong",
        "Zheng Lin",
        "Wei Liu",
        "Jian Luan",
        "Weiping Wang"
      ],
      "institutions": [
        "Institute of Information Engineering, Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences",
        "Xiaomi"
      ],
      "year": 2025,
      "venue": "arXiv",
      "cover": "notes/2025/VecInfer/fig4.png",
      "keywords": [
        "09-Quantization",
        "19-Quantization (KV Cache)"
      ],
      "code_url": null,
      "note_url": "notes/2025/VecInfer/note/",
      "prototxt_path": "meta/2025/VecInfer.prototxt",
      "update_time": 0
    },
    {
      "id": "VENOM",
      "title": "VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores",
      "abbr": "VENOM",
      "url": "http://arxiv.org/abs/2310.02065v1",
      "authors": [
        "Roberto L. Castro",
        "Andrei Ivanov",
        "Diego Andrade",
        "Tal Ben-Nun",
        "Basilio B. Fraguela",
        "Torsten Hoefler"
      ],
      "institutions": [
        "Universidade da Corua",
        "ETH Zurich"
      ],
      "year": 2023,
      "venue": "SC",
      "cover": "notes/2023/VENOM/vnm.png",
      "keywords": [
        "01-Sparse/Pruning",
        "04-Sparsity (Weight)"
      ],
      "code_url": "https://github.com/UDC-GAC/venom",
      "note_url": "notes/2023/VENOM/note/",
      "prototxt_path": "meta/2023/VENOM.prototxt",
      "update_time": 0
    },
    {
      "id": "Vidur",
      "title": "Vidur: A Large-Scale Simulation Framework For LLM Inference",
      "abbr": "Vidur",
      "url": "http://arxiv.org/abs/2405.05465v2",
      "authors": [
        "Amey Agrawal",
        "Nitin Kedia",
        "Jayashree Mohan",
        "Ashish Panwar",
        "Nipun Kwatra",
        "Bhargav Gulavani",
        "Ramachandran Ramjee",
        "Alexey Tumanov"
      ],
      "institutions": [
        "Microsoft Research India"
      ],
      "year": 2024,
      "venue": "MLSys",
      "cover": "notes/2024/Vidur/fig2.png",
      "keywords": [
        "39-Performance Modeling"
      ],
      "code_url": "https://github.com/microsoft/vidur",
      "note_url": "notes/2024/Vidur/note/",
      "prototxt_path": "meta/2024/Vidur.prototxt",
      "update_time": 0
    },
    {
      "id": "SGLang-Code",
      "title": "vLLM",
      "abbr": "vLLM",
      "url": "",
      "authors": [
        "Name2"
      ],
      "institutions": [
        "inst1"
      ],
      "year": 2023,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "40-LLM Deployment",
        "46-Tool"
      ],
      "code_url": "https://github.com/vllm-project/vllm",
      "note_url": "notes/2023/SGLang-Code/note/",
      "prototxt_path": "meta/2023/SGLang-Code.prototxt",
      "update_time": 0
    },
    {
      "id": "VORTA",
      "title": "VORTA: Efficient Video Diffusion via Routing Sparse Attention",
      "abbr": "VORTA",
      "url": "http://arxiv.org/abs/2505.18809v2",
      "authors": [
        "Wenhao Sun",
        "Rong-Cheng Tu",
        "Yifu Ding",
        "Zhao Jin",
        "Jingyi Liao",
        "Shunyu Liu",
        "Dacheng Tao"
      ],
      "institutions": [
        "Nanyang Technological University"
      ],
      "year": 2025,
      "venue": "NeurIPS",
      "cover": "notes/2025/VORTA/fig6.png",
      "keywords": [
        "01-Sparse/Pruning",
        "02-Sparsity (Attention)"
      ],
      "code_url": "https://github.com/wenhao728/VORTA",
      "note_url": "notes/2025/VORTA/note/",
      "prototxt_path": "meta/2025/VORTA.prototxt",
      "update_time": 0
    },
    {
      "id": "XGrammar",
      "title": "XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models",
      "abbr": "XGrammar",
      "url": "http://arxiv.org/abs/2411.15100v2",
      "authors": [
        "Yixin Dong",
        "Charlie F. Ruan",
        "Yaxing Cai",
        "Ruihang Lai",
        "Ziyi Xu",
        "Yilong Zhao",
        "Tianqi Chen"
      ],
      "institutions": [
        "Carnegie Mellon University",
        "Shanghai Jiao Tong University",
        "NVIDIA",
        "University of California, Berkeley"
      ],
      "year": 2024,
      "venue": "arXiv",
      "cover": "notes/2024/XGrammar/fig1.png",
      "keywords": [
        "40-LLM Deployment"
      ],
      "code_url": "https://github.com/mlc-ai/xgrammar",
      "note_url": "notes/2024/XGrammar/note/",
      "prototxt_path": "meta/2024/XGrammar.prototxt",
      "update_time": 0
    },
    {
      "id": "ZeroQuant-V2",
      "title": "ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation",
      "abbr": "ZeroQuant-V2",
      "url": "https://arxiv.org/abs/2303.08302",
      "authors": [
        "Zhewei Yao",
        "Yuxiong He"
      ],
      "institutions": [
        "Microsoft"
      ],
      "year": 2023,
      "venue": "arXiv",
      "cover": null,
      "keywords": [
        "09-Quantization"
      ],
      "code_url": "https://github.com/microsoft/DeepSpeed",
      "note_url": null,
      "prototxt_path": "meta/2023/ZeroQuant-V2.prototxt",
      "update_time": 0
    },
    {
      "id": "zeroquant",
      "title": "ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers",
      "abbr": "ZeroQuant",
      "url": "https://openreview.net/forum?id=f-fVCElZ-G1",
      "authors": [
        "Zhewei Yao",
        "Yuxiong He"
      ],
      "institutions": [
        "Microsoft"
      ],
      "year": 2022,
      "venue": "NeurIPS",
      "cover": null,
      "keywords": [
        "09-Quantization"
      ],
      "code_url": "https://github.com/microsoft/DeepSpeed",
      "note_url": null,
      "prototxt_path": "meta/2022/zeroquant.prototxt",
      "update_time": 0
    },
    {
      "id": "ZipLM",
      "title": "ZipLM: Inference-Aware Structured Pruning of Language Models",
      "abbr": "ZipLM",
      "url": "https://openreview.net/pdf?id=bPFFPueAxm",
      "authors": [
        "Eldar Kurtic",
        "Elias Frantar",
        "Dan Alistarh"
      ],
      "institutions": [
        "IST Austria",
        "Neural Magic"
      ],
      "year": 2023,
      "venue": "NeurIPS",
      "cover": "notes/2023/ZipLM/cover.jpg",
      "keywords": [
        "01-Sparse/Pruning",
        "05-Sparsity (Structured)"
      ],
      "code_url": "https://github.com/IST-DASLab/ZipLM",
      "note_url": null,
      "prototxt_path": "meta/2023/ZipLM.prototxt",
      "update_time": 0
    }
  ],
  "filters": {
    "years": [
      2026,
      2025,
      2024,
      2023,
      2022,
      2021,
      2020,
      2019,
      2018,
      2017,
      2016,
      1993,
      1989
    ],
    "venues": [
      "AAAI",
      "ACL",
      "AISTATS",
      "ASPLOS",
      "ATC",
      "AutoML Workshop",
      "Blog",
      "COLM",
      "CVPR",
      "CVPR workshop",
      "Coling",
      "DAC",
      "DATE",
      "ECCV",
      "EMNLP",
      "EMNLP Findings",
      "ENLSP",
      "EuroSys",
      "Github",
      "ICCV",
      "ICLR",
      "ICLR oral",
      "ICML",
      "ICML Workshop",
      "ISCA",
      "JMLR",
      "KDD Workshop",
      "MICRO",
      "MLSys",
      "NCE",
      "NeurIPS",
      "OSDI",
      "PPoPP",
      "SC",
      "SIGMOD",
      "SOSP",
      "SoCC",
      "TACL",
      "TC",
      "TMLR",
      "UAI",
      "VLDB",
      "VLSI",
      "arXiv",
      "blog",
      "github"
    ],
    "keywords": [
      "00-None",
      "01-Sparse/Pruning",
      "02-Sparsity (Attention)",
      "03-Sparsity (Activation)",
      "04-Sparsity (Weight)",
      "05-Sparsity (Structured)",
      "09-Quantization",
      "19-Quantization (KV Cache)",
      "20-Sparse/Eviction (KV Cache)",
      "21-KV Cache Management",
      "29-Comm-Comp Overlap",
      "30-Speculative Decoding",
      "39-Performance Modeling",
      "40-LLM Deployment",
      "41-Survey",
      "42-Network Structure Design",
      "43-Low Rank Decomposition",
      "44-Layer Fusion (Reduce IO)",
      "45-Efficient Training",
      "46-Tool",
      "47-Benchmark"
    ]
  },
  "total": 396
}