# 2025-11-14

# Table of Contents
* [Lethe Layer- and Time-Adaptive KV Cache Pruning for Reasoning-Intensive LLM Serving](#lethe-layer--and-time-adaptive-kv-cache-pruning-for-reasoning-intensive-llm-serving)


## Lethe Layer- and Time-Adaptive KV Cache Pruning for Reasoning-Intensive LLM Serving

>Authors: Hui Zeng, Daming Zhao, Pengfei Yang, WenXuan Hou, Tianyang Zheng, Hui Li, Weiye Ji, Jidong Zhai

>2025-11-08

> [http://arxiv.org/abs/2511.06029v2](http://arxiv.org/abs/2511.06029v2)

Generative reasoning with large language models (![key](https://img.shields.io/badge/LLM-FF8C00)s) often involves long ![key](https://img.shields.io/badge/decoding-F08080) sequences, leading to substantial memory and latency overheads from accumulating key-value (![key](https://img.shields.io/badge/KV-F08080)) ![key](https://img.shields.io/badge/cache-F08080)s. While existing ![key](https://img.shields.io/badge/KV-F08080) ![key](https://img.shields.io/badge/compression-FF8C00) methods primarily focus on reducing ![key](https://img.shields.io/badge/prefill-F08080) memory from long input sequences, they fall short in addressing the dynamic and layer-sensitive nature of long-form generation, which is central to reasoning tasks. We propose Lethe, a dynamic ![key](https://img.shields.io/badge/KV-F08080) ![key](https://img.shields.io/badge/cache-F08080) management framework that introduces adaptivity along both the spatial and temporal dimensions of ![key](https://img.shields.io/badge/decoding-F08080). Along the spatial dimension, Lethe performs layerwise ![key](https://img.shields.io/badge/sparsity-F08080)-aware allocation, assigning token ![key](https://img.shields.io/badge/pruning-F08080) budgets to each ![key](https://img.shields.io/badge/transformer-FF8C00) layer based on estimated attention redundancy. Along the temporal dimension, Lethe conducts multi-round token ![key](https://img.shields.io/badge/pruning-F08080) during generation, driven by a Recency-Aware Selective Retention} (RASR) mechanism. RASR extends traditional recency-based heuristics by also considering token relevance derived from evolving attention patterns, enabling informed decisions about which tokens to retain or evict. Empirical results demonstrate that Lethe achieves a favorable balance between efficiency and generation quality across diverse models and tasks, increases throughput by up to 2.56x.

