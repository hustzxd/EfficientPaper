<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../../../img/favicon.ico" />
    <title>HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference - Efficient Paper</title>
    <link rel="stylesheet" href="../../../../css/theme.css" />
    <link rel="stylesheet" href="../../../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference";
        var mkdocs_page_input_path = "notes/2025/HATA/note.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../../../.." class="icon icon-home"> Efficient Paper
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../../..">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Paper List</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../../../cls_year/">By Year</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../../cls_keyword/">By Keyword</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../../cls_publication/">By Publication</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../../cls_institution/">By Institution</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../../cls_author/">By Author</a>
                  </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../../../weekly_paper/latest/">Weekly Paper</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../..">Efficient Paper</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../../.." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="hata-trainable-and-hardware-efficient-hash-aware-top-k-attention-for-scalable-large-model-inference">HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference</h1>
<blockquote>
<p>Ping Gong, Jiawei Yi, Shengnan Wang, Juncheng Zhang, Zewen Jin, Ouxiang Zhou, Ruibo Liu, Guanbin Xu, Youhui Bai, Bowen Ye, Kun Yuan, Tong Yang, Gong Zhang, Renhai Chen, Feng Wu, Cheng Li</p>
</blockquote>
<p><img alt="" src="../fig2.png" /></p>
<h2 id="abstract">Abstract</h2>
<p>Large Language Models (LLMs) have emerged as a pivotal research area, yet the
attention module remains a critical bottleneck in LLM inference, even with
techniques like KVCache to mitigate redundant computations. While various
top-$k$ attention mechanisms have been proposed to accelerate LLM inference by
exploiting the inherent sparsity of attention, they often struggled to strike a
balance between efficiency and accuracy. In this paper, we introduce HATA
(Hash-Aware Top-$k$ Attention), a novel approach that systematically integrates
low-overhead learning-to-hash techniques into the Top-$k$ attention process.
Different from the existing top-k attention methods which are devoted to
seeking an absolute estimation of qk score, typically with a great cost, HATA
maps queries and keys into binary hash codes, and acquires the relative qk
score order with a quite low cost, which is sufficient for realizing top-k
attention. Extensive experiments demonstrate that HATA achieves up to
7.2$\times$ speedup compared to vanilla full attention while maintaining model
accuracy. In addition, HATA outperforms the state-of-the-art top-$k$ attention
methods in both accuracy and efficiency across multiple mainstream LLM models
and diverse tasks. HATA is open source at https://github.com/gpzlx1/HATA.</p>
<p>HATA用于decoding时的加速，之前kv压缩的方法，比如snapkv用历史的结果预测当前结果，quest按照block pool预测block的重要性，seerattention通过训练的方法来预测重要性；</p>
<p>HATA提出将QK映射为Hash Code，并实用xor 操作高效的判断K cache的重要性。</p>
<p>映射Hash Code的过程涉及到可学习权重，需要一部分数据集进行学习</p>
<p>和 HashAttention: Semantic Sparsity for Faster Inference 思路很像</p>
              
            </div>
          </div><footer>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
    
  </span>
</div>
    <script src="../../../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../../../..";</script>
    <script src="../../../../js/theme_extra.js"></script>
    <script src="../../../../js/theme.js"></script>
      <script src="../../../../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
