# $ReLU^2$ Wins: Discovering Efficient Activation Functions for Sparse LLMs

![](activation.png)

The paper indicates through experiments that models employing $ReLU^2$ excel across all three evaluation aspects, highlighting its potential as an efficient activation function for sparse LLMs.