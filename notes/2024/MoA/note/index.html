<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../../../../img/favicon.ico" />
    <title>MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression - Efficient Paper</title>
    <link rel="stylesheet" href="../../../../css/theme.css" />
    <link rel="stylesheet" href="../../../../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.css" rel="stylesheet" />
        <link href="../../../../css/extra.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression";
        var mkdocs_page_input_path = "notes/2024/MoA/note.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../../../../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
 <link href="../../../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
    html.glightbox-open { overflow: initial; height: 100%; }
    .gslide-title { margin-top: 0px; user-select: text; }
    .gslide-desc { color: #666; user-select: text; }
    .gslide-image img { background: white; }</style> <script src="../../../../assets/javascripts/glightbox.min.js"></script></head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href="../../../.." class="icon icon-home"> Efficient Paper
        </a>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../../..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../../../baseline_methods_graph/">Graph</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Weekly Paper</span></p>
              <ul>
                  <li class="toctree-l1"><a class="reference internal" href="../../../../weekly_paper/2025-12-05/">2025-12-05</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../../weekly_paper/2025-12-12/">2025-12-12</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../../weekly_paper/2025-12-19/">2025-12-19</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../../weekly_paper/2025-12-26/">2025-12-26</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../../weekly_paper/2025-12-31/">2025-12-31</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../../../../weekly_paper/2026-01-09/">2026-01-09</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" >Legacy</a>
    <ul>
                <li class="toctree-l2"><a class="reference internal" href="../../../../weekly_paper/legacy/2025-10-09/">2025-10-09</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../../../weekly_paper/legacy/2025-10-17/">2025-10-17</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../../../weekly_paper/legacy/2025-10-24/">2025-10-24</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../../../weekly_paper/legacy/2025-10-31/">2025-10-31</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../../../weekly_paper/legacy/2025-11-07/">2025-11-07</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../../../weekly_paper/legacy/2025-11-14/">2025-11-14</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../../../weekly_paper/legacy/2025-11-21/">2025-11-21</a>
                </li>
                <li class="toctree-l2"><a class="reference internal" href="../../../../weekly_paper/legacy/2025-11-28/">2025-11-28</a>
                </li>
    </ul>
                  </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../../../../contributors/">Contributors</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../..">Efficient Paper</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../../.." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
  
                <h1 id="moa-mixture-of-sparse-attention-for-automatic-large-language-model-compression">MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression</h1>
<p><a class="glightbox" href="../moa.png" data-type="image" data-width="auto" data-height="auto" data-desc-position="bottom"><img alt="" src="../moa.png" /></a></p>
<h2 id="abstract">Abstract</h2>
<p>Sparse attention can effectively mitigate the significant memory and
throughput demands of Large Language Models (LLMs) in long contexts. Existing
methods typically employ a uniform sparse attention mask, applying the same
sparse pattern across different attention heads and input lengths. However,
this uniform approach fails to capture the diverse attention patterns inherent
in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this
challenge, we propose the Mixture of Attention (MoA), which automatically
tailors distinct sparse attention configurations to different heads and layers.
MoA constructs and navigates a search space of various attention patterns and
their scaling rules relative to input sequence lengths. It profiles the model,
evaluates potential configurations, and pinpoints the optimal sparse attention
compression plan. MoA adapts to varying input sizes, revealing that some
attention heads expand their focus to accommodate longer sequences, while other
heads consistently concentrate on fixed-length local contexts. Experiments show
that MoA increases the effective context length by <script type="math/tex">3.9\times</script> with the same
average attention span, boosting retrieval accuracy by <script type="math/tex">1.5-7.1\times</script> over the
uniform-attention baseline across Vicuna-{7B,13B}, and Llama3-{8B,70B} models.
Moreover, MoA narrows the capability gaps between sparse and dense models,
reducing the maximum relative performance drop from <script type="math/tex">9\%-36\%</script> to within <script type="math/tex">5\%</script>
across two long-context understanding benchmarks. MoA achieves a
<script type="math/tex">1.2-1.4\times</script> GPU memory reduction, boosting decode throughput by
<script type="math/tex">6.6-8.2\times</script> and <script type="math/tex">1.7-1.9\times</script> compared to FlashAttention2 and vLLM, with
minimal impact on performance. Our code is available at
\url{https://github.com/thu-nics/MoA}.</p>
<p>和DuoAttention的做法非常像，每个 atention head 使用不同的 sparse pattern。</p>
<ul>
<li>定义search space，包含是否不进行sparse，使用streamingLLM方式等。</li>
<li>calibration data采用了dense model生成数据，用来衡量attention head的影响</li>
<li>优化配置sparse pattern，从而满足压缩约束条件下loss最小</li>
</ul>
              
  <!-- Giscus 评论系统 - 只在 notes 文件夹下显示 -->
<script>
  // 使用 JavaScript 来判断 URL 路径
  if (window.location.pathname.includes('/notes/')) {
    document.write(`
      <div id="giscus-container" style="margin-top: 2rem; padding-top: 1rem; border-top: 1px solid #e1e4e8;">
        <script src="https://giscus.app/client.js"
                data-repo="hustzxd/EfficientPaper"
                data-repo-id="R_kgDOKVYtOg"
                data-category="General"
                data-category-id="DIC_kwDOKVYtOs4CukCv"
                data-mapping="pathname"
                data-strict="0"
                data-reactions-enabled="1"
                data-emit-metadata="0"
                data-input-position="bottom"
                data-theme="light"
                data-lang="zh-CN"
                crossorigin="anonymous"
                async>
        <\/script>
      </div>
    `);
  }
</script>

            </div>
          </div><footer>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
    
  </span>
</div>
    <script src="../../../../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "../../../..";</script>
    <script src="../../../../js/theme_extra.js"></script>
    <script src="../../../../js/theme.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-clike.min.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-python.min.js"></script>
      <script src="../../../../js/prism-prototxt.js"></script>
      <script src="../../../../js/preview.js"></script>
      <script src="../../../../js/back-to-top.js"></script>
      <script src="../../../../js/note-editor.js"></script>
      <script src="../../../../js/auto-reload.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

<script id="init-glightbox">const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
</script></body>
</html>
