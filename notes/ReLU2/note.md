# ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs

<p align="center">
<img src="activation.png" width="600" title="blank">
</p>

The paper indicates through experiments that models employing $$ReLU^2$$ excel across all three evaluation aspects, highlighting its potential as an efficient activation function for sparse LLMs.