<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>By Keyword - Efficient Paper</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../css/extra.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "By Keyword";
        var mkdocs_page_input_path = "cls_keyword.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> Efficient Paper
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                </li>
              </ul>
              <p class="caption"><span class="caption-text">Paper List</span></p>
              <ul class="current">
                  <li class="toctree-l1"><a class="reference internal" href="../cls_year/">By Year</a>
                  </li>
                  <li class="toctree-l1 current"><a class="reference internal current" href="#">By Keyword</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#01-communication-computation-overlap">01-Communication-Computation Overlap</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#02-sparsity-attention">02-Sparsity (Attention)</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#03-sparsity-activation">03-Sparsity (Activation)</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#04-modeling">04-Modeling</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#05-sparsity-structured">05-Sparsity (Structured)</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#06-sparsepruning">06-Sparse/Pruning</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#07-quantization">07-Quantization</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#08-survey">08-Survey</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#09-low-rank-decomposition">09-Low Rank Decomposition</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#10-layer-fusion-reduce-io">10-Layer Fusion (Reduce IO)</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#11-tool">11-Tool</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#12-kv-cache-optimizationefficient-attention">12-KV Cache Optimization/Efficient Attention</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#13-efficient-training">13-Efficient Training</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#14-network-structure-design">14-Network Structure Design</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#15-sparsity-weight">15-Sparsity (Weight)</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#16-llm-deployment">16-LLM Deployment</a>
    </li>
    </ul>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../cls_publication/">By Publication</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../cls_institution/">By Institution</a>
                  </li>
                  <li class="toctree-l1"><a class="reference internal" href="../cls_author/">By Author</a>
                  </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../weekly_paper/latest/">Weekly Paper</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">Efficient Paper</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
          <li class="breadcrumb-item">Paper List</li>
      <li class="breadcrumb-item active">By Keyword</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h3 id="keyword">keyword</h3>
<h4 id="01-communication-computation-overlap">01-Communication-Computation Overlap</h4>
<table>
<thead>
<tr>
<th style="text-align: left;">Meta</th>
<th style="text-align: left;">Title</th>
<th style="text-align: left;">Cover</th>
<th style="text-align: left;">Publish</th>
<th style="text-align: left;">Code</th>
<th style="text-align: left;">Note</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><a href="../meta/2021/CoCoNet.prototxt">CoCoNet</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2105.05720v5">Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2021-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2021/CoCoNet/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/RIIWOI3F.prototxt">m</a></td>
<td style="text-align: left;"><a href="https://dl.acm.org/doi/abs/10.1145/3567955.3567959">Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-ASPLOS-9370DB" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2023/RIIWOI3F/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/T3.prototxt">T3</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2401.16677v1">T3: Transparent Tracking &amp; Triggering for Fine-grained Overlap of Compute &amp; Collectives</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/T3/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ASPLOS-9370DB" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/T3/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/DistributedGEMM.prototxt">DistributedGEMM</a></td>
<td style="text-align: left;"><a href="https://blog.shi-labs.com/distributed-gemm-88be6a481e2b">A novel CUTLASS-based implementation of Tensor Parallelism for NVLink-enabled systems</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/DistributedGEMM/fig1.gif" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-Blog-696969" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/NVIDIA/cutlass" /></td>
<td style="text-align: left;"><a href="../notes/2024/DistributedGEMM/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/Domino.prototxt">Domino</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2409.15241v1">Domino: Eliminating Communication in LLM Training via Generic Tensor Slicing and Overlapping</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/deepspeedai/DeepSpeedExamples" /></td>
<td style="text-align: left;"><a href="../notes/2024/Domino/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/FLUX.prototxt">FLUX</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2406.06858v5">FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/FLUX/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/NanoFlow.prototxt">NanoFlow</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2408.12757v2">NanoFlow: Towards Optimal Large Language Model Serving Throughput</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/NanoFlow/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/Async-TP.prototxt">Async-TP</a></td>
<td style="text-align: left;"><a href="https://discuss.pytorch.org/t/distributed-w-torchtitan-introducing-async-tensor-parallelism-in-pytorch/209487/1">[Distributed w/ TorchTitan] Introducing Async Tensor Parallelism in PyTorch</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/Async-TP/Figure_2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/pytorch/torchtitan" /></td>
<td style="text-align: left;"><a href="../notes/2024/Async-TP/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/UC0D8DJ6.prototxt">UC0D8DJ6</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2507.14392v1">Characterizing Communication Patterns in Distributed Large Language Model Inference</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/UC0D8DJ6/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/1DZIJVBI.prototxt">1DZIJVBI</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2507.03114v1">Characterizing Compute-Communication Overlap in GPU-Accelerated Distributed Deep Learning: Performance and Power Implications</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/1DZIJVBI/eq1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/1DZIJVBI/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/CometSeed.prototxt">CometSeed</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.19811v3">Comet: Fine-grained Computation-communication Overlapping for Mixture-of-Experts</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/bytedance/flux" /></td>
<td style="text-align: left;"><a href="../notes/2025/CometSeed/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/FlashOverlap.prototxt">FlashOverlap</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2504.19519v1">FlashOverlap: A Lightweight Design for Efficiently Overlapping Communication and Computation</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/FlashOverlap/fig3.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/infinigence/FlashOverlap" /></td>
<td style="text-align: left;"><a href="../notes/2025/FlashOverlap/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/HelixParallelism.prototxt">HelixParallelism</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2507.07120v1">Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/HelixParallelism/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/MegaScale-MoE.prototxt">MegaScale-MoE</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2505.11432v2">MegaScale-MoE: Large-Scale Communication-Efficient Training of Mixture-of-Experts Models in Production</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/MegaScale-MoE/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/Seesaw.prototxt">Seesaw</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2503.06433v1">Seesaw: High-throughput LLM Inference via Model Re-sharding</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/Seesaw/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/TileLink.prototxt">TileLink</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2503.20313v3">TileLink: Generating Efficient Compute-Communication Overlapping Kernels using Tile-Centric Primitives</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/TileLink/fig7.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ByteDance-Seed/Triton-distributed" /></td>
<td style="text-align: left;"><a href="../notes/2025/TileLink/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/TokenWeave.prototxt">TokenWeave</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2505.11329v1">TokenWeave: Efficient Compute-Communication Overlap for Distributed LLM Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/TokenWeave/fig8.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/tokenweave" /></td>
<td style="text-align: left;"><a href="../notes/2025/TokenWeave/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/Triton-distributed.prototxt">Triton-distributed</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2504.19442v3">Triton-distributed: Programming Overlapping Kernels on Distributed AI Systems with the Triton Compiler</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/Triton-distributed/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ByteDance-Seed/Triton-distributed" /></td>
<td style="text-align: left;"><a href="../notes/2025/Triton-distributed/note/">note</a></td>
</tr>
</tbody>
</table>
<h4 id="02-sparsity-attention">02-Sparsity (Attention)</h4>
<table>
<thead>
<tr>
<th style="text-align: left;">Meta</th>
<th style="text-align: left;">Title</th>
<th style="text-align: left;">Cover</th>
<th style="text-align: left;">Publish</th>
<th style="text-align: left;">Code</th>
<th style="text-align: left;">Note</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><a href="../meta/2022/DSA.prototxt">DSA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2110.11299v1">Transformer Acceleration with Dynamic Sparse Attention</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2022/DSA/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2022-TC-F08080" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2022/DSA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/H2O.prototxt">H2O</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2306.14048">H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/H2O/h2o.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/FMInference/H2O" /></td>
<td style="text-align: left;"><a href="../notes/2023/H2O/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/streaming-llm.prototxt">streaming-llm</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2309.17453v4">Efficient Streaming Language Models with Attention Sinks</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/streaming-llm/cover.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICLR-FF6B6B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mit-han-lab/streaming-llm" /></td>
<td style="text-align: left;"><a href="../notes/2024/streaming-llm/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/Quest.prototxt">Quest</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2406.10774">Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/Quest/quest.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mit-han-lab/quest" /></td>
<td style="text-align: left;"><a href="../notes/2024/Quest/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/MInference.prototxt">MInference</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2407.02490v1">MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/MInference/MInference_3shape.PNG" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-NeurIPS-FF1493" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/MInference" /></td>
<td style="text-align: left;"><a href="../notes/2024/MInference/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/PWGG5HBE.prototxt">PWGG5HBE</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2412.19442v2">A Survey on Large Language Model Acceleration based on KV Cache Management</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/PWGG5HBE/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/TreeAI-Lab/Awesome-KV-Cache-Management" /></td>
<td style="text-align: left;"><a href="../notes/2024/PWGG5HBE/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/AdaKV.prototxt">AdaKV</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2407.11550v3">Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/AdaKV/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/FFY0/AdaKV" /></td>
<td style="text-align: left;"><a href="../notes/2024/AdaKV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/SharedAttention.prototxt">SharedAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2407.12866v1">Beyond KV Caching: Shared Attention for Efficient LLMs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/SharedAttention/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/metacarbon/shareAtt" /></td>
<td style="text-align: left;"><a href="../notes/2024/SharedAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/DuoAttention.prototxt">DuoAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2410.10819v1">DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/DuoAttention/duoattention.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mit-han-lab/duo-attention" /></td>
<td style="text-align: left;"><a href="../notes/2024/DuoAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/FlashMask.prototxt">FlashMask</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2410.01359v1">FlashMask: Efficient and Rich Mask Extension of FlashAttention</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/FlashMask/flashmask.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/PaddlePaddle/PaddleNLP" /></td>
<td style="text-align: left;"><a href="../notes/2024/FlashMask/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/MoA.prototxt">MoA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2406.14909v2">MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/MoA/moa.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/thu-nics/MoA" /></td>
<td style="text-align: left;"><a href="../notes/2024/MoA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/DoubleSparsity.prototxt">DoubleSparsity</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2408.07092v2">Post-Training Sparse Attention with Double Sparsity</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/andy-yang-1/DoubleSparse" /></td>
<td style="text-align: left;"><a href="../notes/2024/DoubleSparsity/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/RecycledAttention.prototxt">Recycled Attention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2411.05787v1">Recycled Attention: Efficient inference for long-context language models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/RecycledAttention/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/carriex/recycled-attention" /></td>
<td style="text-align: left;"><a href="../notes/2024/RecycledAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/SampleAttention.prototxt">SampleAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2406.15486v2">SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/SampleAttention/cover.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/SampleAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/SeerAttention.prototxt">SeerAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2410.13276v2">SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/SeerAttention/seerattention.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/SeerAttention" /></td>
<td style="text-align: left;"><a href="../notes/2024/SeerAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/SnapKV.prototxt">SnapKV</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2404.14469v2">SnapKV: LLM Knows What You are Looking for Before Generation</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/SnapKV/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/FasterDecoding/SnapKV" /></td>
<td style="text-align: left;"><a href="../notes/2024/SnapKV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/TOVA.prototxt">TOVA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2401.06104v2">Transformers are Multi-State RNNs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/TOVA/tova.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/schwartz-lab-NLP/TOVA" /></td>
<td style="text-align: left;"><a href="../notes/2024/TOVA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/ZigZagKV.prototxt">ZigZagKV</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2412.09036v1">ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/ZigZagKV/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/ZigZagKV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/NSA.prototxt">NSA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.11089v1">Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/NSA/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ACL-4169E1" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/NSA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/KVSink.prototxt">KVSink</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2508.04257v1">KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/KVSink/fig8.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-COLM-6495ED" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/KVSink/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/FlexPrefill.prototxt">FlexPrefill</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.20766v1">FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/FlexPrefill/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICLR-FF6B6B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/bytedance/FlexPrefill" /></td>
<td style="text-align: left;"><a href="../notes/2025/FlexPrefill/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/ReAttention.prototxt">ReAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2407.15176v3">ReAttention: Training-Free Infinite Context with Finite Attention Scope</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/ReAttention/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICLR-FF6B6B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/OpenMOSS/ReAttention" /></td>
<td style="text-align: left;"><a href="../notes/2025/ReAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/AdaSplash.prototxt">AdaSplash</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.12082">AdaSplash: Adaptive Sparse Flash Attention</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/deep-spin/adasplash" /></td>
<td style="text-align: left;"><a href="../notes/2025/AdaSplash/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/CateKV.prototxt">CateKV</a></td>
<td style="text-align: left;"><a href="https://openreview.net/forum?id=u7dlwgKstN">CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/CateKV/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/CateKV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/PoD.prototxt">PoD</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2412.02252v1">Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/PoD/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/PoD/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/HashAttention.prototxt">HashAttention</a></td>
<td style="text-align: left;"><a href="https://openreview.net/forum?id=Em2oaXd8Dc">HashAttention: Semantic Sparsity for Faster Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/HashAttention/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/xAlg-ai/HashAttention-1.0" /></td>
<td style="text-align: left;"><a href="../notes/2025/HashAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/MMInference.prototxt">MMInference</a></td>
<td style="text-align: left;"><a href="https://openreview.net/forum?id=me6PfbATWM">MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/MMInference/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/ShadowKV.prototxt">ShadowKV</a></td>
<td style="text-align: left;"><a href="https://openreview.net/forum?id=oa7MYAO6h6">ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/ShadowKV/shadowkv.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/bytedance/ShadowKV" /></td>
<td style="text-align: left;"><a href="../notes/2025/ShadowKV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/SpargeAttn.prototxt">SpargeAttn</a></td>
<td style="text-align: left;"><a href="https://openreview.net/forum?id=74c3Wwk8Tc">SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/SpargeAttn/fig3.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/thu-ml/SpargeAttn" /></td>
<td style="text-align: left;"><a href="../notes/2025/SpargeAttn/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/StarAttention.prototxt">StarAttention</a></td>
<td style="text-align: left;"><a href="https://openreview.net/forum?id=QY7Au9nZwp">Star Attention: Efficient LLM Inference over Long Sequences</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/StarAttention/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/NVIDIA/Star-Attention" /></td>
<td style="text-align: left;"><a href="../notes/2025/StarAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/XAttention.prototxt">XAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2503.16428v1">XAttention: Block Sparse Attention with Antidiagonal Scoring</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/XAttention/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mit-han-lab/x-attention" /></td>
<td style="text-align: left;"><a href="../notes/2025/XAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/07NWF4VE.prototxt">07NWF4VE</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2504.06319v1">Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/07NWF4VE/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/07NWF4VE/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/SharePrefill.prototxt">SharePrefill</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2505.19578v1">Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/SharePrefill/fig3.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/SharePrefill/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/ACP.prototxt">ACP</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2504.06949v1">Adaptive Computation Pruning for the Forgetting Transformer</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zhixuan-lin/arctic-fox" /></td>
<td style="text-align: left;"><a href="../notes/2025/ACP/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/AhaKV.prototxt">AhaKV</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2506.03762v1">AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/AhaKV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/AttentionPredictor.prototxt">AttentionPredictor</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.04077v1">AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/AttentionPredictor/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/AttentionPredictor/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/ChunkKV.prototxt">ChunkKV</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.00299v1">ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/ChunkKV/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/ChunkKV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/DBudgetKV.prototxt">DBudgetKV</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.16886v1">DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/DBudgetKV/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/DBudgetKV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/DeltaAttention.prototxt">DeltaAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2505.11254v1">Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/DeltaAttention/fig4.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/DeltaAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/DeltaLLM.prototxt">DeltaLLM</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2507.19608v1">DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/DeltaLLM/fig5.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/DeltaLLM/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/RaaS.prototxt">RaaS</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.11147v1">Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/RaaS/fig5.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/RaaS/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/topk-decoding.prototxt">topk-decoding</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.06766v2">Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/topk-decoding/alg1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ryansynk/topk-decoding" /></td>
<td style="text-align: left;"><a href="../notes/2025/topk-decoding/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/FastKV.prototxt">FastKV</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.01068v1">FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/FastKV/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/dongwonjo/FastKV" /></td>
<td style="text-align: left;"><a href="../notes/2025/FastKV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/FlashInfer.prototxt">FlashInfer</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2501.01005v2">FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/flashinfer-ai/flashinfer" /></td>
<td style="text-align: left;"><a href="../notes/2025/FlashInfer/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/FreqKV.prototxt">FreqKV</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2505.00570v2">FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/FreqKV/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/FreqKV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/HATA.prototxt">HATA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2506.02572v1">HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/HATA/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/gpzlx1/HATA" /></td>
<td style="text-align: left;"><a href="../notes/2025/HATA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/HCAttention.prototxt">HCAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2507.19823v1">HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/HCAttention/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/HCAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/KVLink.prototxt">KVLink</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.16002v1">KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/KVLink/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/UCSB-NLP-Chang/KVLink" /></td>
<td style="text-align: left;"><a href="../notes/2025/KVLink/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/LServer.prototxt">LServer</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.14866v1">LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/LServer/fig5.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mit-han-lab/omniserve" /></td>
<td style="text-align: left;"><a href="../notes/2025/LServer/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/LeanK.prototxt">LeanK</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2508.02215v1">LeanK: Learnable K Cache Channel Pruning for Efficient Decoding</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/LeanK/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/MInference" /></td>
<td style="text-align: left;"><a href="../notes/2025/LeanK/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/MiniCPM4.prototxt">MiniCPM4</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2506.07900v1">MiniCPM4: Ultra-Efficient LLMs on End Devices</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/MiniCPM4/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/openbmb/minicpm" /></td>
<td style="text-align: left;"><a href="../notes/2025/MiniCPM4/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/MoSA.prototxt">MoSA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2505.00315v1">Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/MoSA/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/piotrpiekos/MoSA" /></td>
<td style="text-align: left;"><a href="../notes/2025/MoSA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/MoR.prototxt">MoR</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2507.10524v1">Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/MoR/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/raymin0223/mixture_of_recursions" /></td>
<td style="text-align: left;"><a href="../notes/2025/MoR/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/MoBA.prototxt">MoBA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.13189v1">MoBA: Mixture of Block Attention for Long-Context LLMs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/MoBA/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/MoonshotAI/MoBA" /></td>
<td style="text-align: left;"><a href="../notes/2025/MoBA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/PowerAttention.prototxt">PowerAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2503.03588v1">PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/PowerAttention/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/PowerAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/PSA.prototxt">PSA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2503.00392v1">Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/PSA/fig4.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ASISys/PSAttention" /></td>
<td style="text-align: left;"><a href="../notes/2025/PSA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/QuickSilver.prototxt">QuickSilver</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2506.22396v1">QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/QuickSilver/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/R-KV.prototxt">R-KV</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2505.24133v2">R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/R-KV/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Zefan-Cai/R-KV" /></td>
<td style="text-align: left;"><a href="../notes/2025/R-KV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/RadialAttention.prototxt">RadialAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2506.19852v1">Radial Attention: $O(n\log n)$ Sparse Attention with Energy Decay for Long Video Generation</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/RadialAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/ReSA.prototxt">ReSA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2506.04108v2">Rectified Sparse Attention</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/ReSA/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/unilm" /></td>
<td style="text-align: left;"><a href="../notes/2025/ReSA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/0VRXJQ3F.prototxt">0VRXJQ3F</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2503.24000v1">Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/0VRXJQ3F/tab1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/LLMkvsys/rethink-kv-compression" /></td>
<td style="text-align: left;"><a href="../notes/2025/0VRXJQ3F/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/SALE.prototxt">SALE</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2505.24179v1">SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/SALE/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/BirdChristopher/SALE" /></td>
<td style="text-align: left;"><a href="../notes/2025/SALE/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/SeerAttention-R.prototxt">SeerAttention-R</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2506.08889v1">SeerAttention-R: Sparse Attention Adaptation for Long Reasoning</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/SeerAttention-R/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/SeerAttention" /></td>
<td style="text-align: left;"><a href="../notes/2025/SeerAttention-R/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/SpindleKV.prototxt">SpindleKV</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2507.06517v1">SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/SpindleKV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/Task-KV.prototxt">Task-KV</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2501.15113v1">Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/Task-KV/fig6.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/Task-KV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/sparse-frontier.prototxt">sparse-frontier</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2504.17768v1">The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/sparse-frontier/tb1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/PiotrNawrot/sparse-frontier" /></td>
<td style="text-align: left;"><a href="../notes/2025/sparse-frontier/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/KVCache-Factory.prototxt">KVCache-Factory</a></td>
<td style="text-align: left;"><a href="https://github.com/Zefan-Cai/KVCache-Factory">Unified KV Cache Compression Methods for Auto-Regressive Models</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-github-2F4F4F" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Zefan-Cai/KVCache-Factory" /></td>
<td style="text-align: left;"><a href="../notes/2025/KVCache-Factory/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/kvpress.prototxt">kvpress</a></td>
<td style="text-align: left;">kvpress</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-github-2F4F4F" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/NVIDIA/kvpress" /></td>
<td style="text-align: left;"><a href="../notes/2025/kvpress/note/">note</a></td>
</tr>
</tbody>
</table>
<h4 id="03-sparsity-activation">03-Sparsity (Activation)</h4>
<table>
<thead>
<tr>
<th style="text-align: left;">Meta</th>
<th style="text-align: left;">Title</th>
<th style="text-align: left;">Cover</th>
<th style="text-align: left;">Publish</th>
<th style="text-align: left;">Code</th>
<th style="text-align: left;">Note</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><a href="../meta/2023/SparseViT.prototxt">SparseViT</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2303.17605">SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/SparseViT/sparsevit.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-CVPR-2E8B57" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mit-han-lab/sparsevit" /></td>
<td style="text-align: left;"><a href="./notes/2023/sparsevit/index.md">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/WMMGA0AR.prototxt">m</a></td>
<td style="text-align: left;"><a href="https://openreview.net/forum?id=TJ2nxciYCk-">The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-ICLR-FF6B6B" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/LLM_in_a_flash.prototxt">LLM in a flash</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2312.11514">LLM in a flash: Efficient Large Language Model Inference with Limited Memory</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/LLM_in_a_flash/windows.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2023/LLM_in_a_flash/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/PowerInfer.prototxt">PowerInfer</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2312.12456v1">PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/SJTU-IPADS/PowerInfer" /></td>
<td style="text-align: left;"><a href="../notes/2023/PowerInfer/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/CATS.prototxt">CATS</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2404.08763v4">CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/CATS/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-COLM-6495ED" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ScalingIntelligence/CATS" /></td>
<td style="text-align: left;"><a href="../notes/2024/CATS/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/SparseInfer.prototxt">SparseInfer</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2411.12692v1">SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-DATE-B22222" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/SparseInfer/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/SCAP.prototxt">SCAP</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2412.07174v1">Post-Training Statistical Calibration for Higher Activation Sparsity</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/SCAP/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ENLSP-00BFFF" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/IntelLabs/SCAP" /></td>
<td style="text-align: left;"><a href="../notes/2024/SCAP/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/SAS.prototxt">SAS</a></td>
<td style="text-align: left;"><a href="https://openreview.net/forum?id=vZfi5to2Xl">SAS: Structured Activation Spasification</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/SAS/sas.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICLR-FF6B6B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/DensoITLab/sas_" /></td>
<td style="text-align: left;"><a href="../notes/2024/SAS/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/HMR7HKFV.prototxt">ReLU Strikes Back</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2310.04564">ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/HMR7HKFV/ReLU_Strikes_Back.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICLR_oral-green" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/sjtu-ipads/powerinfer" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/SMAT.prototxt">SMAT</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2403.08477v3">Unleashing the Power of Meta-tuning for Few-shot Generalization Through Sparse Interpolated Experts</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/SMAT/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/szc12153/sparse_interpolated_experts" /></td>
<td style="text-align: left;"><a href="../notes/2024/SMAT/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/CoreInfer.prototxt">CoreInfer</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2410.18311v1">CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/CoreInfer/framework.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/wangqinsi1/CoreInfer" /></td>
<td style="text-align: left;"><a href="../notes/2024/CoreInfer/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/PowerInfer-2.prototxt">PowerInfer-2</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2406.06282v2">PowerInfer-2: Fast Large Language Model Inference on a Smartphone</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><a href="https://powerinfer.ai/v2/">Website</a></td>
<td style="text-align: left;"><a href="../notes/2024/PowerInfer-2/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/ProSparse.prototxt">ProSparse</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2402.13516">ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/ProSparse/prosparse.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/raincleared-song/sparse_gpu_operator" /></td>
<td style="text-align: left;"><a href="../notes/2024/ProSparse/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/Q-Sparse.prototxt">Q-Sparse</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2407.10969v1">Q-Sparse: All Large Language Models can be Fully Sparsely-Activated</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/Q-Sparse/q-sparse.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/Q-Sparse/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/ReLU2.prototxt">ReLU2</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2402.03804">ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/ReLU2/activation.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/ReLU2/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/TurboSparse.prototxt">Turbo Sparse</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2406.05955v2">Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><a href="https://huggingface.co/PowerInfer">Pytorch</a></td>
<td style="text-align: left;"><a href="../notes/2024/TurboSparse/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/BlockFFN.prototxt">BlockFFN</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2507.08771v1">BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/BlockFFN/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-COLM-6495ED" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/thunlp/BlockFFN" /></td>
<td style="text-align: left;"><a href="../notes/2025/BlockFFN/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/R-Sparse.prototxt">R-Sparse</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2504.19449v1">R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/R-Sparse/fig4.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICLR-FF6B6B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/VITA-Group/R-Sparse" /></td>
<td style="text-align: left;"><a href="../notes/2025/R-Sparse/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/TEAL.prototxt">TEAL</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2408.14690v1">Training-Free Activation Sparsity in Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/TEAL/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICLR-FF6B6B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/FasterDecoding/TEAL" /></td>
<td style="text-align: left;"><a href="../notes/2025/TEAL/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/LaRoSA.prototxt">LaRoSA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2507.01299v1">La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/LaRoSA/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/LaRoSA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/SparsingLaw.prototxt">SparsingLaw</a></td>
<td style="text-align: left;"><a href="https://openreview.net/forum?id=SBUc5wirM8">Sparsing Law: Towards Large Language Models with Greater Activation Sparsity</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/SparsingLaw/fig4.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/thunlp/SparsingLaw" /></td>
<td style="text-align: left;"><a href="../notes/2025/SparsingLaw/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/AmberPruner.prototxt">AmberPruner</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2508.02128v1">Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/AmberPruner/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/IFPruning.prototxt">IFPruning</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2501.02086v2">Instruction-Following Pruning for Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/IFPruning/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/IFPruning/note/">note</a></td>
</tr>
</tbody>
</table>
<h4 id="04-modeling">04-Modeling</h4>
<table>
<thead>
<tr>
<th style="text-align: left;">Meta</th>
<th style="text-align: left;">Title</th>
<th style="text-align: left;">Cover</th>
<th style="text-align: left;">Publish</th>
<th style="text-align: left;">Code</th>
<th style="text-align: left;">Note</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><a href="../meta/2024/Vidur.prototxt">Vidur</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2405.05465v2">Vidur: A Large-Scale Simulation Framework For LLM Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/Vidur/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-MLSys-DDA0DD" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/vidur" /></td>
<td style="text-align: left;"><a href="../notes/2024/Vidur/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/APEX.prototxt">APEX</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2411.17651v2">APEX: An Extensible and Dynamism-Aware Simulator for Automated Parallel Execution in LLM Serving</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/apex_plus" /></td>
<td style="text-align: left;"><a href="../notes/2024/APEX/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/AMALI.prototxt">AMALI</a></td>
<td style="text-align: left;"><a href="https://dl.acm.org/doi/10.1145/3695053.3731064">AMALI: An Analytical Model for Accurately Modeling LLM Inference on Modern GPUs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/AMALI/fig6.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ISCA-9932CC" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/AMALI/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/LIMINAL.prototxt">LIMINAL</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2507.14397v1">Efficient LLM Inference: Bandwidth, Compute, Synchronization, and Capacity are all you need</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/LIMINAL/note/">note</a></td>
</tr>
</tbody>
</table>
<h4 id="05-sparsity-structured">05-Sparsity (Structured)</h4>
<table>
<thead>
<tr>
<th style="text-align: left;">Meta</th>
<th style="text-align: left;">Title</th>
<th style="text-align: left;">Cover</th>
<th style="text-align: left;">Publish</th>
<th style="text-align: left;">Code</th>
<th style="text-align: left;">Note</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><a href="../meta/2022/fisherpruning.prototxt">FisherPruning</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2204.09656v2">A Fast Post-Training Pruning Framework for Transformers</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2022/fisherpruning/cover.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2022-NeurIPS-FF1493" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/WoosukKwon/retraining-free-pruning" /></td>
<td style="text-align: left;"><a href="../notes/2022/fisherpruning/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/simple.prototxt">SIMPLE</a></td>
<td style="text-align: left;"><a href="https://aclanthology.org/2023.findings-acl.692.pdf">Structured Pruning for Efficient Generative Pre-trained Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/simple/cover.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-ACL-4169E1" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2023/simple/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/WDCO13S6.prototxt">m</a></td>
<td style="text-align: left;"><a href="https://openreview.net/forum?id=SHlZcInS6C">Structural Pruning of Large Language Models via Neural Architecture Search</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/nas_pruning/nas_pruning.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-AutoML_Workshop-A9A9A9" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/awslabs/syne-tune" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/dejavu.prototxt">Deja Vu</a></td>
<td style="text-align: left;"><a href="https://openreview.net/forum?id=wIPIhHd00i">Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/dejavu/dejavu.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/FMInference/DejaVu" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/LoSparse.prototxt">LoSparse</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2306.11222">Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/LoSparse/losparse.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/yxli2123/LoSparse" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/ZipLM.prototxt">ZipLM</a></td>
<td style="text-align: left;"><a href="https://openreview.net/pdf?id=bPFFPueAxm">ZipLM: Inference-Aware Structured Pruning of Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/ZipLM/cover.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-NeurIPS-FF1493" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/IST-DASLab/ZipLM" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/Compresso.prototxt">Compresso</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2310.05015">Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/Compresso/cover.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/Moonlit" /></td>
<td style="text-align: left;"><a href="../notes/2023/Compresso/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/kcm.prototxt">KCM</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2303.04185">Gradient-Free Structured Pruning with Unlabeled Data</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/kcm/kcm.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/k_pruning.prototxt">K-pruning</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2308.03449">Knowledge-preserving Pruning for Pre-trained Language Models without Retraining</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/k_pruning/kp.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2023/k_pruning/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/LLM-Pruner.prototxt">LLM-Pruner</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2305.11627v3">LLM-Pruner: On the Structural Pruning of Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/LLM-Pruner/cover.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/horseee/LLM-Pruner" /></td>
<td style="text-align: left;"><a href="../notes/2023/LLM-Pruner/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/lorashear.prototxt">LoRAShear</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2310.18356">LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/LLM_shearing.prototxt">LLM-shearing</a></td>
<td style="text-align: left;"><a href="https://xiamengzhou.github.io/sheared-llama/">Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/LLM_shearing/cover.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/princeton-nlp/LLM-Shearing" /></td>
<td style="text-align: left;"><a href="../notes/2023/LLM_shearing/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/flap.prototxt">FLAP</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2312.11983">Fluctuation-based Adaptive Structured Pruning for Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/del/flap.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-AAAI-FF4500" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/CASIA-IVA-Lab/FLAP" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/SliceGPT.prototxt">SliceGPT</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2401.15024v2">SliceGPT: Compress Large Language Models by Deleting Rows and Columns</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/SliceGPT/sliceGPT.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICLR-FF6B6B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/TransformerCompression" /></td>
<td style="text-align: left;"><a href="../notes/2024/SliceGPT/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/OSSCAR.prototxt">OSSCAR</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2403.12983v1">OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mazumder-lab/OSSCAR" /></td>
<td style="text-align: left;"><a href="../notes/2024/OSSCAR/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/SlimGPT.prototxt">SlimGPT</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2412.18110v1">SlimGPT: Layer-wise Structured Pruning for Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/SlimGPT/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-NeurIPS-FF1493" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/SlimGPT/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/Minitron.prototxt">Minitron</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2408.11796v2">Compact Language Models via Pruning and Knowledge Distillation</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/Minitron/minitron.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/NVlabs/Minitron" /></td>
<td style="text-align: left;"><a href="../notes/2024/Minitron/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/Bonsa.prototxt">Bonsa</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2402.05406">Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ldery/Bonsai" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/AdaSkip.prototxt">AdaSkip</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2501.02336v1">AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/AdaSkip/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-AAAI-FF4500" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ASISys/AdaSkip" /></td>
<td style="text-align: left;"><a href="../notes/2025/AdaSkip/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/SlimLLM.prototxt">SlimLLM</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2505.22689v1">SlimLLM: Accurate Structured Pruning for Large Language Models</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/SlimLLM/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/SpecEE.prototxt">SpecEE</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2504.08850v1">SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/SpecEE/fig9.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ISCA-9932CC" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/infinigence/SpecEE" /></td>
<td style="text-align: left;"><a href="../notes/2025/SpecEE/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/LinearPatch.prototxt">LinearPatch</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2505.24680v1">A Simple Linear Patch Revives Layer-Pruned Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/LinearPatch/fig3.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/LinearPatch/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/FlexiDepth.prototxt">FlexiDepth</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2503.23798v1">Adaptive Layer-skipping in Pre-trained LLMs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/FlexiDepth/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/FlexiDepth/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/Mosaic.prototxt">Mosaic</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2504.06323v1">Mosaic: Composite Projection Pruning for Resource-efficient LLMs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/Mosaic/fig6.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/Mosaic/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/Cus-Prun.prototxt">Cus-Prun</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2506.02561v1">Pruning General Large Language Models into Customized Expert Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/Cus-Prun/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zhaoyiran924/Custom-Prune" /></td>
<td style="text-align: left;"><a href="../notes/2025/Cus-Prun/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/SEAP.prototxt">SEAP</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2503.07605v1">SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/SEAP/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/IAAR-Shanghai/SEAP" /></td>
<td style="text-align: left;"><a href="../notes/2025/SEAP/note/">note</a></td>
</tr>
</tbody>
</table>
<h4 id="06-sparsepruning">06-Sparse/Pruning</h4>
<table>
<thead>
<tr>
<th style="text-align: left;">Meta</th>
<th style="text-align: left;">Title</th>
<th style="text-align: left;">Cover</th>
<th style="text-align: left;">Publish</th>
<th style="text-align: left;">Code</th>
<th style="text-align: left;">Note</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><a href="../meta/1989/obd.prototxt">OBD</a></td>
<td style="text-align: left;"><a href="https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf">Optimal Brain Damage</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/1989-NeurIPS-FF1493" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/1993/obs.prototxt">OBS</a></td>
<td style="text-align: left;"><a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=298572&amp;tag=1">Optimal Brain Surgeon and general network pruning</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/1993--green" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2017/dsd.prototxt">DSD</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/1607.04381.pdf">DSD: Dense-Sparse-Dense Training for Deep Neural Networks</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2017-ICLR-FF6B6B" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2017/lobs.prototxt">L-OBS</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/1705.07565.pdf">Learning to Prune Deep Neural Networks via Layer-wise Optimal Brain Surgeon</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2017-NeurIPS-FF1493" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/csyhhu/L-OBS" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2018/Z9R72EAT.prototxt">ADMM-pruning</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/1804.03294">A Systematic DNN Weight Pruning Framework using Alternating Direction Method of Multipliers</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2018-ECCV-3CB371" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/bzantium/pytorch-admm-pruning" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2020/2AL79IUH.prototxt">m</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Elsen_Fast_Sparse_ConvNets_CVPR_2020_paper.pdf">Fast Sparse ConvNets</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2020-CVPR-2E8B57" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/fastconvnets/cvpr2020" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2020/V3MFIRLV.prototxt">m</a></td>
<td style="text-align: left;"><a href="http://proceedings.mlr.press/v119/kurtz20a/kurtz20a.pdf">Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2020-ICML-FF8C00" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2020/movement_pruning.prototxt">Movement Pruning</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2005.07683">Movement Pruning: Adaptive Sparsity by Fine-Tuning</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2020/movement_pruning/mp.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2020-NeurIPS-FF1493" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/huggingface/block_movement_pruning" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2020/blocksparse.prototxt">blocksparse</a></td>
<td style="text-align: left;"><a href="https://cdn.openai.com/blocksparse/blocksparsepaper.pdf">GPU Kernels for Block-Sparse Weights</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2020-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/openai/blocksparse" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2021/OpenVINO.prototxt">OpenVINO</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/ICCV2021W/LPCV/papers/Lazarevich_Post-Training_Deep_Neural_Network_Pruning_via_Layer-Wise_Calibration_ICCVW_2021_paper.pdf">Post-training deep neural network pruning via layer-wise calibration</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2021-ICCV-green" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2021/sr-ste.prototxt">SR-STE</a></td>
<td style="text-align: left;"><a href="https://openreview.net/forum?id=K9bw7vqp_s">Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2021/sr-ste/sr-ste.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2021-ICLR-FF6B6B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/aojunzz/NM-sparsity" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2021/K7GSWQIC.prototxt">m</a></td>
<td style="text-align: left;"><a href="https://proceedings.neurips.cc/paper/2021/hash/6e8404c3b93a9527c8db241a1846599a-Abstract.html">Channel Permutations for N:M Sparsity</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2021-NeurIPS-FF1493" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/NVIDIA/apex" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2021/PUHJMVCM.prototxt">NMSparse</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2104.08378">Accelerating Sparse Deep Neural Networks</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2021-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2021/ITZS3TU3.prototxt">m</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2102.00554">Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2021-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2022/XZBX1Z9G.prototxt">m</a></td>
<td style="text-align: left;"><a href="https://aclanthology.org/2022.acl-long.16/">Sparse Progressive Distillation: Resolving Overfitting under Pretrain-and-Finetune Paradigm</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2022-ACL-4169E1" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/shaoyiHusky/SparseProgressiveDistillation" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2022/TextPruner.prototxt">TextPruner</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2203.15996">TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2022/TextPruner/textpruner.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2022-ACL-4169E1" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/airaria/TextPruner" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2022/2EQV34KV.prototxt">m</a></td>
<td style="text-align: left;"><a href="https://www.cerebras.net/blog/creating-sparse-gpt-3-models-with-iterative-pruning">Creating Sparse GPT-3 Models with Iterative Pruning</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2022-Blog-696969" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2022/spdy.prototxt">SPDY</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2201.13096">SPDY: Accurate Pruning with Speedup Guarantees</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2022/spdy/cover.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2022-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/IST-DASLab/spdy" /></td>
<td style="text-align: left;"><a href="../notes/2022/spdy/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2022/Sprint.prototxt">Sprint</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2209.00606">Sparse Attention Acceleration with Synergistic In-Memory Pruning and On-Chip Recomputation</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2022-MICRO-BA55D3" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2022/fisherpruning.prototxt">FisherPruning</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2204.09656v2">A Fast Post-Training Pruning Framework for Transformers</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2022/fisherpruning/cover.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2022-NeurIPS-FF1493" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/WoosukKwon/retraining-free-pruning" /></td>
<td style="text-align: left;"><a href="../notes/2022/fisherpruning/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2022/obc.prototxt">OBC</a></td>
<td style="text-align: left;"><a href="https://openreview.net/pdf?id=ksVGCOlOEba">Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2022-NeurIPS-FF1493" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/IST-DASLab/OBC" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2022/ComplementarySparsity.prototxt">Complementary Sparsity</a></td>
<td style="text-align: left;"><a href="https://iopscience.iop.org/article/10.1088/2634-4386/ac7c8a">Two Sparsities Are Better Than One: Unlocking the Performance Benefits of Sparse-Sparse Networks</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2022/ComplementarySparsity/cover.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2022-Neuromorphic_Computing_and_Engineering-40E0D0" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2022/ComplementarySparsity/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2022/DSA.prototxt">DSA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2110.11299v1">Transformer Acceleration with Dynamic Sparse Attention</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2022/DSA/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2022-TC-F08080" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2022/DSA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2022/44KWQAWO.prototxt">STA</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2208.06118">An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2022-VLSI-8B0000" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2022/oBERT.prototxt">oBERT</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2203.07259.pdf">The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2022-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/neuralmagic/sparseml" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/diffuser.prototxt">Diffuser</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2210.11794">Diffuser: Efficient Transformers with Multi-hop Attention Diffusion for Long Sequences</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/diffuser/diffuser.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-AAAI-FF4500" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/asFeng/Diffuser" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/grain.prototxt">GRAIN</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2212.07634">Gradient-based Intra-attention Pruning on Pre-trained Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/grain/grain.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-ACL-4169E1" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/airaria/GRAIN" /></td>
<td style="text-align: left;"><a href="../notes/2023/grain/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/smp.prototxt">SMP</a></td>
<td style="text-align: left;"><a href="https://aclanthology.org/2023.acl-long.35.pdf">Pruning Pre-trained Language Models Without Fine-Tuning</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/smp/smp.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-ACL-4169E1" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/kongds/SMP" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/PINS.prototxt">PINS</a></td>
<td style="text-align: left;"><a href="https://aclanthology.org/2023.findings-acl.573/">Pruning Pre-trained Language Models with Principled Importance and Self-regularization</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-ACL-4169E1" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/drsy/pins" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/simple.prototxt">SIMPLE</a></td>
<td style="text-align: left;"><a href="https://aclanthology.org/2023.findings-acl.692.pdf">Structured Pruning for Efficient Generative Pre-trained Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/simple/cover.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-ACL-4169E1" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2023/simple/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/WDCO13S6.prototxt">m</a></td>
<td style="text-align: left;"><a href="https://openreview.net/forum?id=SHlZcInS6C">Structural Pruning of Large Language Models via Neural Architecture Search</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/nas_pruning/nas_pruning.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-AutoML_Workshop-A9A9A9" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/awslabs/syne-tune" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/SparseViT.prototxt">SparseViT</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2303.17605">SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/SparseViT/sparsevit.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-CVPR-2E8B57" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mit-han-lab/sparsevit" /></td>
<td style="text-align: left;"><a href="./notes/2023/sparsevit/index.md">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/TorchSparse.prototxt">TorchSparse++</a></td>
<td style="text-align: left;"><a href="https://openaccess.thecvf.com/content/CVPR2023W/WAD/papers/Tang_TorchSparse_Efficient_Point_Cloud_Engine_CVPRW_2023_paper.pdf">TorchSparse++: Efficient Point Cloud Engine</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-CVPR_workshop-green" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mit-han-lab/torchsparse" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/MVUE.prototxt">MVUE</a></td>
<td style="text-align: left;"><a href="https://openreview.net/pdf?id=vuD2xEtxZcj">Minimum Variance Unbiased N:M Sparsity for the Neural Gradients</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-ICLR-FF6B6B" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/WMMGA0AR.prototxt">m</a></td>
<td style="text-align: left;"><a href="https://openreview.net/forum?id=TJ2nxciYCk-">The Lazy Neuron Phenomenon: On Emergence of Activation Sparsity in Transformers</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-ICLR-FF6B6B" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/dejavu.prototxt">Deja Vu</a></td>
<td style="text-align: left;"><a href="https://openreview.net/forum?id=wIPIhHd00i">Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/dejavu/dejavu.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/FMInference/DejaVu" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/sparsegpt.prototxt">SparseGPT</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2301.00774.pdf">SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot.</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/IST-DASLab/sparsegpt" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/LoSparse.prototxt">LoSparse</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2306.11222">Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/LoSparse/losparse.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/yxli2123/LoSparse" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/nmSPARSE.prototxt">nmSPARSE</a></td>
<td style="text-align: left;"><a href="https://proceedings.mlsys.org/paper_files/paper/2023/file/4552cedd396a308320209f75f56a5ad5-Paper-mlsys2023.pdf">Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-MLSys-DDA0DD" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/SparTA" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/ZipLM.prototxt">ZipLM</a></td>
<td style="text-align: left;"><a href="https://openreview.net/pdf?id=bPFFPueAxm">ZipLM: Inference-Aware Structured Pruning of Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/ZipLM/cover.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-NeurIPS-FF1493" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/IST-DASLab/ZipLM" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/VENOM.prototxt">VENOM</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2310.02065v1">VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/VENOM/vnm.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-SC-CD5C5C" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/UDC-GAC/venom" /></td>
<td style="text-align: left;"><a href="../notes/2023/VENOM/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/spdf.prototxt">SPDF</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2303.10464">SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-UAI-green" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/GBLM-Pruner.prototxt">GBLM-Pruner</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2311.04902v2">Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/GBLM-Pruner/gblm-pruner.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/VILA-Lab/GBLM-Pruner" /></td>
<td style="text-align: left;"><a href="../notes/2023/GBLM-Pruner/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/Compresso.prototxt">Compresso</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2310.05015">Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/Compresso/cover.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/Moonlit" /></td>
<td style="text-align: left;"><a href="../notes/2023/Compresso/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/adaptively_sparse_attention.prototxt">Adaptively Sparse Attention</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2305.15805">Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/adaptively_sparse_attention/adaptively_sparse_attention.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/kcm.prototxt">KCM</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2303.04185">Gradient-Free Structured Pruning with Unlabeled Data</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/kcm/kcm.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/H2O.prototxt">H2O</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2306.14048">H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/H2O/h2o.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/FMInference/H2O" /></td>
<td style="text-align: left;"><a href="../notes/2023/H2O/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/k_pruning.prototxt">K-pruning</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2308.03449">Knowledge-preserving Pruning for Pre-trained Language Models without Retraining</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/k_pruning/kp.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2023/k_pruning/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/LLM_in_a_flash.prototxt">LLM in a flash</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2312.11514">LLM in a flash: Efficient Large Language Model Inference with Limited Memory</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/LLM_in_a_flash/windows.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2023/LLM_in_a_flash/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/LLM-Pruner.prototxt">LLM-Pruner</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2305.11627v3">LLM-Pruner: On the Structural Pruning of Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/LLM-Pruner/cover.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/horseee/LLM-Pruner" /></td>
<td style="text-align: left;"><a href="../notes/2023/LLM-Pruner/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/lorashear.prototxt">LoRAShear</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2310.18356">LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/PowerInfer.prototxt">PowerInfer</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2312.12456v1">PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/SJTU-IPADS/PowerInfer" /></td>
<td style="text-align: left;"><a href="../notes/2023/PowerInfer/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/gbdt.prototxt">GBDT</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2309.09507">Pruning Large Language Models via Accuracy Predictor</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/gbdt/gbdt.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/LLM_shearing.prototxt">LLM-shearing</a></td>
<td style="text-align: left;"><a href="https://xiamengzhou.github.io/sheared-llama/">Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/LLM_shearing/cover.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/princeton-nlp/LLM-Shearing" /></td>
<td style="text-align: left;"><a href="../notes/2023/LLM_shearing/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/SquareHead.prototxt">SquareHead</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2310.06927.pdf">Sparse Fine-tuning for Inference Acceleration of Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/SquareHead/cover.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/IST-DASLab/SparseFinetuning" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/Sparse-IFT.prototxt">Sparse-IFT</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2303.11525">Sparse Iso-FLOP Transformations for Maximizing Training Efficiency</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/CerebrasResearch/Sparse-IFT" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/sms.prototxt">SMS</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2306.16788">Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/sms/sms.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ZIB-IOL/SMS" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/AYB1XUO5.prototxt">m</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2302.02596">Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/EssentialSparsity.prototxt">Essential Sparsity</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2306.03805">The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/VITA-Group/essential_sparsity" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/selective_context.prototxt">Selective Context</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2304.12102">Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/selective_context/selective_context.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/liyucheng09/Selective_Context" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/flap.prototxt">FLAP</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2312.11983">Fluctuation-based Adaptive Structured Pruning for Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/del/flap.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-AAAI-FF4500" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/CASIA-IVA-Lab/FLAP" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/CATS.prototxt">CATS</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2404.08763v4">CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/CATS/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-COLM-6495ED" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ScalingIntelligence/CATS" /></td>
<td style="text-align: left;"><a href="../notes/2024/CATS/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/SparseInfer.prototxt">SparseInfer</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2411.12692v1">SparseInfer: Training-free Prediction of Activation Sparsity for Fast LLM Inference</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-DATE-B22222" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/SparseInfer/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/SCAP.prototxt">SCAP</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2412.07174v1">Post-Training Statistical Calibration for Higher Activation Sparsity</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/SCAP/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ENLSP-00BFFF" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/IntelLabs/SCAP" /></td>
<td style="text-align: left;"><a href="../notes/2024/SCAP/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/Wanda.prototxt">Wanda</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2306.11695">A Simple and Effective Pruning Approach for Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/Wanda/wanda.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICLR-FF6B6B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/locuslab/wanda" /></td>
<td style="text-align: left;"><a href="../notes/2024/Wanda/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/VB8C61V6.prototxt">LLM-KICK</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2310.01382v2">Compressing LLMs: The Truth is Rarely Pure and Never Simple</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/VB8C61V6/llm-kick.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICLR-FF6B6B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/VITA-Group/llm-kick" /></td>
<td style="text-align: left;"><a href="../notes/2024/VB8C61V6/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/DSnoT.prototxt">DSnoT</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2310.08915v3">Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/DSnoT/dsnot.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICLR-FF6B6B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zyxxmu/DSnoT" /></td>
<td style="text-align: left;"><a href="../notes/2024/DSnoT/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/streaming-llm.prototxt">streaming-llm</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2309.17453v4">Efficient Streaming Language Models with Attention Sinks</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/streaming-llm/cover.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICLR-FF6B6B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mit-han-lab/streaming-llm" /></td>
<td style="text-align: left;"><a href="../notes/2024/streaming-llm/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/IA8CS3VH.prototxt">RIA</a></td>
<td style="text-align: left;"><a href="https://openreview.net/forum?id=Tr0lPx9woF">Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/del/Plug-and-Play.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICLR-FF6B6B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/biomedical-cybernetics/Relative-importance-and-activation-pruning" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/SAS.prototxt">SAS</a></td>
<td style="text-align: left;"><a href="https://openreview.net/forum?id=vZfi5to2Xl">SAS: Structured Activation Spasification</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/SAS/sas.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICLR-FF6B6B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/DensoITLab/sas_" /></td>
<td style="text-align: left;"><a href="../notes/2024/SAS/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/SliceGPT.prototxt">SliceGPT</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2401.15024v2">SliceGPT: Compress Large Language Models by Deleting Rows and Columns</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/SliceGPT/sliceGPT.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICLR-FF6B6B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/TransformerCompression" /></td>
<td style="text-align: left;"><a href="../notes/2024/SliceGPT/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/HMR7HKFV.prototxt">ReLU Strikes Back</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2310.04564">ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/HMR7HKFV/ReLU_Strikes_Back.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICLR_oral-green" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/sjtu-ipads/powerinfer" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/HYPL7G37.prototxt">m</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2404.01847v2">Accelerating Transformer Pre-training with 2:4 Sparsity</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/huyz2023/2by4-pretrain" /></td>
<td style="text-align: left;"><a href="../notes/2024/HYPL7G37/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/OSSCAR.prototxt">OSSCAR</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2403.12983v1">OSSCAR: One-Shot Structured Pruning in Vision and Language Models with Combinatorial Optimization</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mazumder-lab/OSSCAR" /></td>
<td style="text-align: left;"><a href="../notes/2024/OSSCAR/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/owl.prototxt">OWL</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2310.05175.pdf">Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/owl/cover.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/luuyin/OWL" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/Quest.prototxt">Quest</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2406.10774">Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/Quest/quest.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mit-han-lab/quest" /></td>
<td style="text-align: left;"><a href="../notes/2024/Quest/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/SPP.prototxt">SPP</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2405.16057v1">SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/SPP/spp.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Lucky-Lance/SPP" /></td>
<td style="text-align: left;"><a href="../notes/2024/SPP/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/SparQ.prototxt">SparQ</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2312.04985v5">SparQ Attention: Bandwidth-Efficient LLM Inference</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/SparQ/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/Sparse-IFT.prototxt">Sparse-IFT</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2303.11525v3">Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/Sparse-IFT/sparseIFT.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/CerebrasResearch/Sparse-IFT" /></td>
<td style="text-align: left;"><a href="../notes/2024/Sparse-IFT/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/MInference.prototxt">MInference</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2407.02490v1">MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/MInference/MInference_3shape.PNG" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-NeurIPS-FF1493" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/MInference" /></td>
<td style="text-align: left;"><a href="../notes/2024/MInference/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/MaskLLM.prototxt">MaskLLM</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2409.17481v1">MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/MaskLLM/maskllm.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-NeurIPS-FF1493" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/NVlabs/MaskLLM" /></td>
<td style="text-align: left;"><a href="../notes/2024/MaskLLM/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/SlimGPT.prototxt">SlimGPT</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2412.18110v1">SlimGPT: Layer-wise Structured Pruning for Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/SlimGPT/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-NeurIPS-FF1493" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/SlimGPT/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/SparseLLM.prototxt">SparseLLM</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2402.17946v3">SparseLLM: Towards Global Pruning for Pre-trained Language Models</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-NeurIPS-FF1493" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/BaiTheBest/SparseLLM" /></td>
<td style="text-align: left;"><a href="../notes/2024/SparseLLM/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/ADMM-pruning.prototxt">ADMM-pruning</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2401.02938v2">Fast and Effective Weight Update for Pruned Large Language Models</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-TMLR-20B2AA" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/fmfi-compbio/admm-pruning" /></td>
<td style="text-align: left;"><a href="../notes/2024/ADMM-pruning/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/flash_llm.prototxt">Flash-LLM</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2309.10285">Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/flash_llm/cover.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-VLDB-A52A2A" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/AlibabaResearch/flash-llm" /></td>
<td style="text-align: left;"><a href="../notes/2024/flash_llm/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/PWGG5HBE.prototxt">PWGG5HBE</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2412.19442v2">A Survey on Large Language Model Acceleration based on KV Cache Management</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/PWGG5HBE/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/TreeAI-Lab/Awesome-KV-Cache-Management" /></td>
<td style="text-align: left;"><a href="../notes/2024/PWGG5HBE/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/AVSS.prototxt">AVSS</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2411.02117v1">AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/AVSS/avss.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/AVSS/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/AdaKV.prototxt">AdaKV</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2407.11550v3">Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/AdaKV/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/FFY0/AdaKV" /></td>
<td style="text-align: left;"><a href="../notes/2024/AdaKV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/0Y41U1N2.prototxt">m</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2410.16135v1">Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/0Y41U1N2/cover.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/0Y41U1N2/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/SharedAttention.prototxt">SharedAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2407.12866v1">Beyond KV Caching: Shared Attention for Efficient LLMs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/SharedAttention/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/metacarbon/shareAtt" /></td>
<td style="text-align: left;"><a href="../notes/2024/SharedAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/Minitron.prototxt">Minitron</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2408.11796v2">Compact Language Models via Pruning and Knowledge Distillation</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/Minitron/minitron.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/NVlabs/Minitron" /></td>
<td style="text-align: left;"><a href="../notes/2024/Minitron/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/CoreInfer.prototxt">CoreInfer</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2410.18311v1">CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/CoreInfer/framework.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/wangqinsi1/CoreInfer" /></td>
<td style="text-align: left;"><a href="../notes/2024/CoreInfer/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/DuoAttention.prototxt">DuoAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2410.10819v1">DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/DuoAttention/duoattention.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mit-han-lab/duo-attention" /></td>
<td style="text-align: left;"><a href="../notes/2024/DuoAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/ULY1AZGY.prototxt">m</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2405.03594v1">Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/neuralmagic/nm-vllm" /></td>
<td style="text-align: left;"><a href="../notes/2024/ULY1AZGY/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/Bonsa.prototxt">Bonsa</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2402.05406">Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ldery/Bonsai" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/FlashMask.prototxt">FlashMask</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2410.01359v1">FlashMask: Efficient and Rich Mask Extension of FlashAttention</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/FlashMask/flashmask.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/PaddlePaddle/PaddleNLP" /></td>
<td style="text-align: left;"><a href="../notes/2024/FlashMask/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/MoA.prototxt">MoA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2406.14909v2">MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/MoA/moa.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/thu-nics/MoA" /></td>
<td style="text-align: left;"><a href="../notes/2024/MoA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/CHESS.prototxt">CHESS</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2409.01366v1">Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><a href="https://anonymous.4open.science/r/CHESS-BA40/README.md">Pytorch</a></td>
<td style="text-align: left;"><a href="../notes/2024/CHESS/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/DoubleSparsity.prototxt">DoubleSparsity</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2408.07092v2">Post-Training Sparse Attention with Double Sparsity</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/andy-yang-1/DoubleSparse" /></td>
<td style="text-align: left;"><a href="../notes/2024/DoubleSparsity/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/PowerInfer-2.prototxt">PowerInfer-2</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2406.06282v2">PowerInfer-2: Fast Large Language Model Inference on a Smartphone</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><a href="https://powerinfer.ai/v2/">Website</a></td>
<td style="text-align: left;"><a href="../notes/2024/PowerInfer-2/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/ProSparse.prototxt">ProSparse</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2402.13516">ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/ProSparse/prosparse.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/raincleared-song/sparse_gpu_operator" /></td>
<td style="text-align: left;"><a href="../notes/2024/ProSparse/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/Q-Sparse.prototxt">Q-Sparse</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2407.10969v1">Q-Sparse: All Large Language Models can be Fully Sparsely-Activated</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/Q-Sparse/q-sparse.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/Q-Sparse/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/ReLU2.prototxt">ReLU2</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2402.03804">ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/ReLU2/activation.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/ReLU2/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/RecycledAttention.prototxt">Recycled Attention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2411.05787v1">Recycled Attention: Efficient inference for long-context language models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/RecycledAttention/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/carriex/recycled-attention" /></td>
<td style="text-align: left;"><a href="../notes/2024/RecycledAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/SampleAttention.prototxt">SampleAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2406.15486v2">SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/SampleAttention/cover.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/SampleAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/SeerAttention.prototxt">SeerAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2410.13276v2">SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/SeerAttention/seerattention.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/SeerAttention" /></td>
<td style="text-align: left;"><a href="../notes/2024/SeerAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/ShadowLLM.prototxt">ShadowLLM</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2406.16635v1">ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/ShadowLLM/shadowLLM.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/abdelfattah-lab/shadow_llm" /></td>
<td style="text-align: left;"><a href="../notes/2024/ShadowLLM/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/SnapKV.prototxt">SnapKV</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2404.14469v2">SnapKV: LLM Knows What You are Looking for Before Generation</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/SnapKV/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/FasterDecoding/SnapKV" /></td>
<td style="text-align: left;"><a href="../notes/2024/SnapKV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/TOVA.prototxt">TOVA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2401.06104v2">Transformers are Multi-State RNNs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/TOVA/tova.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/schwartz-lab-NLP/TOVA" /></td>
<td style="text-align: left;"><a href="../notes/2024/TOVA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/TurboSparse.prototxt">Turbo Sparse</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2406.05955v2">Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><a href="https://huggingface.co/PowerInfer">Pytorch</a></td>
<td style="text-align: left;"><a href="../notes/2024/TurboSparse/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/ZigZagKV.prototxt">ZigZagKV</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2412.09036v1">ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/ZigZagKV/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/ZigZagKV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/AdaSkip.prototxt">AdaSkip</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2501.02336v1">AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/AdaSkip/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-AAAI-FF4500" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ASISys/AdaSkip" /></td>
<td style="text-align: left;"><a href="../notes/2025/AdaSkip/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/AdaptiveSparseTrainer.prototxt">AdaptiveSparseTrainer</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2407.20584v3">Pruning Large Language Models with Semi-Structural Adaptive Sparse Training</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/AdaptiveSparseTrainer/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-AAAI-FF4500" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/thu-ml/Adaptive-Sparse-Trainer" /></td>
<td style="text-align: left;"><a href="../notes/2025/AdaptiveSparseTrainer/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/NSA.prototxt">NSA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.11089v1">Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/NSA/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ACL-4169E1" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/NSA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/SDS.prototxt">SDS</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2408.10473v1">Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/SDS/sds.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-Coling-1E90FF" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/SDS/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/FlexPrefill.prototxt">FlexPrefill</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.20766v1">FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/FlexPrefill/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICLR-FF6B6B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/bytedance/FlexPrefill" /></td>
<td style="text-align: left;"><a href="../notes/2025/FlexPrefill/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/R-Sparse.prototxt">R-Sparse</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2504.19449v1">R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/R-Sparse/fig4.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICLR-FF6B6B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/VITA-Group/R-Sparse" /></td>
<td style="text-align: left;"><a href="../notes/2025/R-Sparse/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/ReAttention.prototxt">ReAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2407.15176v3">ReAttention: Training-Free Infinite Context with Finite Attention Scope</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/ReAttention/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICLR-FF6B6B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/OpenMOSS/ReAttention" /></td>
<td style="text-align: left;"><a href="../notes/2025/ReAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/TEAL.prototxt">TEAL</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2408.14690v1">Training-Free Activation Sparsity in Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/TEAL/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICLR-FF6B6B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/FasterDecoding/TEAL" /></td>
<td style="text-align: left;"><a href="../notes/2025/TEAL/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/AdaSplash.prototxt">AdaSplash</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.12082">AdaSplash: Adaptive Sparse Flash Attention</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/deep-spin/adasplash" /></td>
<td style="text-align: left;"><a href="../notes/2025/AdaSplash/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/BaWA.prototxt">BaWA</a></td>
<td style="text-align: left;"><a href="https://openreview.net/forum?id=YrCvW1Hx7g">BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/BaWA/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/BaWA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/CateKV.prototxt">CateKV</a></td>
<td style="text-align: left;"><a href="https://openreview.net/forum?id=u7dlwgKstN">CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/CateKV/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/CateKV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/PoD.prototxt">PoD</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2412.02252v1">Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/PoD/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/PoD/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/HashAttention.prototxt">HashAttention</a></td>
<td style="text-align: left;"><a href="https://openreview.net/forum?id=Em2oaXd8Dc">HashAttention: Semantic Sparsity for Faster Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/HashAttention/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/xAlg-ai/HashAttention-1.0" /></td>
<td style="text-align: left;"><a href="../notes/2025/HashAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/LaRoSA.prototxt">LaRoSA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2507.01299v1">La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/LaRoSA/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/LaRoSA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/MMInference.prototxt">MMInference</a></td>
<td style="text-align: left;"><a href="https://openreview.net/forum?id=me6PfbATWM">MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/MMInference/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/ShadowKV.prototxt">ShadowKV</a></td>
<td style="text-align: left;"><a href="https://openreview.net/forum?id=oa7MYAO6h6">ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/ShadowKV/shadowkv.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/bytedance/ShadowKV" /></td>
<td style="text-align: left;"><a href="../notes/2025/ShadowKV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/SlimLLM.prototxt">SlimLLM</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2505.22689v1">SlimLLM: Accurate Structured Pruning for Large Language Models</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/SlimLLM/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/SpargeAttn.prototxt">SpargeAttn</a></td>
<td style="text-align: left;"><a href="https://openreview.net/forum?id=74c3Wwk8Tc">SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/SpargeAttn/fig3.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/thu-ml/SpargeAttn" /></td>
<td style="text-align: left;"><a href="../notes/2025/SpargeAttn/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/SparsingLaw.prototxt">SparsingLaw</a></td>
<td style="text-align: left;"><a href="https://openreview.net/forum?id=SBUc5wirM8">Sparsing Law: Towards Large Language Models with Greater Activation Sparsity</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/SparsingLaw/fig4.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/thunlp/SparsingLaw" /></td>
<td style="text-align: left;"><a href="../notes/2025/SparsingLaw/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/XAttention.prototxt">XAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2503.16428v1">XAttention: Block Sparse Attention with Antidiagonal Scoring</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/XAttention/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mit-han-lab/x-attention" /></td>
<td style="text-align: left;"><a href="../notes/2025/XAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/SpecEE.prototxt">SpecEE</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2504.08850v1">SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/SpecEE/fig9.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ISCA-9932CC" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/infinigence/SpecEE" /></td>
<td style="text-align: left;"><a href="../notes/2025/SpecEE/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/LinearPatch.prototxt">LinearPatch</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2505.24680v1">A Simple Linear Patch Revives Layer-Pruned Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/LinearPatch/fig3.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/LinearPatch/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/Acc-SpMM.prototxt">Acc-SpMM</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2501.09251v1">Acc-SpMM: Accelerating General-purpose Sparse Matrix-Matrix Multiplication with GPU Tensor Cores</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/Acc-SpMM/fig3.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/Acc-SpMM/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/07NWF4VE.prototxt">07NWF4VE</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2504.06319v1">Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/07NWF4VE/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/07NWF4VE/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/SharePrefill.prototxt">SharePrefill</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2505.19578v1">Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/SharePrefill/fig3.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/SharePrefill/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/ACP.prototxt">ACP</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2504.06949v1">Adaptive Computation Pruning for the Forgetting Transformer</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zhixuan-lin/arctic-fox" /></td>
<td style="text-align: left;"><a href="../notes/2025/ACP/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/FlexiDepth.prototxt">FlexiDepth</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2503.23798v1">Adaptive Layer-skipping in Pre-trained LLMs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/FlexiDepth/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/FlexiDepth/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/AhaKV.prototxt">AhaKV</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2506.03762v1">AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/AhaKV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/AmberPruner.prototxt">AmberPruner</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2508.02128v1">Amber Pruner: Leveraging N:M Activation Sparsity for Efficient Prefill in Large Language Models</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/AmberPruner/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/AttentionPredictor.prototxt">AttentionPredictor</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.04077v1">AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/AttentionPredictor/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/AttentionPredictor/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/ChunkKV.prototxt">ChunkKV</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.00299v1">ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/ChunkKV/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/ChunkKV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/DBudgetKV.prototxt">DBudgetKV</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.16886v1">DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/DBudgetKV/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/DBudgetKV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/DeltaAttention.prototxt">DeltaAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2505.11254v1">Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/DeltaAttention/fig4.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/DeltaAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/DeltaLLM.prototxt">DeltaLLM</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2507.19608v1">DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/DeltaLLM/fig5.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/DeltaLLM/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/RaaS.prototxt">RaaS</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.11147v1">Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/RaaS/fig5.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/RaaS/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/topk-decoding.prototxt">topk-decoding</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.06766v2">Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/topk-decoding/alg1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ryansynk/topk-decoding" /></td>
<td style="text-align: left;"><a href="../notes/2025/topk-decoding/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/FastKV.prototxt">FastKV</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.01068v1">FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/FastKV/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/dongwonjo/FastKV" /></td>
<td style="text-align: left;"><a href="../notes/2025/FastKV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/FlashInfer.prototxt">FlashInfer</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2501.01005v2">FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/flashinfer-ai/flashinfer" /></td>
<td style="text-align: left;"><a href="../notes/2025/FlashInfer/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/HATA.prototxt">HATA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2506.02572v1">HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/HATA/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/gpzlx1/HATA" /></td>
<td style="text-align: left;"><a href="../notes/2025/HATA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/HCAttention.prototxt">HCAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2507.19823v1">HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/HCAttention/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/HCAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/IFPruning.prototxt">IFPruning</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2501.02086v2">Instruction-Following Pruning for Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/IFPruning/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/IFPruning/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/LServer.prototxt">LServer</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.14866v1">LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/LServer/fig5.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mit-han-lab/omniserve" /></td>
<td style="text-align: left;"><a href="../notes/2025/LServer/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/LeanK.prototxt">LeanK</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2508.02215v1">LeanK: Learnable K Cache Channel Pruning for Efficient Decoding</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/LeanK/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/MInference" /></td>
<td style="text-align: left;"><a href="../notes/2025/LeanK/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/MiniCPM4.prototxt">MiniCPM4</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2506.07900v1">MiniCPM4: Ultra-Efficient LLMs on End Devices</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/MiniCPM4/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/openbmb/minicpm" /></td>
<td style="text-align: left;"><a href="../notes/2025/MiniCPM4/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/MoSA.prototxt">MoSA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2505.00315v1">Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/MoSA/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/piotrpiekos/MoSA" /></td>
<td style="text-align: left;"><a href="../notes/2025/MoSA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/MoBA.prototxt">MoBA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.13189v1">MoBA: Mixture of Block Attention for Long-Context LLMs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/MoBA/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/MoonshotAI/MoBA" /></td>
<td style="text-align: left;"><a href="../notes/2025/MoBA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/Mosaic.prototxt">Mosaic</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2504.06323v1">Mosaic: Composite Projection Pruning for Resource-efficient LLMs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/Mosaic/fig6.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/Mosaic/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/PowerAttention.prototxt">PowerAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2503.03588v1">PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/PowerAttention/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/PowerAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/PSA.prototxt">PSA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2503.00392v1">Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/PSA/fig4.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ASISys/PSAttention" /></td>
<td style="text-align: left;"><a href="../notes/2025/PSA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/Cus-Prun.prototxt">Cus-Prun</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2506.02561v1">Pruning General Large Language Models into Customized Expert Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/Cus-Prun/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zhaoyiran924/Custom-Prune" /></td>
<td style="text-align: left;"><a href="../notes/2025/Cus-Prun/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/QuickSilver.prototxt">QuickSilver</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2506.22396v1">QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/QuickSilver/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/R-KV.prototxt">R-KV</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2505.24133v2">R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/R-KV/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Zefan-Cai/R-KV" /></td>
<td style="text-align: left;"><a href="../notes/2025/R-KV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/RadialAttention.prototxt">RadialAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2506.19852v1">Radial Attention: $O(n\log n)$ Sparse Attention with Energy Decay for Long Video Generation</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/RadialAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/ReSA.prototxt">ReSA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2506.04108v2">Rectified Sparse Attention</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/ReSA/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/unilm" /></td>
<td style="text-align: left;"><a href="../notes/2025/ReSA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/0VRXJQ3F.prototxt">0VRXJQ3F</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2503.24000v1">Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/0VRXJQ3F/tab1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/LLMkvsys/rethink-kv-compression" /></td>
<td style="text-align: left;"><a href="../notes/2025/0VRXJQ3F/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/SALE.prototxt">SALE</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2505.24179v1">SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/SALE/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/BirdChristopher/SALE" /></td>
<td style="text-align: left;"><a href="../notes/2025/SALE/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/SEAP.prototxt">SEAP</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2503.07605v1">SEAP: Training-free Sparse Expert Activation Pruning Unlock the Brainpower of Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/SEAP/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/IAAR-Shanghai/SEAP" /></td>
<td style="text-align: left;"><a href="../notes/2025/SEAP/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/SeerAttention-R.prototxt">SeerAttention-R</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2506.08889v1">SeerAttention-R: Sparse Attention Adaptation for Long Reasoning</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/SeerAttention-R/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/SeerAttention" /></td>
<td style="text-align: left;"><a href="../notes/2025/SeerAttention-R/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/SpindleKV.prototxt">SpindleKV</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2507.06517v1">SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/SpindleKV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/Task-KV.prototxt">Task-KV</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2501.15113v1">Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/Task-KV/fig6.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/Task-KV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/Super-Experts-Profilling.prototxt">Super-Experts-Profilling</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2507.23279v1">Unveiling Super Experts in Mixture-of-Experts Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/Super-Experts-Profilling/fig3.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ZunhaiSu/Super-Experts-Profilling" /></td>
<td style="text-align: left;"><a href="../notes/2025/Super-Experts-Profilling/note/">note</a></td>
</tr>
</tbody>
</table>
<h4 id="07-quantization">07-Quantization</h4>
<table>
<thead>
<tr>
<th style="text-align: left;">Meta</th>
<th style="text-align: left;">Title</th>
<th style="text-align: left;">Cover</th>
<th style="text-align: left;">Publish</th>
<th style="text-align: left;">Code</th>
<th style="text-align: left;">Note</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><a href="../meta/2016/deepcompression.prototxt">Deep Compression</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/1510.00149.pdf">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2016-ICLR-FF6B6B" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2019/actnn.prototxt">ActNN</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2104.14129">ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2019-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ucbrise/actnn" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2021/brecq.prototxt">BRECQ</a></td>
<td style="text-align: left;"><a href="https://openreview.net/pdf?id=POWv6hDd9XH">BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2021-ICLR-FF6B6B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/yhhhli/BRECQ" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2021/gpfq.prototxt">GPFQ</a></td>
<td style="text-align: left;"><a href="https://jmlr.csail.mit.edu/papers/volume22/20-1233/20-1233.pdf">A Greedy Algorithm for Quantizing Neural Networks</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2021-JMLR-008B8B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/YixuanSeanZhou/Quantized_Neural_Nets" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2022/obc.prototxt">OBC</a></td>
<td style="text-align: left;"><a href="https://openreview.net/pdf?id=ksVGCOlOEba">Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2022-NeurIPS-FF1493" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/IST-DASLab/OBC" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2022/zeroquant.prototxt">ZeroQuant</a></td>
<td style="text-align: left;"><a href="https://openreview.net/forum?id=f-fVCElZ-G1">ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2022-NeurIPS-FF1493" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/DeepSpeed" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/gptq.prototxt">GPTQ</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2210.17323.pdf">GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-ICLR-FF6B6B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/IST-DASLab/gptq" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/loftq.prototxt">LoftQ</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2310.08659">LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/loftq/loftq.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/yxli2123/LoftQ" /></td>
<td style="text-align: left;"><a href="../notes/2023/loftq/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/omniquant.prototxt">OmniQuant</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2308.13137">OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/omniquant/omniquant.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/OpenGVLab/OmniQuant" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/gpfqv2.prototxt">GPFQv2</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2201.11113.pdf">Post-training Quantization for Neural Networks with Provable Guarantees</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/YixuanSeanZhou/Quantized_Neural_Nets" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/qlora.prototxt">QLoRA</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2305.14314">QLoRA: Efficient Finetuning of Quantized LLMs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/qlora/qlora.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/artidoro/qlora" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/QuIP.prototxt">QuIP</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2307.13304.pdf">QuIP: Quantization with Incoherence Processing</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/jerry-chee/QuIP" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/RPTQ.prototxt">RPTQ</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2304.01089.pdf">RPTQ: Reorder-based Post-training Quantization for Large Language Models</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/hahnyuan/RPTQ4LLM" /></td>
<td style="text-align: left;"><a href="note.md">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/spqr.prototxt">SpQR</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2306.03078.pdf">SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Vahe1994/SpQR" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/23LQ9SVH.prototxt">m</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs//2306.11987">Training Transformers with 4-bit Integers</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/xijiu9/Train_Transformers_with_INT4" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/ZeroQuant-V2.prototxt">ZeroQuant-V2</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2303.08302">ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/DeepSpeed" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/QA-LoRA.prototxt">QA-LoRA</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2309.14717">QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/QA-LoRA/qalora.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICLR-FF6B6B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/yuhuixu1993/qa-lora" /></td>
<td style="text-align: left;"><a href="../notes/2024/QA-LoRA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/FrameQuant.prototxt">FrameQuant</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2403.06082v1">FrameQuant: Flexible Low-Bit Quantization for Transformers</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/FrameQuant/framequant.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/vsingh-group/FrameQuant" /></td>
<td style="text-align: left;"><a href="../notes/2024/FrameQuant/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/SqueezeLLM.prototxt">SqueezeLLM</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2306.07629v4">SqueezeLLM: Dense-and-Sparse Quantization</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/SqueezeLLM/squeezeLLM.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/SqueezeAILab/SqueezeLLM" /></td>
<td style="text-align: left;"><a href="../notes/2024/SqueezeLLM/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/awq.prototxt">AWQ</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2306.00978">AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-MLSys-DDA0DD" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mit-han-lab/llm-awq" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/KVQuant.prototxt">KVQuant</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2401.18079v2">KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/SqueezeAILab/KVQuant" /></td>
<td style="text-align: left;"><a href="../notes/2024/KVQuant/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/L4Q.prototxt">L4Q</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2402.04902">L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/L4Q/l4q.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/L4Q/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/MiniKV.prototxt">MiniKV</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2411.18077v2">MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/MiniKV/minikv.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/MiniKV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/Q-Sparse.prototxt">Q-Sparse</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2407.10969v1">Q-Sparse: All Large Language Models can be Fully Sparsely-Activated</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/Q-Sparse/q-sparse.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/Q-Sparse/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/QServer.prototxt">QServe</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2405.04532v2">QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><a href="https://hanlab.mit.edu/projects/qserve">Pytorch</a></td>
<td style="text-align: left;"><a href="../notes/2024/QServer/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/COMET.prototxt">COMET</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2410.12168v1">COMET: Towards Partical W4A4KV4 LLMs Serving</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/COMET/fig5.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ASPLOS-9370DB" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/COMET/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/TorchAO.prototxt">TorchAO</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2507.16099v1">TorchAO: PyTorch-Native Training-to-Serving Model Optimization</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/TorchAO/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML_Workshop-green" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/pytorch/ao" /></td>
<td style="text-align: left;"><a href="../notes/2025/TorchAO/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/CCQ.prototxt">CCQ</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2507.07145v1">CCQ: Convolutional Code for Extreme Low-bit Quantization in LLMs</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/CCQ/note/">note</a></td>
</tr>
</tbody>
</table>
<h4 id="08-survey">08-Survey</h4>
<table>
<thead>
<tr>
<th style="text-align: left;">Meta</th>
<th style="text-align: left;">Title</th>
<th style="text-align: left;">Cover</th>
<th style="text-align: left;">Publish</th>
<th style="text-align: left;">Code</th>
<th style="text-align: left;">Note</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><a href="../meta/2021/ITZS3TU3.prototxt">m</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2102.00554">Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2021-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/68I8KKBV.prototxt">m</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2209.00099">Efficient Methods for Natural Language Processing: A Survey</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/del/survey/efficient_NLP.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-TACL-87CEEB" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/ELILXDQG.prototxt">m</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2307.03109">A Survey on Evaluation of Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/del/survey/eval_LLM.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/L5D7520E.prototxt">m</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2308.07633">A Survey on Model Compression for Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/del/survey/compression_LLM.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/AYB1XUO5.prototxt">m</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2302.02596">Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/JSHWEV0S.prototxt">m</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2407.18003v3">Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/JSHWEV0S/overview.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-COLM-6495ED" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zcli-charlie/Awesome-KV-Cache" /></td>
<td style="text-align: left;"><a href="../notes/2024/JSHWEV0S/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/VB8C61V6.prototxt">LLM-KICK</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2310.01382v2">Compressing LLMs: The Truth is Rarely Pure and Never Simple</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/VB8C61V6/llm-kick.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICLR-FF6B6B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/VITA-Group/llm-kick" /></td>
<td style="text-align: left;"><a href="../notes/2024/VB8C61V6/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/DHIB73MC.prototxt">m</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2404.14294v2">A Survey on Efficient Inference for Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/DHIB73MC/efficientinference.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/DHIB73MC/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/068ZPAME.prototxt">068ZPAME</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2412.14219v2">A Survey on Inference Optimization Techniques for Mixture of Experts Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/068ZPAME/tab1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/MoE-Inf/awesome-moe-inference" /></td>
<td style="text-align: left;"><a href="../notes/2024/068ZPAME/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/PWGG5HBE.prototxt">PWGG5HBE</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2412.19442v2">A Survey on Large Language Model Acceleration based on KV Cache Management</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/PWGG5HBE/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/TreeAI-Lab/Awesome-KV-Cache-Management" /></td>
<td style="text-align: left;"><a href="../notes/2024/PWGG5HBE/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/YS9YTT55.prototxt">YS9YTT55</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2407.12391v1">LLM Inference Serving: Survey of Recent Advances and Opportunities</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/YS9YTT55/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/massive-activations.prototxt">massive-activations</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2402.17762v2">Massive Activations in Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/massive-activations/massive_act.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/locuslab/massive-activations" /></td>
<td style="text-align: left;"><a href="../notes/2024/massive-activations/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/209M5GA7.prototxt">209M5GA7</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2508.06297v1">KV Cache Compression for Inference Efficiency in LLMs: A Review</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/209M5GA7/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/52A7RO95.prototxt">52A7RO95</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2507.11181v1">Mixture of Experts in Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/52A7RO95/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/52A7RO95/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/0VRXJQ3F.prototxt">0VRXJQ3F</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2503.24000v1">Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/0VRXJQ3F/tab1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/LLMkvsys/rethink-kv-compression" /></td>
<td style="text-align: left;"><a href="../notes/2025/0VRXJQ3F/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/sparse-frontier.prototxt">sparse-frontier</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2504.17768v1">The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/sparse-frontier/tb1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/PiotrNawrot/sparse-frontier" /></td>
<td style="text-align: left;"><a href="../notes/2025/sparse-frontier/note/">note</a></td>
</tr>
</tbody>
</table>
<h4 id="09-low-rank-decomposition">09-Low Rank Decomposition</h4>
<table>
<thead>
<tr>
<th style="text-align: left;">Meta</th>
<th style="text-align: left;">Title</th>
<th style="text-align: left;">Cover</th>
<th style="text-align: left;">Publish</th>
<th style="text-align: left;">Code</th>
<th style="text-align: left;">Note</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><a href="../meta/2022/lora.prototxt">LoRA</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2106.09685">LoRA: Low-rank adaptation of large language models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2022/lora/lora.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2022-ICLR-FF6B6B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/LoRA" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/adalora.prototxt">AdaLoRA</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2303.10512.pdf">AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/adalora/adalora.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-ICLR-FF6B6B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/QingruZhang/AdaLoRA" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/LoSparse.prototxt">LoSparse</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2306.11222">Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/LoSparse/losparse.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/yxli2123/LoSparse" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/lorashear.prototxt">LoRAShear</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2310.18356">LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/loftq.prototxt">LoftQ</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2310.08659">LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/loftq/loftq.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/yxli2123/LoftQ" /></td>
<td style="text-align: left;"><a href="../notes/2023/loftq/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/qlora.prototxt">QLoRA</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2305.14314">QLoRA: Efficient Finetuning of Quantized LLMs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/qlora/qlora.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/artidoro/qlora" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/QA-LoRA.prototxt">QA-LoRA</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2309.14717">QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/QA-LoRA/qalora.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICLR-FF6B6B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/yuhuixu1993/qa-lora" /></td>
<td style="text-align: left;"><a href="../notes/2024/QA-LoRA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/L4Q.prototxt">L4Q</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2402.04902">L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/L4Q/l4q.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/L4Q/note/">note</a></td>
</tr>
</tbody>
</table>
<h4 id="10-layer-fusion-reduce-io">10-Layer Fusion (Reduce IO)</h4>
<table>
<thead>
<tr>
<th style="text-align: left;">Meta</th>
<th style="text-align: left;">Title</th>
<th style="text-align: left;">Cover</th>
<th style="text-align: left;">Publish</th>
<th style="text-align: left;">Code</th>
<th style="text-align: left;">Note</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><a href="../meta/2022/flashattention.prototxt">FlashAttention</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2205.14135">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2022/flashattention/cover.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2022-NeurIPS-FF1493" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Dao-AILab/flash-attention" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/flashattention2.prototxt">FlashAttention-2</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2307.08691">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Dao-AILab/flash-attention" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/GLA.prototxt">GLA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2505.21487v1">Hardware-Efficient Attention for Fast Decoding</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/GLA/gla.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Dao-AILab/grouped-latent-attention" /></td>
<td style="text-align: left;"><a href="../notes/2025/GLA/note/">note</a></td>
</tr>
</tbody>
</table>
<h4 id="11-tool">11-Tool</h4>
<table>
<thead>
<tr>
<th style="text-align: left;">Meta</th>
<th style="text-align: left;">Title</th>
<th style="text-align: left;">Cover</th>
<th style="text-align: left;">Publish</th>
<th style="text-align: left;">Code</th>
<th style="text-align: left;">Note</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><a href="../meta/2023/fastertransfomer.prototxt">FT</a></td>
<td style="text-align: left;"><a href="https://github.com/NVIDIA/FasterTransformer">FasterTransformer</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-github-2F4F4F" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/NVIDIA/FasterTransformer" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/SCBench.prototxt">SCBench</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2412.10319v2">SCBench: A KV Cache-Centric Analysis of Long-Context Methods</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/MInference" /></td>
<td style="text-align: left;"><a href="../notes/2024/SCBench/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/TorchAO.prototxt">TorchAO</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2507.16099v1">TorchAO: PyTorch-Native Training-to-Serving Model Optimization</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/TorchAO/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML_Workshop-green" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/pytorch/ao" /></td>
<td style="text-align: left;"><a href="../notes/2025/TorchAO/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/FlashInfer.prototxt">FlashInfer</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2501.01005v2">FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/flashinfer-ai/flashinfer" /></td>
<td style="text-align: left;"><a href="../notes/2025/FlashInfer/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/KVCache-Factory.prototxt">KVCache-Factory</a></td>
<td style="text-align: left;"><a href="https://github.com/Zefan-Cai/KVCache-Factory">Unified KV Cache Compression Methods for Auto-Regressive Models</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-github-2F4F4F" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Zefan-Cai/KVCache-Factory" /></td>
<td style="text-align: left;"><a href="../notes/2025/KVCache-Factory/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/kvpress.prototxt">kvpress</a></td>
<td style="text-align: left;">kvpress</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-github-2F4F4F" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/NVIDIA/kvpress" /></td>
<td style="text-align: left;"><a href="../notes/2025/kvpress/note/">note</a></td>
</tr>
</tbody>
</table>
<h4 id="12-kv-cache-optimizationefficient-attention">12-KV Cache Optimization/Efficient Attention</h4>
<table>
<thead>
<tr>
<th style="text-align: left;">Meta</th>
<th style="text-align: left;">Title</th>
<th style="text-align: left;">Cover</th>
<th style="text-align: left;">Publish</th>
<th style="text-align: left;">Code</th>
<th style="text-align: left;">Note</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><a href="../meta/2022/DSA.prototxt">DSA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2110.11299v1">Transformer Acceleration with Dynamic Sparse Attention</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2022/DSA/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2022-TC-F08080" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2022/DSA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/PagedAttention.prototxt">Paged Attention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2309.06180v1">Efficient Memory Management for Large Language Model Serving with PagedAttention</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/PagedAttention/vllm.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-SOSP-8A2BE2" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/vllm-project/vllm" /></td>
<td style="text-align: left;"><a href="../notes/2023/PagedAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/FlashDecoding.prototxt">Flash-Decoding</a></td>
<td style="text-align: left;"><a href="https://crfm.stanford.edu/2023/10/12/flashdecoding.html">Flash-Decoding for long-context inference</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2023/FlashDecoding/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/H2O.prototxt">H2O</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2306.14048">H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/H2O/h2o.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/FMInference/H2O" /></td>
<td style="text-align: left;"><a href="../notes/2023/H2O/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/ChunkAttention.prototxt">ChunkAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2402.15220v4">ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/ChunkAttention/chunkattn.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ACL-4169E1" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/chunk-attention" /></td>
<td style="text-align: left;"><a href="../notes/2024/ChunkAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/CachedAttention.prototxt">CachedAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2403.19708v3">Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/CachedAttention/fig5.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ATC-DC143C" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/CachedAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/JSHWEV0S.prototxt">m</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2407.18003v3">Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/JSHWEV0S/overview.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-COLM-6495ED" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zcli-charlie/Awesome-KV-Cache" /></td>
<td style="text-align: left;"><a href="../notes/2024/JSHWEV0S/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/streaming-llm.prototxt">streaming-llm</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2309.17453v4">Efficient Streaming Language Models with Attention Sinks</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/streaming-llm/cover.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICLR-FF6B6B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mit-han-lab/streaming-llm" /></td>
<td style="text-align: left;"><a href="../notes/2024/streaming-llm/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/Quest.prototxt">Quest</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2406.10774">Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/Quest/quest.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mit-han-lab/quest" /></td>
<td style="text-align: left;"><a href="../notes/2024/Quest/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/SparQ.prototxt">SparQ</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2312.04985v5">SparQ Attention: Bandwidth-Efficient LLM Inference</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/SparQ/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/MInference.prototxt">MInference</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2407.02490v1">MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/MInference/MInference_3shape.PNG" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-NeurIPS-FF1493" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/MInference" /></td>
<td style="text-align: left;"><a href="../notes/2024/MInference/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/SGLang.prototxt">SGLang</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2312.07104v2">SGLang: Efficient Execution of Structured Language Model Programs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/SGLang/fig9.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-NeurIPS-FF1493" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/sgl-project/sglang" /></td>
<td style="text-align: left;"><a href="../notes/2024/SGLang/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/PWGG5HBE.prototxt">PWGG5HBE</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2412.19442v2">A Survey on Large Language Model Acceleration based on KV Cache Management</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/PWGG5HBE/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/TreeAI-Lab/Awesome-KV-Cache-Management" /></td>
<td style="text-align: left;"><a href="../notes/2024/PWGG5HBE/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/AdaKV.prototxt">AdaKV</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2407.11550v3">Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/AdaKV/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/FFY0/AdaKV" /></td>
<td style="text-align: left;"><a href="../notes/2024/AdaKV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/SharedAttention.prototxt">SharedAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2407.12866v1">Beyond KV Caching: Shared Attention for Efficient LLMs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/SharedAttention/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/metacarbon/shareAtt" /></td>
<td style="text-align: left;"><a href="../notes/2024/SharedAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/DuoAttention.prototxt">DuoAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2410.10819v1">DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/DuoAttention/duoattention.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mit-han-lab/duo-attention" /></td>
<td style="text-align: left;"><a href="../notes/2024/DuoAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/FlashMask.prototxt">FlashMask</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2410.01359v1">FlashMask: Efficient and Rich Mask Extension of FlashAttention</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/FlashMask/flashmask.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/PaddlePaddle/PaddleNLP" /></td>
<td style="text-align: left;"><a href="../notes/2024/FlashMask/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/DistAttention.prototxt">DistAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2401.02669v2">Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/DistAttention/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/DistAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/KVQuant.prototxt">KVQuant</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2401.18079v2">KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/SqueezeAILab/KVQuant" /></td>
<td style="text-align: left;"><a href="../notes/2024/KVQuant/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/MiniKV.prototxt">MiniKV</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2411.18077v2">MiniKV: Pushing the Limits of LLM Inference via 2-Bit Layer-Discriminative KV Cache</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/MiniKV/minikv.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/MiniKV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/MoA.prototxt">MoA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2406.14909v2">MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/MoA/moa.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/thu-nics/MoA" /></td>
<td style="text-align: left;"><a href="../notes/2024/MoA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/MFA.prototxt">MFA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2412.19255v2">Multi-matrix Factorization Attention</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/MFA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/DoubleSparsity.prototxt">DoubleSparsity</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2408.07092v2">Post-Training Sparse Attention with Double Sparsity</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/andy-yang-1/DoubleSparse" /></td>
<td style="text-align: left;"><a href="../notes/2024/DoubleSparsity/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/RecycledAttention.prototxt">Recycled Attention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2411.05787v1">Recycled Attention: Efficient inference for long-context language models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/RecycledAttention/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/carriex/recycled-attention" /></td>
<td style="text-align: left;"><a href="../notes/2024/RecycledAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/CLA.prototxt">CLA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2405.12981v1">Reducing Transformer Key-Value Cache Size with Cross-Layer Attention</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/CLA/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/JerryYin777/Cross-Layer-Attention" /></td>
<td style="text-align: left;"><a href="../notes/2024/CLA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/SampleAttention.prototxt">SampleAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2406.15486v2">SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/SampleAttention/cover.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/SampleAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/SeerAttention.prototxt">SeerAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2410.13276v2">SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/SeerAttention/seerattention.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/SeerAttention" /></td>
<td style="text-align: left;"><a href="../notes/2024/SeerAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/SnapKV.prototxt">SnapKV</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2404.14469v2">SnapKV: LLM Knows What You are Looking for Before Generation</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/SnapKV/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/FasterDecoding/SnapKV" /></td>
<td style="text-align: left;"><a href="../notes/2024/SnapKV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/TOVA.prototxt">TOVA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2401.06104v2">Transformers are Multi-State RNNs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/TOVA/tova.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/schwartz-lab-NLP/TOVA" /></td>
<td style="text-align: left;"><a href="../notes/2024/TOVA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/ZigZagKV.prototxt">ZigZagKV</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2412.09036v1">ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/ZigZagKV/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/ZigZagKV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/NSA.prototxt">NSA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.11089v1">Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/NSA/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ACL-4169E1" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/NSA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/COMET.prototxt">COMET</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2410.12168v1">COMET: Towards Partical W4A4KV4 LLMs Serving</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/COMET/fig5.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ASPLOS-9370DB" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/COMET/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/POD-Attention.prototxt">POD-Attention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2410.18038v2">POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/POD-Attention/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ASPLOS-9370DB" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/vattention" /></td>
<td style="text-align: left;"><a href="../notes/2025/POD-Attention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/vAttention.prototxt">vAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2405.04437v3">vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/vAttention/fig5.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ASPLOS-9370DB" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/vattention" /></td>
<td style="text-align: left;"><a href="../notes/2025/vAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/KVSink.prototxt">KVSink</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2508.04257v1">KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/KVSink/fig8.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-COLM-6495ED" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/KVSink/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/FlexPrefill.prototxt">FlexPrefill</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.20766v1">FlexPrefill: A Context-Aware Sparse Attention Mechanism for Efficient Long-Sequence Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/FlexPrefill/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICLR-FF6B6B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/bytedance/FlexPrefill" /></td>
<td style="text-align: left;"><a href="../notes/2025/FlexPrefill/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/ReAttention.prototxt">ReAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2407.15176v3">ReAttention: Training-Free Infinite Context with Finite Attention Scope</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/ReAttention/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICLR-FF6B6B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/OpenMOSS/ReAttention" /></td>
<td style="text-align: left;"><a href="../notes/2025/ReAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/AdaSplash.prototxt">AdaSplash</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.12082">AdaSplash: Adaptive Sparse Flash Attention</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/deep-spin/adasplash" /></td>
<td style="text-align: left;"><a href="../notes/2025/AdaSplash/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/CateKV.prototxt">CateKV</a></td>
<td style="text-align: left;"><a href="https://openreview.net/forum?id=u7dlwgKstN">CateKV: On Sequential Consistency for Long-Context LLM Inference Acceleration</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/CateKV/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/CateKV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/PoD.prototxt">PoD</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2412.02252v1">Compressing KV Cache for Long-Context LLM Inference with Inter-Layer Attention Similarity</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/PoD/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/PoD/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/HashAttention.prototxt">HashAttention</a></td>
<td style="text-align: left;"><a href="https://openreview.net/forum?id=Em2oaXd8Dc">HashAttention: Semantic Sparsity for Faster Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/HashAttention/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/xAlg-ai/HashAttention-1.0" /></td>
<td style="text-align: left;"><a href="../notes/2025/HashAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/MMInference.prototxt">MMInference</a></td>
<td style="text-align: left;"><a href="https://openreview.net/forum?id=me6PfbATWM">MMInference: Accelerating Pre-filling for Long-Context VLMs via Modality-Aware Permutation Sparse Attention</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/MMInference/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/ShadowKV.prototxt">ShadowKV</a></td>
<td style="text-align: left;"><a href="https://openreview.net/forum?id=oa7MYAO6h6">ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/ShadowKV/shadowkv.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/bytedance/ShadowKV" /></td>
<td style="text-align: left;"><a href="../notes/2025/ShadowKV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/SpargeAttn.prototxt">SpargeAttn</a></td>
<td style="text-align: left;"><a href="https://openreview.net/forum?id=74c3Wwk8Tc">SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/SpargeAttn/fig3.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/thu-ml/SpargeAttn" /></td>
<td style="text-align: left;"><a href="../notes/2025/SpargeAttn/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/StarAttention.prototxt">StarAttention</a></td>
<td style="text-align: left;"><a href="https://openreview.net/forum?id=QY7Au9nZwp">Star Attention: Efficient LLM Inference over Long Sequences</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/StarAttention/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/NVIDIA/Star-Attention" /></td>
<td style="text-align: left;"><a href="../notes/2025/StarAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/XAttention.prototxt">XAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2503.16428v1">XAttention: Block Sparse Attention with Antidiagonal Scoring</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/XAttention/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mit-han-lab/x-attention" /></td>
<td style="text-align: left;"><a href="../notes/2025/XAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/07NWF4VE.prototxt">07NWF4VE</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2504.06319v1">Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/07NWF4VE/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/07NWF4VE/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/SharePrefill.prototxt">SharePrefill</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2505.19578v1">Accelerating Prefilling for Long-Context LLMs via Sparse Pattern Sharing</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/SharePrefill/fig3.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/SharePrefill/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/ACP.prototxt">ACP</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2504.06949v1">Adaptive Computation Pruning for the Forgetting Transformer</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zhixuan-lin/arctic-fox" /></td>
<td style="text-align: left;"><a href="../notes/2025/ACP/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/AhaKV.prototxt">AhaKV</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2506.03762v1">AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/AhaKV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/AttentionPredictor.prototxt">AttentionPredictor</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.04077v1">AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/AttentionPredictor/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/AttentionPredictor/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/ChunkKV.prototxt">ChunkKV</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.00299v1">ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/ChunkKV/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/ChunkKV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/DBudgetKV.prototxt">DBudgetKV</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.16886v1">DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/DBudgetKV/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/DBudgetKV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/DeltaAttention.prototxt">DeltaAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2505.11254v1">Delta Attention: Fast and Accurate Sparse Attention Inference by Delta Correction</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/DeltaAttention/fig4.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/DeltaAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/DeltaLLM.prototxt">DeltaLLM</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2507.19608v1">DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/DeltaLLM/fig5.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/DeltaLLM/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/RaaS.prototxt">RaaS</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.11147v1">Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/RaaS/fig5.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/RaaS/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/topk-decoding.prototxt">topk-decoding</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.06766v2">Exploiting Sparsity for Long Context Inference: Million Token Contexts on Commodity GPUs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/topk-decoding/alg1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ryansynk/topk-decoding" /></td>
<td style="text-align: left;"><a href="../notes/2025/topk-decoding/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/2ZU1IWL6.prototxt">2ZU1IWL6</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2507.02754v1">Fast and Simplex: 2-Simplicial Attention in Triton</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/2ZU1IWL6/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/FastKV.prototxt">FastKV</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.01068v1">FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/FastKV/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/dongwonjo/FastKV" /></td>
<td style="text-align: left;"><a href="../notes/2025/FastKV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/FlashInfer.prototxt">FlashInfer</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2501.01005v2">FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/flashinfer-ai/flashinfer" /></td>
<td style="text-align: left;"><a href="../notes/2025/FlashInfer/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/FreqKV.prototxt">FreqKV</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2505.00570v2">FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/FreqKV/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/FreqKV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/HATA.prototxt">HATA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2506.02572v1">HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/HATA/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/gpzlx1/HATA" /></td>
<td style="text-align: left;"><a href="../notes/2025/HATA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/HCAttention.prototxt">HCAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2507.19823v1">HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/HCAttention/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/HCAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/GLA.prototxt">GLA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2505.21487v1">Hardware-Efficient Attention for Fast Decoding</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/GLA/gla.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Dao-AILab/grouped-latent-attention" /></td>
<td style="text-align: left;"><a href="../notes/2025/GLA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/Adrenaline.prototxt">Adrenaline</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2503.20552v1">Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/Adrenaline/fig4.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ASISys/Adrenaline" /></td>
<td style="text-align: left;"><a href="../notes/2025/Adrenaline/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/209M5GA7.prototxt">209M5GA7</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2508.06297v1">KV Cache Compression for Inference Efficiency in LLMs: A Review</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/209M5GA7/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/KVLink.prototxt">KVLink</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.16002v1">KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/KVLink/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/UCSB-NLP-Chang/KVLink" /></td>
<td style="text-align: left;"><a href="../notes/2025/KVLink/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/KeepKV.prototxt">KeepKV</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2504.09936v1">KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/KeepKV/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/KeepKV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/LServer.prototxt">LServer</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.14866v1">LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/LServer/fig5.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mit-han-lab/omniserve" /></td>
<td style="text-align: left;"><a href="../notes/2025/LServer/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/LeanK.prototxt">LeanK</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2508.02215v1">LeanK: Learnable K Cache Channel Pruning for Efficient Decoding</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/LeanK/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/MInference" /></td>
<td style="text-align: left;"><a href="../notes/2025/LeanK/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/MIRAGE.prototxt">MIRAGE</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2507.11507v1">MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/MIRAGE/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/MiniCPM4.prototxt">MiniCPM4</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2506.07900v1">MiniCPM4: Ultra-Efficient LLMs on End Devices</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/MiniCPM4/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/openbmb/minicpm" /></td>
<td style="text-align: left;"><a href="../notes/2025/MiniCPM4/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/MoSA.prototxt">MoSA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2505.00315v1">Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/MoSA/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/piotrpiekos/MoSA" /></td>
<td style="text-align: left;"><a href="../notes/2025/MoSA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/MoBA.prototxt">MoBA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.13189v1">MoBA: Mixture of Block Attention for Long-Context LLMs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/MoBA/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/MoonshotAI/MoBA" /></td>
<td style="text-align: left;"><a href="../notes/2025/MoBA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/PowerAttention.prototxt">PowerAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2503.03588v1">PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/PowerAttention/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/PowerAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/PSA.prototxt">PSA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2503.00392v1">Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/PSA/fig4.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ASISys/PSAttention" /></td>
<td style="text-align: left;"><a href="../notes/2025/PSA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/QuickSilver.prototxt">QuickSilver</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2506.22396v1">QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/QuickSilver/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/R-KV.prototxt">R-KV</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2505.24133v2">R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/R-KV/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Zefan-Cai/R-KV" /></td>
<td style="text-align: left;"><a href="../notes/2025/R-KV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/RadialAttention.prototxt">RadialAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2506.19852v1">Radial Attention: $O(n\log n)$ Sparse Attention with Energy Decay for Long Video Generation</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/RadialAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/ReSA.prototxt">ReSA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2506.04108v2">Rectified Sparse Attention</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/ReSA/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/unilm" /></td>
<td style="text-align: left;"><a href="../notes/2025/ReSA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/0VRXJQ3F.prototxt">0VRXJQ3F</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2503.24000v1">Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/0VRXJQ3F/tab1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/LLMkvsys/rethink-kv-compression" /></td>
<td style="text-align: left;"><a href="../notes/2025/0VRXJQ3F/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/SALE.prototxt">SALE</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2505.24179v1">SALE : Low-bit Estimation for Efficient Sparse Attention in Long-context LLM Prefilling</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/SALE/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/BirdChristopher/SALE" /></td>
<td style="text-align: left;"><a href="../notes/2025/SALE/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/SeerAttention-R.prototxt">SeerAttention-R</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2506.08889v1">SeerAttention-R: Sparse Attention Adaptation for Long Reasoning</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/SeerAttention-R/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/SeerAttention" /></td>
<td style="text-align: left;"><a href="../notes/2025/SeerAttention-R/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/SpindleKV.prototxt">SpindleKV</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2507.06517v1">SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/SpindleKV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/Task-KV.prototxt">Task-KV</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2501.15113v1">Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation of Attention Heads</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/Task-KV/fig6.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/Task-KV/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/KVCache-Factory.prototxt">KVCache-Factory</a></td>
<td style="text-align: left;"><a href="https://github.com/Zefan-Cai/KVCache-Factory">Unified KV Cache Compression Methods for Auto-Regressive Models</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-github-2F4F4F" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Zefan-Cai/KVCache-Factory" /></td>
<td style="text-align: left;"><a href="../notes/2025/KVCache-Factory/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/kvpress.prototxt">kvpress</a></td>
<td style="text-align: left;">kvpress</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-github-2F4F4F" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/NVIDIA/kvpress" /></td>
<td style="text-align: left;"><a href="../notes/2025/kvpress/note/">note</a></td>
</tr>
</tbody>
</table>
<h4 id="13-efficient-training">13-Efficient Training</h4>
<table>
<thead>
<tr>
<th style="text-align: left;">Meta</th>
<th style="text-align: left;">Title</th>
<th style="text-align: left;">Cover</th>
<th style="text-align: left;">Publish</th>
<th style="text-align: left;">Code</th>
<th style="text-align: left;">Note</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><a href="../meta/2022/lora.prototxt">LoRA</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2106.09685">LoRA: Low-rank adaptation of large language models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2022/lora/lora.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2022-ICLR-FF6B6B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/LoRA" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/adalora.prototxt">AdaLoRA</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2303.10512.pdf">AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/adalora/adalora.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-ICLR-FF6B6B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/QingruZhang/AdaLoRA" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/MeZO.prototxt">MeZO</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2305.17333v3">Fine-Tuning Language Models with Just Forward Passes</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/princeton-nlp/MeZO" /></td>
<td style="text-align: left;"><a href="../notes/2023/MeZO/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/loftq.prototxt">LoftQ</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/abs/2310.08659">LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/loftq/loftq.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/yxli2123/LoftQ" /></td>
<td style="text-align: left;"><a href="../notes/2023/loftq/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/HYPL7G37.prototxt">m</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2404.01847v2">Accelerating Transformer Pre-training with 2:4 Sparsity</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/huyz2023/2by4-pretrain" /></td>
<td style="text-align: left;"><a href="../notes/2024/HYPL7G37/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/LoRA%2B.prototxt">LoRA+</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2402.12354v1">LoRA+: Efficient Low Rank Adaptation of Large Models</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/nikhil-ghosh-berkeley/loraplus" /></td>
<td style="text-align: left;"><a href="../notes/2024/LoRA%2B/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/SIFT.prototxt">SIFT</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2312.11875v3">Sparse is Enough in Fine-tuning Pre-trained Large Language Models</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/song-wx/SIFT" /></td>
<td style="text-align: left;"><a href="../notes/2024/SIFT/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/Sparse-IFT.prototxt">Sparse-IFT</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2303.11525v3">Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/Sparse-IFT/sparseIFT.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/CerebrasResearch/Sparse-IFT" /></td>
<td style="text-align: left;"><a href="../notes/2024/Sparse-IFT/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/TinyTrain.prototxt">TinyTrain</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2307.09988v2">TinyTrain: Resource-Aware Task-Adaptive Sparse Training of DNNs at the Data-Scarce Edge</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/theyoungkwon/TinyTrain" /></td>
<td style="text-align: left;"><a href="../notes/2024/TinyTrain/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/LISA.prototxt">LISA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2403.17919v1">LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/LISA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/SN1PK7EK.prototxt">m</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2402.11592v2">Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ZO-Bench/ZO-LLM" /></td>
<td style="text-align: left;"><a href="../notes/2024/SN1PK7EK/note/">note</a></td>
</tr>
</tbody>
</table>
<h4 id="14-network-structure-design">14-Network Structure Design</h4>
<table>
<thead>
<tr>
<th style="text-align: left;">Meta</th>
<th style="text-align: left;">Title</th>
<th style="text-align: left;">Cover</th>
<th style="text-align: left;">Publish</th>
<th style="text-align: left;">Code</th>
<th style="text-align: left;">Note</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><a href="../meta/2023/CodeGeeX.prototxt">CodeGeeX</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2303.17568v2">CodeGeeX: A Pre-Trained Model for Code Generation with Multilingual Benchmarking on HumanEval-X</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/THUDM/CodeGeeX" /></td>
<td style="text-align: left;"><a href="../notes/2023/CodeGeeX/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/068ZPAME.prototxt">068ZPAME</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2412.14219v2">A Survey on Inference Optimization Techniques for Mixture of Experts Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/068ZPAME/tab1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/MoE-Inf/awesome-moe-inference" /></td>
<td style="text-align: left;"><a href="../notes/2024/068ZPAME/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/DeepSeek-V2.prototxt">DeepSeek-V2</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2405.04434v5">DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/DeepSeek-V2/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2" /></td>
<td style="text-align: left;"><a href="../notes/2024/DeepSeek-V2/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/DeepSeek-V3.prototxt">DeepSeek-V3</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2412.19437v1">DeepSeek-V3 Technical Report</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/DeepSeek-V3/fig5.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3" /></td>
<td style="text-align: left;"><a href="../notes/2024/DeepSeek-V3/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/DeepSeekMoE.prototxt">DeepSeekMoE</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2401.06066v1">DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/DeepSeekMoE/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/deepseek-ai/DeepSeek-MoE" /></td>
<td style="text-align: left;"><a href="../notes/2024/DeepSeekMoE/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/MoD.prototxt">MoD</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2404.02258v1">Mixture-of-Depths: Dynamically allocating compute in transformer-based language models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/MoD/mod.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/MoD/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/MFA.prototxt">MFA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2412.19255v2">Multi-matrix Factorization Attention</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/MFA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/ReMoE.prototxt">ReMoE</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2412.14711v1">ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/ReMoE/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/thu-ml/ReMoE" /></td>
<td style="text-align: left;"><a href="../notes/2024/ReMoE/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/CLA.prototxt">CLA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2405.12981v1">Reducing Transformer Key-Value Cache Size with Cross-Layer Attention</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/CLA/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/JerryYin777/Cross-Layer-Attention" /></td>
<td style="text-align: left;"><a href="../notes/2024/CLA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/AdaSkip.prototxt">AdaSkip</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2501.02336v1">AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/AdaSkip/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-AAAI-FF4500" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ASISys/AdaSkip" /></td>
<td style="text-align: left;"><a href="../notes/2025/AdaSkip/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/NSA.prototxt">NSA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.11089v1">Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/NSA/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ACL-4169E1" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/NSA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/BlockFFN.prototxt">BlockFFN</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2507.08771v1">BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/BlockFFN/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-COLM-6495ED" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/thunlp/BlockFFN" /></td>
<td style="text-align: left;"><a href="../notes/2025/BlockFFN/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/FoX.prototxt">FoX</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2503.02130v2">Forgetting Transformer: Softmax Attention with a Forget Gate</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICLR-FF6B6B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zhixuan-lin/forgetting-transformer" /></td>
<td style="text-align: left;"><a href="../notes/2025/FoX/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/RecursiveTransformers.prototxt">RecursiveTransformers</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2410.20672v3">Relaxed Recursive Transformers: Effective Parameter Sharing with Layer-wise LoRA</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/RecursiveTransformers/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICLR-FF6B6B" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/RecursiveTransformers/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/SpecEE.prototxt">SpecEE</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2504.08850v1">SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/SpecEE/fig9.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ISCA-9932CC" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/infinigence/SpecEE" /></td>
<td style="text-align: left;"><a href="../notes/2025/SpecEE/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/MoE-MLA-RoPE.prototxt">MoE-MLA-RoPE</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2508.01261v1">Unifying Mixture of Experts and Multi-Head Latent Attention for Efficient Language Models</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-KDD_Workshop-green" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/MoE-MLA-RoPE/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/DeepSeek-R1.prototxt">DeepSeek-R1</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2501.12948v1">DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/DeepSeek-R1/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1" /></td>
<td style="text-align: left;"><a href="../notes/2025/DeepSeek-R1/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/2ZU1IWL6.prototxt">2ZU1IWL6</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2507.02754v1">Fast and Simplex: 2-Simplicial Attention in Triton</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/2ZU1IWL6/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/GLA.prototxt">GLA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2505.21487v1">Hardware-Efficient Attention for Fast Decoding</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/GLA/gla.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/Dao-AILab/grouped-latent-attention" /></td>
<td style="text-align: left;"><a href="../notes/2025/GLA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/MiniCPM4.prototxt">MiniCPM4</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2506.07900v1">MiniCPM4: Ultra-Efficient LLMs on End Devices</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/MiniCPM4/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/openbmb/minicpm" /></td>
<td style="text-align: left;"><a href="../notes/2025/MiniCPM4/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/52A7RO95.prototxt">52A7RO95</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2507.11181v1">Mixture of Experts in Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/52A7RO95/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/52A7RO95/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/MoSA.prototxt">MoSA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2505.00315v1">Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/MoSA/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/piotrpiekos/MoSA" /></td>
<td style="text-align: left;"><a href="../notes/2025/MoSA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/MoR.prototxt">MoR</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2507.10524v1">Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/MoR/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/raymin0223/mixture_of_recursions" /></td>
<td style="text-align: left;"><a href="../notes/2025/MoR/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/MoBA.prototxt">MoBA</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2502.13189v1">MoBA: Mixture of Block Attention for Long-Context LLMs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/MoBA/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/MoonshotAI/MoBA" /></td>
<td style="text-align: left;"><a href="../notes/2025/MoBA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/PanguUltra.prototxt">PanguUltra</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2504.07866v2">Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend NPUs</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/PanguUltra/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/Qwen3.prototxt">Qwen3</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2505.09388v1">Qwen3 Technical Report</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/Qwen3/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/QwenLM/Qwen3" /></td>
<td style="text-align: left;"><a href="../notes/2025/Qwen3/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/Step-3.prototxt">Step-3</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2507.19427v1">Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/Step-3/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/Super-Experts-Profilling.prototxt">Super-Experts-Profilling</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2507.23279v1">Unveiling Super Experts in Mixture-of-Experts Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/Super-Experts-Profilling/fig3.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ZunhaiSu/Super-Experts-Profilling" /></td>
<td style="text-align: left;"><a href="../notes/2025/Super-Experts-Profilling/note/">note</a></td>
</tr>
</tbody>
</table>
<h4 id="15-sparsity-weight">15-Sparsity (Weight)</h4>
<table>
<thead>
<tr>
<th style="text-align: left;">Meta</th>
<th style="text-align: left;">Title</th>
<th style="text-align: left;">Cover</th>
<th style="text-align: left;">Publish</th>
<th style="text-align: left;">Code</th>
<th style="text-align: left;">Note</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><a href="../meta/2022/oBERT.prototxt">oBERT</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2203.07259.pdf">The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2022-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/neuralmagic/sparseml" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/sparsegpt.prototxt">SparseGPT</a></td>
<td style="text-align: left;"><a href="https://arxiv.org/pdf/2301.00774.pdf">SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot.</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/IST-DASLab/sparsegpt" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/nmSPARSE.prototxt">nmSPARSE</a></td>
<td style="text-align: left;"><a href="https://proceedings.mlsys.org/paper_files/paper/2023/file/4552cedd396a308320209f75f56a5ad5-Paper-mlsys2023.pdf">Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-MLSys-DDA0DD" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/SparTA" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/VENOM.prototxt">VENOM</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2310.02065v1">VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/VENOM/vnm.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-SC-CD5C5C" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/UDC-GAC/venom" /></td>
<td style="text-align: left;"><a href="../notes/2023/VENOM/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/Wanda.prototxt">Wanda</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2306.11695">A Simple and Effective Pruning Approach for Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/Wanda/wanda.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICLR-FF6B6B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/locuslab/wanda" /></td>
<td style="text-align: left;"><a href="../notes/2024/Wanda/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/DSnoT.prototxt">DSnoT</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2310.08915v3">Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/DSnoT/dsnot.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICLR-FF6B6B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/zyxxmu/DSnoT" /></td>
<td style="text-align: left;"><a href="../notes/2024/DSnoT/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/IA8CS3VH.prototxt">RIA</a></td>
<td style="text-align: left;"><a href="https://openreview.net/forum?id=Tr0lPx9woF">Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/del/Plug-and-Play.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICLR-FF6B6B" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/biomedical-cybernetics/Relative-importance-and-activation-pruning" /></td>
<td style="text-align: left;"></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/MaskLLM.prototxt">MaskLLM</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2409.17481v1">MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/MaskLLM/maskllm.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-NeurIPS-FF1493" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/NVlabs/MaskLLM" /></td>
<td style="text-align: left;"><a href="../notes/2024/MaskLLM/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/SparseLLM.prototxt">SparseLLM</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2402.17946v3">SparseLLM: Towards Global Pruning for Pre-trained Language Models</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-NeurIPS-FF1493" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/BaiTheBest/SparseLLM" /></td>
<td style="text-align: left;"><a href="../notes/2024/SparseLLM/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/ADMM-pruning.prototxt">ADMM-pruning</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2401.02938v2">Fast and Effective Weight Update for Pruned Large Language Models</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-TMLR-20B2AA" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/fmfi-compbio/admm-pruning" /></td>
<td style="text-align: left;"><a href="../notes/2024/ADMM-pruning/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/0Y41U1N2.prototxt">m</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2410.16135v1">Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/0Y41U1N2/cover.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/0Y41U1N2/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/AdaptiveSparseTrainer.prototxt">AdaptiveSparseTrainer</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2407.20584v3">Pruning Large Language Models with Semi-Structural Adaptive Sparse Training</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/AdaptiveSparseTrainer/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-AAAI-FF4500" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/thu-ml/Adaptive-Sparse-Trainer" /></td>
<td style="text-align: left;"><a href="../notes/2025/AdaptiveSparseTrainer/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/SDS.prototxt">SDS</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2408.10473v1">Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/SDS/sds.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-Coling-1E90FF" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/SDS/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/BaWA.prototxt">BaWA</a></td>
<td style="text-align: left;"><a href="https://openreview.net/forum?id=YrCvW1Hx7g">BaWA: Automatic Optimizing Pruning Metric for Large Language Models with Balanced Weight and Activation</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/BaWA/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML-FF8C00" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/BaWA/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/TorchAO.prototxt">TorchAO</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2507.16099v1">TorchAO: PyTorch-Native Training-to-Serving Model Optimization</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/TorchAO/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ICML_Workshop-green" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/pytorch/ao" /></td>
<td style="text-align: left;"><a href="../notes/2025/TorchAO/note/">note</a></td>
</tr>
</tbody>
</table>
<h4 id="16-llm-deployment">16-LLM Deployment</h4>
<table>
<thead>
<tr>
<th style="text-align: left;">Meta</th>
<th style="text-align: left;">Title</th>
<th style="text-align: left;">Cover</th>
<th style="text-align: left;">Publish</th>
<th style="text-align: left;">Code</th>
<th style="text-align: left;">Note</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><a href="../meta/2023/PagedAttention.prototxt">Paged Attention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2309.06180v1">Efficient Memory Management for Large Language Model Serving with PagedAttention</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/PagedAttention/vllm.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-SOSP-8A2BE2" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/vllm-project/vllm" /></td>
<td style="text-align: left;"><a href="../notes/2023/PagedAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2023/IHOT8YP4.prototxt">m</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2307.09702v4">Efficient Guided Generation for Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2023/IHOT8YP4/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2023-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2023/IHOT8YP4/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/ChunkAttention.prototxt">ChunkAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2402.15220v4">ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/ChunkAttention/chunkattn.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ACL-4169E1" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/chunk-attention" /></td>
<td style="text-align: left;"><a href="../notes/2024/ChunkAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/CachedAttention.prototxt">CachedAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2403.19708v3">Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/CachedAttention/fig5.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ATC-DC143C" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/CachedAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/Eagle.prototxt">EAGLE</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2401.15077v2">EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/Eagle/eagle.jpg" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-ICML-FF8C00" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/SafeAILab/EAGLE" /></td>
<td style="text-align: left;"><a href="../notes/2024/Eagle/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/SGLang.prototxt">SGLang</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2312.07104v2">SGLang: Efficient Execution of Structured Language Model Programs</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/SGLang/fig9.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-NeurIPS-FF1493" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/sgl-project/sglang" /></td>
<td style="text-align: left;"><a href="../notes/2024/SGLang/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/DistAttention.prototxt">DistAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2401.02669v2">Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/DistAttention/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/DistAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/YS9YTT55.prototxt">YS9YTT55</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2407.12391v1">LLM Inference Serving: Survey of Recent Advances and Opportunities</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2024/YS9YTT55/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2024/XGrammar.prototxt">XGrammar</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2411.15100v2">XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2024/XGrammar/fig1.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2024-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/mlc-ai/xgrammar" /></td>
<td style="text-align: left;"><a href="../notes/2024/XGrammar/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/POD-Attention.prototxt">POD-Attention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2410.18038v2">POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/POD-Attention/fig2.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ASPLOS-9370DB" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/vattention" /></td>
<td style="text-align: left;"><a href="../notes/2025/POD-Attention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/vAttention.prototxt">vAttention</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2405.04437v3">vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/vAttention/fig5.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-ASPLOS-9370DB" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/microsoft/vattention" /></td>
<td style="text-align: left;"><a href="../notes/2025/vAttention/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/HelixParallelism.prototxt">HelixParallelism</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2507.07120v1">Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/HelixParallelism/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/Adrenaline.prototxt">Adrenaline</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2503.20552v1">Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation</a></td>
<td style="text-align: left;"><img alt="cover" src="../notes/2025/Adrenaline/fig4.png" /></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"><img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/ASISys/Adrenaline" /></td>
<td style="text-align: left;"><a href="../notes/2025/Adrenaline/note/">note</a></td>
</tr>
<tr>
<td style="text-align: left;"><a href="../meta/2025/MIRAGE.prototxt">MIRAGE</a></td>
<td style="text-align: left;"><a href="http://arxiv.org/abs/2507.11507v1">MIRAGE: KV Cache Optimization through Parameter Remapping for Multi-tenant LLM Serving</a></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><img alt="Publish" src="https://img.shields.io/badge/2025-arXiv-1E88E5" /></td>
<td style="text-align: left;"></td>
<td style="text-align: left;"><a href="../notes/2025/MIRAGE/note/">note</a></td>
</tr>
</tbody>
</table>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../cls_year/" class="btn btn-neutral float-left" title="By Year"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../cls_publication/" class="btn btn-neutral float-right" title="By Publication">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../cls_year/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../cls_publication/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
