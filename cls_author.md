<details open><summary>

### author
</summary> 
<p>

<details open><summary><b>Aaron Courville</b></summary> 
<p>

|    | meta                            | ttttttttttttttttttttttttttttttitle                                                                | ccccccccccccccccccover   | Publish                                                    | codeeeee                                                                                     | note                             |
|---:|:--------------------------------|:--------------------------------------------------------------------------------------------------|:-------------------------|:-----------------------------------------------------------|:---------------------------------------------------------------------------------------------|:---------------------------------|
|  0 | [FoX](./meta/2025/FoX.prototxt) | [Forgetting Transformer: Softmax Attention with a Forget Gate](http://arxiv.org/abs/2503.02130v2) |                          | ![Publish](https://img.shields.io/badge/2025-ICLR-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/zhixuan-lin/forgetting-transformer) | [note](./notes/2025/FoX/note.md) |
|  1 | [ACP](./meta/2025/ACP.prototxt) | [Adaptive Computation Pruning for the Forgetting Transformer](http://arxiv.org/abs/2504.06949v1)  |                          | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/zhixuan-lin/arctic-fox)             | [note](./notes/2025/ACP/note.md) |</p>
</details>
<details open><summary><b>Abhay Gupta</b></summary> 
<p>

|    | meta                                          | ttttttttttttttttttttttttttttttitle                                                                                              | ccccccccccccccccccover                                                    | Publish                                                    | codeeeee                                                                              | note                                    |
|---:|:----------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------|:-----------------------------------------------------------|:--------------------------------------------------------------------------------------|:----------------------------------------|
|  0 | [Sparse-IFT](./meta/2024/Sparse-IFT.prototxt) | [Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency](http://arxiv.org/abs/2303.11525v3)             | <img width='400' alt='image' src='./notes/2024/Sparse-IFT/sparseIFT.png'> | ![Publish](https://img.shields.io/badge/2024-ICML-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/CerebrasResearch/Sparse-IFT) | [note](./notes/2024/Sparse-IFT/note.md) |
|  1 | [m](./meta/2024/ULY1AZGY.prototxt)            | [Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment](http://arxiv.org/abs/2405.03594v1) |                                                                           | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/neuralmagic/nm-vllm)         | [note](./notes/2024/ULY1AZGY/note.md)   |</p>
</details>
<details open><summary><b>Aixin Liu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Ajay Jaiswal</b></summary> 
<p>

|    | meta                                                         | ttttttttttttttttttttttttttttttitle                                                                                           | ccccccccccccccccccover                                                 | Publish                                                    | codeeeee                                                                                | note                                  |
|---:|:-------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------------|:--------------------------------------|
|  0 | [Essential Sparsity](./meta/2023/EssentialSparsity.prototxt) | [The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter](https://arxiv.org/abs/2306.03805) |                                                                        | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/VITA-Group/essential_sparsity) |                                       |
|  1 | [LLM-KICK](./meta/2024/VB8C61V6.prototxt)                    | [Compressing LLMs: The Truth is Rarely Pure and Never Simple](http://arxiv.org/abs/2310.01382v2)                             | <img width='400' alt='image' src='./notes/2024/VB8C61V6/llm-kick.jpg'> | ![Publish](https://img.shields.io/badge/2024-ICLR-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/VITA-Group/llm-kick)           | [note](./notes/2024/VB8C61V6/note.md) |</p>
</details>
<details open><summary><b>Amir Gholami</b></summary> 
<p>

|    | meta                                                | ttttttttttttttttttttttttttttttitle                                                                                       | ccccccccccccccccccover                                                     | Publish                                                    | codeeeee                                                                                     | note                                       |
|---:|:----------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------|:-----------------------------------------------------------|:---------------------------------------------------------------------------------------------|:-------------------------------------------|
|  0 | [FisherPruning](./meta/2022/fisherpruning.prototxt) | [A Fast Post-Training Pruning Framework for Transformers](http://arxiv.org/abs/2204.09656v2)                             | <img width='400' alt='image' src='./notes/2022/fisherpruning/cover.jpg'>   | ![Publish](https://img.shields.io/badge/2022-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/WoosukKwon/retraining-free-pruning) | [note](./notes/2022/fisherpruning/note.md) |
|  1 | [SqueezeLLM](./meta/2024/SqueezeLLM.prototxt)       | [SqueezeLLM: Dense-and-Sparse Quantization](http://arxiv.org/abs/2306.07629v4)                                           | <img width='400' alt='image' src='./notes/2024/SqueezeLLM/squeezeLLM.png'> | ![Publish](https://img.shields.io/badge/2024-ICML-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/SqueezeAILab/SqueezeLLM)            | [note](./notes/2024/SqueezeLLM/note.md)    |
|  2 | [KVQuant](./meta/2024/KVQuant.prototxt)             | [KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization](http://arxiv.org/abs/2401.18079v2) |                                                                            | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/SqueezeAILab/KVQuant)               | [note](./notes/2024/KVQuant/note.md)       |</p>
</details>
<details open><summary><b>Amir H. Abdi</b></summary> 
<p>

|    | meta                                          | ttttttttttttttttttttttttttttttitle                                                                                               | ccccccccccccccccccover                                                            | Publish                                                    | codeeeee                                                                       | note                                    |
|---:|:----------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------------------------|:-------------------------------------------------------------------------------|:----------------------------------------|
|  0 | [MInference](./meta/2024/MInference.prototxt) | [MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention](http://arxiv.org/abs/2407.02490v1) | <img width='400' alt='image' src='./notes/2024/MInference/MInference_3shape.PNG'> | ![Publish](https://img.shields.io/badge/2024-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/MInference) | [note](./notes/2024/MInference/note.md) |
|  1 | [SCBench](./meta/2024/SCBench.prototxt)       | [SCBench: A KV Cache-Centric Analysis of Long-Context Methods](http://arxiv.org/abs/2412.10319v2)                                |                                                                                   | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/MInference) | [note](./notes/2024/SCBench/note.md)    |</p>
</details>
<details open><summary><b>Andr√© F. T. Martins</b></summary> 
<p>

|    | meta                                        | ttttttttttttttttttttttttttttttitle                                                              | ccccccccccccccccccover                                                   | Publish                                                    | codeeeee                                                                      | note                                   |
|---:|:--------------------------------------------|:------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------|:-----------------------------------------------------------|:------------------------------------------------------------------------------|:---------------------------------------|
|  0 | [m](./meta/2023/68I8KKBV.prototxt)          | [Efficient Methods for Natural Language Processing: A Survey](https://arxiv.org/abs/2209.00099) | <img width='400' alt='image' src='./notes/del/survey/efficient_NLP.jpg'> | ![Publish](https://img.shields.io/badge/2023-TACL-green)   |                                                                               |                                        |
|  1 | [AdaSplash](./meta/2025/AdaSplash.prototxt) | [AdaSplash: Adaptive Sparse Flash Attention](http://arxiv.org/abs/2502.12082v1)                 |                                                                          | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deep-spin/adasplash) | [note](./notes/2025/AdaSplash/note.md) |</p>
</details>
<details open><summary><b>Aojun Zhou</b></summary> 
<p>

|    | meta                                  | ttttttttttttttttttttttttttttttitle                                                                                        | ccccccccccccccccccover                                             | Publish                                                  | codeeeee                                                                      | note                             |
|---:|:--------------------------------------|:--------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------|:---------------------------------------------------------|:------------------------------------------------------------------------------|:---------------------------------|
|  0 | [SR-STE](./meta/2021/sr-ste.prototxt) | [Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch](https://openreview.net/forum?id=K9bw7vqp_s)    | <img width='400' alt='image' src='./notes/2021/sr-ste/sr-ste.jpg'> | ![Publish](https://img.shields.io/badge/2021-ICLR-blue)  | ![GitHub Repo stars](https://img.shields.io/github/stars/aojunzz/NM-sparsity) |                                  |
|  1 | [STA](./meta/2022/44KWQAWO.prototxt)  | [An Algorithm-Hardware Co-Optimized Framework for Accelerating N:M Sparse Transformers](https://arxiv.org/abs/2208.06118) |                                                                    | ![Publish](https://img.shields.io/badge/2022-VLSI-green) |                                                                               |                                  |
|  2 | [SPP](./meta/2024/SPP.prototxt)       | [SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models](http://arxiv.org/abs/2405.16057v1)    | <img width='400' alt='image' src='./notes/2024/SPP/spp.png'>       | ![Publish](https://img.shields.io/badge/2024-ICML-blue)  | ![GitHub Repo stars](https://img.shields.io/github/stars/Lucky-Lance/SPP)     | [note](./notes/2024/SPP/note.md) |</p>
</details>
<details open><summary><b>Ashish Panwar</b></summary> 
<p>

|    | meta                                                | ttttttttttttttttttttttttttttttitle                                                                                 | ccccccccccccccccccover                                                  | Publish                                                     | codeeeee                                                                       | note                                       |
|---:|:----------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------|:------------------------------------------------------------|:-------------------------------------------------------------------------------|:-------------------------------------------|
|  0 | [POD-Attention](./meta/2025/POD-Attention.prototxt) | [POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference](http://arxiv.org/abs/2410.18038v2) | <img width='400' alt='image' src='./notes/2025/POD-Attention/fig2.png'> | ![Publish](https://img.shields.io/badge/2025-ASPLOS-orange) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/vattention) | [note](./notes/2025/POD-Attention/note.md) |
|  1 | [vAttention](./meta/2025/vAttention.prototxt)       | [vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention](http://arxiv.org/abs/2405.04437v3) | <img width='400' alt='image' src='./notes/2025/vAttention/fig5.png'>    | ![Publish](https://img.shields.io/badge/2025-ASPLOS-orange) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/vattention) | [note](./notes/2025/vAttention/note.md)    |</p>
</details>
<details open><summary><b>Bairu Hou</b></summary> 
<p>

|    | meta                                        | ttttttttttttttttttttttttttttttitle                                                                           | ccccccccccccccccccover                                              | Publish                                                    | codeeeee                                                                        | note                                   |
|---:|:--------------------------------------------|:-------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------|:-----------------------------------------------------------|:--------------------------------------------------------------------------------|:---------------------------------------|
|  0 | [IFPruning](./meta/2025/IFPruning.prototxt) | [Instruction-Following Pruning for Large Language Models](http://arxiv.org/abs/2501.02086v2)                 | <img width='400' alt='image' src='./notes/2025/IFPruning/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) |                                                                                 | [note](./notes/2025/IFPruning/note.md) |
|  1 | [KVLink](./meta/2025/KVLink.prototxt)       | [KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse](http://arxiv.org/abs/2502.16002v1) | <img width='400' alt='image' src='./notes/2025/KVLink/fig1.png'>    | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/UCSB-NLP-Chang/KVLink) | [note](./notes/2025/KVLink/note.md)    |</p>
</details>
<details open><summary><b>Baris Kasikci</b></summary> 
<p>

|    | meta                                          | ttttttttttttttttttttttttttttttitle                                                                                     | ccccccccccccccccccover                                           | Publish                                                    | codeeeee                                                                           | note                                    |
|---:|:----------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------------|:----------------------------------------|
|  0 | [Quest](./meta/2024/Quest.prototxt)           | [Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference](http://arxiv.org/abs/2406.10774)                | <img width='400' alt='image' src='./notes/2024/Quest/quest.png'> | ![Publish](https://img.shields.io/badge/2024-ICML-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/quest)        | [note](./notes/2024/Quest/note.md)      |
|  1 | [FlashInfer](./meta/2025/FlashInfer.prototxt) | [FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving](http://arxiv.org/abs/2501.01005v2) |                                                                  | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/flashinfer-ai/flashinfer) | [note](./notes/2025/FlashInfer/note.md) |</p>
</details>
<details open><summary><b>Bei Feng</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Beidi Chen</b></summary> 
<p>

|    | meta                                                | ttttttttttttttttttttttttttttttitle                                                                                         | ccccccccccccccccccover                                                   | Publish                                                    | codeeeee                                                                            | note                                       |
|---:|:----------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------|:-----------------------------------------------------------|:------------------------------------------------------------------------------------|:-------------------------------------------|
|  0 | [Deja Vu](./meta/2023/dejavu.prototxt)              | [Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time](https://openreview.net/forum?id=wIPIhHd00i)            | <img width='400' alt='image' src='./notes/2023/dejavu/dejavu.jpg'>       | ![Publish](https://img.shields.io/badge/2023-ICML-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/FMInference/DejaVu)        |                                            |
|  1 | [H2O](./meta/2023/H2O.prototxt)                     | [H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models](http://arxiv.org/abs/2306.14048) | <img width='400' alt='image' src='./notes/2023/H2O/h2o.png'>             | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/FMInference/H2O)           | [note](./notes/2023/H2O/note.md)           |
|  2 | [streaming-llm](./meta/2024/streaming-llm.prototxt) | [Efficient Streaming Language Models with Attention Sinks](http://arxiv.org/abs/2309.17453v4)                              | <img width='400' alt='image' src='./notes/2024/streaming-llm/cover.jpg'> | ![Publish](https://img.shields.io/badge/2024-ICLR-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/streaming-llm) | [note](./notes/2024/streaming-llm/note.md) |
|  3 | [ShadowKV](./meta/2024/ShadowKV.prototxt)           | [ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference](http://arxiv.org/abs/2410.21465v1)          | <img width='400' alt='image' src='./notes/2024/ShadowKV/shadowkv.png'>   | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/bytedance/ShadowKV)        | [note](./notes/2024/ShadowKV/note.md)      |</p>
</details>
<details open><summary><b>Bin Gao</b></summary> 
<p>

|    | meta                                                    | ttttttttttttttttttttttttttttttitle                                                                                                 | ccccccccccccccccccover                                                    | Publish                                                  | codeeeee                                                                 | note                                         |
|---:|:--------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------|:---------------------------------------------------------|:-------------------------------------------------------------------------|:---------------------------------------------|
|  0 | [CachedAttention](./meta/2024/CachedAttention.prototxt) | [Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention](http://arxiv.org/abs/2403.19708v3) | <img width='400' alt='image' src='./notes/2024/CachedAttention/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-ATC-orange) |                                                                          | [note](./notes/2024/CachedAttention/note.md) |
|  1 | [AdaSkip](./meta/2025/AdaSkip.prototxt)                 | [AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference](http://arxiv.org/abs/2501.02336v1)               | <img width='400' alt='image' src='./notes/2025/AdaSkip/fig1.png'>         | ![Publish](https://img.shields.io/badge/2025-AAAI-blue)  | ![GitHub Repo stars](https://img.shields.io/github/stars/ASISys/AdaSkip) | [note](./notes/2025/AdaSkip/note.md)         |</p>
</details>
<details open><summary><b>Bin Lin</b></summary> 
<p>

|    | meta                                                | ttttttttttttttttttttttttttttttitle                                                                                                                                              | ccccccccccccccccccover                                                  | Publish                                                    | codeeeee                                                                   | note                                       |
|---:|:----------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------|:-----------------------------------------------------------|:---------------------------------------------------------------------------|:-------------------------------------------|
|  0 | [nmSPARSE](./meta/2023/nmSPARSE.prototxt)           | [Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning](https://proceedings.mlsys.org/paper_files/paper/2023/file/4552cedd396a308320209f75f56a5ad5-Paper-mlsys2023.pdf) |                                                                         | ![Publish](https://img.shields.io/badge/2023-MLSys-orange) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/SparTA) |                                            |
|  1 | [DistAttention](./meta/2024/DistAttention.prototxt) | [Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache](http://arxiv.org/abs/2401.02669v2)                                            | <img width='400' alt='image' src='./notes/2024/DistAttention/fig1.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) |                                                                            | [note](./notes/2024/DistAttention/note.md) |</p>
</details>
<details open><summary><b>Bing Xue</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Bingxuan Wang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Bochao Wu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Chaojun Xiao</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                     | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [ReLU2](./meta/2024/ReLU2.prototxt)             | [ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs](https://arxiv.org/abs/2402.03804)        | <img width='400' alt='image' src='./notes/2024/ReLU2/activation.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) |                                                                              | [note](./notes/2024/ReLU2/note.md)       |
|  1 | [SparsingLaw](./meta/2024/SparsingLaw.prototxt) | [Sparsing Law: Towards Large Language Models with Greater Activation Sparsity](http://arxiv.org/abs/2411.02335v1) | <img width='400' alt='image' src='./notes/2024/SparsingLaw/fig4.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/thunlp/SparsingLaw) | [note](./notes/2024/SparsingLaw/note.md) |
|  2 | [MiniCPM4](./meta/2025/MiniCPM4.prototxt)       | [MiniCPM4: Ultra-Efficient LLMs on End Devices](http://arxiv.org/abs/2506.07900v1)                                | <img width='400' alt='image' src='./notes/2025/MiniCPM4/fig2.png'>    | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/openbmb/minicpm)    | [note](./notes/2025/MiniCPM4/note.md)    |</p>
</details>
<details open><summary><b>Chen Chen</b></summary> 
<p>

|    | meta                                                          | ttttttttttttttttttttttttttttttitle                                                                                                  | ccccccccccccccccccover                                                       | Publish                                                    | codeeeee                                                                                       | note                                            |
|---:|:--------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------------------------|:------------------------------------------------|
|  0 | [ProSparse](./meta/2024/ProSparse.prototxt)                   | [ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models](https://arxiv.org/abs/2402.13516) | <img width='400' alt='image' src='./notes/2024/ProSparse/prosparse.jpg'>     | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/raincleared-song/sparse_gpu_operator) | [note](./notes/2024/ProSparse/note.md)          |
|  1 | [AttentionPredictor](./meta/2025/AttentionPredictor.prototxt) | [AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference](http://arxiv.org/abs/2502.04077v1)                       | <img width='400' alt='image' src='./notes/2025/AttentionPredictor/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) |                                                                                                | [note](./notes/2025/AttentionPredictor/note.md) |</p>
</details>
<details open><summary><b>Chen Zhang</b></summary> 
<p>

|    | meta                                                | ttttttttttttttttttttttttttttttitle                                                                                                   | ccccccccccccccccccover                                                  | Publish                                                    | codeeeee   | note                                       |
|---:|:----------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------|:-----------------------------------------------------------|:-----------|:-------------------------------------------|
|  0 | [DistAttention](./meta/2024/DistAttention.prototxt) | [Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache](http://arxiv.org/abs/2401.02669v2) | <img width='400' alt='image' src='./notes/2024/DistAttention/fig1.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) |            | [note](./notes/2024/DistAttention/note.md) |
|  1 | [ZigZagKV](./meta/2024/ZigZagKV.prototxt)           | [ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty](http://arxiv.org/abs/2412.09036v1)     | <img width='400' alt='image' src='./notes/2024/ZigZagKV/fig1.png'>      | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) |            | [note](./notes/2024/ZigZagKV/note.md)      |</p>
</details>
<details open><summary><b>Chengda Lu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Chenggang Zhao</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                             | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                           | note                                     |
|---:|:------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5)        | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2)  | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                              | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3)  | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeekMoE](./meta/2024/DeepSeekMoE.prototxt) | [DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](http://arxiv.org/abs/2401.06066v1) | <img width='400' alt='image' src='./notes/2024/DeepSeekMoE/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-MoE) | [note](./notes/2024/DeepSeekMoE/note.md) |
|  3 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1)        | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1)  | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Chengqi Deng</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                             | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                           | note                                     |
|---:|:------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                              | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3)  | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeekMoE](./meta/2024/DeepSeekMoE.prototxt) | [DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](http://arxiv.org/abs/2401.06066v1) | <img width='400' alt='image' src='./notes/2024/DeepSeekMoE/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-MoE) | [note](./notes/2024/DeepSeekMoE/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1)        | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1)  | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Chengruidong Zhang</b></summary> 
<p>

|    | meta                                          | ttttttttttttttttttttttttttttttitle                                                                                               | ccccccccccccccccccover                                                            | Publish                                                    | codeeeee                                                                       | note                                    |
|---:|:----------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------------------------|:-------------------------------------------------------------------------------|:----------------------------------------|
|  0 | [MInference](./meta/2024/MInference.prototxt) | [MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention](http://arxiv.org/abs/2407.02490v1) | <img width='400' alt='image' src='./notes/2024/MInference/MInference_3shape.PNG'> | ![Publish](https://img.shields.io/badge/2024-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/MInference) | [note](./notes/2024/MInference/note.md) |
|  1 | [SCBench](./meta/2024/SCBench.prototxt)       | [SCBench: A KV Cache-Centric Analysis of Long-Context Methods](http://arxiv.org/abs/2412.10319v2)                                |                                                                                   | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/MInference) | [note](./notes/2024/SCBench/note.md)    |</p>
</details>
<details open><summary><b>Chenyang Song</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                                  | ccccccccccccccccccover                                                   | Publish                                                    | codeeeee                                                                                       | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [ProSparse](./meta/2024/ProSparse.prototxt)     | [ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models](https://arxiv.org/abs/2402.13516) | <img width='400' alt='image' src='./notes/2024/ProSparse/prosparse.jpg'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/raincleared-song/sparse_gpu_operator) | [note](./notes/2024/ProSparse/note.md)   |
|  1 | [ReLU2](./meta/2024/ReLU2.prototxt)             | [ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs](https://arxiv.org/abs/2402.03804)                          | <img width='400' alt='image' src='./notes/2024/ReLU2/activation.png'>    | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) |                                                                                                | [note](./notes/2024/ReLU2/note.md)       |
|  2 | [SparsingLaw](./meta/2024/SparsingLaw.prototxt) | [Sparsing Law: Towards Large Language Models with Greater Activation Sparsity](http://arxiv.org/abs/2411.02335v1)                   | <img width='400' alt='image' src='./notes/2024/SparsingLaw/fig4.png'>    | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/thunlp/SparsingLaw)                   | [note](./notes/2024/SparsingLaw/note.md) |</p>
</details>
<details open><summary><b>Chenyu Zhang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Chong Ruan</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                             | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                           | note                                     |
|---:|:------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5)        | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2)  | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                              | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3)  | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeekMoE](./meta/2024/DeepSeekMoE.prototxt) | [DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](http://arxiv.org/abs/2401.06066v1) | <img width='400' alt='image' src='./notes/2024/DeepSeekMoE/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-MoE) | [note](./notes/2024/DeepSeekMoE/note.md) |
|  3 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1)        | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1)  | [note](./notes/2025/DeepSeek-R1/note.md) |
|  4 | [NSA](./meta/2025/NSA.prototxt)                 | [Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention](http://arxiv.org/abs/2502.11089v1)         | <img width='400' alt='image' src='./notes/2025/NSA/fig2.png'>         | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) |                                                                                    | [note](./notes/2025/NSA/note.md)         |</p>
</details>
<details open><summary><b>Chuang Gan</b></summary> 
<p>

|    | meta                                   | ttttttttttttttttttttttttttttttitle                                                                                 | ccccccccccccccccccover   | Publish                                                    | codeeeee                                                                      | note                                 |
|---:|:---------------------------------------|:-------------------------------------------------------------------------------------------------------------------|:-------------------------|:-----------------------------------------------------------|:------------------------------------------------------------------------------|:-------------------------------------|
|  0 | [AWQ](./meta/2024/awq.prototxt)        | [AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](https://arxiv.org/abs/2306.00978) |                          | ![Publish](https://img.shields.io/badge/2024-MLSys-orange) | ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/llm-awq) |                                      |
|  1 | [QServe](./meta/2024/QServer.prototxt) | [QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving](http://arxiv.org/abs/2405.04532v2)   |                          | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | [Pytorch](https://hanlab.mit.edu/projects/qserve)                             | [note](./notes/2024/QServer/note.md) |</p>
</details>
<details open><summary><b>Clark Barrett</b></summary> 
<p>

|    | meta                                  | ttttttttttttttttttttttttttttttitle                                                                                         | ccccccccccccccccccover                                           | Publish                                                    | codeeeee                                                                     | note                                |
|---:|:--------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------|:------------------------------------|
|  0 | [H2O](./meta/2023/H2O.prototxt)       | [H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models](http://arxiv.org/abs/2306.14048) | <img width='400' alt='image' src='./notes/2023/H2O/h2o.png'>     | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/FMInference/H2O)    | [note](./notes/2023/H2O/note.md)    |
|  1 | [SGLang](./meta/2024/SGLang.prototxt) | [SGLang: Efficient Execution of Structured Language Model Programs](http://arxiv.org/abs/2312.07104v2)                     | <img width='400' alt='image' src='./notes/2024/SGLang/fig9.png'> | ![Publish](https://img.shields.io/badge/2024-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/sgl-project/sglang) | [note](./notes/2024/SGLang/note.md) |</p>
</details>
<details open><summary><b>Cody Hao Yu</b></summary> 
<p>

|    | meta                                                   | ttttttttttttttttttttttttttttttitle                                                                                    | ccccccccccccccccccover                                                   | Publish                                                    | codeeeee                                                                     | note                                        |
|---:|:-------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------|:--------------------------------------------|
|  0 | [Paged Attention](./meta/2023/PagedAttention.prototxt) | [Efficient Memory Management for Large Language Model Serving with PagedAttention](http://arxiv.org/abs/2309.06180v1) | <img width='400' alt='image' src='./notes/2023/PagedAttention/vllm.png'> | ![Publish](https://img.shields.io/badge/2023-SOSP-orange)  | ![GitHub Repo stars](https://img.shields.io/github/stars/vllm-project/vllm)  | [note](./notes/2023/PagedAttention/note.md) |
|  1 | [SGLang](./meta/2024/SGLang.prototxt)                  | [SGLang: Efficient Execution of Structured Language Model Programs](http://arxiv.org/abs/2312.07104v2)                | <img width='400' alt='image' src='./notes/2024/SGLang/fig9.png'>         | ![Publish](https://img.shields.io/badge/2024-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/sgl-project/sglang) | [note](./notes/2024/SGLang/note.md)         |</p>
</details>
<details open><summary><b>Coleman Hooper</b></summary> 
<p>

|    | meta                                          | ttttttttttttttttttttttttttttttitle                                                                                       | ccccccccccccccccccover                                                     | Publish                                                    | codeeeee                                                                          | note                                    |
|---:|:----------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:----------------------------------------|
|  0 | [SqueezeLLM](./meta/2024/SqueezeLLM.prototxt) | [SqueezeLLM: Dense-and-Sparse Quantization](http://arxiv.org/abs/2306.07629v4)                                           | <img width='400' alt='image' src='./notes/2024/SqueezeLLM/squeezeLLM.png'> | ![Publish](https://img.shields.io/badge/2024-ICML-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/SqueezeAILab/SqueezeLLM) | [note](./notes/2024/SqueezeLLM/note.md) |
|  1 | [KVQuant](./meta/2024/KVQuant.prototxt)       | [KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization](http://arxiv.org/abs/2401.18079v2) |                                                                            | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/SqueezeAILab/KVQuant)    | [note](./notes/2024/KVQuant/note.md)    |</p>
</details>
<details open><summary><b>Damai Dai</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                             | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                           | note                                     |
|---:|:------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5)        | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2)  | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                              | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3)  | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeekMoE](./meta/2024/DeepSeekMoE.prototxt) | [DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](http://arxiv.org/abs/2401.06066v1) | <img width='400' alt='image' src='./notes/2024/DeepSeekMoE/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-MoE) | [note](./notes/2024/DeepSeekMoE/note.md) |
|  3 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1)        | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1)  | [note](./notes/2025/DeepSeek-R1/note.md) |
|  4 | [NSA](./meta/2025/NSA.prototxt)                 | [Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention](http://arxiv.org/abs/2502.11089v1)         | <img width='400' alt='image' src='./notes/2025/NSA/fig2.png'>         | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) |                                                                                    | [note](./notes/2025/NSA/note.md)         |</p>
</details>
<details open><summary><b>Dan Alistarh</b></summary> 
<p>

|    | meta                                          | ttttttttttttttttttttttttttttttitle                                                                                                        | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                              | note                                  |
|---:|:----------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:--------------------------------------------------------------------------------------|:--------------------------------------|
|  0 | [m](./meta/2020/V3MFIRLV.prototxt)            | [Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference](http://proceedings.mlr.press/v119/kurtz20a/kurtz20a.pdf)  |                                                                       | ![Publish](https://img.shields.io/badge/2020-ICML-blue)    |                                                                                       |                                       |
|  1 | [m](./meta/2021/ITZS3TU3.prototxt)            | [Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks](https://arxiv.org/abs/2102.00554) |                                                                       | ![Publish](https://img.shields.io/badge/2021-arXiv-violet) |                                                                                       |                                       |
|  2 | [SPDY](./meta/2022/spdy.prototxt)             | [SPDY: Accurate Pruning with Speedup Guarantees](https://arxiv.org/abs/2201.13096)                                                        | <img width='400' alt='image' src='./notes/2022/spdy/cover.jpg'>       | ![Publish](https://img.shields.io/badge/2022-ICML-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/IST-DASLab/spdy)             | [note](./notes/2022/spdy/index.md)    |
|  3 | [OBC](./meta/2022/obc.prototxt)               | [Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning](https://openreview.net/pdf?id=ksVGCOlOEba)   |                                                                       | ![Publish](https://img.shields.io/badge/2022-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/IST-DASLab/OBC)              |                                       |
|  4 | [oBERT](./meta/2022/oBERT.prototxt)           | [The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models](https://arxiv.org/pdf/2203.07259.pdf)    |                                                                       | ![Publish](https://img.shields.io/badge/2022-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/neuralmagic/sparseml)        |                                       |
|  5 | [GPTQ](./meta/2023/gptq.prototxt)             | [GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers](https://arxiv.org/pdf/2210.17323.pdf)                 |                                                                       | ![Publish](https://img.shields.io/badge/2023-ICLR-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/IST-DASLab/gptq)             |                                       |
|  6 | [SparseGPT](./meta/2023/sparsegpt.prototxt)   | [SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot.](https://arxiv.org/pdf/2301.00774.pdf)                          |                                                                       | ![Publish](https://img.shields.io/badge/2023-ICML-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/IST-DASLab/sparsegpt)        |                                       |
|  7 | [ZipLM](./meta/2023/ZipLM.prototxt)           | [ZipLM: Inference-Aware Structured Pruning of Language Models](https://openreview.net/pdf?id=bPFFPueAxm)                                  | <img width='400' alt='image' src='./notes/2023/ZipLM/cover.jpg'>      | ![Publish](https://img.shields.io/badge/2023-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/IST-DASLab/ZipLM)            |                                       |
|  8 | [SpQR](./meta/2023/spqr.prototxt)             | [SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression](https://arxiv.org/pdf/2306.03078.pdf)                  |                                                                       | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/Vahe1994/SpQR)               |                                       |
|  9 | [SquareHead](./meta/2023/SquareHead.prototxt) | [Sparse Fine-tuning for Inference Acceleration of Large Language Models](https://arxiv.org/pdf/2310.06927.pdf)                            | <img width='400' alt='image' src='./notes/2023/SquareHead/cover.png'> | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/IST-DASLab/SparseFinetuning) |                                       |
| 10 | [m](./meta/2024/ULY1AZGY.prototxt)            | [Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment](http://arxiv.org/abs/2405.03594v1)           |                                                                       | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/neuralmagic/nm-vllm)         | [note](./notes/2024/ULY1AZGY/note.md) |</p>
</details>
<details open><summary><b>Daya Guo</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>DeepSeek-AI</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Dejian Yang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Deli Chen</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                             | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                           | note                                     |
|---:|:------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5)        | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2)  | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                              | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3)  | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeekMoE](./meta/2024/DeepSeekMoE.prototxt) | [DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](http://arxiv.org/abs/2401.06066v1) | <img width='400' alt='image' src='./notes/2024/DeepSeekMoE/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-MoE) | [note](./notes/2024/DeepSeekMoE/note.md) |
|  3 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1)        | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1)  | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Dongjie Ji</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Dongsheng Li</b></summary> 
<p>

|    | meta                                          | ttttttttttttttttttttttttttttttitle                                                                                               | ccccccccccccccccccover                                                            | Publish                                                    | codeeeee                                                                       | note                                    |
|---:|:----------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------------------------|:-------------------------------------------------------------------------------|:----------------------------------------|
|  0 | [MInference](./meta/2024/MInference.prototxt) | [MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention](http://arxiv.org/abs/2407.02490v1) | <img width='400' alt='image' src='./notes/2024/MInference/MInference_3shape.PNG'> | ![Publish](https://img.shields.io/badge/2024-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/MInference) | [note](./notes/2024/MInference/note.md) |
|  1 | [SCBench](./meta/2024/SCBench.prototxt)       | [SCBench: A KV Cache-Centric Analysis of Long-Context Methods](http://arxiv.org/abs/2412.10319v2)                                |                                                                                   | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/MInference) | [note](./notes/2024/SCBench/note.md)    |</p>
</details>
<details open><summary><b>Eldar Kurtic</b></summary> 
<p>

|    | meta                                          | ttttttttttttttttttttttttttttttitle                                                                                                     | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                              | note                                  |
|---:|:----------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:--------------------------------------------------------------------------------------|:--------------------------------------|
|  0 | [oBERT](./meta/2022/oBERT.prototxt)           | [The Optimal BERT Surgeon: Scalable and Accurate Second-Order Pruning for Large Language Models](https://arxiv.org/pdf/2203.07259.pdf) |                                                                       | ![Publish](https://img.shields.io/badge/2022-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/neuralmagic/sparseml)        |                                       |
|  1 | [ZipLM](./meta/2023/ZipLM.prototxt)           | [ZipLM: Inference-Aware Structured Pruning of Language Models](https://openreview.net/pdf?id=bPFFPueAxm)                               | <img width='400' alt='image' src='./notes/2023/ZipLM/cover.jpg'>      | ![Publish](https://img.shields.io/badge/2023-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/IST-DASLab/ZipLM)            |                                       |
|  2 | [SquareHead](./meta/2023/SquareHead.prototxt) | [Sparse Fine-tuning for Inference Acceleration of Large Language Models](https://arxiv.org/pdf/2310.06927.pdf)                         | <img width='400' alt='image' src='./notes/2023/SquareHead/cover.png'> | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/IST-DASLab/SparseFinetuning) |                                       |
|  3 | [m](./meta/2024/ULY1AZGY.prototxt)            | [Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment](http://arxiv.org/abs/2405.03594v1)        |                                                                       | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/neuralmagic/nm-vllm)         | [note](./notes/2024/ULY1AZGY/note.md) |</p>
</details>
<details open><summary><b>Elias Frantar</b></summary> 
<p>

|    | meta                                          | ttttttttttttttttttttttttttttttitle                                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                              | note                               |
|---:|:----------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:--------------------------------------------------------------------------------------|:-----------------------------------|
|  0 | [SPDY](./meta/2022/spdy.prototxt)             | [SPDY: Accurate Pruning with Speedup Guarantees](https://arxiv.org/abs/2201.13096)                                                      | <img width='400' alt='image' src='./notes/2022/spdy/cover.jpg'>       | ![Publish](https://img.shields.io/badge/2022-ICML-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/IST-DASLab/spdy)             | [note](./notes/2022/spdy/index.md) |
|  1 | [OBC](./meta/2022/obc.prototxt)               | [Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning](https://openreview.net/pdf?id=ksVGCOlOEba) |                                                                       | ![Publish](https://img.shields.io/badge/2022-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/IST-DASLab/OBC)              |                                    |
|  2 | [GPTQ](./meta/2023/gptq.prototxt)             | [GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers](https://arxiv.org/pdf/2210.17323.pdf)               |                                                                       | ![Publish](https://img.shields.io/badge/2023-ICLR-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/IST-DASLab/gptq)             |                                    |
|  3 | [SparseGPT](./meta/2023/sparsegpt.prototxt)   | [SparseGPT: Massive Language Models Can be Accurately Pruned in one-shot.](https://arxiv.org/pdf/2301.00774.pdf)                        |                                                                       | ![Publish](https://img.shields.io/badge/2023-ICML-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/IST-DASLab/sparsegpt)        |                                    |
|  4 | [ZipLM](./meta/2023/ZipLM.prototxt)           | [ZipLM: Inference-Aware Structured Pruning of Language Models](https://openreview.net/pdf?id=bPFFPueAxm)                                | <img width='400' alt='image' src='./notes/2023/ZipLM/cover.jpg'>      | ![Publish](https://img.shields.io/badge/2023-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/IST-DASLab/ZipLM)            |                                    |
|  5 | [SquareHead](./meta/2023/SquareHead.prototxt) | [Sparse Fine-tuning for Inference Acceleration of Large Language Models](https://arxiv.org/pdf/2310.06927.pdf)                          | <img width='400' alt='image' src='./notes/2023/SquareHead/cover.png'> | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/IST-DASLab/SparseFinetuning) |                                    |</p>
</details>
<details open><summary><b>Erhang Li</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Fan Yang</b></summary> 
<p>

|    | meta                                                    | ttttttttttttttttttttttttttttttitle                                                                                                                                              | ccccccccccccccccccover                                                           | Publish                                                    | codeeeee                                                                          | note                                         |
|---:|:--------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:---------------------------------------------|
|  0 | [nmSPARSE](./meta/2023/nmSPARSE.prototxt)               | [Efficient GPU Kernels for N:M-Sparse Weights in Deep Learning](https://proceedings.mlsys.org/paper_files/paper/2023/file/4552cedd396a308320209f75f56a5ad5-Paper-mlsys2023.pdf) |                                                                                  | ![Publish](https://img.shields.io/badge/2023-MLSys-orange) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/SparTA)        |                                              |
|  1 | [SeerAttention](./meta/2024/SeerAttention.prototxt)     | [SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs](http://arxiv.org/abs/2410.13276v2)                                                                            | <img width='400' alt='image' src='./notes/2024/SeerAttention/seerattention.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/SeerAttention) | [note](./notes/2024/SeerAttention/note.md)   |
|  2 | [SeerAttention-R](./meta/2025/SeerAttention-R.prototxt) | [SeerAttention-R: Sparse Attention Adaptation for Long Reasoning](http://arxiv.org/abs/2506.08889v1)                                                                            | <img width='400' alt='image' src='./notes/2025/SeerAttention-R/fig1.png'>        | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/SeerAttention) | [note](./notes/2025/SeerAttention-R/note.md) |</p>
</details>
<details open><summary><b>Fangyun Lin</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Fucong Dai</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Fuli Luo</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                             | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                           | note                                     |
|---:|:------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5)        | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2)  | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                              | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3)  | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeekMoE](./meta/2024/DeepSeekMoE.prototxt) | [DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](http://arxiv.org/abs/2401.06066v1) | <img width='400' alt='image' src='./notes/2024/DeepSeekMoE/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-MoE) | [note](./notes/2024/DeepSeekMoE/note.md) |
|  3 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1)        | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1)  | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Furu Wei</b></summary> 
<p>

|    | meta                                      | ttttttttttttttttttttttttttttttitle                                                                       | ccccccccccccccccccover                                                 | Publish                                                    | codeeeee                                                                  | note                                  |
|---:|:------------------------------------------|:---------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------|:-----------------------------------------------------------|:--------------------------------------------------------------------------|:--------------------------------------|
|  0 | [Q-Sparse](./meta/2024/Q-Sparse.prototxt) | [Q-Sparse: All Large Language Models can be Fully Sparsely-Activated](http://arxiv.org/abs/2407.10969v1) | <img width='400' alt='image' src='./notes/2024/Q-Sparse/q-sparse.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) |                                                                           | [note](./notes/2024/Q-Sparse/note.md) |
|  1 | [ReSA](./meta/2025/ReSA.prototxt)         | [Rectified Sparse Attention](http://arxiv.org/abs/2506.04108v2)                                          | <img width='400' alt='image' src='./notes/2025/ReSA/fig2.png'>         | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/unilm) | [note](./notes/2025/ReSA/note.md)     |</p>
</details>
<details open><summary><b>Genghan Zhang</b></summary> 
<p>

|    | meta                              | ttttttttttttttttttttttttttttttitle                                                                                   | ccccccccccccccccccover                                         | Publish                                                    | codeeeee                                                                           | note                              |
|---:|:----------------------------------|:---------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------------|:----------------------------------|
|  0 | [CATS](./meta/2024/CATS.prototxt) | [CATS: Contextually-Aware Thresholding for Sparsity in Large Language Models](http://arxiv.org/abs/2404.08763v4)     | <img width='400' alt='image' src='./notes/2024/CATS/fig1.png'> | ![Publish](https://img.shields.io/badge/2024-COLM-green)   | ![GitHub Repo stars](https://img.shields.io/github/stars/ScalingIntelligence/CATS) | [note](./notes/2024/CATS/note.md) |
|  1 | [MoA](./meta/2024/MoA.prototxt)   | [MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression](http://arxiv.org/abs/2406.14909v2) | <img width='400' alt='image' src='./notes/2024/MoA/moa.png'>   | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/thu-nics/MoA)             | [note](./notes/2024/MoA/note.md)  |</p>
</details>
<details open><summary><b>Gongfan Fang</b></summary> 
<p>

|    | meta                                          | ttttttttttttttttttttttttttttttitle                                                                         | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                     | note                                    |
|---:|:----------------------------------------------|:-----------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------|:----------------------------------------|
|  0 | [LLM-Pruner](./meta/2023/LLM-Pruner.prototxt) | [LLM-Pruner: On the Structural Pruning of Large Language Models](http://arxiv.org/abs/2305.11627v3)        | <img width='400' alt='image' src='./notes/2023/LLM-Pruner/cover.jpg'> | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/horseee/LLM-Pruner) | [note](./notes/2023/LLM-Pruner/note.md) |
|  1 | [MaskLLM](./meta/2024/MaskLLM.prototxt)       | [MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models](http://arxiv.org/abs/2409.17481v1) | <img width='400' alt='image' src='./notes/2024/MaskLLM/maskllm.png'>  | ![Publish](https://img.shields.io/badge/2024-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/NVlabs/MaskLLM)     | [note](./notes/2024/MaskLLM/note.md)    |</p>
</details>
<details open><summary><b>Guangbo Hao</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Guangxuan Xiao</b></summary> 
<p>

|    | meta                                                | ttttttttttttttttttttttttttttttitle                                                                                         | ccccccccccccccccccover                                                         | Publish                                                    | codeeeee                                                                            | note                                       |
|---:|:----------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------|:-----------------------------------------------------------|:------------------------------------------------------------------------------------|:-------------------------------------------|
|  0 | [streaming-llm](./meta/2024/streaming-llm.prototxt) | [Efficient Streaming Language Models with Attention Sinks](http://arxiv.org/abs/2309.17453v4)                              | <img width='400' alt='image' src='./notes/2024/streaming-llm/cover.jpg'>       | ![Publish](https://img.shields.io/badge/2024-ICLR-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/streaming-llm) | [note](./notes/2024/streaming-llm/note.md) |
|  1 | [Quest](./meta/2024/Quest.prototxt)                 | [Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference](http://arxiv.org/abs/2406.10774)                    | <img width='400' alt='image' src='./notes/2024/Quest/quest.png'>               | ![Publish](https://img.shields.io/badge/2024-ICML-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/quest)         | [note](./notes/2024/Quest/note.md)         |
|  2 | [DuoAttention](./meta/2024/DuoAttention.prototxt)   | [DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads](http://arxiv.org/abs/2410.10819v1) | <img width='400' alt='image' src='./notes/2024/DuoAttention/duoattention.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/duo-attention) | [note](./notes/2024/DuoAttention/note.md)  |
|  3 | [QServe](./meta/2024/QServer.prototxt)              | [QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving](http://arxiv.org/abs/2405.04532v2)           |                                                                                | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | [Pytorch](https://hanlab.mit.edu/projects/qserve)                                   | [note](./notes/2024/QServer/note.md)       |
|  4 | [LServer](./meta/2025/LServer.prototxt)             | [LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention](http://arxiv.org/abs/2502.14866v1)             | <img width='400' alt='image' src='./notes/2025/LServer/fig5.png'>              | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/omniserve)     | [note](./notes/2025/LServer/note.md)       |
|  5 | [XAttention](./meta/2025/XAttention.prototxt)       | [XAttention: Block Sparse Attention with Antidiagonal Scoring](http://arxiv.org/abs/2503.16428v1)                          | <img width='400' alt='image' src='./notes/2025/XAttention/fig1.png'>           | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/x-attention)   | [note](./notes/2025/XAttention/note.md)    |</p>
</details>
<details open><summary><b>Guanting Chen</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Guohao Dai</b></summary> 
<p>

|    | meta                                  | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                           | Publish                                                    | codeeeee                                                                     | note                                  |
|---:|:--------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------|:--------------------------------------|
|  0 | [m](./meta/2024/DHIB73MC.prototxt)    | [A Survey on Efficient Inference for Large Language Models](http://arxiv.org/abs/2404.14294v2)                          | <img width='400' alt='image' src='./notes/2024/DHIB73MC/efficientinference.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) |                                                                              | [note](./notes/2024/DHIB73MC/note.md) |
|  1 | [MoA](./meta/2024/MoA.prototxt)       | [MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression](http://arxiv.org/abs/2406.14909v2)    | <img width='400' alt='image' src='./notes/2024/MoA/moa.png'>                     | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/thu-nics/MoA)       | [note](./notes/2024/MoA/note.md)      |
|  2 | [SpecEE](./meta/2025/SpecEE.prototxt) | [SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting](http://arxiv.org/abs/2504.08850v1) | <img width='400' alt='image' src='./notes/2025/SpecEE/fig9.png'>                 | ![Publish](https://img.shields.io/badge/2025-ISCA-orange)  | ![GitHub Repo stars](https://img.shields.io/github/stars/infinigence/SpecEE) | [note](./notes/2025/SpecEE/note.md)   |</p>
</details>
<details open><summary><b>Guowei Li</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>H. Zhang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Hai Zhao</b></summary> 
<p>

|    | meta                               | ttttttttttttttttttttttttttttttitle                                                                                   | ccccccccccccccccccover                                                 | Publish                                                  | codeeeee                                                                                | note                                  |
|---:|:-----------------------------------|:---------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------|:---------------------------------------------------------|:----------------------------------------------------------------------------------------|:--------------------------------------|
|  0 | [m](./meta/2024/JSHWEV0S.prototxt) | [Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption](http://arxiv.org/abs/2407.18003v3) | <img width='400' alt='image' src='./notes/2024/JSHWEV0S/overview.png'> | ![Publish](https://img.shields.io/badge/2024-COLM-green) | ![GitHub Repo stars](https://img.shields.io/github/stars/zcli-charlie/Awesome-KV-Cache) | [note](./notes/2024/JSHWEV0S/note.md) |
|  1 | [SIFT](./meta/2024/SIFT.prototxt)  | [Sparse is Enough in Fine-tuning Pre-trained Large Language Models](http://arxiv.org/abs/2312.11875v3)               |                                                                        | ![Publish](https://img.shields.io/badge/2024-ICML-blue)  | ![GitHub Repo stars](https://img.shields.io/github/stars/song-wx/SIFT)                  | [note](./notes/2024/SIFT/note.md)     |</p>
</details>
<details open><summary><b>Haibo Chen</b></summary> 
<p>

|    | meta                                              | ttttttttttttttttttttttttttttttitle                                                                                  | ccccccccccccccccccover   | Publish                                                    | codeeeee                                                                        | note                                      |
|---:|:--------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------|:-------------------------|:-----------------------------------------------------------|:--------------------------------------------------------------------------------|:------------------------------------------|
|  0 | [PowerInfer](./meta/2023/PowerInfer.prototxt)     | [PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU](http://arxiv.org/abs/2312.12456v1)        |                          | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/SJTU-IPADS/PowerInfer) | [note](./notes/2023/PowerInfer/note.md)   |
|  1 | [PowerInfer-2](./meta/2024/PowerInfer-2.prototxt) | [PowerInfer-2: Fast Large Language Model Inference on a Smartphone](http://arxiv.org/abs/2406.06282v2)              |                          | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | [Website](https://powerinfer.ai/v2/)                                            | [note](./notes/2024/PowerInfer-2/note.md) |
|  2 | [Turbo Sparse](./meta/2024/TurboSparse.prototxt)  | [Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters](http://arxiv.org/abs/2406.05955v2) |                          | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | [Pytorch](https://huggingface.co/PowerInfer)                                    | [note](./notes/2024/TurboSparse/note.md)  |</p>
</details>
<details open><summary><b>Han Bao</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [m](./meta/2024/0Y41U1N2.prototxt)              | [Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs](http://arxiv.org/abs/2410.16135v1)   | <img width='400' alt='image' src='./notes/2024/0Y41U1N2/cover.png'>   | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) |                                                                                   | [note](./notes/2024/0Y41U1N2/note.md)    |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Hanshi Sun</b></summary> 
<p>

|    | meta                                      | ttttttttttttttttttttttttttttttitle                                                                                               | ccccccccccccccccccover                                                 | Publish                                                    | codeeeee                                                                     | note                                  |
|---:|:------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------|:--------------------------------------|
|  0 | [ShadowKV](./meta/2024/ShadowKV.prototxt) | [ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference](http://arxiv.org/abs/2410.21465v1)                | <img width='400' alt='image' src='./notes/2024/ShadowKV/shadowkv.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/bytedance/ShadowKV) | [note](./notes/2024/ShadowKV/note.md) |
|  1 | [R-KV](./meta/2025/R-KV.prototxt)         | [R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration](http://arxiv.org/abs/2505.24133v2) | <img width='400' alt='image' src='./notes/2025/R-KV/fig1.png'>         | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/Zefan-Cai/R-KV)     | [note](./notes/2025/R-KV/note.md)     |</p>
</details>
<details open><summary><b>Hanwei Xu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Haocheng Wang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Haocheng Xi</b></summary> 
<p>

|    | meta                                          | ttttttttttttttttttttttttttttttitle                                                                          | ccccccccccccccccccover                                               | Publish                                                    | codeeeee                                                                                      | note                                    |
|---:|:----------------------------------------------|:------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------------------|:----------------------------------------|
|  0 | [m](./meta/2023/23LQ9SVH.prototxt)            | [Training Transformers with 4-bit Integers](https://arxiv.org/abs//2306.11987)                              |                                                                      | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/xijiu9/Train_Transformers_with_INT4) |                                         |
|  1 | [SpargeAttn](./meta/2025/SpargeAttn.prototxt) | [SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference](http://arxiv.org/abs/2502.18137v1) | <img width='400' alt='image' src='./notes/2025/SpargeAttn/fig3.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/thu-ml/SpargeAttn)                   | [note](./notes/2025/SpargeAttn/note.md) |</p>
</details>
<details open><summary><b>Haofeng Huang</b></summary> 
<p>

|    | meta                                          | ttttttttttttttttttttttttttttttitle                                                                                   | ccccccccccccccccccover                                               | Publish                                                    | codeeeee                                                                          | note                                    |
|---:|:----------------------------------------------|:---------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:----------------------------------------|
|  0 | [MoA](./meta/2024/MoA.prototxt)               | [MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression](http://arxiv.org/abs/2406.14909v2) | <img width='400' alt='image' src='./notes/2024/MoA/moa.png'>         | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/thu-nics/MoA)            | [note](./notes/2024/MoA/note.md)        |
|  1 | [SpargeAttn](./meta/2025/SpargeAttn.prototxt) | [SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference](http://arxiv.org/abs/2502.18137v1)          | <img width='400' alt='image' src='./notes/2025/SpargeAttn/fig3.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/thu-ml/SpargeAttn)       | [note](./notes/2025/SpargeAttn/note.md) |
|  2 | [XAttention](./meta/2025/XAttention.prototxt) | [XAttention: Block Sparse Attention with Antidiagonal Scoring](http://arxiv.org/abs/2503.16428v1)                    | <img width='400' alt='image' src='./notes/2025/XAttention/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/x-attention) | [note](./notes/2025/XAttention/note.md) |</p>
</details>
<details open><summary><b>Haoli Bai</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                               | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                                                                    | note                                     |
|---:|:------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [RIA](./meta/2024/IA8CS3VH.prototxt)            | [Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models](https://openreview.net/forum?id=Tr0lPx9woF) | <img width='400' alt='image' src='./notes/del/Plug-and-Play.jpg'>     | ![Publish](https://img.shields.io/badge/2024-ICLR-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/biomedical-cybernetics/Relative-importance-and-activation-pruning) |                                          |
|  1 | [LinearPatch](./meta/2025/LinearPatch.prototxt) | [A Simple Linear Patch Revives Layer-Pruned Large Language Models](http://arxiv.org/abs/2505.24680v1)                            | <img width='400' alt='image' src='./notes/2025/LinearPatch/fig3.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) |                                                                                                                             | [note](./notes/2025/LinearPatch/note.md) |
|  2 | [FreqKV](./meta/2025/FreqKV.prototxt)           | [FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension](http://arxiv.org/abs/2505.00570v2)       | <img width='400' alt='image' src='./notes/2025/FreqKV/fig2.png'>      | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) |                                                                                                                             | [note](./notes/2025/FreqKV/note.md)      |</p>
</details>
<details open><summary><b>Haotian Tang</b></summary> 
<p>

|    | meta                                              | ttttttttttttttttttttttttttttttitle                                                                                                                                           | ccccccccccccccccccover                                                         | Publish                                                           | codeeeee                                                                            | note                                      |
|---:|:--------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------|:------------------------------------------------------------------|:------------------------------------------------------------------------------------|:------------------------------------------|
|  0 | [TorchSparse++](./meta/2023/TorchSparse.prototxt) | [TorchSparse++: Efficient Point Cloud Engine](https://openaccess.thecvf.com/content/CVPR2023W/WAD/papers/Tang_TorchSparse_Efficient_Point_Cloud_Engine_CVPRW_2023_paper.pdf) |                                                                                | ![Publish](https://img.shields.io/badge/2023-CVPR_workshop-green) | ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/torchsparse)   |                                           |
|  1 | [AWQ](./meta/2024/awq.prototxt)                   | [AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](https://arxiv.org/abs/2306.00978)                                                           |                                                                                | ![Publish](https://img.shields.io/badge/2024-MLSys-orange)        | ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/llm-awq)       |                                           |
|  2 | [DuoAttention](./meta/2024/DuoAttention.prototxt) | [DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads](http://arxiv.org/abs/2410.10819v1)                                                   | <img width='400' alt='image' src='./notes/2024/DuoAttention/duoattention.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet)        | ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/duo-attention) | [note](./notes/2024/DuoAttention/note.md) |
|  3 | [QServe](./meta/2024/QServer.prototxt)            | [QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving](http://arxiv.org/abs/2405.04532v2)                                                             |                                                                                | ![Publish](https://img.shields.io/badge/2024-arXiv-violet)        | [Pytorch](https://hanlab.mit.edu/projects/qserve)                                   | [note](./notes/2024/QServer/note.md)      |
|  4 | [LServer](./meta/2025/LServer.prototxt)           | [LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention](http://arxiv.org/abs/2502.14866v1)                                                               | <img width='400' alt='image' src='./notes/2025/LServer/fig5.png'>              | ![Publish](https://img.shields.io/badge/2025-arXiv-violet)        | ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/omniserve)     | [note](./notes/2025/LServer/note.md)      |</p>
</details>
<details open><summary><b>Haotong Xie</b></summary> 
<p>

|    | meta                                             | ttttttttttttttttttttttttttttttitle                                                                                  | ccccccccccccccccccover   | Publish                                                    | codeeeee                                                                        | note                                     |
|---:|:-------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------|:-------------------------|:-----------------------------------------------------------|:--------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [PowerInfer](./meta/2023/PowerInfer.prototxt)    | [PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU](http://arxiv.org/abs/2312.12456v1)        |                          | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/SJTU-IPADS/PowerInfer) | [note](./notes/2023/PowerInfer/note.md)  |
|  1 | [Turbo Sparse](./meta/2024/TurboSparse.prototxt) | [Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters](http://arxiv.org/abs/2406.05955v2) |                          | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | [Pytorch](https://huggingface.co/PowerInfer)                                    | [note](./notes/2024/TurboSparse/note.md) |</p>
</details>
<details open><summary><b>Haowei Zhang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Hayden Kwok-Hay So</b></summary> 
<p>

|    | meta                                                    | ttttttttttttttttttttttttttttttitle                                                                   | ccccccccccccccccccover                                                           | Publish                                                    | codeeeee                                                                          | note                                         |
|---:|:--------------------------------------------------------|:-----------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:---------------------------------------------|
|  0 | [SeerAttention](./meta/2024/SeerAttention.prototxt)     | [SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs](http://arxiv.org/abs/2410.13276v2) | <img width='400' alt='image' src='./notes/2024/SeerAttention/seerattention.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/SeerAttention) | [note](./notes/2024/SeerAttention/note.md)   |
|  1 | [SeerAttention-R](./meta/2025/SeerAttention-R.prototxt) | [SeerAttention-R: Sparse Attention Adaptation for Long Reasoning](http://arxiv.org/abs/2506.08889v1) | <img width='400' alt='image' src='./notes/2025/SeerAttention-R/fig1.png'>        | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/SeerAttention) | [note](./notes/2025/SeerAttention-R/note.md) |</p>
</details>
<details open><summary><b>Honghui Ding</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Hongsheng Li</b></summary> 
<p>

|    | meta                                  | ttttttttttttttttttttttttttttttitle                                                                                     | ccccccccccccccccccover                                             | Publish                                                 | codeeeee                                                                      | note                             |
|---:|:--------------------------------------|:-----------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------|:--------------------------------------------------------|:------------------------------------------------------------------------------|:---------------------------------|
|  0 | [SR-STE](./meta/2021/sr-ste.prototxt) | [Learning N:M Fine-grained Structured Sparse Neural Networks From Scratch](https://openreview.net/forum?id=K9bw7vqp_s) | <img width='400' alt='image' src='./notes/2021/sr-ste/sr-ste.jpg'> | ![Publish](https://img.shields.io/badge/2021-ICLR-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/aojunzz/NM-sparsity) |                                  |
|  1 | [SPP](./meta/2024/SPP.prototxt)       | [SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models](http://arxiv.org/abs/2405.16057v1) | <img width='400' alt='image' src='./notes/2024/SPP/spp.png'>       | ![Publish](https://img.shields.io/badge/2024-ICML-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/Lucky-Lance/SPP)     | [note](./notes/2024/SPP/note.md) |</p>
</details>
<details open><summary><b>Huajian Xin</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Huazuo Gao</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                             | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                           | note                                     |
|---:|:------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5)        | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2)  | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                              | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3)  | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeekMoE](./meta/2024/DeepSeekMoE.prototxt) | [DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](http://arxiv.org/abs/2401.06066v1) | <img width='400' alt='image' src='./notes/2024/DeepSeekMoE/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-MoE) | [note](./notes/2024/DeepSeekMoE/note.md) |
|  3 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1)        | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1)  | [note](./notes/2025/DeepSeek-R1/note.md) |
|  4 | [NSA](./meta/2025/NSA.prototxt)                 | [Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention](http://arxiv.org/abs/2502.11089v1)         | <img width='400' alt='image' src='./notes/2025/NSA/fig2.png'>         | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) |                                                                                    | [note](./notes/2025/NSA/note.md)         |</p>
</details>
<details open><summary><b>Hui Li</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Hui Qu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Huiqiang Jiang</b></summary> 
<p>

|    | meta                                          | ttttttttttttttttttttttttttttttitle                                                                                               | ccccccccccccccccccover                                                            | Publish                                                    | codeeeee                                                                       | note                                    |
|---:|:----------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------------------------|:-------------------------------------------------------------------------------|:----------------------------------------|
|  0 | [MInference](./meta/2024/MInference.prototxt) | [MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention](http://arxiv.org/abs/2407.02490v1) | <img width='400' alt='image' src='./notes/2024/MInference/MInference_3shape.PNG'> | ![Publish](https://img.shields.io/badge/2024-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/MInference) | [note](./notes/2024/MInference/note.md) |
|  1 | [SCBench](./meta/2024/SCBench.prototxt)       | [SCBench: A KV Cache-Centric Analysis of Long-Context Methods](http://arxiv.org/abs/2412.10319v2)                                |                                                                                   | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/MInference) | [note](./notes/2024/SCBench/note.md)    |</p>
</details>
<details open><summary><b>Iman Mirzadeh</b></summary> 
<p>

|    | meta                                                  | ttttttttttttttttttttttttttttttitle                                                                              | ccccccccccccccccccover                                                          | Publish                                                       | codeeeee                                                                        | note                                        |
|---:|:------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------|:--------------------------------------------------------------|:--------------------------------------------------------------------------------|:--------------------------------------------|
|  0 | [LLM in a flash](./meta/2023/LLM_in_a_flash.prototxt) | [LLM in a flash: Efficient Large Language Model Inference with Limited Memory](http://arxiv.org/abs/2312.11514) | <img width='400' alt='image' src='./notes/2023/LLM_in_a_flash/windows.png'>     | ![Publish](https://img.shields.io/badge/2023-arXiv-violet)    |                                                                                 | [note](./notes/2023/LLM_in_a_flash/note.md) |
|  1 | [ReLU Strikes Back](./meta/2024/HMR7HKFV.prototxt)    | [ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models](https://arxiv.org/abs/2310.04564)  | <img width='400' alt='image' src='./notes/2024/HMR7HKFV/ReLU_Strikes_Back.jpg'> | ![Publish](https://img.shields.io/badge/2024-ICLR_oral-green) | ![GitHub Repo stars](https://img.shields.io/github/stars/sjtu-ipads/powerinfer) |                                             |</p>
</details>
<details open><summary><b>Ion Stoica</b></summary> 
<p>

|    | meta                                                   | ttttttttttttttttttttttttttttttitle                                                                                     | ccccccccccccccccccover                                                   | Publish                                                    | codeeeee                                                                           | note                                        |
|---:|:-------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------------|:--------------------------------------------|
|  0 | [ActNN](./meta/2019/actnn.prototxt)                    | [ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training](https://arxiv.org/abs/2104.14129) |                                                                          | ![Publish](https://img.shields.io/badge/2019-ICML-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/ucbrise/actnn)            |                                             |
|  1 | [Paged Attention](./meta/2023/PagedAttention.prototxt) | [Efficient Memory Management for Large Language Model Serving with PagedAttention](http://arxiv.org/abs/2309.06180v1)  | <img width='400' alt='image' src='./notes/2023/PagedAttention/vllm.png'> | ![Publish](https://img.shields.io/badge/2023-SOSP-orange)  | ![GitHub Repo stars](https://img.shields.io/github/stars/vllm-project/vllm)        | [note](./notes/2023/PagedAttention/note.md) |
|  2 | [SGLang](./meta/2024/SGLang.prototxt)                  | [SGLang: Efficient Execution of Structured Language Model Programs](http://arxiv.org/abs/2312.07104v2)                 | <img width='400' alt='image' src='./notes/2024/SGLang/fig9.png'>         | ![Publish](https://img.shields.io/badge/2024-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/sgl-project/sglang)       | [note](./notes/2024/SGLang/note.md)         |
|  3 | [DoubleSparsity](./meta/2024/DoubleSparsity.prototxt)  | [Post-Training Sparse Attention with Double Sparsity](http://arxiv.org/abs/2408.07092v2)                               |                                                                          | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/andy-yang-1/DoubleSparse) | [note](./notes/2024/DoubleSparsity/note.md) |</p>
</details>
<details open><summary><b>J. L. Cai</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>J. Zico Kolter</b></summary> 
<p>

|    | meta                                                            | ttttttttttttttttttttttttttttttitle                                                                   | ccccccccccccccccccover                                                               | Publish                                                    | codeeeee                                                                               | note                                             |
|---:|:----------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------|:-----------------------------------------------------------|:---------------------------------------------------------------------------------------|:-------------------------------------------------|
|  0 | [Wanda](./meta/2024/Wanda.prototxt)                             | [A Simple and Effective Pruning Approach for Large Language Models](http://arxiv.org/abs/2306.11695) | <img width='400' alt='image' src='./notes/2024/Wanda/wanda.png'>                     | ![Publish](https://img.shields.io/badge/2024-ICLR-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/locuslab/wanda)               | [note](./notes/2024/Wanda/note.md)               |
|  1 | [massive-activations](./meta/2024/massive-activations.prototxt) | [Massive Activations in Large Language Models](http://arxiv.org/abs/2402.17762v2)                    | <img width='400' alt='image' src='./notes/2024/massive-activations/massive_act.jpg'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/locuslab/massive-activations) | [note](./notes/2024/massive-activations/note.md) |</p>
</details>
<details open><summary><b>Jan Kautz</b></summary> 
<p>

|    | meta                                      | ttttttttttttttttttttttttttttttitle                                                                         | ccccccccccccccccccover                                                 | Publish                                                    | codeeeee                                                                  | note                                  |
|---:|:------------------------------------------|:-----------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------|:-----------------------------------------------------------|:--------------------------------------------------------------------------|:--------------------------------------|
|  0 | [MaskLLM](./meta/2024/MaskLLM.prototxt)   | [MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models](http://arxiv.org/abs/2409.17481v1) | <img width='400' alt='image' src='./notes/2024/MaskLLM/maskllm.png'>   | ![Publish](https://img.shields.io/badge/2024-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/NVlabs/MaskLLM)  | [note](./notes/2024/MaskLLM/note.md)  |
|  1 | [Minitron](./meta/2024/Minitron.prototxt) | [Compact Language Models via Pruning and Knowledge Distillation](https://arxiv.org/abs/2408.11796v2)       | <img width='400' alt='image' src='./notes/2024/Minitron/minitron.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/NVlabs/Minitron) | [note](./notes/2024/Minitron/note.md) |</p>
</details>
<details open><summary><b>Jason D. Lee</b></summary> 
<p>

|    | meta                               | ttttttttttttttttttttttttttttttitle                                                                                          | ccccccccccccccccccover   | Publish                                                    | codeeeee                                                                     | note                                  |
|---:|:-----------------------------------|:----------------------------------------------------------------------------------------------------------------------------|:-------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------|:--------------------------------------|
|  0 | [MeZO](./meta/2023/MeZO.prototxt)  | [Fine-Tuning Language Models with Just Forward Passes](http://arxiv.org/abs/2305.17333v3)                                   |                          | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/princeton-nlp/MeZO) | [note](./notes/2023/MeZO/note.md)     |
|  1 | [m](./meta/2024/SN1PK7EK.prototxt) | [Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark](http://arxiv.org/abs/2402.11592v2) |                          | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/ZO-Bench/ZO-LLM)    | [note](./notes/2024/SN1PK7EK/note.md) |</p>
</details>
<details open><summary><b>Jayashree Mohan</b></summary> 
<p>

|    | meta                                                | ttttttttttttttttttttttttttttttitle                                                                                 | ccccccccccccccccccover                                                  | Publish                                                     | codeeeee                                                                       | note                                       |
|---:|:----------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------|:------------------------------------------------------------|:-------------------------------------------------------------------------------|:-------------------------------------------|
|  0 | [POD-Attention](./meta/2025/POD-Attention.prototxt) | [POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference](http://arxiv.org/abs/2410.18038v2) | <img width='400' alt='image' src='./notes/2025/POD-Attention/fig2.png'> | ![Publish](https://img.shields.io/badge/2025-ASPLOS-orange) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/vattention) | [note](./notes/2025/POD-Attention/note.md) |
|  1 | [vAttention](./meta/2025/vAttention.prototxt)       | [vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention](http://arxiv.org/abs/2405.04437v3) | <img width='400' alt='image' src='./notes/2025/vAttention/fig5.png'>    | ![Publish](https://img.shields.io/badge/2025-ASPLOS-orange) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/vattention) | [note](./notes/2025/vAttention/note.md)    |</p>
</details>
<details open><summary><b>Jeff Pool</b></summary> 
<p>

|    | meta                                    | ttttttttttttttttttttttttttttttitle                                                                                                     | ccccccccccccccccccover                                               | Publish                                                    | codeeeee                                                                 | note                                 |
|---:|:----------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------|:-----------------------------------------------------------|:-------------------------------------------------------------------------|:-------------------------------------|
|  0 | [m](./meta/2021/K7GSWQIC.prototxt)      | [Channel Permutations for N:M Sparsity](https://proceedings.neurips.cc/paper/2021/hash/6e8404c3b93a9527c8db241a1846599a-Abstract.html) |                                                                      | ![Publish](https://img.shields.io/badge/2021-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/NVIDIA/apex)    |                                      |
|  1 | [MaskLLM](./meta/2024/MaskLLM.prototxt) | [MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models](http://arxiv.org/abs/2409.17481v1)                             | <img width='400' alt='image' src='./notes/2024/MaskLLM/maskllm.png'> | ![Publish](https://img.shields.io/badge/2024-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/NVlabs/MaskLLM) | [note](./notes/2024/MaskLLM/note.md) |</p>
</details>
<details open><summary><b>Jiaming Tang</b></summary> 
<p>

|    | meta                                              | ttttttttttttttttttttttttttttttitle                                                                                         | ccccccccccccccccccover                                                         | Publish                                                    | codeeeee                                                                            | note                                      |
|---:|:--------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------|:-----------------------------------------------------------|:------------------------------------------------------------------------------------|:------------------------------------------|
|  0 | [Quest](./meta/2024/Quest.prototxt)               | [Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference](http://arxiv.org/abs/2406.10774)                    | <img width='400' alt='image' src='./notes/2024/Quest/quest.png'>               | ![Publish](https://img.shields.io/badge/2024-ICML-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/quest)         | [note](./notes/2024/Quest/note.md)        |
|  1 | [AWQ](./meta/2024/awq.prototxt)                   | [AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](https://arxiv.org/abs/2306.00978)         |                                                                                | ![Publish](https://img.shields.io/badge/2024-MLSys-orange) | ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/llm-awq)       |                                           |
|  2 | [DuoAttention](./meta/2024/DuoAttention.prototxt) | [DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads](http://arxiv.org/abs/2410.10819v1) | <img width='400' alt='image' src='./notes/2024/DuoAttention/duoattention.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/duo-attention) | [note](./notes/2024/DuoAttention/note.md) |
|  3 | [LServer](./meta/2025/LServer.prototxt)           | [LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention](http://arxiv.org/abs/2502.14866v1)             | <img width='400' alt='image' src='./notes/2025/LServer/fig5.png'>              | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/omniserve)     | [note](./notes/2025/LServer/note.md)      |</p>
</details>
<details open><summary><b>Jiaming Xu</b></summary> 
<p>

|    | meta                                  | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                           | Publish                                                    | codeeeee                                                                     | note                                  |
|---:|:--------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------|:--------------------------------------|
|  0 | [m](./meta/2024/DHIB73MC.prototxt)    | [A Survey on Efficient Inference for Large Language Models](http://arxiv.org/abs/2404.14294v2)                          | <img width='400' alt='image' src='./notes/2024/DHIB73MC/efficientinference.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) |                                                                              | [note](./notes/2024/DHIB73MC/note.md) |
|  1 | [SpecEE](./meta/2025/SpecEE.prototxt) | [SpecEE: Accelerating Large Language Model Inference with Speculative Early Exiting](http://arxiv.org/abs/2504.08850v1) | <img width='400' alt='image' src='./notes/2025/SpecEE/fig9.png'>                 | ![Publish](https://img.shields.io/badge/2025-ISCA-orange)  | ![GitHub Repo stars](https://img.shields.io/github/stars/infinigence/SpecEE) | [note](./notes/2025/SpecEE/note.md)   |</p>
</details>
<details open><summary><b>Jian Liang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Jianfei Chen</b></summary> 
<p>

|    | meta                                                                | ttttttttttttttttttttttttttttttitle                                                                                     | ccccccccccccccccccover                                                          | Publish                                                    | codeeeee                                                                                      | note                                               |
|---:|:--------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------------------|:---------------------------------------------------|
|  0 | [ActNN](./meta/2019/actnn.prototxt)                                 | [ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training](https://arxiv.org/abs/2104.14129) |                                                                                 | ![Publish](https://img.shields.io/badge/2019-ICML-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/ucbrise/actnn)                       |                                                    |
|  1 | [m](./meta/2023/23LQ9SVH.prototxt)                                  | [Training Transformers with 4-bit Integers](https://arxiv.org/abs//2306.11987)                                         |                                                                                 | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/xijiu9/Train_Transformers_with_INT4) |                                                    |
|  2 | [m](./meta/2024/HYPL7G37.prototxt)                                  | [Accelerating Transformer Pre-training with 2:4 Sparsity](http://arxiv.org/abs/2404.01847v2)                           |                                                                                 | ![Publish](https://img.shields.io/badge/2024-ICML-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/huyz2023/2by4-pretrain)              | [note](./notes/2024/HYPL7G37/note.md)              |
|  3 | [m](./meta/2024/0Y41U1N2.prototxt)                                  | [Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs](http://arxiv.org/abs/2410.16135v1)  | <img width='400' alt='image' src='./notes/2024/0Y41U1N2/cover.png'>             | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) |                                                                                               | [note](./notes/2024/0Y41U1N2/note.md)              |
|  4 | [ReMoE](./meta/2024/ReMoE.prototxt)                                 | [ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing](http://arxiv.org/abs/2412.14711v1)                  | <img width='400' alt='image' src='./notes/2024/ReMoE/fig1.png'>                 | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/thu-ml/ReMoE)                        | [note](./notes/2024/ReMoE/note.md)                 |
|  5 | [AdaptiveSparseTrainer](./meta/2025/AdaptiveSparseTrainer.prototxt) | [Pruning Large Language Models with Semi-Structural Adaptive Sparse Training](http://arxiv.org/abs/2407.20584v3)       | <img width='400' alt='image' src='./notes/2025/AdaptiveSparseTrainer/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-AAAI-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/thu-ml/Adaptive-Sparse-Trainer)      | [note](./notes/2025/AdaptiveSparseTrainer/note.md) |
|  6 | [SpargeAttn](./meta/2025/SpargeAttn.prototxt)                       | [SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference](http://arxiv.org/abs/2502.18137v1)            | <img width='400' alt='image' src='./notes/2025/SpargeAttn/fig3.png'>            | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/thu-ml/SpargeAttn)                   | [note](./notes/2025/SpargeAttn/note.md)            |</p>
</details>
<details open><summary><b>Jianzhong Guo</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Jiaqi Ni</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Jiashi Li</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                             | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                           | note                                     |
|---:|:------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5)        | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2)  | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                              | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3)  | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeekMoE](./meta/2024/DeepSeekMoE.prototxt) | [DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](http://arxiv.org/abs/2401.06066v1) | <img width='400' alt='image' src='./notes/2024/DeepSeekMoE/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-MoE) | [note](./notes/2024/DeepSeekMoE/note.md) |
|  3 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1)        | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1)  | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Jiawei Wang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Jie Zhou</b></summary> 
<p>

|    | meta                                        | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                              | Publish                                                    | codeeeee                                                                  | note                                   |
|---:|:--------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------|:-----------------------------------------------------------|:--------------------------------------------------------------------------|:---------------------------------------|
|  0 | [DBudgetKV](./meta/2025/DBudgetKV.prototxt) | [DBudgetKV: Dynamic Budget in KV Cache Compression for Ensuring Optimal Performance](http://arxiv.org/abs/2502.16886v1) | <img width='400' alt='image' src='./notes/2025/DBudgetKV/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) |                                                                           | [note](./notes/2025/DBudgetKV/note.md) |
|  1 | [MiniCPM4](./meta/2025/MiniCPM4.prototxt)   | [MiniCPM4: Ultra-Efficient LLMs on End Devices](http://arxiv.org/abs/2506.07900v1)                                      | <img width='400' alt='image' src='./notes/2025/MiniCPM4/fig2.png'>  | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/openbmb/minicpm) | [note](./notes/2025/MiniCPM4/note.md)  |</p>
</details>
<details open><summary><b>Jin Chen</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Jingchang Chen</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Jingyang Yuan</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |
|  3 | [NSA](./meta/2025/NSA.prototxt)                 | [Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention](http://arxiv.org/abs/2502.11089v1)  | <img width='400' alt='image' src='./notes/2025/NSA/fig2.png'>         | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) |                                                                                   | [note](./notes/2025/NSA/note.md)         |</p>
</details>
<details open><summary><b>Joseph E. Gonzalez</b></summary> 
<p>

|    | meta                                                   | ttttttttttttttttttttttttttttttitle                                                                                     | ccccccccccccccccccover                                                   | Publish                                                    | codeeeee                                                                           | note                                        |
|---:|:-------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------------|:--------------------------------------------|
|  0 | [ActNN](./meta/2019/actnn.prototxt)                    | [ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training](https://arxiv.org/abs/2104.14129) |                                                                          | ![Publish](https://img.shields.io/badge/2019-ICML-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/ucbrise/actnn)            |                                             |
|  1 | [Paged Attention](./meta/2023/PagedAttention.prototxt) | [Efficient Memory Management for Large Language Model Serving with PagedAttention](http://arxiv.org/abs/2309.06180v1)  | <img width='400' alt='image' src='./notes/2023/PagedAttention/vllm.png'> | ![Publish](https://img.shields.io/badge/2023-SOSP-orange)  | ![GitHub Repo stars](https://img.shields.io/github/stars/vllm-project/vllm)        | [note](./notes/2023/PagedAttention/note.md) |
|  2 | [SGLang](./meta/2024/SGLang.prototxt)                  | [SGLang: Efficient Execution of Structured Language Model Programs](http://arxiv.org/abs/2312.07104v2)                 | <img width='400' alt='image' src='./notes/2024/SGLang/fig9.png'>         | ![Publish](https://img.shields.io/badge/2024-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/sgl-project/sglang)       | [note](./notes/2024/SGLang/note.md)         |
|  3 | [DoubleSparsity](./meta/2024/DoubleSparsity.prototxt)  | [Post-Training Sparse Attention with Double Sparsity](http://arxiv.org/abs/2408.07092v2)                               |                                                                          | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/andy-yang-1/DoubleSparse) | [note](./notes/2024/DoubleSparsity/note.md) |</p>
</details>
<details open><summary><b>Jun Zhu</b></summary> 
<p>

|    | meta                                                                | ttttttttttttttttttttttttttttttitle                                                                               | ccccccccccccccccccover                                                          | Publish                                                    | codeeeee                                                                                      | note                                               |
|---:|:--------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------------------|:---------------------------------------------------|
|  0 | [m](./meta/2023/23LQ9SVH.prototxt)                                  | [Training Transformers with 4-bit Integers](https://arxiv.org/abs//2306.11987)                                   |                                                                                 | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/xijiu9/Train_Transformers_with_INT4) |                                                    |
|  1 | [m](./meta/2024/HYPL7G37.prototxt)                                  | [Accelerating Transformer Pre-training with 2:4 Sparsity](http://arxiv.org/abs/2404.01847v2)                     |                                                                                 | ![Publish](https://img.shields.io/badge/2024-ICML-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/huyz2023/2by4-pretrain)              | [note](./notes/2024/HYPL7G37/note.md)              |
|  2 | [ReMoE](./meta/2024/ReMoE.prototxt)                                 | [ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing](http://arxiv.org/abs/2412.14711v1)            | <img width='400' alt='image' src='./notes/2024/ReMoE/fig1.png'>                 | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/thu-ml/ReMoE)                        | [note](./notes/2024/ReMoE/note.md)                 |
|  3 | [AdaptiveSparseTrainer](./meta/2025/AdaptiveSparseTrainer.prototxt) | [Pruning Large Language Models with Semi-Structural Adaptive Sparse Training](http://arxiv.org/abs/2407.20584v3) | <img width='400' alt='image' src='./notes/2025/AdaptiveSparseTrainer/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-AAAI-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/thu-ml/Adaptive-Sparse-Trainer)      | [note](./notes/2025/AdaptiveSparseTrainer/note.md) |
|  4 | [SpargeAttn](./meta/2025/SpargeAttn.prototxt)                       | [SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference](http://arxiv.org/abs/2502.18137v1)      | <img width='400' alt='image' src='./notes/2025/SpargeAttn/fig3.png'>            | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/thu-ml/SpargeAttn)                   | [note](./notes/2025/SpargeAttn/note.md)            |</p>
</details>
<details open><summary><b>Junjie Qiu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Junlong Li</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Junxian Guo</b></summary> 
<p>

|    | meta                                              | ttttttttttttttttttttttttttttttitle                                                                                         | ccccccccccccccccccover                                                         | Publish                                                    | codeeeee                                                                            | note                                      |
|---:|:--------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------|:-----------------------------------------------------------|:------------------------------------------------------------------------------------|:------------------------------------------|
|  0 | [DuoAttention](./meta/2024/DuoAttention.prototxt) | [DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads](http://arxiv.org/abs/2410.10819v1) | <img width='400' alt='image' src='./notes/2024/DuoAttention/duoattention.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/duo-attention) | [note](./notes/2024/DuoAttention/note.md) |
|  1 | [LServer](./meta/2025/LServer.prototxt)           | [LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention](http://arxiv.org/abs/2502.14866v1)             | <img width='400' alt='image' src='./notes/2025/LServer/fig5.png'>              | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/omniserve)     | [note](./notes/2025/LServer/note.md)      |
|  2 | [XAttention](./meta/2025/XAttention.prototxt)     | [XAttention: Block Sparse Attention with Antidiagonal Scoring](http://arxiv.org/abs/2503.16428v1)                          | <img width='400' alt='image' src='./notes/2025/XAttention/fig1.png'>           | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/x-attention)   | [note](./notes/2025/XAttention/note.md)   |</p>
</details>
<details open><summary><b>Junxiao Song</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Kai Dong</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Kai Hu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Kaige Gao</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Kang Guan</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Kang Zhao</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                    | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                         | note                                     |
|---:|:------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:---------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [m](./meta/2024/HYPL7G37.prototxt)              | [Accelerating Transformer Pre-training with 2:4 Sparsity](http://arxiv.org/abs/2404.01847v2)                          |                                                                       | ![Publish](https://img.shields.io/badge/2024-ICML-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/huyz2023/2by4-pretrain) | [note](./notes/2024/HYPL7G37/note.md)    |
|  1 | [m](./meta/2024/0Y41U1N2.prototxt)              | [Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs](http://arxiv.org/abs/2410.16135v1) | <img width='400' alt='image' src='./notes/2024/0Y41U1N2/cover.png'>   | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) |                                                                                  | [note](./notes/2024/0Y41U1N2/note.md)    |
|  2 | [LinearPatch](./meta/2025/LinearPatch.prototxt) | [A Simple Linear Patch Revives Layer-Pruned Large Language Models](http://arxiv.org/abs/2505.24680v1)                 | <img width='400' alt='image' src='./notes/2025/LinearPatch/fig3.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) |                                                                                  | [note](./notes/2025/LinearPatch/note.md) |</p>
</details>
<details open><summary><b>Kexin Huang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Kuai Yu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Kurt Keutzer</b></summary> 
<p>

|    | meta                                                | ttttttttttttttttttttttttttttttitle                                                                                       | ccccccccccccccccccover                                                     | Publish                                                    | codeeeee                                                                                     | note                                       |
|---:|:----------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------|:-----------------------------------------------------------|:---------------------------------------------------------------------------------------------|:-------------------------------------------|
|  0 | [FisherPruning](./meta/2022/fisherpruning.prototxt) | [A Fast Post-Training Pruning Framework for Transformers](http://arxiv.org/abs/2204.09656v2)                             | <img width='400' alt='image' src='./notes/2022/fisherpruning/cover.jpg'>   | ![Publish](https://img.shields.io/badge/2022-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/WoosukKwon/retraining-free-pruning) | [note](./notes/2022/fisherpruning/note.md) |
|  1 | [SqueezeLLM](./meta/2024/SqueezeLLM.prototxt)       | [SqueezeLLM: Dense-and-Sparse Quantization](http://arxiv.org/abs/2306.07629v4)                                           | <img width='400' alt='image' src='./notes/2024/SqueezeLLM/squeezeLLM.png'> | ![Publish](https://img.shields.io/badge/2024-ICML-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/SqueezeAILab/SqueezeLLM)            | [note](./notes/2024/SqueezeLLM/note.md)    |
|  2 | [KVQuant](./meta/2024/KVQuant.prototxt)             | [KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization](http://arxiv.org/abs/2401.18079v2) |                                                                            | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/SqueezeAILab/KVQuant)               | [note](./notes/2024/KVQuant/note.md)       |</p>
</details>
<details open><summary><b>Lean Wang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |
|  3 | [NSA](./meta/2025/NSA.prototxt)                 | [Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention](http://arxiv.org/abs/2502.11089v1)  | <img width='400' alt='image' src='./notes/2025/NSA/fig2.png'>         | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) |                                                                                   | [note](./notes/2025/NSA/note.md)         |</p>
</details>
<details open><summary><b>Lecong Zhang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Lei Chen</b></summary> 
<p>

|    | meta                                                          | ttttttttttttttttttttttttttttttitle                                                                              | ccccccccccccccccccover                                                       | Publish                                                    | codeeeee                                                                                         | note                                            |
|---:|:--------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------|:-----------------------------------------------------------|:-------------------------------------------------------------------------------------------------|:------------------------------------------------|
|  0 | [PWGG5HBE](./meta/2024/PWGG5HBE.prototxt)                     | [A Survey on Large Language Model Acceleration based on KV Cache Management](http://arxiv.org/abs/2412.19442v2) | <img width='400' alt='image' src='./notes/2024/PWGG5HBE/fig2.png'>           | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/TreeAI-Lab/Awesome-KV-Cache-Management) | [note](./notes/2024/PWGG5HBE/note.md)           |
|  1 | [AttentionPredictor](./meta/2025/AttentionPredictor.prototxt) | [AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference](http://arxiv.org/abs/2502.04077v1)   | <img width='400' alt='image' src='./notes/2025/AttentionPredictor/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) |                                                                                                  | [note](./notes/2025/AttentionPredictor/note.md) |</p>
</details>
<details open><summary><b>Lei Xu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Leyi Xia</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Li Dong</b></summary> 
<p>

|    | meta                                                    | ttttttttttttttttttttttttttttttitle                                                                   | ccccccccccccccccccover                                                    | Publish                                                    | codeeeee                                                                          | note                                         |
|---:|:--------------------------------------------------------|:-----------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:---------------------------------------------|
|  0 | [ReSA](./meta/2025/ReSA.prototxt)                       | [Rectified Sparse Attention](http://arxiv.org/abs/2506.04108v2)                                      | <img width='400' alt='image' src='./notes/2025/ReSA/fig2.png'>            | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/unilm)         | [note](./notes/2025/ReSA/note.md)            |
|  1 | [SeerAttention-R](./meta/2025/SeerAttention-R.prototxt) | [SeerAttention-R: Sparse Attention Adaptation for Long Reasoning](http://arxiv.org/abs/2506.08889v1) | <img width='400' alt='image' src='./notes/2025/SeerAttention-R/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/SeerAttention) | [note](./notes/2025/SeerAttention-R/note.md) |</p>
</details>
<details open><summary><b>Li-Wen Chang</b></summary> 
<p>

|    | meta                                      | ttttttttttttttttttttttttttttttitle                                                                                               | ccccccccccccccccccover                                                 | Publish                                                    | codeeeee                                                                     | note                                  |
|---:|:------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------|:--------------------------------------|
|  0 | [ShadowKV](./meta/2024/ShadowKV.prototxt) | [ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference](http://arxiv.org/abs/2410.21465v1)                | <img width='400' alt='image' src='./notes/2024/ShadowKV/shadowkv.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/bytedance/ShadowKV) | [note](./notes/2024/ShadowKV/note.md) |
|  1 | [R-KV](./meta/2025/R-KV.prototxt)         | [R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration](http://arxiv.org/abs/2505.24133v2) | <img width='400' alt='image' src='./notes/2025/R-KV/fig1.png'>         | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/Zefan-Cai/R-KV)     | [note](./notes/2025/R-KV/note.md)     |</p>
</details>
<details open><summary><b>Lian Liu</b></summary> 
<p>

|    | meta                                | ttttttttttttttttttttttttttttttitle                                                                                               | ccccccccccccccccccover                                          | Publish                                                     | codeeeee   | note                               |
|---:|:------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------|:------------------------------------------------------------|:-----------|:-----------------------------------|
|  0 | [COMET](./meta/2025/COMET.prototxt) | [COMET: Towards Partical W4A4KV4 LLMs Serving](http://arxiv.org/abs/2410.12168v1)                                                | <img width='400' alt='image' src='./notes/2025/COMET/fig5.png'> | ![Publish](https://img.shields.io/badge/2025-ASPLOS-orange) |            | [note](./notes/2025/COMET/note.md) |
|  1 | [SDS](./meta/2025/SDS.prototxt)     | [Enhancing One-shot Pruned Pre-trained Language Models through Sparse-Dense-Sparse Mechanism](http://arxiv.org/abs/2408.10473v1) | <img width='400' alt='image' src='./notes/2025/SDS/sds.png'>    | ![Publish](https://img.shields.io/badge/2025-Coling-green)  |            | [note](./notes/2025/SDS/note.md)   |</p>
</details>
<details open><summary><b>Liang Zhao</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [SparseLLM](./meta/2024/SparseLLM.prototxt)     | [SparseLLM: Towards Global Pruning for Pre-trained Language Models](http://arxiv.org/abs/2402.17946v3)                  |                                                                       | ![Publish](https://img.shields.io/badge/2024-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/BaiTheBest/SparseLLM)    | [note](./notes/2024/SparseLLM/note.md)   |
|  1 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  2 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  3 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |
|  4 | [NSA](./meta/2025/NSA.prototxt)                 | [Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention](http://arxiv.org/abs/2502.11089v1)  | <img width='400' alt='image' src='./notes/2025/NSA/fig2.png'>         | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) |                                                                                   | [note](./notes/2025/NSA/note.md)         |</p>
</details>
<details open><summary><b>Lianmin Zheng</b></summary> 
<p>

|    | meta                                                   | ttttttttttttttttttttttttttttttitle                                                                                         | ccccccccccccccccccover                                                   | Publish                                                    | codeeeee                                                                           | note                                        |
|---:|:-------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------------|:--------------------------------------------|
|  0 | [ActNN](./meta/2019/actnn.prototxt)                    | [ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training](https://arxiv.org/abs/2104.14129)     |                                                                          | ![Publish](https://img.shields.io/badge/2019-ICML-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/ucbrise/actnn)            |                                             |
|  1 | [Paged Attention](./meta/2023/PagedAttention.prototxt) | [Efficient Memory Management for Large Language Model Serving with PagedAttention](http://arxiv.org/abs/2309.06180v1)      | <img width='400' alt='image' src='./notes/2023/PagedAttention/vllm.png'> | ![Publish](https://img.shields.io/badge/2023-SOSP-orange)  | ![GitHub Repo stars](https://img.shields.io/github/stars/vllm-project/vllm)        | [note](./notes/2023/PagedAttention/note.md) |
|  2 | [H2O](./meta/2023/H2O.prototxt)                        | [H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models](http://arxiv.org/abs/2306.14048) | <img width='400' alt='image' src='./notes/2023/H2O/h2o.png'>             | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/FMInference/H2O)          | [note](./notes/2023/H2O/note.md)            |
|  3 | [SGLang](./meta/2024/SGLang.prototxt)                  | [SGLang: Efficient Execution of Structured Language Model Programs](http://arxiv.org/abs/2312.07104v2)                     | <img width='400' alt='image' src='./notes/2024/SGLang/fig9.png'>         | ![Publish](https://img.shields.io/badge/2024-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/sgl-project/sglang)       | [note](./notes/2024/SGLang/note.md)         |
|  4 | [DoubleSparsity](./meta/2024/DoubleSparsity.prototxt)  | [Post-Training Sparse Attention with Double Sparsity](http://arxiv.org/abs/2408.07092v2)                                   |                                                                          | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/andy-yang-1/DoubleSparse) | [note](./notes/2024/DoubleSparsity/note.md) |</p>
</details>
<details open><summary><b>Lili Qiu</b></summary> 
<p>

|    | meta                                          | ttttttttttttttttttttttttttttttitle                                                                                               | ccccccccccccccccccover                                                            | Publish                                                    | codeeeee                                                                       | note                                    |
|---:|:----------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------------------------|:-------------------------------------------------------------------------------|:----------------------------------------|
|  0 | [MInference](./meta/2024/MInference.prototxt) | [MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention](http://arxiv.org/abs/2407.02490v1) | <img width='400' alt='image' src='./notes/2024/MInference/MInference_3shape.PNG'> | ![Publish](https://img.shields.io/badge/2024-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/MInference) | [note](./notes/2024/MInference/note.md) |
|  1 | [SCBench](./meta/2024/SCBench.prototxt)       | [SCBench: A KV Cache-Centric Analysis of Long-Context Methods](http://arxiv.org/abs/2412.10319v2)                                |                                                                                   | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/MInference) | [note](./notes/2024/SCBench/note.md)    |</p>
</details>
<details open><summary><b>Litong Wang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Liyue Zhang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Lu Hou</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                               | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                                                                    | note                                     |
|---:|:------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [RIA](./meta/2024/IA8CS3VH.prototxt)            | [Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models](https://openreview.net/forum?id=Tr0lPx9woF) | <img width='400' alt='image' src='./notes/del/Plug-and-Play.jpg'>     | ![Publish](https://img.shields.io/badge/2024-ICLR-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/biomedical-cybernetics/Relative-importance-and-activation-pruning) |                                          |
|  1 | [LinearPatch](./meta/2025/LinearPatch.prototxt) | [A Simple Linear Patch Revives Layer-Pruned Large Language Models](http://arxiv.org/abs/2505.24680v1)                            | <img width='400' alt='image' src='./notes/2025/LinearPatch/fig3.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) |                                                                                                                             | [note](./notes/2025/LinearPatch/note.md) |</p>
</details>
<details open><summary><b>Mao Yang</b></summary> 
<p>

|    | meta                                                    | ttttttttttttttttttttttttttttttitle                                                                                                  | ccccccccccccccccccover                                                           | Publish                                                    | codeeeee                                                                          | note                                         |
|---:|:--------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:---------------------------------------------|
|  0 | [Compresso](./meta/2023/Compresso.prototxt)             | [Compresso: Structured Pruning with Collaborative Prompting Learns Compact Large Language Models](https://arxiv.org/abs/2310.05015) | <img width='400' alt='image' src='./notes/2023/Compresso/cover.jpg'>             | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/Moonlit)       | [note](./notes/2023/Compresso/note.md)       |
|  1 | [SeerAttention](./meta/2024/SeerAttention.prototxt)     | [SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs](http://arxiv.org/abs/2410.13276v2)                                | <img width='400' alt='image' src='./notes/2024/SeerAttention/seerattention.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/SeerAttention) | [note](./notes/2024/SeerAttention/note.md)   |
|  2 | [SeerAttention-R](./meta/2025/SeerAttention-R.prototxt) | [SeerAttention-R: Sparse Attention Adaptation for Long Reasoning](http://arxiv.org/abs/2506.08889v1)                                | <img width='400' alt='image' src='./notes/2025/SeerAttention-R/fig1.png'>        | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/SeerAttention) | [note](./notes/2025/SeerAttention-R/note.md) |</p>
</details>
<details open><summary><b>Maosong Sun</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                                  | ccccccccccccccccccover                                                   | Publish                                                    | codeeeee                                                                                       | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [ProSparse](./meta/2024/ProSparse.prototxt)     | [ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models](https://arxiv.org/abs/2402.13516) | <img width='400' alt='image' src='./notes/2024/ProSparse/prosparse.jpg'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/raincleared-song/sparse_gpu_operator) | [note](./notes/2024/ProSparse/note.md)   |
|  1 | [ReLU2](./meta/2024/ReLU2.prototxt)             | [ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs](https://arxiv.org/abs/2402.03804)                          | <img width='400' alt='image' src='./notes/2024/ReLU2/activation.png'>    | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) |                                                                                                | [note](./notes/2024/ReLU2/note.md)       |
|  2 | [SparsingLaw](./meta/2024/SparsingLaw.prototxt) | [Sparsing Law: Towards Large Language Models with Greater Activation Sparsity](http://arxiv.org/abs/2411.02335v1)                   | <img width='400' alt='image' src='./notes/2024/SparsingLaw/fig4.png'>    | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/thunlp/SparsingLaw)                   | [note](./notes/2024/SparsingLaw/note.md) |
|  3 | [MiniCPM4](./meta/2025/MiniCPM4.prototxt)       | [MiniCPM4: Ultra-Efficient LLMs on End Devices](http://arxiv.org/abs/2506.07900v1)                                                  | <img width='400' alt='image' src='./notes/2025/MiniCPM4/fig2.png'>       | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/openbmb/minicpm)                      | [note](./notes/2025/MiniCPM4/note.md)    |</p>
</details>
<details open><summary><b>Marcos Treviso</b></summary> 
<p>

|    | meta                                        | ttttttttttttttttttttttttttttttitle                                                              | ccccccccccccccccccover                                                   | Publish                                                    | codeeeee                                                                      | note                                   |
|---:|:--------------------------------------------|:------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------|:-----------------------------------------------------------|:------------------------------------------------------------------------------|:---------------------------------------|
|  0 | [m](./meta/2023/68I8KKBV.prototxt)          | [Efficient Methods for Natural Language Processing: A Survey](https://arxiv.org/abs/2209.00099) | <img width='400' alt='image' src='./notes/del/survey/efficient_NLP.jpg'> | ![Publish](https://img.shields.io/badge/2023-TACL-green)   |                                                                               |                                        |
|  1 | [AdaSplash](./meta/2025/AdaSplash.prototxt) | [AdaSplash: Adaptive Sparse Flash Attention](http://arxiv.org/abs/2502.12082v1)                 |                                                                          | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deep-spin/adasplash) | [note](./notes/2025/AdaSplash/note.md) |</p>
</details>
<details open><summary><b>Mark Kurtz</b></summary> 
<p>

|    | meta                               | ttttttttttttttttttttttttttttttitle                                                                                                       | ccccccccccccccccccover   | Publish                                                    | codeeeee                                                                      | note                                  |
|---:|:-----------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------|:-------------------------|:-----------------------------------------------------------|:------------------------------------------------------------------------------|:--------------------------------------|
|  0 | [m](./meta/2020/V3MFIRLV.prototxt) | [Inducing and Exploiting Activation Sparsity for Fast Neural Network Inference](http://proceedings.mlr.press/v119/kurtz20a/kurtz20a.pdf) |                          | ![Publish](https://img.shields.io/badge/2020-ICML-blue)    |                                                                               |                                       |
|  1 | [m](./meta/2024/ULY1AZGY.prototxt) | [Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment](http://arxiv.org/abs/2405.03594v1)          |                          | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/neuralmagic/nm-vllm) | [note](./notes/2024/ULY1AZGY/note.md) |</p>
</details>
<details open><summary><b>Mehrdad Farajtabar</b></summary> 
<p>

|    | meta                                                  | ttttttttttttttttttttttttttttttitle                                                                              | ccccccccccccccccccover                                                          | Publish                                                       | codeeeee                                                                        | note                                        |
|---:|:------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------|:--------------------------------------------------------------|:--------------------------------------------------------------------------------|:--------------------------------------------|
|  0 | [LLM in a flash](./meta/2023/LLM_in_a_flash.prototxt) | [LLM in a flash: Efficient Large Language Model Inference with Limited Memory](http://arxiv.org/abs/2312.11514) | <img width='400' alt='image' src='./notes/2023/LLM_in_a_flash/windows.png'>     | ![Publish](https://img.shields.io/badge/2023-arXiv-violet)    |                                                                                 | [note](./notes/2023/LLM_in_a_flash/note.md) |
|  1 | [ReLU Strikes Back](./meta/2024/HMR7HKFV.prototxt)    | [ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models](https://arxiv.org/abs/2310.04564)  | <img width='400' alt='image' src='./notes/2024/HMR7HKFV/ReLU_Strikes_Back.jpg'> | ![Publish](https://img.shields.io/badge/2024-ICLR_oral-green) | ![GitHub Repo stars](https://img.shields.io/github/stars/sjtu-ipads/powerinfer) |                                             |</p>
</details>
<details open><summary><b>Meng Li</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Miaojun Wang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Michael Goin</b></summary> 
<p>

|    | meta                                          | ttttttttttttttttttttttttttttttitle                                                                                              | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                              | note                                  |
|---:|:----------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:--------------------------------------------------------------------------------------|:--------------------------------------|
|  0 | [SquareHead](./meta/2023/SquareHead.prototxt) | [Sparse Fine-tuning for Inference Acceleration of Large Language Models](https://arxiv.org/pdf/2310.06927.pdf)                  | <img width='400' alt='image' src='./notes/2023/SquareHead/cover.png'> | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/IST-DASLab/SparseFinetuning) |                                       |
|  1 | [m](./meta/2024/ULY1AZGY.prototxt)            | [Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment](http://arxiv.org/abs/2405.03594v1) |                                                                       | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/neuralmagic/nm-vllm)         | [note](./notes/2024/ULY1AZGY/note.md) |</p>
</details>
<details open><summary><b>Michael Hassid</b></summary> 
<p>

|    | meta                               | ttttttttttttttttttttttttttttttitle                                                              | ccccccccccccccccccover                                                   | Publish                                                    | codeeeee                                                                        | note                              |
|---:|:-----------------------------------|:------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------|:-----------------------------------------------------------|:--------------------------------------------------------------------------------|:----------------------------------|
|  0 | [m](./meta/2023/68I8KKBV.prototxt) | [Efficient Methods for Natural Language Processing: A Survey](https://arxiv.org/abs/2209.00099) | <img width='400' alt='image' src='./notes/del/survey/efficient_NLP.jpg'> | ![Publish](https://img.shields.io/badge/2023-TACL-green)   |                                                                                 |                                   |
|  1 | [TOVA](./meta/2024/TOVA.prototxt)  | [Transformers are Multi-State RNNs](http://arxiv.org/abs/2401.06104v2)                          | <img width='400' alt='image' src='./notes/2024/TOVA/tova.png'>           | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/schwartz-lab-NLP/TOVA) | [note](./notes/2024/TOVA/note.md) |</p>
</details>
<details open><summary><b>Michael W. Mahoney</b></summary> 
<p>

|    | meta                                                | ttttttttttttttttttttttttttttttitle                                                                                       | ccccccccccccccccccover                                                     | Publish                                                    | codeeeee                                                                                     | note                                       |
|---:|:----------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------|:-----------------------------------------------------------|:---------------------------------------------------------------------------------------------|:-------------------------------------------|
|  0 | [ActNN](./meta/2019/actnn.prototxt)                 | [ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training](https://arxiv.org/abs/2104.14129)   |                                                                            | ![Publish](https://img.shields.io/badge/2019-ICML-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/ucbrise/actnn)                      |                                            |
|  1 | [FisherPruning](./meta/2022/fisherpruning.prototxt) | [A Fast Post-Training Pruning Framework for Transformers](http://arxiv.org/abs/2204.09656v2)                             | <img width='400' alt='image' src='./notes/2022/fisherpruning/cover.jpg'>   | ![Publish](https://img.shields.io/badge/2022-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/WoosukKwon/retraining-free-pruning) | [note](./notes/2022/fisherpruning/note.md) |
|  2 | [SqueezeLLM](./meta/2024/SqueezeLLM.prototxt)       | [SqueezeLLM: Dense-and-Sparse Quantization](http://arxiv.org/abs/2306.07629v4)                                           | <img width='400' alt='image' src='./notes/2024/SqueezeLLM/squeezeLLM.png'> | ![Publish](https://img.shields.io/badge/2024-ICML-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/SqueezeAILab/SqueezeLLM)            | [note](./notes/2024/SqueezeLLM/note.md)    |
|  3 | [KVQuant](./meta/2024/KVQuant.prototxt)             | [KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization](http://arxiv.org/abs/2401.18079v2) |                                                                            | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/SqueezeAILab/KVQuant)               | [note](./notes/2024/KVQuant/note.md)       |</p>
</details>
<details open><summary><b>Mingchuan Zhang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Minghua Zhang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Minghui Tang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Mingjie Sun</b></summary> 
<p>

|    | meta                                                            | ttttttttttttttttttttttttttttttitle                                                                               | ccccccccccccccccccover                                                               | Publish                                                    | codeeeee                                                                               | note                                             |
|---:|:----------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------|:-----------------------------------------------------------|:---------------------------------------------------------------------------------------|:-------------------------------------------------|
|  0 | [GBLM-Pruner](./meta/2023/GBLM-Pruner.prototxt)                 | [Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models](http://arxiv.org/abs/2311.04902v2) | <img width='400' alt='image' src='./notes/2023/GBLM-Pruner/gblm-pruner.jpg'>         | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/VILA-Lab/GBLM-Pruner)         | [note](./notes/2023/GBLM-Pruner/note.md)         |
|  1 | [Wanda](./meta/2024/Wanda.prototxt)                             | [A Simple and Effective Pruning Approach for Large Language Models](http://arxiv.org/abs/2306.11695)             | <img width='400' alt='image' src='./notes/2024/Wanda/wanda.png'>                     | ![Publish](https://img.shields.io/badge/2024-ICLR-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/locuslab/wanda)               | [note](./notes/2024/Wanda/note.md)               |
|  2 | [massive-activations](./meta/2024/massive-activations.prototxt) | [Massive Activations in Large Language Models](http://arxiv.org/abs/2402.17762v2)                                | <img width='400' alt='image' src='./notes/2024/massive-activations/massive_act.jpg'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/locuslab/massive-activations) | [note](./notes/2024/massive-activations/note.md) |</p>
</details>
<details open><summary><b>Mingming Li</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Ning Tian</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Panpan Huang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                             | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                           | note                                     |
|---:|:------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5)        | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2)  | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                              | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3)  | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeekMoE](./meta/2024/DeepSeekMoE.prototxt) | [DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](http://arxiv.org/abs/2401.06066v1) | <img width='400' alt='image' src='./notes/2024/DeepSeekMoE/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-MoE) | [note](./notes/2024/DeepSeekMoE/note.md) |
|  3 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1)        | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1)  | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Pavlo Molchanov</b></summary> 
<p>

|    | meta                                      | ttttttttttttttttttttttttttttttitle                                                                         | ccccccccccccccccccover                                                 | Publish                                                    | codeeeee                                                                  | note                                  |
|---:|:------------------------------------------|:-----------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------|:-----------------------------------------------------------|:--------------------------------------------------------------------------|:--------------------------------------|
|  0 | [MaskLLM](./meta/2024/MaskLLM.prototxt)   | [MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models](http://arxiv.org/abs/2409.17481v1) | <img width='400' alt='image' src='./notes/2024/MaskLLM/maskllm.png'>   | ![Publish](https://img.shields.io/badge/2024-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/NVlabs/MaskLLM)  | [note](./notes/2024/MaskLLM/note.md)  |
|  1 | [Minitron](./meta/2024/Minitron.prototxt) | [Compact Language Models via Pruning and Knowledge Distillation](https://arxiv.org/abs/2408.11796v2)       | <img width='400' alt='image' src='./notes/2024/Minitron/minitron.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/NVlabs/Minitron) | [note](./notes/2024/Minitron/note.md) |</p>
</details>
<details open><summary><b>Peiyi Wang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Peng Zhang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Pengcheng He</b></summary> 
<p>

|    | meta                                    | ttttttttttttttttttttttttttttttitle                                                                              | ccccccccccccccccccover                                               | Publish                                                    | codeeeee                                                                      | note                               |
|---:|:----------------------------------------|:----------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------|:-----------------------------------------------------------|:------------------------------------------------------------------------------|:-----------------------------------|
|  0 | [AdaLoRA](./meta/2023/adalora.prototxt) | [AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning](https://arxiv.org/pdf/2303.10512.pdf) | <img width='400' alt='image' src='./notes/2023/adalora/adalora.jpg'> | ![Publish](https://img.shields.io/badge/2023-ICLR-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/QingruZhang/AdaLoRA) |                                    |
|  1 | [LoftQ](./meta/2023/loftq.prototxt)     | [LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models](https://arxiv.org/abs/2310.08659)        | <img width='400' alt='image' src='./notes/2023/loftq/loftq.jpg'>     | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/yxli2123/LoftQ)      | [note](./notes/2023/loftq/note.md) |</p>
</details>
<details open><summary><b>Pengfei Zuo</b></summary> 
<p>

|    | meta                                                    | ttttttttttttttttttttttttttttttitle                                                                                                                    | ccccccccccccccccccover                                                    | Publish                                                    | codeeeee                                                                     | note                                         |
|---:|:--------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------|:---------------------------------------------|
|  0 | [CachedAttention](./meta/2024/CachedAttention.prototxt) | [Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention](http://arxiv.org/abs/2403.19708v3)                    | <img width='400' alt='image' src='./notes/2024/CachedAttention/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-ATC-orange)   |                                                                              | [note](./notes/2024/CachedAttention/note.md) |
|  1 | [AdaSkip](./meta/2025/AdaSkip.prototxt)                 | [AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference](http://arxiv.org/abs/2501.02336v1)                                  | <img width='400' alt='image' src='./notes/2025/AdaSkip/fig1.png'>         | ![Publish](https://img.shields.io/badge/2025-AAAI-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/ASISys/AdaSkip)     | [note](./notes/2025/AdaSkip/note.md)         |
|  2 | [Adrenaline](./meta/2025/Adrenaline.prototxt)           | [Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation](http://arxiv.org/abs/2503.20552v1) | <img width='400' alt='image' src='./notes/2025/Adrenaline/fig4.png'>      | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/ASISys/Adrenaline)  | [note](./notes/2025/Adrenaline/note.md)      |
|  3 | [PSA](./meta/2025/PSA.prototxt)                         | [Progressive Sparse Attention: Algorithm and System Co-design for Efficient Attention in LLM Serving](http://arxiv.org/abs/2503.00392v1)              | <img width='400' alt='image' src='./notes/2025/PSA/fig4.png'>             | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/ASISys/PSAttention) | [note](./notes/2025/PSA/note.md)             |</p>
</details>
<details open><summary><b>Qiancheng Wang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Qianhui Wu</b></summary> 
<p>

|    | meta                                          | ttttttttttttttttttttttttttttttitle                                                                                               | ccccccccccccccccccover                                                            | Publish                                                    | codeeeee                                                                       | note                                    |
|---:|:----------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------------------------|:-------------------------------------------------------------------------------|:----------------------------------------|
|  0 | [MInference](./meta/2024/MInference.prototxt) | [MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention](http://arxiv.org/abs/2407.02490v1) | <img width='400' alt='image' src='./notes/2024/MInference/MInference_3shape.PNG'> | ![Publish](https://img.shields.io/badge/2024-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/MInference) | [note](./notes/2024/MInference/note.md) |
|  1 | [SCBench](./meta/2024/SCBench.prototxt)       | [SCBench: A KV Cache-Centric Analysis of Long-Context Methods](http://arxiv.org/abs/2412.10319v2)                                |                                                                                   | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/MInference) | [note](./notes/2024/SCBench/note.md)    |</p>
</details>
<details open><summary><b>Qihao Zhu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Qinyu Chen</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Qiushi Du</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>R. J. Chen</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>R. L. Jin</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Ramachandran Ramjee</b></summary> 
<p>

|    | meta                                                | ttttttttttttttttttttttttttttttitle                                                                                 | ccccccccccccccccccover                                                  | Publish                                                     | codeeeee                                                                       | note                                       |
|---:|:----------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------|:------------------------------------------------------------|:-------------------------------------------------------------------------------|:-------------------------------------------|
|  0 | [POD-Attention](./meta/2025/POD-Attention.prototxt) | [POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference](http://arxiv.org/abs/2410.18038v2) | <img width='400' alt='image' src='./notes/2025/POD-Attention/fig2.png'> | ![Publish](https://img.shields.io/badge/2025-ASPLOS-orange) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/vattention) | [note](./notes/2025/POD-Attention/note.md) |
|  1 | [vAttention](./meta/2025/vAttention.prototxt)       | [vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention](http://arxiv.org/abs/2405.04437v3) | <img width='400' alt='image' src='./notes/2025/vAttention/fig5.png'>    | ![Publish](https://img.shields.io/badge/2025-ASPLOS-orange) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/vattention) | [note](./notes/2025/vAttention/note.md)    |</p>
</details>
<details open><summary><b>Ramya Prabhu</b></summary> 
<p>

|    | meta                                                | ttttttttttttttttttttttttttttttitle                                                                                 | ccccccccccccccccccover                                                  | Publish                                                     | codeeeee                                                                       | note                                       |
|---:|:----------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------|:------------------------------------------------------------|:-------------------------------------------------------------------------------|:-------------------------------------------|
|  0 | [POD-Attention](./meta/2025/POD-Attention.prototxt) | [POD-Attention: Unlocking Full Prefill-Decode Overlap for Faster LLM Inference](http://arxiv.org/abs/2410.18038v2) | <img width='400' alt='image' src='./notes/2025/POD-Attention/fig2.png'> | ![Publish](https://img.shields.io/badge/2025-ASPLOS-orange) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/vattention) | [note](./notes/2025/POD-Attention/note.md) |
|  1 | [vAttention](./meta/2025/vAttention.prototxt)       | [vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention](http://arxiv.org/abs/2405.04437v3) | <img width='400' alt='image' src='./notes/2025/vAttention/fig5.png'>    | ![Publish](https://img.shields.io/badge/2025-ASPLOS-orange) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/vattention) | [note](./notes/2025/vAttention/note.md)    |</p>
</details>
<details open><summary><b>Rayan Saab</b></summary> 
<p>

|    | meta                                  | ttttttttttttttttttttttttttttttitle                                                                                  | ccccccccccccccccccover   | Publish                                                    | codeeeee                                                                                       | note   |
|---:|:--------------------------------------|:--------------------------------------------------------------------------------------------------------------------|:-------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------------------------|:-------|
|  0 | [GPFQ](./meta/2021/gpfq.prototxt)     | [A Greedy Algorithm for Quantizing Neural Networks](https://jmlr.csail.mit.edu/papers/volume22/20-1233/20-1233.pdf) |                          | ![Publish](https://img.shields.io/badge/2021-JMLR-green)   | ![GitHub Repo stars](https://img.shields.io/github/stars/YixuanSeanZhou/Quantized_Neural_Nets) |        |
|  1 | [GPFQv2](./meta/2023/gpfqv2.prototxt) | [Post-training Quantization for Neural Networks with Provable Guarantees](https://arxiv.org/pdf/2201.11113.pdf)     |                          | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/YixuanSeanZhou/Quantized_Neural_Nets) |        |</p>
</details>
<details open><summary><b>Roy Schwartz</b></summary> 
<p>

|    | meta                               | ttttttttttttttttttttttttttttttitle                                                              | ccccccccccccccccccover                                                   | Publish                                                    | codeeeee                                                                        | note                              |
|---:|:-----------------------------------|:------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------|:-----------------------------------------------------------|:--------------------------------------------------------------------------------|:----------------------------------|
|  0 | [m](./meta/2023/68I8KKBV.prototxt) | [Efficient Methods for Natural Language Processing: A Survey](https://arxiv.org/abs/2209.00099) | <img width='400' alt='image' src='./notes/del/survey/efficient_NLP.jpg'> | ![Publish](https://img.shields.io/badge/2023-TACL-green)   |                                                                                 |                                   |
|  1 | [TOVA](./meta/2024/TOVA.prototxt)  | [Transformers are Multi-State RNNs](http://arxiv.org/abs/2401.06104v2)                          | <img width='400' alt='image' src='./notes/2024/TOVA/tova.png'>           | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/schwartz-lab-NLP/TOVA) | [note](./notes/2024/TOVA/note.md) |</p>
</details>
<details open><summary><b>Ruihang Lai</b></summary> 
<p>

|    | meta                                          | ttttttttttttttttttttttttttttttitle                                                                                           | ccccccccccccccccccover                                             | Publish                                                    | codeeeee                                                                           | note                                    |
|---:|:----------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------------|:----------------------------------------|
|  0 | [XGrammar](./meta/2024/XGrammar.prototxt)     | [XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models](http://arxiv.org/abs/2411.15100v2) | <img width='400' alt='image' src='./notes/2024/XGrammar/fig1.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/mlc-ai/xgrammar)          | [note](./notes/2024/XGrammar/note.md)   |
|  1 | [FlashInfer](./meta/2025/FlashInfer.prototxt) | [FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving](http://arxiv.org/abs/2501.01005v2)       |                                                                    | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/flashinfer-ai/flashinfer) | [note](./notes/2025/FlashInfer/note.md) |</p>
</details>
<details open><summary><b>Ruiqi Ge</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Ruisong Zhang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Ruizhe Pan</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Runji Wang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Runxin Xu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Ruoyu Zhang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Ruyi Chen</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>S. S. Li</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Saurav Muralidharan</b></summary> 
<p>

|    | meta                                      | ttttttttttttttttttttttttttttttitle                                                                         | ccccccccccccccccccover                                                 | Publish                                                    | codeeeee                                                                  | note                                  |
|---:|:------------------------------------------|:-----------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------|:-----------------------------------------------------------|:--------------------------------------------------------------------------|:--------------------------------------|
|  0 | [MaskLLM](./meta/2024/MaskLLM.prototxt)   | [MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models](http://arxiv.org/abs/2409.17481v1) | <img width='400' alt='image' src='./notes/2024/MaskLLM/maskllm.png'>   | ![Publish](https://img.shields.io/badge/2024-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/NVlabs/MaskLLM)  | [note](./notes/2024/MaskLLM/note.md)  |
|  1 | [Minitron](./meta/2024/Minitron.prototxt) | [Compact Language Models via Pruning and Knowledge Distillation](https://arxiv.org/abs/2408.11796v2)       | <img width='400' alt='image' src='./notes/2024/Minitron/minitron.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/NVlabs/Minitron) | [note](./notes/2024/Minitron/note.md) |</p>
</details>
<details open><summary><b>Sean Lie</b></summary> 
<p>

|    | meta                                          | ttttttttttttttttttttttttttttttitle                                                                                              | ccccccccccccccccccover                                                    | Publish                                                    | codeeeee                                                                              | note                                    |
|---:|:----------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------|:-----------------------------------------------------------|:--------------------------------------------------------------------------------------|:----------------------------------------|
|  0 | [Sparse-IFT](./meta/2023/Sparse-IFT.prototxt) | [Sparse Iso-FLOP Transformations for Maximizing Training Efficiency](https://arxiv.org/abs/2303.11525)                          |                                                                           | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/CerebrasResearch/Sparse-IFT) |                                         |
|  1 | [Sparse-IFT](./meta/2024/Sparse-IFT.prototxt) | [Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency](http://arxiv.org/abs/2303.11525v3)             | <img width='400' alt='image' src='./notes/2024/Sparse-IFT/sparseIFT.png'> | ![Publish](https://img.shields.io/badge/2024-ICML-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/CerebrasResearch/Sparse-IFT) | [note](./notes/2024/Sparse-IFT/note.md) |
|  2 | [m](./meta/2024/ULY1AZGY.prototxt)            | [Enabling High-Sparsity Foundational Llama Models with Efficient Pretraining and Deployment](http://arxiv.org/abs/2405.03594v1) |                                                                           | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/neuralmagic/nm-vllm)         | [note](./notes/2024/ULY1AZGY/note.md)   |</p>
</details>
<details open><summary><b>Sehoon Kim</b></summary> 
<p>

|    | meta                                                | ttttttttttttttttttttttttttttttitle                                                                                       | ccccccccccccccccccover                                                     | Publish                                                    | codeeeee                                                                                     | note                                       |
|---:|:----------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------|:-----------------------------------------------------------|:---------------------------------------------------------------------------------------------|:-------------------------------------------|
|  0 | [FisherPruning](./meta/2022/fisherpruning.prototxt) | [A Fast Post-Training Pruning Framework for Transformers](http://arxiv.org/abs/2204.09656v2)                             | <img width='400' alt='image' src='./notes/2022/fisherpruning/cover.jpg'>   | ![Publish](https://img.shields.io/badge/2022-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/WoosukKwon/retraining-free-pruning) | [note](./notes/2022/fisherpruning/note.md) |
|  1 | [SqueezeLLM](./meta/2024/SqueezeLLM.prototxt)       | [SqueezeLLM: Dense-and-Sparse Quantization](http://arxiv.org/abs/2306.07629v4)                                           | <img width='400' alt='image' src='./notes/2024/SqueezeLLM/squeezeLLM.png'> | ![Publish](https://img.shields.io/badge/2024-ICML-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/SqueezeAILab/SqueezeLLM)            | [note](./notes/2024/SqueezeLLM/note.md)    |
|  2 | [KVQuant](./meta/2024/KVQuant.prototxt)             | [KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization](http://arxiv.org/abs/2401.18079v2) |                                                                            | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/SqueezeAILab/KVQuant)               | [note](./notes/2024/KVQuant/note.md)       |</p>
</details>
<details open><summary><b>Shang Yang</b></summary> 
<p>

|    | meta                                              | ttttttttttttttttttttttttttttttitle                                                                                         | ccccccccccccccccccover                                                         | Publish                                                    | codeeeee                                                                            | note                                      |
|---:|:--------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------|:-----------------------------------------------------------|:------------------------------------------------------------------------------------|:------------------------------------------|
|  0 | [AWQ](./meta/2024/awq.prototxt)                   | [AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](https://arxiv.org/abs/2306.00978)         |                                                                                | ![Publish](https://img.shields.io/badge/2024-MLSys-orange) | ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/llm-awq)       |                                           |
|  1 | [DuoAttention](./meta/2024/DuoAttention.prototxt) | [DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads](http://arxiv.org/abs/2410.10819v1) | <img width='400' alt='image' src='./notes/2024/DuoAttention/duoattention.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/duo-attention) | [note](./notes/2024/DuoAttention/note.md) |
|  2 | [QServe](./meta/2024/QServer.prototxt)            | [QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving](http://arxiv.org/abs/2405.04532v2)           |                                                                                | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | [Pytorch](https://hanlab.mit.edu/projects/qserve)                                   | [note](./notes/2024/QServer/note.md)      |
|  3 | [LServer](./meta/2025/LServer.prototxt)           | [LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention](http://arxiv.org/abs/2502.14866v1)             | <img width='400' alt='image' src='./notes/2025/LServer/fig5.png'>              | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/omniserve)     | [note](./notes/2025/LServer/note.md)      |</p>
</details>
<details open><summary><b>Shanghao Lu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Shangyan Zhou</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Shanhuang Chen</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Shaoqing Wu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Shengen Yan</b></summary> 
<p>

|    | meta                               | ttttttttttttttttttttttttttttttitle                                                                                   | ccccccccccccccccccover                                                           | Publish                                                    | codeeeee                                                               | note                                  |
|---:|:-----------------------------------|:---------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------|:--------------------------------------|
|  0 | [m](./meta/2024/DHIB73MC.prototxt) | [A Survey on Efficient Inference for Large Language Models](http://arxiv.org/abs/2404.14294v2)                       | <img width='400' alt='image' src='./notes/2024/DHIB73MC/efficientinference.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) |                                                                        | [note](./notes/2024/DHIB73MC/note.md) |
|  1 | [MoA](./meta/2024/MoA.prototxt)    | [MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression](http://arxiv.org/abs/2406.14909v2) | <img width='400' alt='image' src='./notes/2024/MoA/moa.png'>                     | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/thu-nics/MoA) | [note](./notes/2024/MoA/note.md)      |</p>
</details>
<details open><summary><b>Shengfeng Ye</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                                                                                                                                                     | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/![GitHub_Repo_stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1)-![Publish](https://img.shields.io/badge/2025-arXiv-violet)-green) | [note](./notes/2025/DeepSeek-R1/note.md)                                          |                                          |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/![GitHub_Repo_stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1)-![Publish](https://img.shields.io/badge/2025-arXiv-violet)-green) | [note](./notes/2025/DeepSeek-R1/note.md)                                          |                                          |
|  2 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/![GitHub_Repo_stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3)-![Publish](https://img.shields.io/badge/2024-arXiv-violet)-green) | [note](./notes/2024/DeepSeek-V3/note.md)                                          |                                          |
|  3 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/![GitHub_Repo_stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3)-![Publish](https://img.shields.io/badge/2024-arXiv-violet)-green) | [note](./notes/2024/DeepSeek-V3/note.md)                                          |                                          |
|  4 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet)                                                                                                                                  | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |</p>
</details>
<details open><summary><b>Shijie Cao</b></summary> 
<p>

|    | meta                                                    | ttttttttttttttttttttttttttttttitle                                                                   | ccccccccccccccccccover                                                           | Publish                                                    | codeeeee                                                                          | note                                         |
|---:|:--------------------------------------------------------|:-----------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:---------------------------------------------|
|  0 | [SeerAttention](./meta/2024/SeerAttention.prototxt)     | [SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs](http://arxiv.org/abs/2410.13276v2) | <img width='400' alt='image' src='./notes/2024/SeerAttention/seerattention.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/SeerAttention) | [note](./notes/2024/SeerAttention/note.md)   |
|  1 | [ReSA](./meta/2025/ReSA.prototxt)                       | [Rectified Sparse Attention](http://arxiv.org/abs/2506.04108v2)                                      | <img width='400' alt='image' src='./notes/2025/ReSA/fig2.png'>                   | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/unilm)         | [note](./notes/2025/ReSA/note.md)            |
|  2 | [SeerAttention-R](./meta/2025/SeerAttention-R.prototxt) | [SeerAttention-R: Sparse Attention Adaptation for Long Reasoning](http://arxiv.org/abs/2506.08889v1) | <img width='400' alt='image' src='./notes/2025/SeerAttention-R/fig1.png'>        | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/SeerAttention) | [note](./notes/2025/SeerAttention-R/note.md) |</p>
</details>
<details open><summary><b>Shirong Ma</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Shiwei Liu</b></summary> 
<p>

|    | meta                                                         | ttttttttttttttttttttttttttttttitle                                                                                                            | ccccccccccccccccccover                                           | Publish                                                    | codeeeee                                                                                | note                               |
|---:|:-------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------------|:-----------------------------------|
|  0 | [m](./meta/2023/AYB1XUO5.prototxt)                           | [Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers](https://arxiv.org/abs/2302.02596) |                                                                  | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) |                                                                                         |                                    |
|  1 | [Essential Sparsity](./meta/2023/EssentialSparsity.prototxt) | [The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter](https://arxiv.org/abs/2306.03805)                  |                                                                  | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/VITA-Group/essential_sparsity) |                                    |
|  2 | [DSnoT](./meta/2024/DSnoT.prototxt)                          | [Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs](http://arxiv.org/abs/2310.08915v3)                                    | <img width='400' alt='image' src='./notes/2024/DSnoT/dsnot.png'> | ![Publish](https://img.shields.io/badge/2024-ICLR-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/zyxxmu/DSnoT)                  | [note](./notes/2024/DSnoT/note.md) |
|  3 | [OWL](./meta/2024/owl.prototxt)                              | [Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity](https://arxiv.org/pdf/2310.05175.pdf)    | <img width='400' alt='image' src='./notes/2024/owl/cover.jpg'>   | ![Publish](https://img.shields.io/badge/2024-ICML-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/luuyin/OWL)                    |                                    |</p>
</details>
<details open><summary><b>Shiyao Li</b></summary> 
<p>

|    | meta                               | ttttttttttttttttttttttttttttttitle                                                                                   | ccccccccccccccccccover                                                           | Publish                                                    | codeeeee                                                               | note                                  |
|---:|:-----------------------------------|:---------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------|:--------------------------------------|
|  0 | [m](./meta/2024/DHIB73MC.prototxt) | [A Survey on Efficient Inference for Large Language Models](http://arxiv.org/abs/2404.14294v2)                       | <img width='400' alt='image' src='./notes/2024/DHIB73MC/efficientinference.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) |                                                                        | [note](./notes/2024/DHIB73MC/note.md) |
|  1 | [MoA](./meta/2024/MoA.prototxt)    | [MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression](http://arxiv.org/abs/2406.14909v2) | <img width='400' alt='image' src='./notes/2024/MoA/moa.png'>                     | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/thu-nics/MoA) | [note](./notes/2024/MoA/note.md)      |</p>
</details>
<details open><summary><b>Shiyu Chang</b></summary> 
<p>

|    | meta                                        | ttttttttttttttttttttttttttttttitle                                                                           | ccccccccccccccccccover                                              | Publish                                                    | codeeeee                                                                        | note                                   |
|---:|:--------------------------------------------|:-------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------|:-----------------------------------------------------------|:--------------------------------------------------------------------------------|:---------------------------------------|
|  0 | [IFPruning](./meta/2025/IFPruning.prototxt) | [Instruction-Following Pruning for Large Language Models](http://arxiv.org/abs/2501.02086v2)                 | <img width='400' alt='image' src='./notes/2025/IFPruning/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) |                                                                                 | [note](./notes/2025/IFPruning/note.md) |
|  1 | [KVLink](./meta/2025/KVLink.prototxt)       | [KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse](http://arxiv.org/abs/2502.16002v1) | <img width='400' alt='image' src='./notes/2025/KVLink/fig1.png'>    | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/UCSB-NLP-Chang/KVLink) | [note](./notes/2025/KVLink/note.md)    |</p>
</details>
<details open><summary><b>Shiyu Wang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Shreyas Saxena</b></summary> 
<p>

|    | meta                                          | ttttttttttttttttttttttttttttttitle                                                                                  | ccccccccccccccccccover                                                    | Publish                                                    | codeeeee                                                                              | note                                    |
|---:|:----------------------------------------------|:--------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------|:-----------------------------------------------------------|:--------------------------------------------------------------------------------------|:----------------------------------------|
|  0 | [SPDF](./meta/2023/spdf.prototxt)             | [SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models](https://arxiv.org/abs/2303.10464)       |                                                                           | ![Publish](https://img.shields.io/badge/2023-UAI-green)    |                                                                                       |                                         |
|  1 | [Sparse-IFT](./meta/2023/Sparse-IFT.prototxt) | [Sparse Iso-FLOP Transformations for Maximizing Training Efficiency](https://arxiv.org/abs/2303.11525)              |                                                                           | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/CerebrasResearch/Sparse-IFT) |                                         |
|  2 | [Sparse-IFT](./meta/2024/Sparse-IFT.prototxt) | [Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency](http://arxiv.org/abs/2303.11525v3) | <img width='400' alt='image' src='./notes/2024/Sparse-IFT/sparseIFT.png'> | ![Publish](https://img.shields.io/badge/2024-ICML-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/CerebrasResearch/Sparse-IFT) | [note](./notes/2024/Sparse-IFT/note.md) |</p>
</details>
<details open><summary><b>Shuang Zhou</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Shuiping Yu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Shunfeng Zhou</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Shuting Pan</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Size Zheng</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                 | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'>  | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [ShadowKV](./meta/2024/ShadowKV.prototxt)       | [ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference](http://arxiv.org/abs/2410.21465v1)       | <img width='400' alt='image' src='./notes/2024/ShadowKV/shadowkv.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/bytedance/ShadowKV)      | [note](./notes/2024/ShadowKV/note.md)    |</p>
</details>
<details open><summary><b>Song Han</b></summary> 
<p>

|    | meta                                                     | ttttttttttttttttttttttttttttttitle                                                                                                                                           | ccccccccccccccccccover                                                         | Publish                                                           | codeeeee                                                                            | note                                       |
|---:|:---------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------|:------------------------------------------------------------------|:------------------------------------------------------------------------------------|:-------------------------------------------|
|  0 | [Deep Compression](./meta/2016/deepcompression.prototxt) | [Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding](https://arxiv.org/pdf/1510.00149.pdf)                             |                                                                                | ![Publish](https://img.shields.io/badge/2016-ICLR-blue)           |                                                                                     |                                            |
|  1 | [DSD](./meta/2017/dsd.prototxt)                          | [DSD: Dense-Sparse-Dense Training for Deep Neural Networks](https://arxiv.org/pdf/1607.04381.pdf)                                                                            |                                                                                | ![Publish](https://img.shields.io/badge/2017-ICLR-blue)           |                                                                                     |                                            |
|  2 | [SparseViT](./meta/2023/SparseViT.prototxt)              | [SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer](https://arxiv.org/abs/2303.17605)                                               | <img width='400' alt='image' src='./notes/2023/SparseViT/sparsevit.jpg'>       | ![Publish](https://img.shields.io/badge/2023-CVPR-blue)           | ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/sparsevit)     | [note](./notes/2023/sparsevit/index.md)    |
|  3 | [TorchSparse++](./meta/2023/TorchSparse.prototxt)        | [TorchSparse++: Efficient Point Cloud Engine](https://openaccess.thecvf.com/content/CVPR2023W/WAD/papers/Tang_TorchSparse_Efficient_Point_Cloud_Engine_CVPRW_2023_paper.pdf) |                                                                                | ![Publish](https://img.shields.io/badge/2023-CVPR_workshop-green) | ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/torchsparse)   |                                            |
|  4 | [streaming-llm](./meta/2024/streaming-llm.prototxt)      | [Efficient Streaming Language Models with Attention Sinks](http://arxiv.org/abs/2309.17453v4)                                                                                | <img width='400' alt='image' src='./notes/2024/streaming-llm/cover.jpg'>       | ![Publish](https://img.shields.io/badge/2024-ICLR-blue)           | ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/streaming-llm) | [note](./notes/2024/streaming-llm/note.md) |
|  5 | [Quest](./meta/2024/Quest.prototxt)                      | [Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference](http://arxiv.org/abs/2406.10774)                                                                      | <img width='400' alt='image' src='./notes/2024/Quest/quest.png'>               | ![Publish](https://img.shields.io/badge/2024-ICML-blue)           | ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/quest)         | [note](./notes/2024/Quest/note.md)         |
|  6 | [AWQ](./meta/2024/awq.prototxt)                          | [AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](https://arxiv.org/abs/2306.00978)                                                           |                                                                                | ![Publish](https://img.shields.io/badge/2024-MLSys-orange)        | ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/llm-awq)       |                                            |
|  7 | [DuoAttention](./meta/2024/DuoAttention.prototxt)        | [DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads](http://arxiv.org/abs/2410.10819v1)                                                   | <img width='400' alt='image' src='./notes/2024/DuoAttention/duoattention.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet)        | ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/duo-attention) | [note](./notes/2024/DuoAttention/note.md)  |
|  8 | [QServe](./meta/2024/QServer.prototxt)                   | [QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving](http://arxiv.org/abs/2405.04532v2)                                                             |                                                                                | ![Publish](https://img.shields.io/badge/2024-arXiv-violet)        | [Pytorch](https://hanlab.mit.edu/projects/qserve)                                   | [note](./notes/2024/QServer/note.md)       |
|  9 | [LServer](./meta/2025/LServer.prototxt)                  | [LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention](http://arxiv.org/abs/2502.14866v1)                                                               | <img width='400' alt='image' src='./notes/2025/LServer/fig5.png'>              | ![Publish](https://img.shields.io/badge/2025-arXiv-violet)        | ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/omniserve)     | [note](./notes/2025/LServer/note.md)       |
| 10 | [XAttention](./meta/2025/XAttention.prototxt)            | [XAttention: Block Sparse Attention with Antidiagonal Scoring](http://arxiv.org/abs/2503.16428v1)                                                                            | <img width='400' alt='image' src='./notes/2025/XAttention/fig1.png'>           | ![Publish](https://img.shields.io/badge/2025-arXiv-violet)        | ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/x-attention)   | [note](./notes/2025/XAttention/note.md)    |</p>
</details>
<details open><summary><b>Surin Ahn</b></summary> 
<p>

|    | meta                                          | ttttttttttttttttttttttttttttttitle                                                                                               | ccccccccccccccccccover                                                            | Publish                                                    | codeeeee                                                                       | note                                    |
|---:|:----------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------------------------|:-------------------------------------------------------------------------------|:----------------------------------------|
|  0 | [MInference](./meta/2024/MInference.prototxt) | [MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention](http://arxiv.org/abs/2407.02490v1) | <img width='400' alt='image' src='./notes/2024/MInference/MInference_3shape.PNG'> | ![Publish](https://img.shields.io/badge/2024-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/MInference) | [note](./notes/2024/MInference/note.md) |
|  1 | [SCBench](./meta/2024/SCBench.prototxt)       | [SCBench: A KV Cache-Centric Analysis of Long-Context Methods](http://arxiv.org/abs/2412.10319v2)                                |                                                                                   | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/MInference) | [note](./notes/2024/SCBench/note.md)    |</p>
</details>
<details open><summary><b>T. Wang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Tao Xie</b></summary> 
<p>

|    | meta                                                | ttttttttttttttttttttttttttttttitle                                                                                                   | ccccccccccccccccccover                                                  | Publish                                                    | codeeeee   | note                                       |
|---:|:----------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------|:-----------------------------------------------------------|:-----------|:-------------------------------------------|
|  0 | [DistAttention](./meta/2024/DistAttention.prototxt) | [Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache](http://arxiv.org/abs/2401.02669v2) | <img width='400' alt='image' src='./notes/2024/DistAttention/fig1.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) |            | [note](./notes/2024/DistAttention/note.md) |
|  1 | [RaaS](./meta/2025/RaaS.prototxt)                   | [Efficient Long-Decoding Inference with Reasoning-Aware Attention Sparsity](http://arxiv.org/abs/2502.11147v1)                       | <img width='400' alt='image' src='./notes/2025/RaaS/fig5.png'>          | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) |            | [note](./notes/2025/RaaS/note.md)          |</p>
</details>
<details open><summary><b>Tao Yuan</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                    | ccccccccccccccccccover                                                | Publish                                                    | codeeeee   | note                                     |
|---:|:------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:-----------|:-----------------------------------------|
|  0 | [m](./meta/2024/0Y41U1N2.prototxt)              | [Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs](http://arxiv.org/abs/2410.16135v1) | <img width='400' alt='image' src='./notes/2024/0Y41U1N2/cover.png'>   | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) |            | [note](./notes/2024/0Y41U1N2/note.md)    |
|  1 | [LinearPatch](./meta/2025/LinearPatch.prototxt) | [A Simple Linear Patch Revives Layer-Pruned Large Language Models](http://arxiv.org/abs/2505.24680v1)                 | <img width='400' alt='image' src='./notes/2025/LinearPatch/fig3.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) |            | [note](./notes/2025/LinearPatch/note.md) |</p>
</details>
<details open><summary><b>Tao Yun</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Tian Pei</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Tianle Cai</b></summary> 
<p>

|    | meta                                  | ttttttttttttttttttttttttttttttitle                                                                | ccccccccccccccccccover                                           | Publish                                                    | codeeeee                                                                        | note                                |
|---:|:--------------------------------------|:--------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------|:-----------------------------------------------------------|:--------------------------------------------------------------------------------|:------------------------------------|
|  0 | [SnapKV](./meta/2024/SnapKV.prototxt) | [SnapKV: LLM Knows What You are Looking for Before Generation](http://arxiv.org/abs/2404.14469v2) | <img width='400' alt='image' src='./notes/2024/SnapKV/fig1.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/FasterDecoding/SnapKV) | [note](./notes/2024/SnapKV/note.md) |
|  1 | [TEAL](./meta/2025/TEAL.prototxt)     | [Training-Free Activation Sparsity in Large Language Models](http://arxiv.org/abs/2408.14690v1)   | <img width='400' alt='image' src='./notes/2025/TEAL/fig1.png'>   | ![Publish](https://img.shields.io/badge/2025-ICLR-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/FasterDecoding/TEAL)   | [note](./notes/2025/TEAL/note.md)   |</p>
</details>
<details open><summary><b>Tianlong Chen</b></summary> 
<p>

|    | meta                               | ttttttttttttttttttttttttttttttitle                                                                                          | ccccccccccccccccccover                                       | Publish                                                    | codeeeee                                                                  | note                                  |
|---:|:-----------------------------------|:----------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------|:-----------------------------------------------------------|:--------------------------------------------------------------------------|:--------------------------------------|
|  0 | [H2O](./meta/2023/H2O.prototxt)    | [H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models](http://arxiv.org/abs/2306.14048)  | <img width='400' alt='image' src='./notes/2023/H2O/h2o.png'> | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/FMInference/H2O) | [note](./notes/2023/H2O/note.md)      |
|  1 | [m](./meta/2024/SN1PK7EK.prototxt) | [Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark](http://arxiv.org/abs/2402.11592v2) |                                                              | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/ZO-Bench/ZO-LLM) | [note](./notes/2024/SN1PK7EK/note.md) |</p>
</details>
<details open><summary><b>Tianqi Chen</b></summary> 
<p>

|    | meta                                          | ttttttttttttttttttttttttttttttitle                                                                                           | ccccccccccccccccccover                                             | Publish                                                    | codeeeee                                                                           | note                                    |
|---:|:----------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------------|:----------------------------------------|
|  0 | [XGrammar](./meta/2024/XGrammar.prototxt)     | [XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models](http://arxiv.org/abs/2411.15100v2) | <img width='400' alt='image' src='./notes/2024/XGrammar/fig1.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/mlc-ai/xgrammar)          | [note](./notes/2024/XGrammar/note.md)   |
|  1 | [FlashInfer](./meta/2025/FlashInfer.prototxt) | [FlashInfer: Efficient and Customizable Attention Engine for LLM Inference Serving](http://arxiv.org/abs/2501.01005v2)       |                                                                    | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/flashinfer-ai/flashinfer) | [note](./notes/2025/FlashInfer/note.md) |</p>
</details>
<details open><summary><b>Tianyu Fu</b></summary> 
<p>

|    | meta                               | ttttttttttttttttttttttttttttttitle                                                                                   | ccccccccccccccccccover                                                           | Publish                                                    | codeeeee                                                               | note                                  |
|---:|:-----------------------------------|:---------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------|:--------------------------------------|
|  0 | [m](./meta/2024/DHIB73MC.prototxt) | [A Survey on Efficient Inference for Large Language Models](http://arxiv.org/abs/2404.14294v2)                       | <img width='400' alt='image' src='./notes/2024/DHIB73MC/efficientinference.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) |                                                                        | [note](./notes/2024/DHIB73MC/note.md) |
|  1 | [MoA](./meta/2024/MoA.prototxt)    | [MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression](http://arxiv.org/abs/2406.14909v2) | <img width='400' alt='image' src='./notes/2024/MoA/moa.png'>                     | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/thu-nics/MoA) | [note](./notes/2024/MoA/note.md)      |</p>
</details>
<details open><summary><b>Tianyu Gao</b></summary> 
<p>

|    | meta                                              | ttttttttttttttttttttttttttttttitle                                                                                             | ccccccccccccccccccover                                                  | Publish                                                    | codeeeee                                                                             | note                                      |
|---:|:--------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------|:-----------------------------------------------------------|:-------------------------------------------------------------------------------------|:------------------------------------------|
|  0 | [MeZO](./meta/2023/MeZO.prototxt)                 | [Fine-Tuning Language Models with Just Forward Passes](http://arxiv.org/abs/2305.17333v3)                                      |                                                                         | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/princeton-nlp/MeZO)         | [note](./notes/2023/MeZO/note.md)         |
|  1 | [LLM-shearing](./meta/2023/LLM_shearing.prototxt) | [Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning](https://xiamengzhou.github.io/sheared-llama/) | <img width='400' alt='image' src='./notes/2023/LLM_shearing/cover.jpg'> | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/princeton-nlp/LLM-Shearing) | [note](./notes/2023/LLM_shearing/note.md) |</p>
</details>
<details open><summary><b>Tianyu Sun</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Tianzhu Ye</b></summary> 
<p>

|    | meta                                                    | ttttttttttttttttttttttttttttttitle                                                                   | ccccccccccccccccccover                                                    | Publish                                                    | codeeeee                                                                          | note                                         |
|---:|:--------------------------------------------------------|:-----------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:---------------------------------------------|
|  0 | [ReSA](./meta/2025/ReSA.prototxt)                       | [Rectified Sparse Attention](http://arxiv.org/abs/2506.04108v2)                                      | <img width='400' alt='image' src='./notes/2025/ReSA/fig2.png'>            | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/unilm)         | [note](./notes/2025/ReSA/note.md)            |
|  1 | [SeerAttention-R](./meta/2025/SeerAttention-R.prototxt) | [SeerAttention-R: Sparse Attention Adaptation for Long Reasoning](http://arxiv.org/abs/2506.08889v1) | <img width='400' alt='image' src='./notes/2025/SeerAttention-R/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/SeerAttention) | [note](./notes/2025/SeerAttention-R/note.md) |</p>
</details>
<details open><summary><b>Tim Dettmers</b></summary> 
<p>

|    | meta                                | ttttttttttttttttttttttttttttttitle                                                                                       | ccccccccccccccccccover                                           | Publish                                                    | codeeeee                                                                 | note   |
|---:|:------------------------------------|:-------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------|:-----------------------------------------------------------|:-------------------------------------------------------------------------|:-------|
|  0 | [QLoRA](./meta/2023/qlora.prototxt) | [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)                                        | <img width='400' alt='image' src='./notes/2023/qlora/qlora.jpg'> | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/artidoro/qlora) |        |
|  1 | [SpQR](./meta/2023/spqr.prototxt)   | [SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression](https://arxiv.org/pdf/2306.03078.pdf) |                                                                  | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/Vahe1994/SpQR)  |        |</p>
</details>
<details open><summary><b>Ting Cao</b></summary> 
<p>

|    | meta                                                    | ttttttttttttttttttttttttttttttitle                                                                   | ccccccccccccccccccover                                                           | Publish                                                    | codeeeee                                                                          | note                                         |
|---:|:--------------------------------------------------------|:-----------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:---------------------------------------------|
|  0 | [SeerAttention](./meta/2024/SeerAttention.prototxt)     | [SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs](http://arxiv.org/abs/2410.13276v2) | <img width='400' alt='image' src='./notes/2024/SeerAttention/seerattention.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/SeerAttention) | [note](./notes/2024/SeerAttention/note.md)   |
|  1 | [SeerAttention-R](./meta/2025/SeerAttention-R.prototxt) | [SeerAttention-R: Sparse Attention Adaptation for Long Reasoning](http://arxiv.org/abs/2506.08889v1) | <img width='400' alt='image' src='./notes/2025/SeerAttention-R/fig1.png'>        | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/SeerAttention) | [note](./notes/2025/SeerAttention-R/note.md) |</p>
</details>
<details open><summary><b>Tong Yang</b></summary> 
<p>

|    | meta                                  | ttttttttttttttttttttttttttttttitle                                                                                                        | ccccccccccccccccccover                                           | Publish                                                    | codeeeee                                                              | note                                |
|---:|:--------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------|:------------------------------------|
|  0 | [HATA](./meta/2025/HATA.prototxt)     | [HATA: Trainable and Hardware-Efficient Hash-Aware Top-k Attention for Scalable Large Model Inference](http://arxiv.org/abs/2506.02572v1) | <img width='400' alt='image' src='./notes/2025/HATA/fig2.png'>   | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/gpzlx1/HATA) | [note](./notes/2025/HATA/note.md)   |
|  1 | [KeepKV](./meta/2025/KeepKV.prototxt) | [KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference](http://arxiv.org/abs/2504.09936v1)         | <img width='400' alt='image' src='./notes/2025/KeepKV/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) |                                                                       | [note](./notes/2025/KeepKV/note.md) |</p>
</details>
<details open><summary><b>Torsten Hoefler</b></summary> 
<p>

|    | meta                                      | ttttttttttttttttttttttttttttttitle                                                                                                        | ccccccccccccccccccover                                                 | Publish                                                    | codeeeee                                                                                   | note                                  |
|---:|:------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------|:-----------------------------------------------------------|:-------------------------------------------------------------------------------------------|:--------------------------------------|
|  0 | [m](./meta/2021/ITZS3TU3.prototxt)        | [Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks](https://arxiv.org/abs/2102.00554) |                                                                        | ![Publish](https://img.shields.io/badge/2021-arXiv-violet) |                                                                                            |                                       |
|  1 | [VENOM](./meta/2023/VENOM.prototxt)       | [VENOM: A Vectorized N:M Format for Unleashing the Power of Sparse Tensor Cores](http://arxiv.org/abs/2310.02065v1)                       | <img width='400' alt='image' src='./notes/2023/VENOM/vnm.png'>         | ![Publish](https://img.shields.io/badge/2023-SC-orange)    | ![GitHub Repo stars](https://img.shields.io/github/stars/UDC-GAC/venom)                    | [note](./notes/2023/VENOM/note.md)    |
|  2 | [SliceGPT](./meta/2024/SliceGPT.prototxt) | [SliceGPT: Compress Large Language Models by Deleting Rows and Columns](http://arxiv.org/abs/2401.15024v2)                                | <img width='400' alt='image' src='./notes/2024/SliceGPT/sliceGPT.jpg'> | ![Publish](https://img.shields.io/badge/2024-ICLR-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/TransformerCompression) | [note](./notes/2024/SliceGPT/note.md) |</p>
</details>
<details open><summary><b>Tri Dao</b></summary> 
<p>

|    | meta                                                     | ttttttttttttttttttttttttttttttitle                                                                                   | ccccccccccccccccccover                                                    | Publish                                                    | codeeeee                                                                                     | note                                       |
|---:|:---------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------|:-----------------------------------------------------------|:---------------------------------------------------------------------------------------------|:-------------------------------------------|
|  0 | [FlashAttention](./meta/2022/flashattention.prototxt)    | [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)      | <img width='400' alt='image' src='./notes/2022/flashattention/cover.jpg'> | ![Publish](https://img.shields.io/badge/2022-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/Dao-AILab/flash-attention)          |                                            |
|  1 | [Flash-Decoding](./meta/2023/FlashDecoding.prototxt)     | [Flash-Decoding for long-context inference](https://crfm.stanford.edu/2023/10/12/flashdecoding.html)                 |                                                                           | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) |                                                                                              | [note](./notes/2023/FlashDecoding/note.md) |
|  2 | [FlashAttention-2](./meta/2023/flashattention2.prototxt) | [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/abs/2307.08691) |                                                                           | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/Dao-AILab/flash-attention)          |                                            |
|  3 | [GLA](./meta/2025/GLA.prototxt)                          | [Hardware-Efficient Attention for Fast Decoding](http://arxiv.org/abs/2505.21487v1)                                  | <img width='400' alt='image' src='./notes/2025/GLA/gla.png'>              | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/Dao-AILab/grouped-latent-attention) | [note](./notes/2025/GLA/note.md)           |</p>
</details>
<details open><summary><b>Tuo Zhao</b></summary> 
<p>

|    | meta                                      | ttttttttttttttttttttttttttttttitle                                                                                             | ccccccccccccccccccover                                                 | Publish                                                    | codeeeee                                                                      | note                               |
|---:|:------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------|:-----------------------------------------------------------|:------------------------------------------------------------------------------|:-----------------------------------|
|  0 | [AdaLoRA](./meta/2023/adalora.prototxt)   | [AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning](https://arxiv.org/pdf/2303.10512.pdf)                | <img width='400' alt='image' src='./notes/2023/adalora/adalora.jpg'>   | ![Publish](https://img.shields.io/badge/2023-ICLR-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/QingruZhang/AdaLoRA) |                                    |
|  1 | [LoSparse](./meta/2023/LoSparse.prototxt) | [Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation](https://arxiv.org/abs/2306.11222) | <img width='400' alt='image' src='./notes/2023/LoSparse/losparse.jpg'> | ![Publish](https://img.shields.io/badge/2023-ICML-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/yxli2123/LoSparse)   |                                    |
|  2 | [LoftQ](./meta/2023/loftq.prototxt)       | [LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models](https://arxiv.org/abs/2310.08659)                       | <img width='400' alt='image' src='./notes/2023/loftq/loftq.jpg'>       | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/yxli2123/LoftQ)      | [note](./notes/2023/loftq/note.md) |</p>
</details>
<details open><summary><b>Vithursan Thangarasa</b></summary> 
<p>

|    | meta                                          | ttttttttttttttttttttttttttttttitle                                                                                  | ccccccccccccccccccover                                                    | Publish                                                    | codeeeee                                                                              | note                                    |
|---:|:----------------------------------------------|:--------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------|:-----------------------------------------------------------|:--------------------------------------------------------------------------------------|:----------------------------------------|
|  0 | [SPDF](./meta/2023/spdf.prototxt)             | [SPDF: Sparse Pre-training and Dense Fine-tuning for Large Language Models](https://arxiv.org/abs/2303.10464)       |                                                                           | ![Publish](https://img.shields.io/badge/2023-UAI-green)    |                                                                                       |                                         |
|  1 | [Sparse-IFT](./meta/2023/Sparse-IFT.prototxt) | [Sparse Iso-FLOP Transformations for Maximizing Training Efficiency](https://arxiv.org/abs/2303.11525)              |                                                                           | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/CerebrasResearch/Sparse-IFT) |                                         |
|  2 | [Sparse-IFT](./meta/2024/Sparse-IFT.prototxt) | [Sparse-IFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency](http://arxiv.org/abs/2303.11525v3) | <img width='400' alt='image' src='./notes/2024/Sparse-IFT/sparseIFT.png'> | ![Publish](https://img.shields.io/badge/2024-ICML-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/CerebrasResearch/Sparse-IFT) | [note](./notes/2024/Sparse-IFT/note.md) |</p>
</details>
<details open><summary><b>W. L. Xiao</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Wangding Zeng</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                             | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                           | note                                     |
|---:|:------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5)        | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2)  | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                              | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3)  | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeekMoE](./meta/2024/DeepSeekMoE.prototxt) | [DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](http://arxiv.org/abs/2401.06066v1) | <img width='400' alt='image' src='./notes/2024/DeepSeekMoE/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-MoE) | [note](./notes/2024/DeepSeekMoE/note.md) |
|  3 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1)        | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1)  | [note](./notes/2025/DeepSeek-R1/note.md) |
|  4 | [NSA](./meta/2025/NSA.prototxt)                 | [Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention](http://arxiv.org/abs/2502.11089v1)         | <img width='400' alt='image' src='./notes/2025/NSA/fig2.png'>         | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) |                                                                                    | [note](./notes/2025/NSA/note.md)         |</p>
</details>
<details open><summary><b>Wanjia Zhao</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Wei An</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Wei Lin</b></summary> 
<p>

|    | meta                                                | ttttttttttttttttttttttttttttttitle                                                                                                                      | ccccccccccccccccccover                                                  | Publish                                                    | codeeeee                                                                            | note                                       |
|---:|:----------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------|:-----------------------------------------------------------|:------------------------------------------------------------------------------------|:-------------------------------------------|
|  0 | [Flash-LLM](./meta/2024/flash_llm.prototxt)         | [Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity](https://arxiv.org/abs/2309.10285) | <img width='400' alt='image' src='./notes/2024/flash_llm/cover.jpg'>    | ![Publish](https://img.shields.io/badge/2024-VLDB-green)   | ![GitHub Repo stars](https://img.shields.io/github/stars/AlibabaResearch/flash-llm) | [note](./notes/2024/flash_llm/index.md)    |
|  1 | [DistAttention](./meta/2024/DistAttention.prototxt) | [Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache](http://arxiv.org/abs/2401.02669v2)                    | <img width='400' alt='image' src='./notes/2024/DistAttention/fig1.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) |                                                                                     | [note](./notes/2024/DistAttention/note.md) |</p>
</details>
<details open><summary><b>Wei Wang</b></summary> 
<p>

|    | meta                                                  | ttttttttttttttttttttttttttttttitle                                                                                            | ccccccccccccccccccover                                                   | Publish                                                    | codeeeee                                                               | note                                        |
|---:|:------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------|:--------------------------------------------|
|  0 | [BRECQ](./meta/2021/brecq.prototxt)                   | [BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction](https://openreview.net/pdf?id=POWv6hDd9XH)   |                                                                          | ![Publish](https://img.shields.io/badge/2021-ICLR-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/yhhhli/BRECQ) |                                             |
|  1 | [PowerAttention](./meta/2025/PowerAttention.prototxt) | [PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention](http://arxiv.org/abs/2503.03588v1) | <img width='400' alt='image' src='./notes/2025/PowerAttention/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) |                                                                        | [note](./notes/2025/PowerAttention/note.md) |</p>
</details>
<details open><summary><b>Weiyu Huang</b></summary> 
<p>

|    | meta                                                                | ttttttttttttttttttttttttttttttitle                                                                               | ccccccccccccccccccover                                                          | Publish                                                 | codeeeee                                                                                 | note                                               |
|---:|:--------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------|:--------------------------------------------------------|:-----------------------------------------------------------------------------------------|:---------------------------------------------------|
|  0 | [m](./meta/2024/HYPL7G37.prototxt)                                  | [Accelerating Transformer Pre-training with 2:4 Sparsity](http://arxiv.org/abs/2404.01847v2)                     |                                                                                 | ![Publish](https://img.shields.io/badge/2024-ICML-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/huyz2023/2by4-pretrain)         | [note](./notes/2024/HYPL7G37/note.md)              |
|  1 | [AdaptiveSparseTrainer](./meta/2025/AdaptiveSparseTrainer.prototxt) | [Pruning Large Language Models with Semi-Structural Adaptive Sparse Training](http://arxiv.org/abs/2407.20584v3) | <img width='400' alt='image' src='./notes/2025/AdaptiveSparseTrainer/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-AAAI-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/thu-ml/Adaptive-Sparse-Trainer) | [note](./notes/2025/AdaptiveSparseTrainer/note.md) |</p>
</details>
<details open><summary><b>Weizhu Chen</b></summary> 
<p>

|    | meta                                    | ttttttttttttttttttttttttttttttitle                                                                              | ccccccccccccccccccover                                               | Publish                                                    | codeeeee                                                                      | note                               |
|---:|:----------------------------------------|:----------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------|:-----------------------------------------------------------|:------------------------------------------------------------------------------|:-----------------------------------|
|  0 | [LoRA](./meta/2022/lora.prototxt)       | [LoRA: Low-rank adaptation of large language models](https://arxiv.org/abs/2106.09685)                          | <img width='400' alt='image' src='./notes/2022/lora/lora.jpg'>       | ![Publish](https://img.shields.io/badge/2022-ICLR-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/LoRA)      |                                    |
|  1 | [AdaLoRA](./meta/2023/adalora.prototxt) | [AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning](https://arxiv.org/pdf/2303.10512.pdf) | <img width='400' alt='image' src='./notes/2023/adalora/adalora.jpg'> | ![Publish](https://img.shields.io/badge/2023-ICLR-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/QingruZhang/AdaLoRA) |                                    |
|  2 | [LoftQ](./meta/2023/loftq.prototxt)     | [LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models](https://arxiv.org/abs/2310.08659)        | <img width='400' alt='image' src='./notes/2023/loftq/loftq.jpg'>     | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/yxli2123/LoftQ)      | [note](./notes/2023/loftq/note.md) |</p>
</details>
<details open><summary><b>Wen Liu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Wenfeng Liang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                             | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                           | note                                     |
|---:|:------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5)        | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2)  | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                              | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3)  | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeekMoE](./meta/2024/DeepSeekMoE.prototxt) | [DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](http://arxiv.org/abs/2401.06066v1) | <img width='400' alt='image' src='./notes/2024/DeepSeekMoE/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-MoE) | [note](./notes/2024/DeepSeekMoE/note.md) |
|  3 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1)        | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1)  | [note](./notes/2025/DeepSeek-R1/note.md) |
|  4 | [NSA](./meta/2025/NSA.prototxt)                 | [Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention](http://arxiv.org/abs/2502.11089v1)         | <img width='400' alt='image' src='./notes/2025/NSA/fig2.png'>         | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) |                                                                                    | [note](./notes/2025/NSA/note.md)         |</p>
</details>
<details open><summary><b>Wenjun Gao</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Wenqin Yu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Wentao Zhang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Woosuk Kwon</b></summary> 
<p>

|    | meta                                                   | ttttttttttttttttttttttttttttttitle                                                                                    | ccccccccccccccccccover                                                   | Publish                                                    | codeeeee                                                                                     | note                                        |
|---:|:-------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------|:-----------------------------------------------------------|:---------------------------------------------------------------------------------------------|:--------------------------------------------|
|  0 | [FisherPruning](./meta/2022/fisherpruning.prototxt)    | [A Fast Post-Training Pruning Framework for Transformers](http://arxiv.org/abs/2204.09656v2)                          | <img width='400' alt='image' src='./notes/2022/fisherpruning/cover.jpg'> | ![Publish](https://img.shields.io/badge/2022-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/WoosukKwon/retraining-free-pruning) | [note](./notes/2022/fisherpruning/note.md)  |
|  1 | [Paged Attention](./meta/2023/PagedAttention.prototxt) | [Efficient Memory Management for Large Language Model Serving with PagedAttention](http://arxiv.org/abs/2309.06180v1) | <img width='400' alt='image' src='./notes/2023/PagedAttention/vllm.png'> | ![Publish](https://img.shields.io/badge/2023-SOSP-orange)  | ![GitHub Repo stars](https://img.shields.io/github/stars/vllm-project/vllm)                  | [note](./notes/2023/PagedAttention/note.md) |</p>
</details>
<details open><summary><b>X. Q. Li</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Xiafei Qiu</b></summary> 
<p>

|    | meta                                                | ttttttttttttttttttttttttttttttitle                                                                                                                      | ccccccccccccccccccover                                                  | Publish                                                    | codeeeee                                                                            | note                                       |
|---:|:----------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------|:-----------------------------------------------------------|:------------------------------------------------------------------------------------|:-------------------------------------------|
|  0 | [Flash-LLM](./meta/2024/flash_llm.prototxt)         | [Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity](https://arxiv.org/abs/2309.10285) | <img width='400' alt='image' src='./notes/2024/flash_llm/cover.jpg'>    | ![Publish](https://img.shields.io/badge/2024-VLDB-green)   | ![GitHub Repo stars](https://img.shields.io/github/stars/AlibabaResearch/flash-llm) | [note](./notes/2024/flash_llm/index.md)    |
|  1 | [DistAttention](./meta/2024/DistAttention.prototxt) | [Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache](http://arxiv.org/abs/2401.02669v2)                    | <img width='400' alt='image' src='./notes/2024/DistAttention/fig1.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) |                                                                                     | [note](./notes/2024/DistAttention/note.md) |</p>
</details>
<details open><summary><b>Xiang Liu</b></summary> 
<p>

|    | meta                                    | ttttttttttttttttttttttttttttttitle                                                                                              | ccccccccccccccccccover                                            | Publish                                                    | codeeeee   | note                                 |
|---:|:----------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------|:-----------------------------------------------------------|:-----------|:-------------------------------------|
|  0 | [LISA](./meta/2024/LISA.prototxt)       | [LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning](http://arxiv.org/abs/2403.17919v1)  |                                                                   | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) |            | [note](./notes/2024/LISA/note.md)    |
|  1 | [ChunkKV](./meta/2025/ChunkKV.prototxt) | [ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference](http://arxiv.org/abs/2502.00299v1) | <img width='400' alt='image' src='./notes/2025/ChunkKV/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) |            | [note](./notes/2025/ChunkKV/note.md) |</p>
</details>
<details open><summary><b>Xiangyue Jin</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Xianzhi Yu</b></summary> 
<p>

|    | meta                                                          | ttttttttttttttttttttttttttttttitle                                                                            | ccccccccccccccccccover                                                       | Publish                                                    | codeeeee   | note                                            |
|---:|:--------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------|:-----------------------------------------------------------|:-----------|:------------------------------------------------|
|  0 | [LinearPatch](./meta/2025/LinearPatch.prototxt)               | [A Simple Linear Patch Revives Layer-Pruned Large Language Models](http://arxiv.org/abs/2505.24680v1)         | <img width='400' alt='image' src='./notes/2025/LinearPatch/fig3.png'>        | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) |            | [note](./notes/2025/LinearPatch/note.md)        |
|  1 | [AttentionPredictor](./meta/2025/AttentionPredictor.prototxt) | [AttentionPredictor: Temporal Pattern Matters for Efficient LLM Inference](http://arxiv.org/abs/2502.04077v1) | <img width='400' alt='image' src='./notes/2025/AttentionPredictor/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) |            | [note](./notes/2025/AttentionPredictor/note.md) |</p>
</details>
<details open><summary><b>Xianzu Wang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Xiao Bi</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Xiaodong Liu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Xiaohan Wang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Xiaojin Shen</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Xiaokang Chen</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Xiaokang Zhang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Xiaosha Chen</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Xiaotao Nie</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Xiaowen Sun</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Xiaoxiang Wang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Xin Cheng</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Xin Liu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                                | ccccccccccccccccccover                                                 | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5)           | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'>  | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                                 | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'>  | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [ShadowKV](./meta/2024/ShadowKV.prototxt)       | [ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference](http://arxiv.org/abs/2410.21465v1)                 | <img width='400' alt='image' src='./notes/2024/ShadowKV/shadowkv.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/bytedance/ShadowKV)      | [note](./notes/2024/ShadowKV/note.md)    |
|  3 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1)           | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'>  | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |
|  4 | [KeepKV](./meta/2025/KeepKV.prototxt)           | [KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference](http://arxiv.org/abs/2504.09936v1) | <img width='400' alt='image' src='./notes/2025/KeepKV/fig1.png'>       | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) |                                                                                   | [note](./notes/2025/KeepKV/note.md)      |</p>
</details>
<details open><summary><b>Xin Xie</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Xinchao Wang</b></summary> 
<p>

|    | meta                                          | ttttttttttttttttttttttttttttttitle                                                                         | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                     | note                                    |
|---:|:----------------------------------------------|:-----------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------|:----------------------------------------|
|  0 | [LLM-Pruner](./meta/2023/LLM-Pruner.prototxt) | [LLM-Pruner: On the Structural Pruning of Large Language Models](http://arxiv.org/abs/2305.11627v3)        | <img width='400' alt='image' src='./notes/2023/LLM-Pruner/cover.jpg'> | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/horseee/LLM-Pruner) | [note](./notes/2023/LLM-Pruner/note.md) |
|  1 | [MaskLLM](./meta/2024/MaskLLM.prototxt)       | [MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models](http://arxiv.org/abs/2409.17481v1) | <img width='400' alt='image' src='./notes/2024/MaskLLM/maskllm.png'>  | ![Publish](https://img.shields.io/badge/2024-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/NVlabs/MaskLLM)     | [note](./notes/2024/MaskLLM/note.md)    |</p>
</details>
<details open><summary><b>Xingchao Liu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Xingkai Yu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                             | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                           | note                                     |
|---:|:------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5)        | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2)  | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                              | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3)  | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeekMoE](./meta/2024/DeepSeekMoE.prototxt) | [DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](http://arxiv.org/abs/2401.06066v1) | <img width='400' alt='image' src='./notes/2024/DeepSeekMoE/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-MoE) | [note](./notes/2024/DeepSeekMoE/note.md) |
|  3 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1)        | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1)  | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Xinnan Song</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Xinxia Shan</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Xinyi Zhou</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Xinyu Yang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Xinyu Zhou</b></summary> 
<p>

|    | meta                                      | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                             | Publish                                                    | codeeeee                                                                                  | note                                  |
|---:|:------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------|:-----------------------------------------------------------|:------------------------------------------------------------------------------------------|:--------------------------------------|
|  0 | [MoBA](./meta/2025/MoBA.prototxt)         | [MoBA: Mixture of Block Attention for Long-Context LLMs](http://arxiv.org/abs/2502.13189v1)                             | <img width='400' alt='image' src='./notes/2025/MoBA/fig1.png'>     | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/MoonshotAI/MoBA)                 | [note](./notes/2025/MoBA/note.md)     |
|  1 | [0VRXJQ3F](./meta/2025/0VRXJQ3F.prototxt) | [Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving](http://arxiv.org/abs/2503.24000v1) | <img width='400' alt='image' src='./notes/2025/0VRXJQ3F/tab1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/LLMkvsys/rethink-kv-compression) | [note](./notes/2025/0VRXJQ3F/note.md) |</p>
</details>
<details open><summary><b>Xinyuan Li</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Xiuhong Li</b></summary> 
<p>

|    | meta                                                    | ttttttttttttttttttttttttttttttitle                                                                                                                       | ccccccccccccccccccover                                                           | Publish                                                    | codeeeee   | note                                         |
|---:|:--------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------|:-----------------------------------------------------------|:-----------|:---------------------------------------------|
|  0 | [m](./meta/2024/DHIB73MC.prototxt)                      | [A Survey on Efficient Inference for Large Language Models](http://arxiv.org/abs/2404.14294v2)                                                           | <img width='400' alt='image' src='./notes/2024/DHIB73MC/efficientinference.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) |            | [note](./notes/2024/DHIB73MC/note.md)        |
|  1 | [SampleAttention](./meta/2024/SampleAttention.prototxt) | [SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention](http://arxiv.org/abs/2406.15486v2) | <img width='400' alt='image' src='./notes/2024/SampleAttention/cover.png'>       | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) |            | [note](./notes/2024/SampleAttention/note.md) |</p>
</details>
<details open><summary><b>Xu Han</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                                  | ccccccccccccccccccover                                                   | Publish                                                    | codeeeee                                                                                       | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [ProSparse](./meta/2024/ProSparse.prototxt)     | [ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models](https://arxiv.org/abs/2402.13516) | <img width='400' alt='image' src='./notes/2024/ProSparse/prosparse.jpg'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/raincleared-song/sparse_gpu_operator) | [note](./notes/2024/ProSparse/note.md)   |
|  1 | [ReLU2](./meta/2024/ReLU2.prototxt)             | [ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs](https://arxiv.org/abs/2402.03804)                          | <img width='400' alt='image' src='./notes/2024/ReLU2/activation.png'>    | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) |                                                                                                | [note](./notes/2024/ReLU2/note.md)       |
|  2 | [SparsingLaw](./meta/2024/SparsingLaw.prototxt) | [Sparsing Law: Towards Large Language Models with Greater Activation Sparsity](http://arxiv.org/abs/2411.02335v1)                   | <img width='400' alt='image' src='./notes/2024/SparsingLaw/fig4.png'>    | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/thunlp/SparsingLaw)                   | [note](./notes/2024/SparsingLaw/note.md) |
|  3 | [MiniCPM4](./meta/2025/MiniCPM4.prototxt)       | [MiniCPM4: Ultra-Efficient LLMs on End Devices](http://arxiv.org/abs/2506.07900v1)                                                  | <img width='400' alt='image' src='./notes/2025/MiniCPM4/fig2.png'>       | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/openbmb/minicpm)                      | [note](./notes/2025/MiniCPM4/note.md)    |</p>
</details>
<details open><summary><b>Xu Owen He</b></summary> 
<p>

|    | meta                            | ttttttttttttttttttttttttttttttitle                                                                | ccccccccccccccccccover   | Publish                                                    | codeeeee                                                                                     | note                             |
|---:|:--------------------------------|:--------------------------------------------------------------------------------------------------|:-------------------------|:-----------------------------------------------------------|:---------------------------------------------------------------------------------------------|:---------------------------------|
|  0 | [FoX](./meta/2025/FoX.prototxt) | [Forgetting Transformer: Softmax Attention with a Forget Gate](http://arxiv.org/abs/2503.02130v2) |                          | ![Publish](https://img.shields.io/badge/2025-ICLR-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/zhixuan-lin/forgetting-transformer) | [note](./notes/2025/FoX/note.md) |
|  1 | [ACP](./meta/2025/ACP.prototxt) | [Adaptive Computation Pruning for the Forgetting Transformer](http://arxiv.org/abs/2504.06949v1)  |                          | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/zhixuan-lin/arctic-fox)             | [note](./notes/2025/ACP/note.md) |</p>
</details>
<details open><summary><b>Xuecheng Su</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Xuefei Ning</b></summary> 
<p>

|    | meta                               | ttttttttttttttttttttttttttttttitle                                                                                   | ccccccccccccccccccover                                                           | Publish                                                    | codeeeee                                                               | note                                  |
|---:|:-----------------------------------|:---------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------|:--------------------------------------|
|  0 | [m](./meta/2024/DHIB73MC.prototxt) | [A Survey on Efficient Inference for Large Language Models](http://arxiv.org/abs/2404.14294v2)                       | <img width='400' alt='image' src='./notes/2024/DHIB73MC/efficientinference.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) |                                                                        | [note](./notes/2024/DHIB73MC/note.md) |
|  1 | [MoA](./meta/2024/MoA.prototxt)    | [MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression](http://arxiv.org/abs/2406.14909v2) | <img width='400' alt='image' src='./notes/2024/MoA/moa.png'>                     | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/thu-nics/MoA) | [note](./notes/2024/MoA/note.md)      |</p>
</details>
<details open><summary><b>Xufang Luo</b></summary> 
<p>

|    | meta                                          | ttttttttttttttttttttttttttttttitle                                                                                               | ccccccccccccccccccover                                                            | Publish                                                    | codeeeee                                                                       | note                                    |
|---:|:----------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------------------------|:-------------------------------------------------------------------------------|:----------------------------------------|
|  0 | [MInference](./meta/2024/MInference.prototxt) | [MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention](http://arxiv.org/abs/2407.02490v1) | <img width='400' alt='image' src='./notes/2024/MInference/MInference_3shape.PNG'> | ![Publish](https://img.shields.io/badge/2024-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/MInference) | [note](./notes/2024/MInference/note.md) |
|  1 | [SCBench](./meta/2024/SCBench.prototxt)       | [SCBench: A KV Cache-Centric Analysis of Long-Context Methods](http://arxiv.org/abs/2412.10319v2)                                |                                                                                   | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/MInference) | [note](./notes/2024/SCBench/note.md)    |</p>
</details>
<details open><summary><b>Xuheng Lin</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Y. K. Li</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                             | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                           | note                                     |
|---:|:------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5)        | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2)  | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                              | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3)  | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeekMoE](./meta/2024/DeepSeekMoE.prototxt) | [DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](http://arxiv.org/abs/2401.06066v1) | <img width='400' alt='image' src='./notes/2024/DeepSeekMoE/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-MoE) | [note](./notes/2024/DeepSeekMoE/note.md) |
|  3 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1)        | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1)  | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Y. Q. Wang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Y. Wu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                             | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                           | note                                     |
|---:|:------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5)        | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2)  | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeekMoE](./meta/2024/DeepSeekMoE.prototxt) | [DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](http://arxiv.org/abs/2401.06066v1) | <img width='400' alt='image' src='./notes/2024/DeepSeekMoE/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-MoE) | [note](./notes/2024/DeepSeekMoE/note.md) |</p>
</details>
<details open><summary><b>Y. X. Wei</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |
|  3 | [NSA](./meta/2025/NSA.prototxt)                 | [Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention](http://arxiv.org/abs/2502.11089v1)  | <img width='400' alt='image' src='./notes/2025/NSA/fig2.png'>         | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) |                                                                                   | [note](./notes/2025/NSA/note.md)         |</p>
</details>
<details open><summary><b>Y. X. Zhu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Yang Zhang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Yanhong Xu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                                                                                                                                                     | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/![GitHub_Repo_stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1)-![Publish](https://img.shields.io/badge/2025-arXiv-violet)-green) | [note](./notes/2025/DeepSeek-R1/note.md)                                          |                                          |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/![GitHub_Repo_stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1)-![Publish](https://img.shields.io/badge/2025-arXiv-violet)-green) | [note](./notes/2025/DeepSeek-R1/note.md)                                          |                                          |
|  2 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/![GitHub_Repo_stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3)-![Publish](https://img.shields.io/badge/2024-arXiv-violet)-green) | [note](./notes/2024/DeepSeek-V3/note.md)                                          |                                          |
|  3 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/![GitHub_Repo_stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3)-![Publish](https://img.shields.io/badge/2024-arXiv-violet)-green) | [note](./notes/2024/DeepSeek-V3/note.md)                                          |                                          |
|  4 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet)                                                                                                                                  | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |</p>
</details>
<details open><summary><b>Yankai Lin</b></summary> 
<p>

|    | meta                                      | ttttttttttttttttttttttttttttttitle                                                                         | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                  | note                                  |
|---:|:------------------------------------------|:-----------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:--------------------------------------------------------------------------|:--------------------------------------|
|  0 | [ReLU2](./meta/2024/ReLU2.prototxt)       | [ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs](https://arxiv.org/abs/2402.03804) | <img width='400' alt='image' src='./notes/2024/ReLU2/activation.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) |                                                                           | [note](./notes/2024/ReLU2/note.md)    |
|  1 | [MiniCPM4](./meta/2025/MiniCPM4.prototxt) | [MiniCPM4: Ultra-Efficient LLMs on End Devices](http://arxiv.org/abs/2506.07900v1)                         | <img width='400' alt='image' src='./notes/2025/MiniCPM4/fig2.png'>    | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/openbmb/minicpm) | [note](./notes/2025/MiniCPM4/note.md) |</p>
</details>
<details open><summary><b>Yanping Huang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Yao Li</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Yao Zhao</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Yaofeng Sun</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Yaohui Li</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Yaohui Wang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Yi Yu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Yi Zheng</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Yichao Zhang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Yifan Shi</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Yikai Zhang</b></summary> 
<p>

|    | meta                                                  | ttttttttttttttttttttttttttttttitle                                                                                               | ccccccccccccccccccover                                                   | Publish                                                    | codeeeee                                                                 | note                                        |
|---:|:------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------|:-----------------------------------------------------------|:-------------------------------------------------------------------------|:--------------------------------------------|
|  0 | [PowerAttention](./meta/2025/PowerAttention.prototxt) | [PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention](http://arxiv.org/abs/2503.03588v1)    | <img width='400' alt='image' src='./notes/2025/PowerAttention/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) |                                                                          | [note](./notes/2025/PowerAttention/note.md) |
|  1 | [R-KV](./meta/2025/R-KV.prototxt)                     | [R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration](http://arxiv.org/abs/2505.24133v2) | <img width='400' alt='image' src='./notes/2025/R-KV/fig1.png'>           | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/Zefan-Cai/R-KV) | [note](./notes/2025/R-KV/note.md)           |</p>
</details>
<details open><summary><b>Yiliang Xiong</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Yilong Zhao</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                           | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [Quest](./meta/2024/Quest.prototxt)             | [Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference](http://arxiv.org/abs/2406.10774)                      | <img width='400' alt='image' src='./notes/2024/Quest/quest.png'>      | ![Publish](https://img.shields.io/badge/2024-ICML-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/quest)       | [note](./notes/2024/Quest/note.md)       |
|  1 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5)      | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  2 | [XGrammar](./meta/2024/XGrammar.prototxt)       | [XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models](http://arxiv.org/abs/2411.15100v2) | <img width='400' alt='image' src='./notes/2024/XGrammar/fig1.png'>    | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/mlc-ai/xgrammar)         | [note](./notes/2024/XGrammar/note.md)    |</p>
</details>
<details open><summary><b>Ying He</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Ying Sheng</b></summary> 
<p>

|    | meta                                                   | ttttttttttttttttttttttttttttttitle                                                                                         | ccccccccccccccccccover                                                   | Publish                                                    | codeeeee                                                                           | note                                        |
|---:|:-------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------------|:--------------------------------------------|
|  0 | [Paged Attention](./meta/2023/PagedAttention.prototxt) | [Efficient Memory Management for Large Language Model Serving with PagedAttention](http://arxiv.org/abs/2309.06180v1)      | <img width='400' alt='image' src='./notes/2023/PagedAttention/vllm.png'> | ![Publish](https://img.shields.io/badge/2023-SOSP-orange)  | ![GitHub Repo stars](https://img.shields.io/github/stars/vllm-project/vllm)        | [note](./notes/2023/PagedAttention/note.md) |
|  1 | [H2O](./meta/2023/H2O.prototxt)                        | [H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models](http://arxiv.org/abs/2306.14048) | <img width='400' alt='image' src='./notes/2023/H2O/h2o.png'>             | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/FMInference/H2O)          | [note](./notes/2023/H2O/note.md)            |
|  2 | [SGLang](./meta/2024/SGLang.prototxt)                  | [SGLang: Efficient Execution of Structured Language Model Programs](http://arxiv.org/abs/2312.07104v2)                     | <img width='400' alt='image' src='./notes/2024/SGLang/fig9.png'>         | ![Publish](https://img.shields.io/badge/2024-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/sgl-project/sglang)       | [note](./notes/2024/SGLang/note.md)         |
|  3 | [DoubleSparsity](./meta/2024/DoubleSparsity.prototxt)  | [Post-Training Sparse Attention with Double Sparsity](http://arxiv.org/abs/2408.07092v2)                                   |                                                                          | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/andy-yang-1/DoubleSparse) | [note](./notes/2024/DoubleSparsity/note.md) |</p>
</details>
<details open><summary><b>Ying Tang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Yishi Piao</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Yisong Wang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Yixiao Li</b></summary> 
<p>

|    | meta                                      | ttttttttttttttttttttttttttttttitle                                                                                             | ccccccccccccccccccover                                                 | Publish                                                    | codeeeee                                                                    | note                               |
|---:|:------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------|:-----------------------------------|
|  0 | [LoSparse](./meta/2023/LoSparse.prototxt) | [Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation](https://arxiv.org/abs/2306.11222) | <img width='400' alt='image' src='./notes/2023/LoSparse/losparse.jpg'> | ![Publish](https://img.shields.io/badge/2023-ICML-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/yxli2123/LoSparse) |                                    |
|  1 | [LoftQ](./meta/2023/loftq.prototxt)       | [LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models](https://arxiv.org/abs/2310.08659)                       | <img width='400' alt='image' src='./notes/2023/loftq/loftq.jpg'>       | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/yxli2123/LoftQ)    | [note](./notes/2023/loftq/note.md) |</p>
</details>
<details open><summary><b>Yixin Dong</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                           | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5)      | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [XGrammar](./meta/2024/XGrammar.prototxt)       | [XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models](http://arxiv.org/abs/2411.15100v2) | <img width='400' alt='image' src='./notes/2024/XGrammar/fig1.png'>    | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/mlc-ai/xgrammar)         | [note](./notes/2024/XGrammar/note.md)    |</p>
</details>
<details open><summary><b>Yixin Song</b></summary> 
<p>

|    | meta                                              | ttttttttttttttttttttttttttttttitle                                                                                  | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                        | note                                      |
|---:|:--------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:--------------------------------------------------------------------------------|:------------------------------------------|
|  0 | [PowerInfer](./meta/2023/PowerInfer.prototxt)     | [PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU](http://arxiv.org/abs/2312.12456v1)        |                                                                       | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/SJTU-IPADS/PowerInfer) | [note](./notes/2023/PowerInfer/note.md)   |
|  1 | [PowerInfer-2](./meta/2024/PowerInfer-2.prototxt) | [PowerInfer-2: Fast Large Language Model Inference on a Smartphone](http://arxiv.org/abs/2406.06282v2)              |                                                                       | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | [Website](https://powerinfer.ai/v2/)                                            | [note](./notes/2024/PowerInfer-2/note.md) |
|  2 | [ReLU2](./meta/2024/ReLU2.prototxt)               | [ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs](https://arxiv.org/abs/2402.03804)          | <img width='400' alt='image' src='./notes/2024/ReLU2/activation.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) |                                                                                 | [note](./notes/2024/ReLU2/note.md)        |
|  3 | [Turbo Sparse](./meta/2024/TurboSparse.prototxt)  | [Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters](http://arxiv.org/abs/2406.05955v2) |                                                                       | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | [Pytorch](https://huggingface.co/PowerInfer)                                    | [note](./notes/2024/TurboSparse/note.md)  |</p>
</details>
<details open><summary><b>Yixuan Tan</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Yiyang Ma</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Yiyuan Liu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Yizhao Gao</b></summary> 
<p>

|    | meta                                                    | ttttttttttttttttttttttttttttttitle                                                                   | ccccccccccccccccccover                                                           | Publish                                                    | codeeeee                                                                          | note                                         |
|---:|:--------------------------------------------------------|:-----------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:---------------------------------------------|
|  0 | [SeerAttention](./meta/2024/SeerAttention.prototxt)     | [SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs](http://arxiv.org/abs/2410.13276v2) | <img width='400' alt='image' src='./notes/2024/SeerAttention/seerattention.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/SeerAttention) | [note](./notes/2024/SeerAttention/note.md)   |
|  1 | [ReSA](./meta/2025/ReSA.prototxt)                       | [Rectified Sparse Attention](http://arxiv.org/abs/2506.04108v2)                                      | <img width='400' alt='image' src='./notes/2025/ReSA/fig2.png'>                   | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/unilm)         | [note](./notes/2025/ReSA/note.md)            |
|  2 | [SeerAttention-R](./meta/2025/SeerAttention-R.prototxt) | [SeerAttention-R: Sparse Attention Adaptation for Long Reasoning](http://arxiv.org/abs/2506.08889v1) | <img width='400' alt='image' src='./notes/2025/SeerAttention-R/fig1.png'>        | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/SeerAttention) | [note](./notes/2025/SeerAttention-R/note.md) |</p>
</details>
<details open><summary><b>Yong Li</b></summary> 
<p>

|    | meta                                                | ttttttttttttttttttttttttttttttitle                                                                                                                      | ccccccccccccccccccover                                                  | Publish                                                    | codeeeee                                                                            | note                                       |
|---:|:----------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------|:-----------------------------------------------------------|:------------------------------------------------------------------------------------|:-------------------------------------------|
|  0 | [Flash-LLM](./meta/2024/flash_llm.prototxt)         | [Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity](https://arxiv.org/abs/2309.10285) | <img width='400' alt='image' src='./notes/2024/flash_llm/cover.jpg'>    | ![Publish](https://img.shields.io/badge/2024-VLDB-green)   | ![GitHub Repo stars](https://img.shields.io/github/stars/AlibabaResearch/flash-llm) | [note](./notes/2024/flash_llm/index.md)    |
|  1 | [DistAttention](./meta/2024/DistAttention.prototxt) | [Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache](http://arxiv.org/abs/2401.02669v2)                    | <img width='400' alt='image' src='./notes/2024/DistAttention/fig1.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) |                                                                                     | [note](./notes/2024/DistAttention/note.md) |</p>
</details>
<details open><summary><b>Yongqiang Guo</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Yu Cheng</b></summary> 
<p>

|    | meta                                                    | ttttttttttttttttttttttttttttttitle                                                                              | ccccccccccccccccccover                                                    | Publish                                                    | codeeeee                                                                          | note                                         |
|---:|:--------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:---------------------------------------------|
|  0 | [AdaLoRA](./meta/2023/adalora.prototxt)                 | [AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning](https://arxiv.org/pdf/2303.10512.pdf) | <img width='400' alt='image' src='./notes/2023/adalora/adalora.jpg'>      | ![Publish](https://img.shields.io/badge/2023-ICLR-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/QingruZhang/AdaLoRA)     |                                              |
|  1 | [SeerAttention-R](./meta/2025/SeerAttention-R.prototxt) | [SeerAttention-R: Sparse Attention Adaptation for Long Reasoning](http://arxiv.org/abs/2506.08889v1)            | <img width='400' alt='image' src='./notes/2025/SeerAttention-R/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/SeerAttention) | [note](./notes/2025/SeerAttention-R/note.md) |</p>
</details>
<details open><summary><b>Yu Wang</b></summary> 
<p>

|    | meta                               | ttttttttttttttttttttttttttttttitle                                                                                   | ccccccccccccccccccover                                                           | Publish                                                    | codeeeee                                                               | note                                  |
|---:|:-----------------------------------|:---------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------|:--------------------------------------|
|  0 | [m](./meta/2024/DHIB73MC.prototxt) | [A Survey on Efficient Inference for Large Language Models](http://arxiv.org/abs/2404.14294v2)                       | <img width='400' alt='image' src='./notes/2024/DHIB73MC/efficientinference.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) |                                                                        | [note](./notes/2024/DHIB73MC/note.md) |
|  1 | [MoA](./meta/2024/MoA.prototxt)    | [MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression](http://arxiv.org/abs/2406.14909v2) | <img width='400' alt='image' src='./notes/2024/MoA/moa.png'>                     | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/thu-nics/MoA) | [note](./notes/2024/MoA/note.md)      |</p>
</details>
<details open><summary><b>Yu Wu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Yuan Ou</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Yuandong Tian</b></summary> 
<p>

|    | meta                                                | ttttttttttttttttttttttttttttttitle                                                                                         | ccccccccccccccccccover                                                   | Publish                                                    | codeeeee                                                                            | note                                       |
|---:|:----------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------|:-----------------------------------------------------------|:------------------------------------------------------------------------------------|:-------------------------------------------|
|  0 | [H2O](./meta/2023/H2O.prototxt)                     | [H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models](http://arxiv.org/abs/2306.14048) | <img width='400' alt='image' src='./notes/2023/H2O/h2o.png'>             | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/FMInference/H2O)           | [note](./notes/2023/H2O/note.md)           |
|  1 | [streaming-llm](./meta/2024/streaming-llm.prototxt) | [Efficient Streaming Language Models with Attention Sinks](http://arxiv.org/abs/2309.17453v4)                              | <img width='400' alt='image' src='./notes/2024/streaming-llm/cover.jpg'> | ![Publish](https://img.shields.io/badge/2024-ICLR-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/streaming-llm) | [note](./notes/2024/streaming-llm/note.md) |
|  2 | [R-Sparse](./meta/2025/R-Sparse.prototxt)           | [R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference](http://arxiv.org/abs/2504.19449v1)                  | <img width='400' alt='image' src='./notes/2025/R-Sparse/fig4.png'>       | ![Publish](https://img.shields.io/badge/2025-ICLR-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/VITA-Group/R-Sparse)       | [note](./notes/2025/R-Sparse/note.md)      |</p>
</details>
<details open><summary><b>Yuchen Zhu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Yucheng Li</b></summary> 
<p>

|    | meta                                                        | ttttttttttttttttttttttttttttttitle                                                                                                                            | ccccccccccccccccccover                                                                   | Publish                                                    | codeeeee                                                                                | note                                    |
|---:|:------------------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------------|:----------------------------------------|
|  0 | [Selective Context](./meta/2023/selective_context.prototxt) | [Unlocking Context Constraints of LLMs: Enhancing Context Efficiency of LLMs with Self-Information-Based Content Filtering](https://arxiv.org/abs/2304.12102) | <img width='400' alt='image' src='./notes/2023/selective_context/selective_context.jpg'> | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/liyucheng09/Selective_Context) |                                         |
|  1 | [MInference](./meta/2024/MInference.prototxt)               | [MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention](http://arxiv.org/abs/2407.02490v1)                              | <img width='400' alt='image' src='./notes/2024/MInference/MInference_3shape.PNG'>        | ![Publish](https://img.shields.io/badge/2024-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/MInference)          | [note](./notes/2024/MInference/note.md) |
|  2 | [SCBench](./meta/2024/SCBench.prototxt)                     | [SCBench: A KV Cache-Centric Analysis of Long-Context Methods](http://arxiv.org/abs/2412.10319v2)                                                             |                                                                                          | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/MInference)          | [note](./notes/2024/SCBench/note.md)    |
|  3 | [R-KV](./meta/2025/R-KV.prototxt)                           | [R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration](http://arxiv.org/abs/2505.24133v2)                              | <img width='400' alt='image' src='./notes/2025/R-KV/fig1.png'>                           | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/Zefan-Cai/R-KV)                | [note](./notes/2025/R-KV/note.md)       |</p>
</details>
<details open><summary><b>Yuduan Wang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Yue Gong</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Yuezhou Hu</b></summary> 
<p>

|    | meta                                                                | ttttttttttttttttttttttttttttttitle                                                                               | ccccccccccccccccccover                                                          | Publish                                                 | codeeeee                                                                                 | note                                               |
|---:|:--------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------------|:--------------------------------------------------------|:-----------------------------------------------------------------------------------------|:---------------------------------------------------|
|  0 | [m](./meta/2024/HYPL7G37.prototxt)                                  | [Accelerating Transformer Pre-training with 2:4 Sparsity](http://arxiv.org/abs/2404.01847v2)                     |                                                                                 | ![Publish](https://img.shields.io/badge/2024-ICML-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/huyz2023/2by4-pretrain)         | [note](./notes/2024/HYPL7G37/note.md)              |
|  1 | [AdaptiveSparseTrainer](./meta/2025/AdaptiveSparseTrainer.prototxt) | [Pruning Large Language Models with Semi-Structural Adaptive Sparse Training](http://arxiv.org/abs/2407.20584v3) | <img width='400' alt='image' src='./notes/2025/AdaptiveSparseTrainer/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-AAAI-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/thu-ml/Adaptive-Sparse-Trainer) | [note](./notes/2025/AdaptiveSparseTrainer/note.md) |</p>
</details>
<details open><summary><b>Yuheng Zou</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Yuhui Xu</b></summary> 
<p>

|    | meta                                    | ttttttttttttttttttttttttttttttitle                                                                                     | ccccccccccccccccccover                                              | Publish                                                 | codeeeee                                                                      | note                                 |
|---:|:----------------------------------------|:-----------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------|:--------------------------------------------------------|:------------------------------------------------------------------------------|:-------------------------------------|
|  0 | [QA-LoRA](./meta/2024/QA-LoRA.prototxt) | [QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2309.14717)           | <img width='400' alt='image' src='./notes/2024/QA-LoRA/qalora.jpg'> | ![Publish](https://img.shields.io/badge/2024-ICLR-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/yuhuixu1993/qa-lora) | [note](./notes/2024/QA-LoRA/note.md) |
|  1 | [SPP](./meta/2024/SPP.prototxt)         | [SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models](http://arxiv.org/abs/2405.16057v1) | <img width='400' alt='image' src='./notes/2024/SPP/spp.png'>        | ![Publish](https://img.shields.io/badge/2024-ICML-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/Lucky-Lance/SPP)     | [note](./notes/2024/SPP/note.md)     |</p>
</details>
<details open><summary><b>Yujia He</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Yujun Lin</b></summary> 
<p>

|    | meta                                    | ttttttttttttttttttttttttttttttitle                                                                               | ccccccccccccccccccover                                            | Publish                                                    | codeeeee                                                                        | note                                 |
|---:|:----------------------------------------|:-----------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------|:-----------------------------------------------------------|:--------------------------------------------------------------------------------|:-------------------------------------|
|  0 | [QServe](./meta/2024/QServer.prototxt)  | [QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving](http://arxiv.org/abs/2405.04532v2) |                                                                   | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | [Pytorch](https://hanlab.mit.edu/projects/qserve)                               | [note](./notes/2024/QServer/note.md) |
|  1 | [LServer](./meta/2025/LServer.prototxt) | [LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention](http://arxiv.org/abs/2502.14866v1)   | <img width='400' alt='image' src='./notes/2025/LServer/fig5.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/mit-han-lab/omniserve) | [note](./notes/2025/LServer/note.md) |</p>
</details>
<details open><summary><b>Yukun Zha</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Yulhwa Kim</b></summary> 
<p>

|    | meta                                  | ttttttttttttttttttttttttttttttitle                                                                                                  | ccccccccccccccccccover                                           | Publish                                                    | codeeeee                                                                   | note                                |
|---:|:--------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------|:-----------------------------------------------------------|:---------------------------------------------------------------------------|:------------------------------------|
|  0 | [L4Q](./meta/2024/L4Q.prototxt)       | [L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ](https://arxiv.org/abs/2402.04902) | <img width='400' alt='image' src='./notes/2024/L4Q/l4q.jpg'>     | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) |                                                                            | [note](./notes/2024/L4Q/note.md)    |
|  1 | [FastKV](./meta/2025/FastKV.prototxt) | [FastKV: KV Cache Compression for Fast Long-Context Processing with Token-Selective Propagation](http://arxiv.org/abs/2502.01068v1) | <img width='400' alt='image' src='./notes/2025/FastKV/fig2.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/dongwonjo/FastKV) | [note](./notes/2025/FastKV/note.md) |</p>
</details>
<details open><summary><b>Yunfan Xiong</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Yunxian Ma</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Yuqing Xia</b></summary> 
<p>

|    | meta                                                    | ttttttttttttttttttttttttttttttitle                                                                   | ccccccccccccccccccover                                                    | Publish                                                    | codeeeee                                                                          | note                                         |
|---:|:--------------------------------------------------------|:-----------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:---------------------------------------------|
|  0 | [ReSA](./meta/2025/ReSA.prototxt)                       | [Rectified Sparse Attention](http://arxiv.org/abs/2506.04108v2)                                      | <img width='400' alt='image' src='./notes/2025/ReSA/fig2.png'>            | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/unilm)         | [note](./notes/2025/ReSA/note.md)            |
|  1 | [SeerAttention-R](./meta/2025/SeerAttention-R.prototxt) | [SeerAttention-R: Sparse Attention Adaptation for Long Reasoning](http://arxiv.org/abs/2506.08889v1) | <img width='400' alt='image' src='./notes/2025/SeerAttention-R/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/SeerAttention) | [note](./notes/2025/SeerAttention-R/note.md) |</p>
</details>
<details open><summary><b>Yuqing Yang</b></summary> 
<p>

|    | meta                                          | ttttttttttttttttttttttttttttttitle                                                                                               | ccccccccccccccccccover                                                            | Publish                                                    | codeeeee                                                                       | note                                    |
|---:|:----------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------------------------|:-------------------------------------------------------------------------------|:----------------------------------------|
|  0 | [MInference](./meta/2024/MInference.prototxt) | [MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention](http://arxiv.org/abs/2407.02490v1) | <img width='400' alt='image' src='./notes/2024/MInference/MInference_3shape.PNG'> | ![Publish](https://img.shields.io/badge/2024-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/MInference) | [note](./notes/2024/MInference/note.md) |
|  1 | [SCBench](./meta/2024/SCBench.prototxt)       | [SCBench: A KV Cache-Centric Analysis of Long-Context Methods](http://arxiv.org/abs/2412.10319v2)                                |                                                                                   | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/MInference) | [note](./notes/2024/SCBench/note.md)    |</p>
</details>
<details open><summary><b>Yutao Sun</b></summary> 
<p>

|    | meta                                                    | ttttttttttttttttttttttttttttttitle                                                                   | ccccccccccccccccccover                                                    | Publish                                                    | codeeeee                                                                          | note                                         |
|---:|:--------------------------------------------------------|:-----------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:---------------------------------------------|
|  0 | [ReSA](./meta/2025/ReSA.prototxt)                       | [Rectified Sparse Attention](http://arxiv.org/abs/2506.04108v2)                                      | <img width='400' alt='image' src='./notes/2025/ReSA/fig2.png'>            | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/unilm)         | [note](./notes/2025/ReSA/note.md)            |
|  1 | [SeerAttention-R](./meta/2025/SeerAttention-R.prototxt) | [SeerAttention-R: Sparse Attention Adaptation for Long Reasoning](http://arxiv.org/abs/2506.08889v1) | <img width='400' alt='image' src='./notes/2025/SeerAttention-R/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/SeerAttention) | [note](./notes/2025/SeerAttention-R/note.md) |</p>
</details>
<details open><summary><b>Yuting Yan</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Yuxiang Luo</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Yuxiang You</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Yuxin Wu</b></summary> 
<p>

|    | meta                              | ttttttttttttttttttttttttttttttitle                                                                                                        | ccccccccccccccccccover                                         | Publish                                                    | codeeeee                                                                  | note                              |
|---:|:----------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------|:-----------------------------------------------------------|:--------------------------------------------------------------------------|:----------------------------------|
|  0 | [AVSS](./meta/2024/AVSS.prototxt) | [AVSS: Layer Importance Evaluation in Large Language Models via Activation Variance-Sparsity Analysis](http://arxiv.org/abs/2411.02117v1) | <img width='400' alt='image' src='./notes/2024/AVSS/avss.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) |                                                                           | [note](./notes/2024/AVSS/note.md) |
|  1 | [MoBA](./meta/2025/MoBA.prototxt) | [MoBA: Mixture of Block Attention for Long-Context LLMs](http://arxiv.org/abs/2502.13189v1)                                               | <img width='400' alt='image' src='./notes/2025/MoBA/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/MoonshotAI/MoBA) | [note](./notes/2025/MoBA/note.md) |</p>
</details>
<details open><summary><b>Yuxiong He</b></summary> 
<p>

|    | meta                                              | ttttttttttttttttttttttttttttttitle                                                                                                               | ccccccccccccccccccover   | Publish                                                    | codeeeee                                                                      | note   |
|---:|:--------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------|:-----------------------------------------------------------|:------------------------------------------------------------------------------|:-------|
|  0 | [ZeroQuant](./meta/2022/zeroquant.prototxt)       | [ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers](https://openreview.net/forum?id=f-fVCElZ-G1)       |                          | ![Publish](https://img.shields.io/badge/2022-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/DeepSpeed) |        |
|  1 | [ZeroQuant-V2](./meta/2023/ZeroQuant-V2.prototxt) | [ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation](https://arxiv.org/abs/2303.08302) |                          | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/DeepSpeed) |        |</p>
</details>
<details open><summary><b>Yuxuan Liu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Yuyang Zhou</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Z. F. Wu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Z. Z. Ren</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Zehui Ren</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Zeyu Mi</b></summary> 
<p>

|    | meta                                              | ttttttttttttttttttttttttttttttitle                                                                                  | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                        | note                                      |
|---:|:--------------------------------------------------|:--------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:--------------------------------------------------------------------------------|:------------------------------------------|
|  0 | [PowerInfer](./meta/2023/PowerInfer.prototxt)     | [PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU](http://arxiv.org/abs/2312.12456v1)        |                                                                       | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/SJTU-IPADS/PowerInfer) | [note](./notes/2023/PowerInfer/note.md)   |
|  1 | [PowerInfer-2](./meta/2024/PowerInfer-2.prototxt) | [PowerInfer-2: Fast Large Language Model Inference on a Smartphone](http://arxiv.org/abs/2406.06282v2)              |                                                                       | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | [Website](https://powerinfer.ai/v2/)                                            | [note](./notes/2024/PowerInfer-2/note.md) |
|  2 | [ReLU2](./meta/2024/ReLU2.prototxt)               | [ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs](https://arxiv.org/abs/2402.03804)          | <img width='400' alt='image' src='./notes/2024/ReLU2/activation.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) |                                                                                 | [note](./notes/2024/ReLU2/note.md)        |
|  3 | [Turbo Sparse](./meta/2024/TurboSparse.prototxt)  | [Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters](http://arxiv.org/abs/2406.05955v2) |                                                                       | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | [Pytorch](https://huggingface.co/PowerInfer)                                    | [note](./notes/2024/TurboSparse/note.md)  |</p>
</details>
<details open><summary><b>Zhangli Sha</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Zhangyang Wang</b></summary> 
<p>

|    | meta                                                         | ttttttttttttttttttttttttttttttitle                                                                                                            | ccccccccccccccccccover                                                 | Publish                                                    | codeeeee                                                                                | note                                  |
|---:|:-------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------------|:--------------------------------------|
|  0 | [H2O](./meta/2023/H2O.prototxt)                              | [H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models](http://arxiv.org/abs/2306.14048)                    | <img width='400' alt='image' src='./notes/2023/H2O/h2o.png'>           | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/FMInference/H2O)               | [note](./notes/2023/H2O/note.md)      |
|  1 | [m](./meta/2023/AYB1XUO5.prototxt)                           | [Ten Lessons We Have Learned in the New Sparseland: A Short Handbook for Sparse Neural Network Researchers](https://arxiv.org/abs/2302.02596) |                                                                        | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) |                                                                                         |                                       |
|  2 | [Essential Sparsity](./meta/2023/EssentialSparsity.prototxt) | [The Emergence of Essential Sparsity in Large Pre-trained Models: The Weights that Matter](https://arxiv.org/abs/2306.03805)                  |                                                                        | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/VITA-Group/essential_sparsity) |                                       |
|  3 | [LLM-KICK](./meta/2024/VB8C61V6.prototxt)                    | [Compressing LLMs: The Truth is Rarely Pure and Never Simple](http://arxiv.org/abs/2310.01382v2)                                              | <img width='400' alt='image' src='./notes/2024/VB8C61V6/llm-kick.jpg'> | ![Publish](https://img.shields.io/badge/2024-ICLR-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/VITA-Group/llm-kick)           | [note](./notes/2024/VB8C61V6/note.md) |
|  4 | [OWL](./meta/2024/owl.prototxt)                              | [Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity](https://arxiv.org/pdf/2310.05175.pdf)    | <img width='400' alt='image' src='./notes/2024/owl/cover.jpg'>         | ![Publish](https://img.shields.io/badge/2024-ICML-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/luuyin/OWL)                    |                                       |
|  5 | [m](./meta/2024/SN1PK7EK.prototxt)                           | [Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark](http://arxiv.org/abs/2402.11592v2)                   |                                                                        | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/ZO-Bench/ZO-LLM)               | [note](./notes/2024/SN1PK7EK/note.md) |
|  6 | [R-Sparse](./meta/2025/R-Sparse.prototxt)                    | [R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference](http://arxiv.org/abs/2504.19449v1)                                     | <img width='400' alt='image' src='./notes/2025/R-Sparse/fig4.png'>     | ![Publish](https://img.shields.io/badge/2025-ICLR-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/VITA-Group/R-Sparse)           | [note](./notes/2025/R-Sparse/note.md) |</p>
</details>
<details open><summary><b>Zhe Fu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Zhean Xu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Zhen Dong</b></summary> 
<p>

|    | meta                                          | ttttttttttttttttttttttttttttttitle                                                                                               | ccccccccccccccccccover                                                     | Publish                                                    | codeeeee                                                                          | note                                    |
|---:|:----------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:----------------------------------------|
|  0 | [SqueezeLLM](./meta/2024/SqueezeLLM.prototxt) | [SqueezeLLM: Dense-and-Sparse Quantization](http://arxiv.org/abs/2306.07629v4)                                                   | <img width='400' alt='image' src='./notes/2024/SqueezeLLM/squeezeLLM.png'> | ![Publish](https://img.shields.io/badge/2024-ICML-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/SqueezeAILab/SqueezeLLM) | [note](./notes/2024/SqueezeLLM/note.md) |
|  1 | [R-KV](./meta/2025/R-KV.prototxt)             | [R-KV: Redundancy-aware KV Cache Compression for Training-Free Reasoning Models Acceleration](http://arxiv.org/abs/2505.24133v2) | <img width='400' alt='image' src='./notes/2025/R-KV/fig1.png'>             | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/Zefan-Cai/R-KV)          | [note](./notes/2025/R-KV/note.md)       |</p>
</details>
<details open><summary><b>Zhen Huang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Zhen Zhang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Zhenda Xie</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                             | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                           | note                                     |
|---:|:------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5)        | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2)  | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                              | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3)  | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeekMoE](./meta/2024/DeepSeekMoE.prototxt) | [DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](http://arxiv.org/abs/2401.06066v1) | <img width='400' alt='image' src='./notes/2024/DeepSeekMoE/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-MoE) | [note](./notes/2024/DeepSeekMoE/note.md) |
|  3 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1)        | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1)  | [note](./notes/2025/DeepSeek-R1/note.md) |
|  4 | [NSA](./meta/2025/NSA.prototxt)                 | [Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention](http://arxiv.org/abs/2502.11089v1)         | <img width='400' alt='image' src='./notes/2025/NSA/fig2.png'>         | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) |                                                                                    | [note](./notes/2025/NSA/note.md)         |</p>
</details>
<details open><summary><b>Zhengyan Zhang</b></summary> 
<p>

|    | meta                                             | ttttttttttttttttttttttttttttttitle                                                                                                  | ccccccccccccccccccover                                                   | Publish                                                    | codeeeee                                                                                       | note                                     |
|---:|:-------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt)  | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                                   | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'>    | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3)              | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [ProSparse](./meta/2024/ProSparse.prototxt)      | [ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models](https://arxiv.org/abs/2402.13516) | <img width='400' alt='image' src='./notes/2024/ProSparse/prosparse.jpg'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/raincleared-song/sparse_gpu_operator) | [note](./notes/2024/ProSparse/note.md)   |
|  2 | [ReLU2](./meta/2024/ReLU2.prototxt)              | [ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs](https://arxiv.org/abs/2402.03804)                          | <img width='400' alt='image' src='./notes/2024/ReLU2/activation.png'>    | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) |                                                                                                | [note](./notes/2024/ReLU2/note.md)       |
|  3 | [Turbo Sparse](./meta/2024/TurboSparse.prototxt) | [Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters](http://arxiv.org/abs/2406.05955v2)                 |                                                                          | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | [Pytorch](https://huggingface.co/PowerInfer)                                                   | [note](./notes/2024/TurboSparse/note.md) |
|  4 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt)  | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1)             | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'>    | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1)              | [note](./notes/2025/DeepSeek-R1/note.md) |
|  5 | [NSA](./meta/2025/NSA.prototxt)                  | [Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention](http://arxiv.org/abs/2502.11089v1)              | <img width='400' alt='image' src='./notes/2025/NSA/fig2.png'>            | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) |                                                                                                | [note](./notes/2025/NSA/note.md)         |</p>
</details>
<details open><summary><b>Zhenyu Zhang</b></summary> 
<p>

|    | meta                                      | ttttttttttttttttttttttttttttttitle                                                                                                         | ccccccccccccccccccover                                             | Publish                                                    | codeeeee                                                                      | note                                  |
|---:|:------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------|:-----------------------------------------------------------|:------------------------------------------------------------------------------|:--------------------------------------|
|  0 | [H2O](./meta/2023/H2O.prototxt)           | [H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models](http://arxiv.org/abs/2306.14048)                 | <img width='400' alt='image' src='./notes/2023/H2O/h2o.png'>       | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/FMInference/H2O)     | [note](./notes/2023/H2O/note.md)      |
|  1 | [OWL](./meta/2024/owl.prototxt)           | [Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity](https://arxiv.org/pdf/2310.05175.pdf) | <img width='400' alt='image' src='./notes/2024/owl/cover.jpg'>     | ![Publish](https://img.shields.io/badge/2024-ICML-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/luuyin/OWL)          |                                       |
|  2 | [R-Sparse](./meta/2025/R-Sparse.prototxt) | [R-Sparse: Rank-Aware Activation Sparsity for Efficient LLM Inference](http://arxiv.org/abs/2504.19449v1)                                  | <img width='400' alt='image' src='./notes/2025/R-Sparse/fig4.png'> | ![Publish](https://img.shields.io/badge/2025-ICLR-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/VITA-Group/R-Sparse) | [note](./notes/2025/R-Sparse/note.md) |</p>
</details>
<details open><summary><b>Zhewei Yao</b></summary> 
<p>

|    | meta                                              | ttttttttttttttttttttttttttttttitle                                                                                                               | ccccccccccccccccccover   | Publish                                                    | codeeeee                                                                      | note   |
|---:|:--------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------|:-----------------------------------------------------------|:------------------------------------------------------------------------------|:-------|
|  0 | [ActNN](./meta/2019/actnn.prototxt)               | [ActNN: Reducing Training Memory Footprint via 2-Bit Activation Compressed Training](https://arxiv.org/abs/2104.14129)                           |                          | ![Publish](https://img.shields.io/badge/2019-ICML-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/ucbrise/actnn)       |        |
|  1 | [ZeroQuant](./meta/2022/zeroquant.prototxt)       | [ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers](https://openreview.net/forum?id=f-fVCElZ-G1)       |                          | ![Publish](https://img.shields.io/badge/2022-NeurIPS-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/DeepSpeed) |        |
|  2 | [ZeroQuant-V2](./meta/2023/ZeroQuant-V2.prototxt) | [ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation](https://arxiv.org/abs/2303.08302) |                          | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/microsoft/DeepSpeed) |        |</p>
</details>
<details open><summary><b>Zhewen Hao</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Zhibin Gou</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Zhicheng Ma</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Zhigang Yan</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Zhihang Yuan</b></summary> 
<p>

|    | meta                               | ttttttttttttttttttttttttttttttitle                                                                               | ccccccccccccccccccover                                                           | Publish                                                    | codeeeee                                                                    | note                                  |
|---:|:-----------------------------------|:-----------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------|:--------------------------------------|
|  0 | [RPTQ](./meta/2023/RPTQ.prototxt)  | [RPTQ: Reorder-based Post-training Quantization for Large Language Models](https://arxiv.org/pdf/2304.01089.pdf) |                                                                                  | ![Publish](https://img.shields.io/badge/2023-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/hahnyuan/RPTQ4LLM) | [note](note.md)                       |
|  1 | [m](./meta/2024/DHIB73MC.prototxt) | [A Survey on Efficient Inference for Large Language Models](http://arxiv.org/abs/2404.14294v2)                   | <img width='400' alt='image' src='./notes/2024/DHIB73MC/efficientinference.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) |                                                                             | [note](./notes/2024/DHIB73MC/note.md) |</p>
</details>
<details open><summary><b>Zhihong Shao</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Zhipeng Xu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Zhixuan Lin</b></summary> 
<p>

|    | meta                            | ttttttttttttttttttttttttttttttitle                                                                | ccccccccccccccccccover   | Publish                                                    | codeeeee                                                                                     | note                             |
|---:|:--------------------------------|:--------------------------------------------------------------------------------------------------|:-------------------------|:-----------------------------------------------------------|:---------------------------------------------------------------------------------------------|:---------------------------------|
|  0 | [FoX](./meta/2025/FoX.prototxt) | [Forgetting Transformer: Softmax Attention with a Forget Gate](http://arxiv.org/abs/2503.02130v2) |                          | ![Publish](https://img.shields.io/badge/2025-ICLR-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/zhixuan-lin/forgetting-transformer) | [note](./notes/2025/FoX/note.md) |
|  1 | [ACP](./meta/2025/ACP.prototxt) | [Adaptive Computation Pruning for the Forgetting Transformer](http://arxiv.org/abs/2504.06949v1)  |                          | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/zhixuan-lin/arctic-fox)             | [note](./notes/2025/ACP/note.md) |</p>
</details>
<details open><summary><b>Zhiyu Wu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Zhiyuan Liu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                                  | ccccccccccccccccccover                                                   | Publish                                                    | codeeeee                                                                                       | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------|:-----------------------------------------------------------|:-----------------------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [ProSparse](./meta/2024/ProSparse.prototxt)     | [ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models](https://arxiv.org/abs/2402.13516) | <img width='400' alt='image' src='./notes/2024/ProSparse/prosparse.jpg'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/raincleared-song/sparse_gpu_operator) | [note](./notes/2024/ProSparse/note.md)   |
|  1 | [ReLU2](./meta/2024/ReLU2.prototxt)             | [ReLU2 Wins: Discovering Efficient Activation Functions for Sparse LLMs](https://arxiv.org/abs/2402.03804)                          | <img width='400' alt='image' src='./notes/2024/ReLU2/activation.png'>    | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) |                                                                                                | [note](./notes/2024/ReLU2/note.md)       |
|  2 | [SparsingLaw](./meta/2024/SparsingLaw.prototxt) | [Sparsing Law: Towards Large Language Models with Greater Activation Sparsity](http://arxiv.org/abs/2411.02335v1)                   | <img width='400' alt='image' src='./notes/2024/SparsingLaw/fig4.png'>    | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/thunlp/SparsingLaw)                   | [note](./notes/2024/SparsingLaw/note.md) |
|  3 | [MiniCPM4](./meta/2025/MiniCPM4.prototxt)       | [MiniCPM4: Ultra-Efficient LLMs on End Devices](http://arxiv.org/abs/2506.07900v1)                                                  | <img width='400' alt='image' src='./notes/2025/MiniCPM4/fig2.png'>       | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/openbmb/minicpm)                      | [note](./notes/2025/MiniCPM4/note.md)    |</p>
</details>
<details open><summary><b>Zhongyu Zhang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Zhou Yu</b></summary> 
<p>

|    | meta                                                    | ttttttttttttttttttttttttttttttitle                                                                                                                    | ccccccccccccccccccover                                                    | Publish                                                    | codeeeee                                                                    | note                                         |
|---:|:--------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------|:---------------------------------------------|
|  0 | [CachedAttention](./meta/2024/CachedAttention.prototxt) | [Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention](http://arxiv.org/abs/2403.19708v3)                    | <img width='400' alt='image' src='./notes/2024/CachedAttention/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-ATC-orange)   |                                                                             | [note](./notes/2024/CachedAttention/note.md) |
|  1 | [Adrenaline](./meta/2025/Adrenaline.prototxt)           | [Injecting Adrenaline into LLM Serving: Boosting Resource Utilization and Throughput via Attention Disaggregation](http://arxiv.org/abs/2503.20552v1) | <img width='400' alt='image' src='./notes/2025/Adrenaline/fig4.png'>      | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/ASISys/Adrenaline) | [note](./notes/2025/Adrenaline/note.md)      |</p>
</details>
<details open><summary><b>Zhuang Liu</b></summary> 
<p>

|    | meta                                                            | ttttttttttttttttttttttttttttttitle                                                                   | ccccccccccccccccccover                                                               | Publish                                                    | codeeeee                                                                               | note                                             |
|---:|:----------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------|:-----------------------------------------------------------|:---------------------------------------------------------------------------------------|:-------------------------------------------------|
|  0 | [Wanda](./meta/2024/Wanda.prototxt)                             | [A Simple and Effective Pruning Approach for Large Language Models](http://arxiv.org/abs/2306.11695) | <img width='400' alt='image' src='./notes/2024/Wanda/wanda.png'>                     | ![Publish](https://img.shields.io/badge/2024-ICLR-blue)    | ![GitHub Repo stars](https://img.shields.io/github/stars/locuslab/wanda)               | [note](./notes/2024/Wanda/note.md)               |
|  1 | [massive-activations](./meta/2024/massive-activations.prototxt) | [Massive Activations in Large Language Models](http://arxiv.org/abs/2402.17762v2)                    | <img width='400' alt='image' src='./notes/2024/massive-activations/massive_act.jpg'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/locuslab/massive-activations) | [note](./notes/2024/massive-activations/note.md) |</p>
</details>
<details open><summary><b>Zhuomin He</b></summary> 
<p>

|    | meta                                                    | ttttttttttttttttttttttttttttttitle                                                                                                 | ccccccccccccccccccover                                                    | Publish                                                  | codeeeee                                                                 | note                                         |
|---:|:--------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------|:--------------------------------------------------------------------------|:---------------------------------------------------------|:-------------------------------------------------------------------------|:---------------------------------------------|
|  0 | [CachedAttention](./meta/2024/CachedAttention.prototxt) | [Cost-Efficient Large Language Model Serving for Multi-turn Conversations with CachedAttention](http://arxiv.org/abs/2403.19708v3) | <img width='400' alt='image' src='./notes/2024/CachedAttention/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-ATC-orange) |                                                                          | [note](./notes/2024/CachedAttention/note.md) |
|  1 | [AdaSkip](./meta/2025/AdaSkip.prototxt)                 | [AdaSkip: Adaptive Sublayer Skipping for Accelerating Long-Context LLM Inference](http://arxiv.org/abs/2501.02336v1)               | <img width='400' alt='image' src='./notes/2025/AdaSkip/fig1.png'>         | ![Publish](https://img.shields.io/badge/2025-AAAI-blue)  | ![GitHub Repo stars](https://img.shields.io/github/stars/ASISys/AdaSkip) | [note](./notes/2025/AdaSkip/note.md)         |</p>
</details>
<details open><summary><b>Zhuoshu Li</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Zihan Wang</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                                | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5)           | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [KeepKV](./meta/2025/KeepKV.prototxt)           | [KeepKV: Eliminating Output Perturbation in KV Cache Compression for Efficient LLMs Inference](http://arxiv.org/abs/2504.09936v1) | <img width='400' alt='image' src='./notes/2025/KeepKV/fig1.png'>      | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) |                                                                                   | [note](./notes/2025/KeepKV/note.md)      |</p>
</details>
<details open><summary><b>Zihui Gu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Zijia Zhu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Zijun Liu</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Zilin Li</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Ziqing Yang</b></summary> 
<p>

|    | meta                                          | ttttttttttttttttttttttttttttttitle                                                                        | ccccccccccccccccccover                                                     | Publish                                                | codeeeee                                                                     | note                                |
|---:|:----------------------------------------------|:----------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------|:-------------------------------------------------------|:-----------------------------------------------------------------------------|:------------------------------------|
|  0 | [TextPruner](./meta/2022/TextPruner.prototxt) | [TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models](https://arxiv.org/abs/2203.15996)   | <img width='400' alt='image' src='./notes/2022/TextPruner/textpruner.jpg'> | ![Publish](https://img.shields.io/badge/2022-ACL-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/airaria/TextPruner) |                                     |
|  1 | [GRAIN](./meta/2023/grain.prototxt)           | [Gradient-based Intra-attention Pruning on Pre-trained Language Models](https://arxiv.org/abs/2212.07634) | <img width='400' alt='image' src='./notes/2023/grain/grain.jpg'>           | ![Publish](https://img.shields.io/badge/2023-ACL-blue) | ![GitHub Repo stars](https://img.shields.io/github/stars/airaria/GRAIN)      | [note](./notes/2023/grain/index.md) |</p>
</details>
<details open><summary><b>Ziwei Xie</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V2](./meta/2024/DeepSeek-V2.prototxt) | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](http://arxiv.org/abs/2405.04434v5) | <img width='400' alt='image' src='./notes/2024/DeepSeek-V2/fig2.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2) | [note](./notes/2024/DeepSeek-V2/note.md) |
|  1 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  2 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Zixuan Zhou</b></summary> 
<p>

|    | meta                                      | ttttttttttttttttttttttttttttttitle                                                             | ccccccccccccccccccover                                                           | Publish                                                    | codeeeee                                                                  | note                                  |
|---:|:------------------------------------------|:-----------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------|:-----------------------------------------------------------|:--------------------------------------------------------------------------|:--------------------------------------|
|  0 | [m](./meta/2024/DHIB73MC.prototxt)        | [A Survey on Efficient Inference for Large Language Models](http://arxiv.org/abs/2404.14294v2) | <img width='400' alt='image' src='./notes/2024/DHIB73MC/efficientinference.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) |                                                                           | [note](./notes/2024/DHIB73MC/note.md) |
|  1 | [MiniCPM4](./meta/2025/MiniCPM4.prototxt) | [MiniCPM4: Ultra-Efficient LLMs on End Devices](http://arxiv.org/abs/2506.07900v1)             | <img width='400' alt='image' src='./notes/2025/MiniCPM4/fig2.png'>               | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/openbmb/minicpm) | [note](./notes/2025/MiniCPM4/note.md) |</p>
</details>
<details open><summary><b>Ziyang Song</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Ziyi Gao</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Zizheng Pan</b></summary> 
<p>

|    | meta                                            | ttttttttttttttttttttttttttttttitle                                                                                      | ccccccccccccccccccover                                                | Publish                                                    | codeeeee                                                                          | note                                     |
|---:|:------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------|:-----------------------------------------------------------|:----------------------------------------------------------------------------------|:-----------------------------------------|
|  0 | [DeepSeek-V3](./meta/2024/DeepSeek-V3.prototxt) | [DeepSeek-V3 Technical Report](http://arxiv.org/abs/2412.19437v1)                                                       | <img width='400' alt='image' src='./notes/2024/DeepSeek-V3/fig5.png'> | ![Publish](https://img.shields.io/badge/2024-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V3) | [note](./notes/2024/DeepSeek-V3/note.md) |
|  1 | [DeepSeek-R1](./meta/2025/DeepSeek-R1.prototxt) | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](http://arxiv.org/abs/2501.12948v1) | <img width='400' alt='image' src='./notes/2025/DeepSeek-R1/fig1.png'> | ![Publish](https://img.shields.io/badge/2025-arXiv-violet) | ![GitHub Repo stars](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-R1) | [note](./notes/2025/DeepSeek-R1/note.md) |</p>
</details>
<details open><summary><b>Zuchao Li</b></summary> 
<p>

|    | meta                               | ttttttttttttttttttttttttttttttitle                                                                                   | ccccccccccccccccccover                                                 | Publish                                                  | codeeeee                                                                                | note                                  |
|---:|:-----------------------------------|:---------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------|:---------------------------------------------------------|:----------------------------------------------------------------------------------------|:--------------------------------------|
|  0 | [m](./meta/2024/JSHWEV0S.prototxt) | [Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption](http://arxiv.org/abs/2407.18003v3) | <img width='400' alt='image' src='./notes/2024/JSHWEV0S/overview.png'> | ![Publish](https://img.shields.io/badge/2024-COLM-green) | ![GitHub Repo stars](https://img.shields.io/github/stars/zcli-charlie/Awesome-KV-Cache) | [note](./notes/2024/JSHWEV0S/note.md) |
|  1 | [SIFT](./meta/2024/SIFT.prototxt)  | [Sparse is Enough in Fine-tuning Pre-trained Large Language Models](http://arxiv.org/abs/2312.11875v3)               |                                                                        | ![Publish](https://img.shields.io/badge/2024-ICML-blue)  | ![GitHub Repo stars](https://img.shields.io/github/stars/song-wx/SIFT)                  | [note](./notes/2024/SIFT/note.md)     |</p>
</details>
</p>
</details>

